{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to Cellects!","text":"<p>Cellects is a software designed to analyze organisms that grow or move on an immobile surface.</p> <p>It processes timelapse experiments provided either as a set of images or as a video, recorded with a fixed camera observing one or several stationary arenas.</p> Figure: Tracking the growth of 6 plasmodia of Physarum polycephalum  <p>The user guide is structured in two main parts:</p> <ul> <li>Setting up a first analysis, which explains how to quickly set up a first analysis using the three core steps of Cellects:   data specification, initial image analysis, and video tracking.</li> <li>Improving the analysis, which details all available options to adapt Cellects to specific experimental conditions and to fine-tune an existing workflow.</li> </ul> Getting Started Discover Cellects and install it. <ul> <li>Quick start</li> <li>What is Cellects?</li> <li>Installation</li> </ul> Setting up a first analysis From data localisation to image analysis and tracking. <ul> <li>Data localisation</li> <li>Image analysis window</li> <li>Video tracking window</li> </ul> Improving the analysis Tune advanced parameters and scale to multiple folders. <ul> <li>Advanced parameters</li> <li>Required outputs</li> <li>Multiple folders</li> </ul> Use Cases Concrete examples and typical pipelines. Contributing How to help, development workflow, guidelines. API Reference Auto-generated API documentation."},{"location":"contributing/","title":"Contributing to Cellects","text":""},{"location":"contributing/#how-to-report-issues","title":"How to Report Issues","text":"<ul> <li>File bugs/feature requests on GitHub.</li> </ul>"},{"location":"contributing/#code-contributions","title":"Code Contributions","text":"<ol> <li>Fork the repository and create a new branch: <pre><code>   git checkout -b feature/new-widget\n</code></pre></li> <li>Write tests for new features (see tests/).</li> <li>Submit a pull request with a clear description.</li> </ol>"},{"location":"contributing/#testing","title":"Testing","text":""},{"location":"contributing/#run-tests","title":"Run Tests","text":"<p>Cellects uses <code>pytest</code> + <code>pytest-cov</code>. Install test dependencies:</p> <pre><code>pip install -e \".[test]\"\n</code></pre> <p>Run the test suite (with coverage enabled by default via <code>pyproject.toml</code>):</p> <pre><code>pytest\n</code></pre> <p>You can access the coverage report with <code>coverage html</code> and open <code>htmlcov/index.html</code> in your browser.</p> <pre><code>open htmlcov/index.html        # macOS\nxdg-open htmlcov/index.html    # Linux\nstart htmlcov\\index.html       # Windows (PowerShell)\n</code></pre> <p>Or explicitly: <pre><code>pytest --cov=src/cellects --cov-report=term-missing\n</code></pre></p>"},{"location":"contributing/#build-documentation","title":"Build Documentation","text":"<p>Install doc dependencies:</p> <pre><code>pip install -e \".[doc]\"\n</code></pre> <p>Serve the docs locally: <pre><code>mkdocs serve\n</code></pre></p> <p>Open http://127.0.0.1:8000 in your browser.</p>"},{"location":"contributing/#create-windows-executable","title":"Create windows executable","text":"<p>Start by creating a new python environment.  If it already exists, save the main.spec file elsewhere and delete the environment to get the last Cellects version using pip. And run: <pre><code>python -m venv ./cellects_env\ndeactivate\ncellects_env\\Scripts\\activate\npip install .\npip install pyinstaller\ncd cellects_env/Lib/site-packages/cellects\n</code></pre> Generate the .exe <pre><code>pyinstaller __main__.py\n</code></pre> To modify the .exe (icons, terminal...), add a .spec file in 'cellects_env/Lib/site-packages/cellects/' and run: <pre><code>pyinstaller __main__.spec\n</code></pre> Then, compress the dist folder into Cellects.zip and use NSIS to generate the installer</p> <p>Note</p> <ul> <li>When installing Cellects dependencies, do not use editable mode:</li> <li>Lighter installer with the \"Solid\" option</li> </ul>"},{"location":"getting-started/","title":"Getting Started with Cellects","text":""},{"location":"getting-started/#installation-short-version","title":"Installation (Short version)","text":"<p>Install using our Windows installer: Cellects_installer.exe</p> <p>Or, install via pip: <pre><code>pip install cellects\n</code></pre> Any difficulties? follow our complete installation tutorial</p>"},{"location":"getting-started/#quick-start","title":"Quick Start","text":"<p>Run in terminal: <pre><code>Cellects\n</code></pre></p>"},{"location":"getting-started/#testing","title":"Testing","text":"<p>In order to validate your installation, you can run the test using the following commands:</p> <pre><code>pip install -e \".[test]\"\npytest\n</code></pre>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#installation","title":"Installation","text":"<p>Choose the method that matches your OS and how you want to use Cellects.</p> Windows (Installer)All OS (pip)All OS (Source)"},{"location":"installation/#install-with-cellects_installerexe","title":"Install with <code>Cellects_installer.exe</code>","text":"<ol> <li> <p>Download Cellects_installer.exe: Cellects_installer.exe</p> </li> <li> <p>Double-click <code>Cellects_installer.exe</code> to start the installation.</p> </li> <li> <p>To run Cellects, open the newly created folder and launch <code>Cellects.exe</code>.</p> </li> </ol> <p>Windows security warning</p> <p>Windows may warn you that the installer is unsafe (because we are not a registered developer). Click More info \u2192 Run anyway.</p> <p>Antivirus software</p> <p>Some antivirus software may block or slow down the installation for the same reason.</p>"},{"location":"installation/#install-with-pip-macos-windows-linux","title":"Install with <code>pip</code> (macOS / Windows / Linux)","text":"<p>Prerequisite</p> <p>Install Python 3.13: Python 3.13</p>"},{"location":"installation/#optional-but-recommended-use-a-virtual-environment","title":"Optional but recommended: use a virtual environment","text":"<pre><code>cd path/toward/an/existing/folder/\npython -m venv ./cellects_env\n</code></pre> <p>Activate it:</p> WindowsmacOS / Linux <pre><code>cellects_env\\Scripts\\activate\n</code></pre> <pre><code>source cellects_env/bin/activate\n</code></pre> <p>Why use a virtual environment?</p> <p>It prevents compatibility issues with other Python projects. You\u2019ll just need to activate it each time before running Cellects.</p>"},{"location":"installation/#install","title":"Install","text":"<pre><code>pip install cellects\n</code></pre>"},{"location":"installation/#run","title":"Run","text":"<pre><code>cellects\n</code></pre>"},{"location":"installation/#uninstall","title":"Uninstall","text":"<pre><code>pip uninstall cellects\n</code></pre>"},{"location":"installation/#install-from-source-macos-windows-linux","title":"Install from source (macOS / Windows / Linux)","text":"<p>Prerequisites</p> <ul> <li>Install Python 3.13: Python 3.13 </li> <li>Install git: git </li> <li>On macOS, install Homebrew: brew</li> </ul>"},{"location":"installation/#clone-the-repository","title":"Clone the repository","text":"<pre><code>cd path/toward/an/existing/folder/\n</code></pre> <p>Folder choice</p> <p>The repository will be cloned into this folder. If you are updating an existing project, clone into a new folder name and rename it only after verifying the new version.</p> <pre><code>git clone https://github.com/Aurele-B/Cellects.git\ncd ./Cellects\npip install --upgrade pip\npython -m venv ./cellects_env\n</code></pre> <p>Activate the environment:</p> WindowsmacOS / Linux <pre><code>cellects_env\\Scripts\\activate\n</code></pre> <pre><code>source cellects_env/bin/activate\n</code></pre>"},{"location":"installation/#install-dependencies-editable-mode","title":"Install dependencies (editable mode)","text":"<pre><code>pip install -e .\n</code></pre>"},{"location":"installation/#run_1","title":"Run","text":"<pre><code>cellects\n</code></pre>"},{"location":"license/","title":"License & Citation","text":""},{"location":"license/#license","title":"License","text":"<p>GNU GPL3 License (see LICENSE).</p>"},{"location":"license/#citation","title":"Citation","text":"<p>To cite Cellects, use: <pre><code>@article{boussard2024cellects,\n  title={Cellects, a software to quantify cell expansion and motion},\n  author={Boussard, Aur{\\`e}le and Arrufat, Patrick and Dussutour, Audrey and P{\\'e}rez-Escudero, Alfonso},\n  journal={bioRxiv},\n  pages={2024--03},\n  year={2024},\n  publisher={Cold Spring Harbor Laboratory}\n}\n</code></pre></p>"},{"location":"troubleshooting/","title":"Troubleshooting","text":""},{"location":"troubleshooting/#common-errors-and-fixes","title":"Common Errors and Fixes","text":""},{"location":"troubleshooting/#error-anything-causing-the-application-to-freeze","title":"Error: Anything causing the application to freeze","text":"<p>Cause: Nothing happens when clicking on any widget. Fix: Restart the application, use Advanced parameters and Reset all settings. Restart the application again.</p>"},{"location":"troubleshooting/#error-gui-fails-to-launch","title":"Error: \"GUI Fails to Launch\"","text":"<p>Cause: Wrong python version (e.g., python3.10). Fix: Reinstall with python 3.13</p>"},{"location":"troubleshooting/#error-gui-fails-to-launch_1","title":"Error: \"GUI Fails to Launch\"","text":"<p>Cause: Missing dependency (e.g., PyQt5). Fix: Reinstall with <code>pip install Cellects</code>.</p>"},{"location":"troubleshooting/#error-please-enter-a-valid-path","title":"Error: \"Please, enter a valid path\"","text":"<p>Cause: Incorrect file path in Data Localisation. Fix: Ensure the path uses forward slashes (<code>/</code>) or double backslashes (<code>\\\\</code>). Fix: Ensure the path leads to a directory containing images with the right prefix and extension.</p>"},{"location":"troubleshooting/#debugging-tips","title":"Debugging Tips","text":"<ul> <li>Run Cellects in verbose mode: <pre><code>cellects --verbose gui\n</code></pre></li> </ul>"},{"location":"use-cases/","title":"Use cases","text":"In\u00a0[1]: Copied! <pre>from pathlib import Path\n\nDATA_DIR = Path(\"..\") / \"data\" / \"single_experiment\"\nDATA_DIR.resolve()\n</pre> from pathlib import Path  DATA_DIR = Path(\"..\") / \"data\" / \"single_experiment\" DATA_DIR.resolve() Out[1]: <pre>PosixPath('/home/runner/work/Cellects/Cellects/data/single_experiment')</pre> In\u00a0[2]: Copied! <pre>from matplotlib import pyplot as plt\nfrom cellects.core.script_based_run import load_data, run_image_analysis, write_videos, run_all_arenas\nfrom cellects.utils.load_display_save import show\n\nif not DATA_DIR.exists():\n    raise FileNotFoundError(\n        f\"Example data folder not found: {DATA_DIR}. \"\n        \"Run this notebook from the project root or provide the example dataset.\"\n    )\n\npo = load_data(pathway=str(DATA_DIR), sample_number=1, extension='tif')\npo = run_image_analysis(po)\npo = write_videos(po)\npo.vars['frame_by_frame_segmentation'] = True\npo = run_all_arenas(po)\n\nshow(po.last_image.bgr[:, :, ::-1], show=False)\n\n# 6) Plot growth curve\nplt.figure()\nplt.plot(po.one_row_per_frame['time'], po.one_row_per_frame['area'])\nplt.xlabel('Time (hours)')\nplt.ylabel('Surface area (pixels)')\nplt.title('Plasmodium growth: area over time')\nplt.show()\n</pre> from matplotlib import pyplot as plt from cellects.core.script_based_run import load_data, run_image_analysis, write_videos, run_all_arenas from cellects.utils.load_display_save import show  if not DATA_DIR.exists():     raise FileNotFoundError(         f\"Example data folder not found: {DATA_DIR}. \"         \"Run this notebook from the project root or provide the example dataset.\"     )  po = load_data(pathway=str(DATA_DIR), sample_number=1, extension='tif') po = run_image_analysis(po) po = write_videos(po) po.vars['frame_by_frame_segmentation'] = True po = run_all_arenas(po)  show(po.last_image.bgr[:, :, ::-1], show=False)  # 6) Plot growth curve plt.figure() plt.plot(po.one_row_per_frame['time'], po.one_row_per_frame['area']) plt.xlabel('Time (hours)') plt.ylabel('Surface area (pixels)') plt.title('Plasmodium growth: area over time') plt.show() <pre>\nSaving the bunch n: 1 / 1 of videos: </pre> In\u00a0[3]: Copied! <pre>import numpy as np\nfrom matplotlib import pyplot as plt\n\nfrom cellects.utils.load_display_save import show\nfrom cellects.core.script_based_run import (\n    generate_colony_like_video,\n    load_data,\n    run_image_analysis,\n    run_one_video_analysis,\n)\n\n# 1) Generate synthetic data\nrgb_video = generate_colony_like_video()\n\n# 2) Display (optional)\n# movie(rgb_video)\n\n# 3) Segment\npo = load_data(rgb_video)\npo.vars['several_blob_per_arena'] = True\npo = run_image_analysis(po, last_im=rgb_video[-1, ...])\n\n\n# 4) Video tracking\npo.vars['maximal_growth_factor'] = 0.5\nMA = run_one_video_analysis(po, with_video_in_ram=True)\n\n# 5) Show video segmentation mask (color represent time)\nshow(np.cumsum(MA.binary, 0)[-1], show=False)\n\n# 6) Plot growth curve\nplt.figure()\nplt.plot(MA.one_row_per_frame['time'], MA.one_row_per_frame['area_total'])\nplt.xlabel('Time (frame)')\nplt.ylabel('Summed surface area (pixels)')\nplt.title('Total growth of simulated Colonies ')\nplt.show()\n</pre> import numpy as np from matplotlib import pyplot as plt  from cellects.utils.load_display_save import show from cellects.core.script_based_run import (     generate_colony_like_video,     load_data,     run_image_analysis,     run_one_video_analysis, )  # 1) Generate synthetic data rgb_video = generate_colony_like_video()  # 2) Display (optional) # movie(rgb_video)  # 3) Segment po = load_data(rgb_video) po.vars['several_blob_per_arena'] = True po = run_image_analysis(po, last_im=rgb_video[-1, ...])   # 4) Video tracking po.vars['maximal_growth_factor'] = 0.5 MA = run_one_video_analysis(po, with_video_in_ram=True)  # 5) Show video segmentation mask (color represent time) show(np.cumsum(MA.binary, 0)[-1], show=False)  # 6) Plot growth curve plt.figure() plt.plot(MA.one_row_per_frame['time'], MA.one_row_per_frame['area_total']) plt.xlabel('Time (frame)') plt.ylabel('Summed surface area (pixels)') plt.title('Total growth of simulated Colonies ') plt.show()  <pre>\r  0%|          | 0/20 [00:00&lt;?, ?it/s]</pre> <pre>\r  5%|\u258c         | 1/20 [00:00&lt;00:02,  6.78it/s]</pre> <pre>\r 10%|\u2588         | 2/20 [00:00&lt;00:02,  6.34it/s]</pre> <pre>\r 15%|\u2588\u258c        | 3/20 [00:00&lt;00:02,  6.22it/s]</pre> <pre>\r 20%|\u2588\u2588        | 4/20 [00:00&lt;00:02,  6.18it/s]</pre> <pre>\r 25%|\u2588\u2588\u258c       | 5/20 [00:00&lt;00:02,  6.18it/s]</pre> <pre>\r 30%|\u2588\u2588\u2588       | 6/20 [00:00&lt;00:02,  6.16it/s]</pre> <pre>\r 35%|\u2588\u2588\u2588\u258c      | 7/20 [00:01&lt;00:02,  6.25it/s]</pre> <pre>\r 40%|\u2588\u2588\u2588\u2588      | 8/20 [00:01&lt;00:01,  6.29it/s]</pre> <pre>\r 45%|\u2588\u2588\u2588\u2588\u258c     | 9/20 [00:01&lt;00:01,  6.34it/s]</pre> <pre>\r 50%|\u2588\u2588\u2588\u2588\u2588     | 10/20 [00:01&lt;00:01,  6.59it/s]</pre> <pre>\r 55%|\u2588\u2588\u2588\u2588\u2588\u258c    | 11/20 [00:01&lt;00:01,  6.97it/s]</pre> <pre>\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 12/20 [00:01&lt;00:01,  7.39it/s]</pre> <pre>\r 65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 13/20 [00:01&lt;00:00,  7.69it/s]</pre> <pre>\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 14/20 [00:02&lt;00:00,  8.18it/s]</pre> <pre>\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 16/20 [00:02&lt;00:00,  9.24it/s]</pre> <pre>\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 18/20 [00:02&lt;00:00,  9.93it/s]</pre> <pre>\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:02&lt;00:00, 11.07it/s]</pre> <pre>\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:02&lt;00:00,  7.87it/s]</pre> <pre>\n</pre>"},{"location":"use-cases/#use-case-with-the-graphical-user-interface-gui","title":"Use-case with the graphical user interface (GUI)\u00b6","text":""},{"location":"use-cases/#gui-case-1-automated-physarum-polycephalum-tracking","title":"GUI case 1: Automated Physarum polycephalum tracking\u00b6","text":"<p>Problem: Track the surface area over time of one individual.</p>"},{"location":"use-cases/#steps","title":"Steps\u00b6","text":"<ol> <li>Launch the GUI:</li> </ol> <pre>Cellects\n</pre> <ol> <li>Load images via Data Localisation:</li> </ol> <ul> <li>Prefix: <code>im</code></li> <li>Extension: <code>.tif</code></li> <li>Arena number per folder: <code>1</code></li> </ul> <ol> <li><p>Image analysis \u2192 apply config, validate specimen/arena, etc.</p> </li> <li><p>Video tracking \u2192 Done \u2192 Post processing \u2192 Read</p> </li> <li><p>Save one result to export CSV time series.</p> </li> </ol>"},{"location":"use-cases/#use-cases-with-an-application-programming-interface-api","title":"Use-cases with an application programming interface (API)\u00b6","text":""},{"location":"use-cases/#api-case-1-automated-physarum-polycephalum-tracking","title":"API case 1: Automated Physarum polycephalum tracking\u00b6","text":"<p>This section runs the script-based pipeline (if example data is present).</p>"},{"location":"use-cases/#api-case-2-colony-growth-tracking-synthetic-example","title":"API case 2: Colony growth tracking (synthetic example)\u00b6","text":"<p>Problem: Get the surface area over time of several appearing colonies.</p> <p>This example is self-contained and should produce plots.</p>"},{"location":"what-is-cellects/","title":"What is Cellects?","text":""},{"location":"what-is-cellects/#purpose","title":"Purpose","text":"<p>Cellects analyzes organisms that grow or move on an immobile surface.</p>"},{"location":"what-is-cellects/#to-generate-usable-data","title":"To generate usable data","text":"<p>Cellects allows you to analyze timelapses in the form of a set of images or a video.  To work properly, these timelapses must be taken with a camera capturing specimen(s) in one or several stationary arena(s). Arena's position and illumination conditions have to remain constant throughout the experiment. Then, run the software and help it find the location of the timelapse (see Data localisation).</p>"},{"location":"what-is-cellects/#user-guide-description","title":"User guide description","text":"In the first section of this user manual, we will explain how to set up a first quick analysis with Cellects.  Any Cellects analysis takes place in 3 steps: data specification, first image analysis and video tracking.  The second section of this user manual runs thoroughly through every option to run Cellects in particular conditions  and to finetune an already working setup."},{"location":"advanced/","title":"Improving and personalizing the analysis","text":"<p>This section covers advanced use of Cellects:</p> <ul> <li>Advanced parameters </li> <li>Required outputs </li> <li>Multi-folder analyses</li> </ul>"},{"location":"advanced/multiple-folders/","title":"Analyze multiple folders at once","text":"<p>Analyzing multiple folders at once (Fig. 11) streamlines high-throughput experimentation by applying validated analysis parameters to entire datasets with minimal user intervention.  By configuring folder selection and per-folder arena counts, users ensure that Cellects applies the same detection logic uniformly while respecting experimental heterogeneity.</p>"},{"location":"advanced/multiple-folders/#detailed-description","title":"Detailed description","text":"Figure 11: Select folders to analyze window <p>The window of Figure 11 appears after the first window when the selected folder contains subfolders with images matching the Image prefix and Images extension patterns.  </p>"},{"location":"advanced/multiple-folders/#check-to-select-all-folders","title":"Check to select all folders:","text":"<p>Select this option to run the analysis on all folders containing images matching the Image prefix and Images extension. Otherwise, use Ctrl/Cmd to select specific folders for analysis.</p> <p>Note</p> <ul> <li>This setting affects only the Run All functionality.</li> <li>To apply saved masks (e.g., background or specimen initiation regions) across selected folders,enable    Keep Cell and Back drawing for all folders in Advanced parameters.</li> </ul> <p>Here, the user can adjust the number of arenas for each folder  </p> <p>Click Next to go to the image analysis window (Fig. 2) of the first selected folder.  </p>"},{"location":"advanced/multiple-folders/#remarks","title":"Remarks","text":"<ul> <li> <p>If the Select and draw option is required for more than one folder, analyze them separately.   \u2192 In that case, click Previous and select only one subfolder.  </p> </li> <li> <p>If only one folder requires the Select and draw option, analyze it first.   \u2192 To do so, deselect all folders, select the one that needs drawing, then hold <code>Ctrl/Cmd</code> while selecting the others.  </p> </li> <li> <p>If the first selected folder has already been analyzed, two shortcuts appear:  </p> </li> <li>Video tracking window \u2192 skips the image analysis window, lets the user tune video tracking before running all  </li> <li>Run all directly \u2192 skips both image analysis and video tracking, applies saved settings to all folders directly  </li> </ul>"},{"location":"advanced/outputs/","title":"Required outputs","text":"<p>The Required outputs window (Fig. 11) defines what quantitative descriptors Cellects computes and exports during analysis\u2014a critical step in ensuring reproducible, shareable results aligned with open science standards.  Users select from a range of spatial and temporal metrics (e.g., area, perimeter, oscillation patterns) as well as pixel coordinates, geometrical graphs (vertices.csv, edges.csv), or network skeletons.  These outputs bridge raw imaging data to downstream analysis in statistical tools, modeling software, or collaborative platforms. </p> <p>For example:</p> <ul> <li>Pixel-level tracking (e.g., [Pixels covered by the specimen(s)]) enables high-resolution morphodynamics studies.</li> <li>Graph/network outputs (e.g., [Graph of the specimen(s)]) are essential for organisms with internal branching structures (Physarum polycephalum), capturing topological changes over time.</li> </ul>"},{"location":"advanced/outputs/#detailed-description","title":"Detailed description","text":"Figure 10: Required output window"},{"location":"advanced/outputs/#pixels-covered-by-the-specimens","title":"Pixels covered by the specimen(s):","text":"<p>Save a .npy file containing coordinates (t, y, x) of specimen pixel presence as detected by current parameters.</p> <p>Note</p> <ul> <li>These files may consume significant memory depending on the total frame count.</li> </ul>"},{"location":"advanced/outputs/#graph-of-the-specimens-or-network","title":"Graph of the specimen(s) (or network):","text":"<p>Compute a geometrical graph describing the specimen based on current detection parameters.  Cellects generates this graph using the skeleton of the largest connected component per frame.  If network detection is enabled, it will be computed on the detected network instead. The output includes:</p> <ul> <li>A .csv file for vertices with coordinates (t, y, x), IDs, tip status, part of the specimen's initial position, connection status with other vertices.</li> <li>A .csv file for edges with IDs, vertex pairs, lengths, average width, and intensity.</li> </ul> <p>Note</p> <ul> <li>These files may consume significant memory depending on the total frame count.</li> <li>Network and graph detection together are relevant only for organisms with a distinct internalnetwork (e.g., Physarum polycephalum).</li> </ul>"},{"location":"advanced/outputs/#oscillating-areas-in-the-specimens","title":"Oscillating areas in the specimen(s):","text":"<p>Compute and save (as .npy files) coordinates (t, y, x) of oscillating areas in the specimen(s).  Two files are generated: one for thickening regions and one for slimming regions.</p>"},{"location":"advanced/outputs/#network-in-the-specimens","title":"Network in the specimen(s):","text":"<p>Detect and save (as .npy file) coordinates (t, y, x) of a distinct network within the specimen(s). specimen(s).</p>"},{"location":"advanced/outputs/#save-descriptors","title":"Save descriptors","text":"<p>Saves selected descriptors in <code>.csv</code> files at the end of the analysis (Run all).  </p> <p>Available descriptors include:  </p> <ul> <li>Area  </li> <li>Perimeter  </li> <li>Circularity  </li> <li>Rectangularity  </li> <li>Total hole area  </li> <li>Solidity  </li> <li>Convexity  </li> <li>Eccentricity  </li> <li>Euler number  </li> <li>Standard deviation (x, y)  </li> <li>Skewness (x, y)  </li> <li>Kurtosis (x, y)  </li> <li>Major axes lengths and angles  </li> <li>Growth transitions</li> <li>Oscillations</li> <li>Minkowski dimension</li> </ul>"},{"location":"advanced/parameters/","title":"Advanced parameters","text":"<p>The Advanced parameters window (Fig. 9) provides fine-grained control over Cellects\u2019 algorithmic behavior, enabling researchers to customize analysis pipelines for specific datasets or experimental conditions. These settings extend the flexibility introduced in earlier stages (Data localisation, Image analysis, and Video tracking) by allowing tuning of segmentation logic (e.g., mesh resolution, intensity thresholds), computational performance (parallel processing, memory allocation).  For instance, users can optimize for low-noise environments with [Mesh minimal intensity variation] or facilitate high-throughput computations using [Run analysis in parallel]. This section is particularly critical for troubleshooting edge cases (e.g., ambiguous specimen boundaries) or adapting Cellects to non-standard substrates (e.g., heterogeneous growth media). By documenting and reusing these configurations, researchers ensure both technical precision and reproducibility.</p>"},{"location":"advanced/parameters/#detailed-description","title":"Detailed description","text":"Figure 9: Advanced parameters window"},{"location":"advanced/parameters/#automatically-crop-images","title":"Automatically crop images:","text":"<p>Uses initial image detection to crop all images and improve arena/last image detection.</p> <p>Note</p> <ul> <li>Unselect this option if analysis fails or crashes during image analysis.</li> </ul>"},{"location":"advanced/parameters/#subtract-background","title":"Subtract background:","text":"<p>Takes the first image and subtracts it from subsequent images. This can improve or degrade detection depending on dataset characteristics.</p>"},{"location":"advanced/parameters/#keep-cell-and-back-drawings-for-all-folders","title":"Keep Cell and Back drawings for all folders:","text":"<p>During initial image analysis, if the user drew cell/back regions to assist detection, this option saves and uses these annotations across all folders. In summary:</p> <ul> <li>Checked \u2192 retain annotations for all folders</li> <li>Unchecked \u2192 apply only to current folder</li> </ul>"},{"location":"advanced/parameters/#correct-errors-around-initial-specimens-position","title":"Correct errors around initial specimen's position:","text":"<p>Applies an algorithm to correct detection errors near the initial specimen position due to color variations (e.g., from nutrient patches). Technical workflow:</p> <ul> <li>Identifies potential gaps around initial position</li> <li>Monitors local growth velocity</li> <li>Fills gaps using growth patterns from adjacent pixels</li> </ul> <p>Note</p> <ul> <li>\u26a0\ufe0f Not recommended if the substrate has the same transparency everywhere (i.e. no differencebetween starting and growth regions).</li> </ul>"},{"location":"advanced/parameters/#prevent-fast-growth-near-periphery","title":"Prevent fast growth near periphery:","text":"<p>During video analysis, prevents false specimen detection at arena borders by filtering rapid periphery growth.</p> <ul> <li>Checked \u2192 Exclude fast -moving detections near boundaries</li> <li>Unchecked \u2192 Use standard detection criteria</li> </ul>"},{"location":"advanced/parameters/#connect-distant-shapes","title":"Connect distant shapes:","text":"<p>Algorithm for connecting disjoint specimen regions in cases where there should be only one connected specimen per arena.  This is useful when the specimen's heterogeneity create wrong disconnections and the detection is smaller than the true specimen. Technical implementation:</p> <ul> <li>Identifies disconnected subregions</li> <li>Analyzes local growth dynamics</li> <li>Recreates connections using spatially consistent growth patterns</li> </ul> <p>Note</p> <ul> <li>Increases analysis time substantially.</li> </ul>"},{"location":"advanced/parameters/#all-specimens-have-the-same-direction","title":"All specimens have the same direction:","text":"<p>Select to optimize arena detection for specimens moving move in the same direction.</p> <ul> <li>Checked \u2192 Uses motion pattern analysis for arena localization.</li> <li>Unchecked \u2192 Employs standard centroid based algorithm.</li> </ul> <p>Note</p> <ul> <li>Both options work equally when growth is roughly isotropic.</li> </ul>"},{"location":"advanced/parameters/#appearance-size-threshold-automatic-if-checked","title":"Appearance size threshold (automatic if checked):","text":"<p>Minimum pixel count threshold for identifying specimen emergence (e.g., bacterial colony formation).</p> <ul> <li>Checked \u2192 Automatic threshold calculation.</li> <li>Unchecked \u2192 Manual user -defined threshold.</li> </ul>"},{"location":"advanced/parameters/#appearance-detection-method","title":"Appearance detection method:","text":"<p>Selection criteria for initial specimen detection:</p> <ul> <li>Largest: Based on component size metric.</li> <li>Most central: Based on arena center proximity.</li> </ul> <p>Note</p> <ul> <li>Applicable only to progressively emerging specimens.</li> </ul>"},{"location":"advanced/parameters/#mesh-side-length","title":"Mesh side length:","text":"<p>Pixel dimension for analysis window size.</p> <p>Note</p> <ul> <li>Must not exceed minimum image dimension</li> </ul>"},{"location":"advanced/parameters/#mesh-step","title":"Mesh step:","text":"<p>The size of the step (in pixels) between consecutive rolling window positions.</p> <p>Note</p> <ul> <li>Must not exceed the mesh side length to ensure full coverage of the image.</li> </ul>"},{"location":"advanced/parameters/#mesh-minimal-intensity-variation","title":"Mesh minimal intensity variation:","text":"<p>The minimal variation in intensity to consider that a given window does contain the specimen(s).</p> <p>Note</p> <ul> <li>This threshold is an intensity value ranging from 0 to 255 (generally small).</li> <li>Correspond to the level of noise in the background.</li> </ul> Figure 9-1: Remaining parameters"},{"location":"advanced/parameters/#expected-oscillation-period","title":"Expected oscillation period:","text":"<p>The period (in minutes) of biological oscillations to detect within the specimen(s). Computation is based on luminosity variations.</p>"},{"location":"advanced/parameters/#minimal-oscillating-cluster-size","title":"Minimal oscillating cluster size:","text":"<p>When looking for oscillatory patterns, Cellects detects connected components that are thickening or slimming synchronously in the specimen(s). This parameter thresholds the minimal size of these groups of connected pixels. This threshold is useful to filter out small noisy oscillations.</p>"},{"location":"advanced/parameters/#spatio-temporal-scaling","title":"Spatio-temporal scaling:","text":"<p>Defines the spatiotemporal scale of the dataset:</p> <ul> <li>Time between images or frames (minutes).</li> <li>An option to convert areas/distances from pixels to mm/mm\u00b2.</li> </ul>"},{"location":"advanced/parameters/#run-analysis-in-parallel","title":"Run analysis in parallel:","text":"<p>Allow the use of more than one core of the computer processor.</p> <ul> <li>Checked \u2192 Uses multiple CPU cores to analyze arenas in parallel (faster).</li> <li>Unchecked \u2192 Single core analysis.</li> </ul>"},{"location":"advanced/parameters/#proc-max-core-number","title":"Proc max core number:","text":"<p>Maximum number of logical CPU cores to use during analysis. The default value is set to the total number of available CPU cores minus one.</p>"},{"location":"advanced/parameters/#minimal-ram-let-free","title":"Minimal RAM let free:","text":"<p>Amount of RAM that should be left available for other programs. Setting to <code>0</code> gives Cellects all memory, but increases crash risk if other apps are open.</p>"},{"location":"advanced/parameters/#lose-accuracy-to-save-ram","title":"Lose accuracy to save RAM:","text":"<p>For low memory systems:</p> <ul> <li>Converts video from <code>np.float64</code> to <code>uint8</code></li> <li>Saves RAM at the cost of a slight precision loss</li> </ul>"},{"location":"advanced/parameters/#video-fps","title":"Video fps:","text":"<p>Frames per second of validation videos.</p>"},{"location":"advanced/parameters/#keep-unaltered-videos","title":"Keep unaltered videos:","text":"<p>Keeps unaltered <code>.npy</code> videos in hard drive.</p> <ul> <li>Checked \u2192 Rerunning the same analysis will be faster.</li> <li>Unchecked \u2192 These videos will be written and removed each run of the same analysis.</li> </ul> <p>Note</p> <ul> <li>Large files: it is recommended to remove them once analysis is entirely finalized.</li> </ul>"},{"location":"advanced/parameters/#save-processed-videos","title":"Save processed videos:","text":"<p>Saves lightweight processed validation videos (recommended over unaltered videos). These videos assess analysis accuracy and can be read in standard video players.</p>"},{"location":"advanced/parameters/#color-space-combination-for-video-analysis","title":"Color space combination for video analysis:","text":"<p>Advanced option: Changes the way RGB processing directly in video tracking. Useful for testing new color spaces without (re)running image analysis.</p>"},{"location":"advanced/parameters/#night-mode","title":"Night mode:","text":"<p>Switches the application background between light and dark themes.</p>"},{"location":"advanced/parameters/#reset-all-settings","title":"Reset all settings:","text":"<p>Useful when the software freezes with no apparent reason. To reset all settings, it removes the config file in the  current folder as well as the config file in the software folder. Then, it retrieves and saves the default parameters.</p>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#main-modules","title":"Main modules","text":"<ul> <li><code>cellects.core</code></li> <li><code>cellects.gui</code></li> <li><code>cellects.image_analysis</code></li> <li><code>cellects.utils</code></li> </ul>"},{"location":"api/SUMMARY/","title":"SUMMARY","text":"<ul> <li>Overview</li> <li>cellects<ul> <li>core<ul> <li>cellects_paths</li> <li>cellects_threads</li> <li>motion_analysis</li> <li>one_image_analysis</li> <li>program_organizer</li> <li>script_based_run</li> </ul> </li> <li>gui<ul> <li>advanced_parameters</li> <li>cellects</li> <li>custom_widgets</li> <li>first_window</li> <li>if_several_folders_window</li> <li>image_analysis_window</li> <li>required_output</li> <li>ui_strings</li> <li>video_analysis_window</li> </ul> </li> <li>image_analysis<ul> <li>cell_leaving_detection</li> <li>image_segmentation</li> <li>morphological_operations</li> <li>network_functions</li> <li>one_image_analysis_threads</li> <li>oscillations_functions</li> <li>progressively_add_distant_shapes</li> <li>shape_descriptors</li> </ul> </li> <li>utils<ul> <li>decorators</li> <li>formulas</li> <li>load_display_save</li> <li>utilitarian</li> </ul> </li> </ul> </li> </ul>"},{"location":"api/cellects/core/","title":"<code>cellects.core</code>","text":""},{"location":"api/cellects/core/#cellects.core","title":"<code>cellects.core</code>","text":""},{"location":"api/cellects/core/cellects_paths/","title":"<code>cellects.core.cellects_paths</code>","text":""},{"location":"api/cellects/core/cellects_paths/#cellects.core.cellects_paths","title":"<code>cellects.core.cellects_paths</code>","text":"<p>Generate the different paths used by cellects. Adjust the path names according to the current position of the software.</p>"},{"location":"api/cellects/core/cellects_threads/","title":"<code>cellects.core.cellects_threads</code>","text":""},{"location":"api/cellects/core/cellects_threads/#cellects.core.cellects_threads","title":"<code>cellects.core.cellects_threads</code>","text":"<p>Cellects GUI module implementing threaded image/video analysis workflows.</p> <p>This module provides a Qt-based interface for analyzing biological motion and growth through color space combinations, segmentation strategies, arena delineation, and video processing. Uses QThreaded workers to maintain UI responsiveness during computationally intensive tasks like segmentation, motion tracking, network detection, oscillation and fractal analysis.</p> <p>Main Components LoadDataToRunCellectsQuicklyThread : Loads necessary data asynchronously for quick Cellects execution. FirstImageAnalysisThread : Analyzes first image with automatic color space selection and segmentation. LastImageAnalysisThread : Processes last frame analysis for optimized color space combinations. CropScaleSubtractDelineateThread : Handles cropping, scaling, and arena boundary detection. OneArenaThread : Performs complete motion analysis on a single arena with post-processing. RunAllThread : Executes full batch analysis across multiple arenas/experiments.</p> <p>Notes Uses QThread for background operations to maintain UI responsiveness. Key workflows include automated color space optimization, adaptive segmentation algorithms, multithreaded video processing, and arena delineation via geometric analysis or manual drawing. Implements special post-processing for Physarum polycephalum network detection and oscillatory activity tracking.</p>"},{"location":"api/cellects/core/cellects_threads/#cellects.core.cellects_threads.ChangeOneRepResultThread","title":"<code>ChangeOneRepResultThread</code>","text":"<p>               Bases: <code>QThread</code></p> <p>Thread for modifying the results of one arena.</p> Signals <p>message_from_thread : Signal(str)     Signal emitted when the result is changed.</p> Notes <p>This class uses <code>QThread</code> to manage the process asynchronously.</p> Source code in <code>src/cellects/core/cellects_threads.py</code> <pre><code>class ChangeOneRepResultThread(QtCore.QThread):\n    \"\"\"\n    Thread for modifying the results of one arena.\n\n    Signals\n    --------\n    message_from_thread : Signal(str)\n        Signal emitted when the result is changed.\n\n    Notes\n    -----\n    This class uses `QThread` to manage the process asynchronously.\n    \"\"\"\n    message_from_thread = QtCore.Signal(str)\n\n    def __init__(self, parent=None):\n        \"\"\"\n        Initialize the worker thread for changing the saved results in the current folder, for a particular arena\n\n        Parameters\n        ----------\n        parent : QObject, optional\n            The parent object of this thread instance. In use, an instance of CellectsMainWidget class. Default is None.\n        \"\"\"\n        super(ChangeOneRepResultThread, self).__init__(parent)\n        self.setParent(parent)\n\n    def run(self):\n        \"\"\"\n        Modify the motion and results of an arena.\n\n        Extended Description\n        --------------------\n        This method performs various operations on the motion data of an arena,\n        including binary mask creation, descriptor computation, and transition\n        detection. It also handles optional computations like fading effects and\n        segmentation based on different video options.\n        \"\"\"\n        self.message_from_thread.emit(\n            f\"Arena n\u00b0{self.parent().po.all['arena']}: modifying its results...\")\n        if self.parent().po.motion.start is None:\n            self.parent().po.motion.binary = np.repeat(np.expand_dims(self.parent().po.motion.origin, 0),\n                                                     self.parent().po.motion.converted_video.shape[0], axis=0).astype(np.uint8)\n        else:\n            if self.parent().po.all['compute_all_options']:\n                if self.parent().po.all['video_option'] == 0:\n                    self.parent().po.motion.binary = self.parent().po.motion.segmented\n                else:\n                    if self.parent().po.all['video_option'] == 1:\n                        mask = self.parent().po.motion.luminosity_segmentation\n                    elif self.parent().po.all['video_option'] == 2:\n                        mask = self.parent().po.motion.gradient_segmentation\n                    elif self.parent().po.all['video_option'] == 3:\n                        mask = self.parent().po.motion.logical_and\n                    elif self.parent().po.all['video_option'] == 4:\n                        mask = self.parent().po.motion.logical_or\n                    self.parent().po.motion.binary = np.zeros(self.parent().po.motion.dims, dtype=np.uint8)\n                    self.parent().po.motion.binary[mask[0], mask[1], mask[2]] = 1\n            else:\n                self.parent().po.motion.binary = np.zeros(self.parent().po.motion.dims[:3], dtype=np.uint8)\n                if self.parent().po.computed_video_options[self.parent().po.all['video_option']]:\n                    self.parent().po.motion.binary = self.parent().po.motion.segmented\n\n        if self.parent().po.vars['do_fading']:\n            self.parent().po.motion.newly_explored_area = self.parent().po.newly_explored_area[:, self.parent().po.all['video_option']]\n        self.parent().po.motion.max_distance = 9 * self.parent().po.vars['detection_range_factor']\n        self.parent().po.motion.get_descriptors_from_binary(release_memory=False)\n        self.parent().po.motion.detect_growth_transitions()\n        self.parent().po.motion.networks_analysis(False)\n        self.parent().po.motion.study_cytoscillations(False)\n        self.parent().po.motion.fractal_descriptions()\n        self.parent().po.motion.change_results_of_one_arena()\n        self.parent().po.motion = None\n        # self.parent().po.motion = None\n        self.message_from_thread.emit(f\"Arena n\u00b0{self.parent().po.all['arena']}: analysis finished.\")\n</code></pre>"},{"location":"api/cellects/core/cellects_threads/#cellects.core.cellects_threads.ChangeOneRepResultThread.__init__","title":"<code>__init__(parent=None)</code>","text":"<p>Initialize the worker thread for changing the saved results in the current folder, for a particular arena</p> <p>Parameters:</p> Name Type Description Default <code>parent</code> <code>QObject</code> <p>The parent object of this thread instance. In use, an instance of CellectsMainWidget class. Default is None.</p> <code>None</code> Source code in <code>src/cellects/core/cellects_threads.py</code> <pre><code>def __init__(self, parent=None):\n    \"\"\"\n    Initialize the worker thread for changing the saved results in the current folder, for a particular arena\n\n    Parameters\n    ----------\n    parent : QObject, optional\n        The parent object of this thread instance. In use, an instance of CellectsMainWidget class. Default is None.\n    \"\"\"\n    super(ChangeOneRepResultThread, self).__init__(parent)\n    self.setParent(parent)\n</code></pre>"},{"location":"api/cellects/core/cellects_threads/#cellects.core.cellects_threads.ChangeOneRepResultThread.run","title":"<code>run()</code>","text":"<p>Modify the motion and results of an arena.</p> Extended Description <p>This method performs various operations on the motion data of an arena, including binary mask creation, descriptor computation, and transition detection. It also handles optional computations like fading effects and segmentation based on different video options.</p> Source code in <code>src/cellects/core/cellects_threads.py</code> <pre><code>def run(self):\n    \"\"\"\n    Modify the motion and results of an arena.\n\n    Extended Description\n    --------------------\n    This method performs various operations on the motion data of an arena,\n    including binary mask creation, descriptor computation, and transition\n    detection. It also handles optional computations like fading effects and\n    segmentation based on different video options.\n    \"\"\"\n    self.message_from_thread.emit(\n        f\"Arena n\u00b0{self.parent().po.all['arena']}: modifying its results...\")\n    if self.parent().po.motion.start is None:\n        self.parent().po.motion.binary = np.repeat(np.expand_dims(self.parent().po.motion.origin, 0),\n                                                 self.parent().po.motion.converted_video.shape[0], axis=0).astype(np.uint8)\n    else:\n        if self.parent().po.all['compute_all_options']:\n            if self.parent().po.all['video_option'] == 0:\n                self.parent().po.motion.binary = self.parent().po.motion.segmented\n            else:\n                if self.parent().po.all['video_option'] == 1:\n                    mask = self.parent().po.motion.luminosity_segmentation\n                elif self.parent().po.all['video_option'] == 2:\n                    mask = self.parent().po.motion.gradient_segmentation\n                elif self.parent().po.all['video_option'] == 3:\n                    mask = self.parent().po.motion.logical_and\n                elif self.parent().po.all['video_option'] == 4:\n                    mask = self.parent().po.motion.logical_or\n                self.parent().po.motion.binary = np.zeros(self.parent().po.motion.dims, dtype=np.uint8)\n                self.parent().po.motion.binary[mask[0], mask[1], mask[2]] = 1\n        else:\n            self.parent().po.motion.binary = np.zeros(self.parent().po.motion.dims[:3], dtype=np.uint8)\n            if self.parent().po.computed_video_options[self.parent().po.all['video_option']]:\n                self.parent().po.motion.binary = self.parent().po.motion.segmented\n\n    if self.parent().po.vars['do_fading']:\n        self.parent().po.motion.newly_explored_area = self.parent().po.newly_explored_area[:, self.parent().po.all['video_option']]\n    self.parent().po.motion.max_distance = 9 * self.parent().po.vars['detection_range_factor']\n    self.parent().po.motion.get_descriptors_from_binary(release_memory=False)\n    self.parent().po.motion.detect_growth_transitions()\n    self.parent().po.motion.networks_analysis(False)\n    self.parent().po.motion.study_cytoscillations(False)\n    self.parent().po.motion.fractal_descriptions()\n    self.parent().po.motion.change_results_of_one_arena()\n    self.parent().po.motion = None\n    # self.parent().po.motion = None\n    self.message_from_thread.emit(f\"Arena n\u00b0{self.parent().po.all['arena']}: analysis finished.\")\n</code></pre>"},{"location":"api/cellects/core/cellects_threads/#cellects.core.cellects_threads.CompleteImageAnalysisThread","title":"<code>CompleteImageAnalysisThread</code>","text":"<p>               Bases: <code>QThread</code></p> <p>Thread for completing the last image analysis.</p> Signals <p>message_when_thread_finished : Signal(bool)     Signal emitted upon completion of the thread's task.</p> Notes <p>This class uses <code>QThread</code> to manage the process asynchronously.</p> Source code in <code>src/cellects/core/cellects_threads.py</code> <pre><code>class CompleteImageAnalysisThread(QtCore.QThread):\n    \"\"\"\n    Thread for completing the last image analysis.\n\n    Signals\n    -------\n    message_when_thread_finished : Signal(bool)\n        Signal emitted upon completion of the thread's task.\n\n    Notes\n    -----\n    This class uses `QThread` to manage the process asynchronously.\n    \"\"\"\n    message_when_thread_finished = QtCore.Signal(bool)\n\n    def __init__(self, parent=None):\n        \"\"\"\n        Initialize the worker thread for completing the last image analysis\n\n        Parameters\n        ----------\n        parent : QObject, optional\n            The parent object of this thread instance. In use, an instance of CellectsMainWidget class. Default is None.\n        \"\"\"\n        super(CompleteImageAnalysisThread, self).__init__(parent)\n        self.setParent(parent)\n\n    def run(self):\n        self.parent().po.get_background_to_subtract()\n        self.parent().po.get_origins_and_backgrounds_lists()\n        self.parent().po.data_to_save['exif'] = True\n        self.parent().po.save_data_to_run_cellects_quickly()\n        self.parent().po.all['bio_mask'] = None\n        self.parent().po.all['back_mask'] = None\n        if self.parent().imageanalysiswindow.bio_masks_number != 0:\n            self.parent().po.all['bio_mask'] = np.nonzero(self.parent().imageanalysiswindow.bio_mask)\n        if self.parent().imageanalysiswindow.back_masks_number != 0:\n            self.parent().po.all['back_mask'] = np.nonzero(self.parent().imageanalysiswindow.back_mask)\n        self.parent().po.complete_image_analysis()\n        self.message_when_thread_finished.emit(True)\n</code></pre>"},{"location":"api/cellects/core/cellects_threads/#cellects.core.cellects_threads.CompleteImageAnalysisThread.__init__","title":"<code>__init__(parent=None)</code>","text":"<p>Initialize the worker thread for completing the last image analysis</p> <p>Parameters:</p> Name Type Description Default <code>parent</code> <code>QObject</code> <p>The parent object of this thread instance. In use, an instance of CellectsMainWidget class. Default is None.</p> <code>None</code> Source code in <code>src/cellects/core/cellects_threads.py</code> <pre><code>def __init__(self, parent=None):\n    \"\"\"\n    Initialize the worker thread for completing the last image analysis\n\n    Parameters\n    ----------\n    parent : QObject, optional\n        The parent object of this thread instance. In use, an instance of CellectsMainWidget class. Default is None.\n    \"\"\"\n    super(CompleteImageAnalysisThread, self).__init__(parent)\n    self.setParent(parent)\n</code></pre>"},{"location":"api/cellects/core/cellects_threads/#cellects.core.cellects_threads.CropScaleSubtractDelineateThread","title":"<code>CropScaleSubtractDelineateThread</code>","text":"<p>               Bases: <code>QThread</code></p> <p>Thread for detecting crop and arena coordinates.</p> Signals <p>message_from_thread : Signal(str)     Signal emitted when progress messages are available. message_when_thread_finished : Signal(dict)     Signal emitted upon completion of the thread's task.</p> Notes <p>This class uses <code>QThread</code> to manage the process asynchronously.</p> Source code in <code>src/cellects/core/cellects_threads.py</code> <pre><code>class CropScaleSubtractDelineateThread(QtCore.QThread):\n    \"\"\"\n    Thread for detecting crop and arena coordinates.\n\n    Signals\n    -------\n    message_from_thread : Signal(str)\n        Signal emitted when progress messages are available.\n    message_when_thread_finished : Signal(dict)\n        Signal emitted upon completion of the thread's task.\n\n    Notes\n    -----\n    This class uses `QThread` to manage the process asynchronously.\n    \"\"\"\n    message_from_thread = QtCore.Signal(str)\n    message_when_thread_finished = QtCore.Signal(dict)\n\n    def __init__(self, parent=None):\n        \"\"\"\n        Initialize the worker thread for detecting crop and arena coordinates in the first image\n\n        Parameters\n        ----------\n        parent : QObject, optional\n            The parent object of this thread instance. In use, an instance of CellectsMainWidget class. Default is None.\n        \"\"\"\n\n        super(CropScaleSubtractDelineateThread, self).__init__(parent)\n        self.setParent(parent)\n\n    def run(self):\n        \"\"\"\n        Start cropping if required, perform initial processing,\n        and handle subsequent operations based on configuration.\n\n        Extended Description\n        --------------------\n        This method initiates the cropping process if necessary,\n        performs initial processing steps, and manages subsequent operations\n        depending on whether multiple blobs are detected per arena.\n\n        Notes\n        -----\n        This method uses several logging operations to track its progress.\n        It interacts with various components of the parent object\n        to perform necessary image processing tasks.\n        \"\"\"\n        logging.info(\"Start cropping if required\")\n        analysis_status = {\"continue\": True, \"message\": \"\"}\n        self.parent().po.cropping(is_first_image=True)\n        self.parent().po.get_average_pixel_size()\n        if os.path.isfile('Data to run Cellects quickly.pkl'):\n            os.remove('Data to run Cellects quickly.pkl')\n        logging.info(\"Save data to run Cellects quickly\")\n        self.parent().po.data_to_save['first_image'] = True\n        self.parent().po.save_data_to_run_cellects_quickly()\n        self.parent().po.data_to_save['first_image'] = False\n        if not self.parent().po.vars['several_blob_per_arena']:\n            logging.info(\"Check whether the detected shape number is ok\")\n            nb, shapes, stats, centroids = cv2.connectedComponentsWithStats(self.parent().po.first_image.validated_shapes)\n            y_lim = self.parent().po.first_image.y_boundaries\n            if ((nb - 1) != self.parent().po.sample_number or np.any(stats[:, 4] == 1)):\n                analysis_status[\"message\"] = \"Image analysis failed to detect the right cell(s) number: restart the analysis.\"\n                analysis_status['continue'] = False\n            elif y_lim is None:\n                analysis_status[\"message\"] = \"The shapes detected in the image did not allow automatic arena delineation.\"\n                analysis_status['continue'] = False\n            elif (y_lim == - 1).sum() != (y_lim == 1).sum():\n                analysis_status[\"message\"] = \"Automatic arena delineation cannot work if one cell touches the image border.\"\n                self.parent().po.first_image.y_boundaries = None\n                analysis_status['continue'] = False\n        if analysis_status['continue']:\n            logging.info(\"Start automatic video delineation\")\n            analysis_status = self.parent().po.delineate_each_arena()\n        else:\n            self.parent().po.first_image.validated_shapes = np.zeros(self.parent().po.first_image.image.shape[:2], dtype=np.uint8)\n            logging.info(analysis_status[\"message\"])\n        self.message_when_thread_finished.emit(analysis_status)\n</code></pre>"},{"location":"api/cellects/core/cellects_threads/#cellects.core.cellects_threads.CropScaleSubtractDelineateThread.__init__","title":"<code>__init__(parent=None)</code>","text":"<p>Initialize the worker thread for detecting crop and arena coordinates in the first image</p> <p>Parameters:</p> Name Type Description Default <code>parent</code> <code>QObject</code> <p>The parent object of this thread instance. In use, an instance of CellectsMainWidget class. Default is None.</p> <code>None</code> Source code in <code>src/cellects/core/cellects_threads.py</code> <pre><code>def __init__(self, parent=None):\n    \"\"\"\n    Initialize the worker thread for detecting crop and arena coordinates in the first image\n\n    Parameters\n    ----------\n    parent : QObject, optional\n        The parent object of this thread instance. In use, an instance of CellectsMainWidget class. Default is None.\n    \"\"\"\n\n    super(CropScaleSubtractDelineateThread, self).__init__(parent)\n    self.setParent(parent)\n</code></pre>"},{"location":"api/cellects/core/cellects_threads/#cellects.core.cellects_threads.CropScaleSubtractDelineateThread.run","title":"<code>run()</code>","text":"<p>Start cropping if required, perform initial processing, and handle subsequent operations based on configuration.</p> Extended Description <p>This method initiates the cropping process if necessary, performs initial processing steps, and manages subsequent operations depending on whether multiple blobs are detected per arena.</p> Notes <p>This method uses several logging operations to track its progress. It interacts with various components of the parent object to perform necessary image processing tasks.</p> Source code in <code>src/cellects/core/cellects_threads.py</code> <pre><code>def run(self):\n    \"\"\"\n    Start cropping if required, perform initial processing,\n    and handle subsequent operations based on configuration.\n\n    Extended Description\n    --------------------\n    This method initiates the cropping process if necessary,\n    performs initial processing steps, and manages subsequent operations\n    depending on whether multiple blobs are detected per arena.\n\n    Notes\n    -----\n    This method uses several logging operations to track its progress.\n    It interacts with various components of the parent object\n    to perform necessary image processing tasks.\n    \"\"\"\n    logging.info(\"Start cropping if required\")\n    analysis_status = {\"continue\": True, \"message\": \"\"}\n    self.parent().po.cropping(is_first_image=True)\n    self.parent().po.get_average_pixel_size()\n    if os.path.isfile('Data to run Cellects quickly.pkl'):\n        os.remove('Data to run Cellects quickly.pkl')\n    logging.info(\"Save data to run Cellects quickly\")\n    self.parent().po.data_to_save['first_image'] = True\n    self.parent().po.save_data_to_run_cellects_quickly()\n    self.parent().po.data_to_save['first_image'] = False\n    if not self.parent().po.vars['several_blob_per_arena']:\n        logging.info(\"Check whether the detected shape number is ok\")\n        nb, shapes, stats, centroids = cv2.connectedComponentsWithStats(self.parent().po.first_image.validated_shapes)\n        y_lim = self.parent().po.first_image.y_boundaries\n        if ((nb - 1) != self.parent().po.sample_number or np.any(stats[:, 4] == 1)):\n            analysis_status[\"message\"] = \"Image analysis failed to detect the right cell(s) number: restart the analysis.\"\n            analysis_status['continue'] = False\n        elif y_lim is None:\n            analysis_status[\"message\"] = \"The shapes detected in the image did not allow automatic arena delineation.\"\n            analysis_status['continue'] = False\n        elif (y_lim == - 1).sum() != (y_lim == 1).sum():\n            analysis_status[\"message\"] = \"Automatic arena delineation cannot work if one cell touches the image border.\"\n            self.parent().po.first_image.y_boundaries = None\n            analysis_status['continue'] = False\n    if analysis_status['continue']:\n        logging.info(\"Start automatic video delineation\")\n        analysis_status = self.parent().po.delineate_each_arena()\n    else:\n        self.parent().po.first_image.validated_shapes = np.zeros(self.parent().po.first_image.image.shape[:2], dtype=np.uint8)\n        logging.info(analysis_status[\"message\"])\n    self.message_when_thread_finished.emit(analysis_status)\n</code></pre>"},{"location":"api/cellects/core/cellects_threads/#cellects.core.cellects_threads.FirstImageAnalysisThread","title":"<code>FirstImageAnalysisThread</code>","text":"<p>               Bases: <code>QThread</code></p> <p>Thread for analyzing the first image of a given folder.</p> Signals <p>message_from_thread : Signal(str)     Signal emitted when progress messages are available. message_when_thread_finished : Signal(bool)     Signal emitted upon completion of the thread's task.</p> Notes <p>This class uses <code>QThread</code> to manage the process asynchronously.</p> Source code in <code>src/cellects/core/cellects_threads.py</code> <pre><code>class FirstImageAnalysisThread(QtCore.QThread):\n    \"\"\"\n    Thread for analyzing the first image of a given folder.\n\n    Signals\n    -------\n    message_from_thread : Signal(str)\n        Signal emitted when progress messages are available.\n    message_when_thread_finished : Signal(bool)\n        Signal emitted upon completion of the thread's task.\n\n    Notes\n    -----\n    This class uses `QThread` to manage the process asynchronously.\n    \"\"\"\n    message_from_thread = QtCore.Signal(str)\n    message_when_thread_finished = QtCore.Signal(bool)\n\n    def __init__(self, parent=None):\n        \"\"\"\n        Initialize the worker thread for analyzing the first image of a given folder\n\n        Parameters\n        ----------\n        parent : QObject, optional\n            The parent object of this thread instance. In use, an instance of CellectsMainWidget class. Default is None.\n        \"\"\"\n        super(FirstImageAnalysisThread, self).__init__(parent)\n        self.setParent(parent)\n\n    def run(self):\n        \"\"\"\n        Perform image analysis and segmentation based on the current state of the application.\n\n        This function handles both bio-mask and background mask processing, emits status messages,\n        computes average pixel size if necessary, and performs image segmentation or generates\n        analysis options.\n\n        Parameters\n        ----------\n        self : object\n            The instance of the class containing this method. Should have attributes:\n            - parent: Reference to the parent object\n            - message_from_thread.emit: Method to emit messages from the thread\n            - message_when_thread_finished.emit: Method to signal thread completion\n\n        Returns\n        -------\n        None\n            This method does not return a value but emits messages and modifies the state of\n            self.parent objects.\n        Notes\n        -----\n        This method performs several complex operations involving image segmentation and\n        analysis generation. It handles both bio-masks and background masks, computes average\n        pixel sizes, and updates various state attributes on the parent object.\n        \"\"\"\n        tic = default_timer()\n        if self.parent().po.visualize or len(self.parent().po.first_im.shape) == 2:\n            self.message_from_thread.emit(\"Image segmentation, wait...\")\n        else:\n            self.message_from_thread.emit(\"Generating segmentation options, wait...\")\n        self.parent().po.full_first_image_segmentation(not self.parent().imageanalysiswindow.asking_first_im_parameters_flag,\n                                                       self.parent().imageanalysiswindow.bio_mask, self.parent().imageanalysiswindow.back_mask)\n\n        logging.info(f\" image analysis lasted {np.floor((default_timer() - tic) / 60).astype(int)} minutes {np.round((default_timer() - tic) % 60).astype(int)} secondes\")\n        self.message_when_thread_finished.emit(True)\n</code></pre>"},{"location":"api/cellects/core/cellects_threads/#cellects.core.cellects_threads.FirstImageAnalysisThread.__init__","title":"<code>__init__(parent=None)</code>","text":"<p>Initialize the worker thread for analyzing the first image of a given folder</p> <p>Parameters:</p> Name Type Description Default <code>parent</code> <code>QObject</code> <p>The parent object of this thread instance. In use, an instance of CellectsMainWidget class. Default is None.</p> <code>None</code> Source code in <code>src/cellects/core/cellects_threads.py</code> <pre><code>def __init__(self, parent=None):\n    \"\"\"\n    Initialize the worker thread for analyzing the first image of a given folder\n\n    Parameters\n    ----------\n    parent : QObject, optional\n        The parent object of this thread instance. In use, an instance of CellectsMainWidget class. Default is None.\n    \"\"\"\n    super(FirstImageAnalysisThread, self).__init__(parent)\n    self.setParent(parent)\n</code></pre>"},{"location":"api/cellects/core/cellects_threads/#cellects.core.cellects_threads.FirstImageAnalysisThread.run","title":"<code>run()</code>","text":"<p>Perform image analysis and segmentation based on the current state of the application.</p> <p>This function handles both bio-mask and background mask processing, emits status messages, computes average pixel size if necessary, and performs image segmentation or generates analysis options.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>object</code> <p>The instance of the class containing this method. Should have attributes: - parent: Reference to the parent object - message_from_thread.emit: Method to emit messages from the thread - message_when_thread_finished.emit: Method to signal thread completion</p> required <p>Returns:</p> Type Description <code>None</code> <p>This method does not return a value but emits messages and modifies the state of self.parent objects.</p> Notes <p>This method performs several complex operations involving image segmentation and analysis generation. It handles both bio-masks and background masks, computes average pixel sizes, and updates various state attributes on the parent object.</p> Source code in <code>src/cellects/core/cellects_threads.py</code> <pre><code>def run(self):\n    \"\"\"\n    Perform image analysis and segmentation based on the current state of the application.\n\n    This function handles both bio-mask and background mask processing, emits status messages,\n    computes average pixel size if necessary, and performs image segmentation or generates\n    analysis options.\n\n    Parameters\n    ----------\n    self : object\n        The instance of the class containing this method. Should have attributes:\n        - parent: Reference to the parent object\n        - message_from_thread.emit: Method to emit messages from the thread\n        - message_when_thread_finished.emit: Method to signal thread completion\n\n    Returns\n    -------\n    None\n        This method does not return a value but emits messages and modifies the state of\n        self.parent objects.\n    Notes\n    -----\n    This method performs several complex operations involving image segmentation and\n    analysis generation. It handles both bio-masks and background masks, computes average\n    pixel sizes, and updates various state attributes on the parent object.\n    \"\"\"\n    tic = default_timer()\n    if self.parent().po.visualize or len(self.parent().po.first_im.shape) == 2:\n        self.message_from_thread.emit(\"Image segmentation, wait...\")\n    else:\n        self.message_from_thread.emit(\"Generating segmentation options, wait...\")\n    self.parent().po.full_first_image_segmentation(not self.parent().imageanalysiswindow.asking_first_im_parameters_flag,\n                                                   self.parent().imageanalysiswindow.bio_mask, self.parent().imageanalysiswindow.back_mask)\n\n    logging.info(f\" image analysis lasted {np.floor((default_timer() - tic) / 60).astype(int)} minutes {np.round((default_timer() - tic) % 60).astype(int)} secondes\")\n    self.message_when_thread_finished.emit(True)\n</code></pre>"},{"location":"api/cellects/core/cellects_threads/#cellects.core.cellects_threads.GetExifDataThread","title":"<code>GetExifDataThread</code>","text":"<p>               Bases: <code>QThread</code></p> <p>Thread for loading exif data from images.</p> Notes <p>This class uses <code>QThread</code> to manage the process asynchronously.</p> Source code in <code>src/cellects/core/cellects_threads.py</code> <pre><code>class GetExifDataThread(QtCore.QThread):\n    \"\"\"\n    Thread for loading exif data from images.\n\n    Notes\n    -----\n    This class uses `QThread` to manage the process asynchronously.\n    \"\"\"\n\n    def __init__(self, parent=None):\n        \"\"\"\n        Initialize the worker thread for looking for the exif data.\n\n        Parameters\n        ----------\n        parent : QObject, optional\n            The parent object of this thread instance. In use, an instance of CellectsMainWidget class. Default is None.\n        \"\"\"\n        super(GetExifDataThread, self).__init__(parent)\n        self.setParent(parent)\n\n    def run(self):\n        \"\"\"\n        Do extract exif data..\n        \"\"\"\n        self.parent().po.extract_exif()\n</code></pre>"},{"location":"api/cellects/core/cellects_threads/#cellects.core.cellects_threads.GetExifDataThread.__init__","title":"<code>__init__(parent=None)</code>","text":"<p>Initialize the worker thread for looking for the exif data.</p> <p>Parameters:</p> Name Type Description Default <code>parent</code> <code>QObject</code> <p>The parent object of this thread instance. In use, an instance of CellectsMainWidget class. Default is None.</p> <code>None</code> Source code in <code>src/cellects/core/cellects_threads.py</code> <pre><code>def __init__(self, parent=None):\n    \"\"\"\n    Initialize the worker thread for looking for the exif data.\n\n    Parameters\n    ----------\n    parent : QObject, optional\n        The parent object of this thread instance. In use, an instance of CellectsMainWidget class. Default is None.\n    \"\"\"\n    super(GetExifDataThread, self).__init__(parent)\n    self.setParent(parent)\n</code></pre>"},{"location":"api/cellects/core/cellects_threads/#cellects.core.cellects_threads.GetExifDataThread.run","title":"<code>run()</code>","text":"<p>Do extract exif data..</p> Source code in <code>src/cellects/core/cellects_threads.py</code> <pre><code>def run(self):\n    \"\"\"\n    Do extract exif data..\n    \"\"\"\n    self.parent().po.extract_exif()\n</code></pre>"},{"location":"api/cellects/core/cellects_threads/#cellects.core.cellects_threads.GetFirstImThread","title":"<code>GetFirstImThread</code>","text":"<p>               Bases: <code>QThread</code></p> <p>Thread for getting the first image.</p> Signals <p>message_when_thread_finished : Signal(bool)     Emitted when the thread finishes execution, indicating whether data loading was successful.</p> Notes <p>This class uses <code>QThread</code> to manage the process asynchronously.</p> Source code in <code>src/cellects/core/cellects_threads.py</code> <pre><code>class GetFirstImThread(QtCore.QThread):\n    \"\"\"\n    Thread for getting the first image.\n\n    Signals\n    -------\n    message_when_thread_finished : Signal(bool)\n        Emitted when the thread finishes execution, indicating whether data loading was successful.\n\n    Notes\n    -----\n    This class uses `QThread` to manage the process asynchronously.\n    \"\"\"\n    message_when_thread_finished = QtCore.Signal(np.ndarray)\n    def __init__(self, parent=None):\n        \"\"\"\n        Initialize the worker thread for loading the first image of one folder.\n\n        Parameters\n        ----------\n        parent : QObject, optional\n            The parent object of this thread instance. In use, an instance of CellectsMainWidget class. Default is None.\n        \"\"\"\n        super(GetFirstImThread, self).__init__(parent)\n        self.setParent(parent)\n\n    def run(self):\n        \"\"\"\n        Run the first image reading task in the parent process and emit a signal when it finishes.\n        \"\"\"\n        self.parent().po.get_first_image()\n        self.message_when_thread_finished.emit(self.parent().po.first_im)\n</code></pre>"},{"location":"api/cellects/core/cellects_threads/#cellects.core.cellects_threads.GetFirstImThread.__init__","title":"<code>__init__(parent=None)</code>","text":"<p>Initialize the worker thread for loading the first image of one folder.</p> <p>Parameters:</p> Name Type Description Default <code>parent</code> <code>QObject</code> <p>The parent object of this thread instance. In use, an instance of CellectsMainWidget class. Default is None.</p> <code>None</code> Source code in <code>src/cellects/core/cellects_threads.py</code> <pre><code>def __init__(self, parent=None):\n    \"\"\"\n    Initialize the worker thread for loading the first image of one folder.\n\n    Parameters\n    ----------\n    parent : QObject, optional\n        The parent object of this thread instance. In use, an instance of CellectsMainWidget class. Default is None.\n    \"\"\"\n    super(GetFirstImThread, self).__init__(parent)\n    self.setParent(parent)\n</code></pre>"},{"location":"api/cellects/core/cellects_threads/#cellects.core.cellects_threads.GetFirstImThread.run","title":"<code>run()</code>","text":"<p>Run the first image reading task in the parent process and emit a signal when it finishes.</p> Source code in <code>src/cellects/core/cellects_threads.py</code> <pre><code>def run(self):\n    \"\"\"\n    Run the first image reading task in the parent process and emit a signal when it finishes.\n    \"\"\"\n    self.parent().po.get_first_image()\n    self.message_when_thread_finished.emit(self.parent().po.first_im)\n</code></pre>"},{"location":"api/cellects/core/cellects_threads/#cellects.core.cellects_threads.GetLastImThread","title":"<code>GetLastImThread</code>","text":"<p>               Bases: <code>QThread</code></p> <p>Thread for getting the last image.</p> Notes <p>This class uses <code>QThread</code> to manage the process asynchronously.</p> Source code in <code>src/cellects/core/cellects_threads.py</code> <pre><code>class GetLastImThread(QtCore.QThread):\n    \"\"\"\n    Thread for getting the last image.\n\n    Notes\n    -----\n    This class uses `QThread` to manage the process asynchronously.\n    \"\"\"\n    def __init__(self, parent=None):\n        \"\"\"\n        Initialize the worker thread for loading the last image of one folder.\n\n        Parameters\n        ----------\n        parent : QObject, optional\n            The parent object of this thread instance. In use, an instance of CellectsMainWidget class. Default is None.\n        \"\"\"\n        super(GetLastImThread, self).__init__(parent)\n        self.setParent(parent)\n\n    def run(self):\n        \"\"\"\n        Run the last image reading task in the parent process.\n        \"\"\"\n        self.parent().po.get_last_image()\n</code></pre>"},{"location":"api/cellects/core/cellects_threads/#cellects.core.cellects_threads.GetLastImThread.__init__","title":"<code>__init__(parent=None)</code>","text":"<p>Initialize the worker thread for loading the last image of one folder.</p> <p>Parameters:</p> Name Type Description Default <code>parent</code> <code>QObject</code> <p>The parent object of this thread instance. In use, an instance of CellectsMainWidget class. Default is None.</p> <code>None</code> Source code in <code>src/cellects/core/cellects_threads.py</code> <pre><code>def __init__(self, parent=None):\n    \"\"\"\n    Initialize the worker thread for loading the last image of one folder.\n\n    Parameters\n    ----------\n    parent : QObject, optional\n        The parent object of this thread instance. In use, an instance of CellectsMainWidget class. Default is None.\n    \"\"\"\n    super(GetLastImThread, self).__init__(parent)\n    self.setParent(parent)\n</code></pre>"},{"location":"api/cellects/core/cellects_threads/#cellects.core.cellects_threads.GetLastImThread.run","title":"<code>run()</code>","text":"<p>Run the last image reading task in the parent process.</p> Source code in <code>src/cellects/core/cellects_threads.py</code> <pre><code>def run(self):\n    \"\"\"\n    Run the last image reading task in the parent process.\n    \"\"\"\n    self.parent().po.get_last_image()\n</code></pre>"},{"location":"api/cellects/core/cellects_threads/#cellects.core.cellects_threads.LastImageAnalysisThread","title":"<code>LastImageAnalysisThread</code>","text":"<p>               Bases: <code>QThread</code></p> <p>Thread for analyzing the last image of a given folder.</p> Signals <p>message_from_thread : Signal(str)     Signal emitted when progress messages are available. message_when_thread_finished : Signal(bool)     Signal emitted upon completion of the thread's task.</p> Notes <p>This class uses <code>QThread</code> to manage the process asynchronously.</p> Source code in <code>src/cellects/core/cellects_threads.py</code> <pre><code>class LastImageAnalysisThread(QtCore.QThread):\n    \"\"\"\n    Thread for analyzing the last image of a given folder.\n\n    Signals\n    -------\n    message_from_thread : Signal(str)\n        Signal emitted when progress messages are available.\n    message_when_thread_finished : Signal(bool)\n        Signal emitted upon completion of the thread's task.\n\n    Notes\n    -----\n    This class uses `QThread` to manage the process asynchronously.\n    \"\"\"\n    message_from_thread = QtCore.Signal(str)\n    message_when_thread_finished = QtCore.Signal(bool)\n\n    def __init__(self, parent=None):\n        \"\"\"\n        Initialize the worker thread for analyzing the last image of a given folder\n\n        Parameters\n        ----------\n        parent : QObject, optional\n            The parent object of this thread instance. In use, an instance of CellectsMainWidget class. Default is None.\n        \"\"\"\n        super(LastImageAnalysisThread, self).__init__(parent)\n        self.setParent(parent)\n\n    def run(self):\n        \"\"\"\n        Summary:\n        Run the image processing and analysis pipeline based on current settings.\n\n        Extended Description:\n        This function initiates the workflow for image processing and analysis,\n        including segmenting images, generating analysis options, and handling\n        various masks and settings based on the current state of the parent object.\n\n        Returns:\n        --------\n        None\n            This method does not return a value. It emits signals to indicate the\n            progress and completion of the processing tasks.\n\n        Notes:\n        ------\n        This function uses various attributes from the parent class to determine\n        how to process and analyze images. The specific behavior is heavily\n        dependent on the state of these attributes.\n\n        Attributes:\n        -----------\n        parent() : object\n            The owner of this instance, containing necessary settings and methods.\n        message_from_thread.emit(s : str) : signal\n            Signal to indicate progress messages from the thread.\n        message_when_thread_finished.emit(success : bool) : signal\n            Signal to indicate the completion of the thread.\n        \"\"\"\n        if self.parent().po.visualize or (len(self.parent().po.first_im.shape) == 2 and not self.parent().po.network_shaped):\n            self.message_from_thread.emit(\"Image segmentation, wait...\")\n        else:\n            self.message_from_thread.emit(\"Generating analysis options, wait...\")\n        self.parent().po.full_last_image_segmentation(self.parent().imageanalysiswindow.bio_mask, self.parent().imageanalysiswindow.back_mask)\n        self.message_when_thread_finished.emit(True)\n</code></pre>"},{"location":"api/cellects/core/cellects_threads/#cellects.core.cellects_threads.LastImageAnalysisThread.__init__","title":"<code>__init__(parent=None)</code>","text":"<p>Initialize the worker thread for analyzing the last image of a given folder</p> <p>Parameters:</p> Name Type Description Default <code>parent</code> <code>QObject</code> <p>The parent object of this thread instance. In use, an instance of CellectsMainWidget class. Default is None.</p> <code>None</code> Source code in <code>src/cellects/core/cellects_threads.py</code> <pre><code>def __init__(self, parent=None):\n    \"\"\"\n    Initialize the worker thread for analyzing the last image of a given folder\n\n    Parameters\n    ----------\n    parent : QObject, optional\n        The parent object of this thread instance. In use, an instance of CellectsMainWidget class. Default is None.\n    \"\"\"\n    super(LastImageAnalysisThread, self).__init__(parent)\n    self.setParent(parent)\n</code></pre>"},{"location":"api/cellects/core/cellects_threads/#cellects.core.cellects_threads.LastImageAnalysisThread.run","title":"<code>run()</code>","text":"<p>Summary: Run the image processing and analysis pipeline based on current settings.</p> <p>Extended Description: This function initiates the workflow for image processing and analysis, including segmenting images, generating analysis options, and handling various masks and settings based on the current state of the parent object.</p> Returns: <p>None     This method does not return a value. It emits signals to indicate the     progress and completion of the processing tasks.</p> Notes: <p>This function uses various attributes from the parent class to determine how to process and analyze images. The specific behavior is heavily dependent on the state of these attributes.</p> Attributes: <p>parent() : object     The owner of this instance, containing necessary settings and methods. message_from_thread.emit(s : str) : signal     Signal to indicate progress messages from the thread. message_when_thread_finished.emit(success : bool) : signal     Signal to indicate the completion of the thread.</p> Source code in <code>src/cellects/core/cellects_threads.py</code> <pre><code>def run(self):\n    \"\"\"\n    Summary:\n    Run the image processing and analysis pipeline based on current settings.\n\n    Extended Description:\n    This function initiates the workflow for image processing and analysis,\n    including segmenting images, generating analysis options, and handling\n    various masks and settings based on the current state of the parent object.\n\n    Returns:\n    --------\n    None\n        This method does not return a value. It emits signals to indicate the\n        progress and completion of the processing tasks.\n\n    Notes:\n    ------\n    This function uses various attributes from the parent class to determine\n    how to process and analyze images. The specific behavior is heavily\n    dependent on the state of these attributes.\n\n    Attributes:\n    -----------\n    parent() : object\n        The owner of this instance, containing necessary settings and methods.\n    message_from_thread.emit(s : str) : signal\n        Signal to indicate progress messages from the thread.\n    message_when_thread_finished.emit(success : bool) : signal\n        Signal to indicate the completion of the thread.\n    \"\"\"\n    if self.parent().po.visualize or (len(self.parent().po.first_im.shape) == 2 and not self.parent().po.network_shaped):\n        self.message_from_thread.emit(\"Image segmentation, wait...\")\n    else:\n        self.message_from_thread.emit(\"Generating analysis options, wait...\")\n    self.parent().po.full_last_image_segmentation(self.parent().imageanalysiswindow.bio_mask, self.parent().imageanalysiswindow.back_mask)\n    self.message_when_thread_finished.emit(True)\n</code></pre>"},{"location":"api/cellects/core/cellects_threads/#cellects.core.cellects_threads.LoadDataToRunCellectsQuicklyThread","title":"<code>LoadDataToRunCellectsQuicklyThread</code>","text":"<p>               Bases: <code>QThread</code></p> <p>Load data to run Cellects quickly in a separate thread.</p> <p>This class is responsible for loading necessary data asynchronously in order to speed up the process of running Cellects.</p> Signals <p>message_when_thread_finished : Signal(str)     Emitted when the thread finishes execution, indicating whether data loading was successful.</p> Notes <p>This class uses <code>QThread</code> to manage the process asynchronously.</p> Source code in <code>src/cellects/core/cellects_threads.py</code> <pre><code>class LoadDataToRunCellectsQuicklyThread(QtCore.QThread):\n    \"\"\"\n    Load data to run Cellects quickly in a separate thread.\n\n    This class is responsible for loading necessary data asynchronously\n    in order to speed up the process of running Cellects.\n\n    Signals\n    -------\n    message_when_thread_finished : Signal(str)\n        Emitted when the thread finishes execution, indicating whether data loading was successful.\n\n    Notes\n    -----\n    This class uses `QThread` to manage the process asynchronously.\n    \"\"\"\n    message_from_thread = QtCore.Signal(str)\n\n    def __init__(self, parent=None):\n        \"\"\"\n        Initialize the worker thread for quickly loading data to run Cellects.\n\n        Parameters\n        ----------\n        parent : QObject, optional\n            The parent object of this thread instance. In use, an instance of CellectsMainWidget class. Default is None.\n        \"\"\"\n        super(LoadDataToRunCellectsQuicklyThread, self).__init__(parent)\n        self.setParent(parent)\n\n    def run(self):\n        \"\"\"\n        Execute the data loading and preparation process for running cellects without setting all parameters in the GUI.\n\n        This method triggers the parent object's methods to look for data and load it,\n        then checks if the first experiment is ready. If so, it emits a message.\n        \"\"\"\n        self.parent().po.look_for_data()\n        self.parent().po.load_data_to_run_cellects_quickly()\n        if self.parent().po.first_exp_ready_to_run:\n            self.message_from_thread.emit(\"Data found, Video tracking window and Run all directly are available\")\n        else:\n            self.message_from_thread.emit(\"\")\n</code></pre>"},{"location":"api/cellects/core/cellects_threads/#cellects.core.cellects_threads.LoadDataToRunCellectsQuicklyThread.__init__","title":"<code>__init__(parent=None)</code>","text":"<p>Initialize the worker thread for quickly loading data to run Cellects.</p> <p>Parameters:</p> Name Type Description Default <code>parent</code> <code>QObject</code> <p>The parent object of this thread instance. In use, an instance of CellectsMainWidget class. Default is None.</p> <code>None</code> Source code in <code>src/cellects/core/cellects_threads.py</code> <pre><code>def __init__(self, parent=None):\n    \"\"\"\n    Initialize the worker thread for quickly loading data to run Cellects.\n\n    Parameters\n    ----------\n    parent : QObject, optional\n        The parent object of this thread instance. In use, an instance of CellectsMainWidget class. Default is None.\n    \"\"\"\n    super(LoadDataToRunCellectsQuicklyThread, self).__init__(parent)\n    self.setParent(parent)\n</code></pre>"},{"location":"api/cellects/core/cellects_threads/#cellects.core.cellects_threads.LoadDataToRunCellectsQuicklyThread.run","title":"<code>run()</code>","text":"<p>Execute the data loading and preparation process for running cellects without setting all parameters in the GUI.</p> <p>This method triggers the parent object's methods to look for data and load it, then checks if the first experiment is ready. If so, it emits a message.</p> Source code in <code>src/cellects/core/cellects_threads.py</code> <pre><code>def run(self):\n    \"\"\"\n    Execute the data loading and preparation process for running cellects without setting all parameters in the GUI.\n\n    This method triggers the parent object's methods to look for data and load it,\n    then checks if the first experiment is ready. If so, it emits a message.\n    \"\"\"\n    self.parent().po.look_for_data()\n    self.parent().po.load_data_to_run_cellects_quickly()\n    if self.parent().po.first_exp_ready_to_run:\n        self.message_from_thread.emit(\"Data found, Video tracking window and Run all directly are available\")\n    else:\n        self.message_from_thread.emit(\"\")\n</code></pre>"},{"location":"api/cellects/core/cellects_threads/#cellects.core.cellects_threads.LoadFirstFolderIfSeveralThread","title":"<code>LoadFirstFolderIfSeveralThread</code>","text":"<p>               Bases: <code>QThread</code></p> <p>Thread for loading data from the first folder if there are several folders.</p> Signals <p>message_when_thread_finished : Signal(bool)     Emitted when the thread finishes execution, indicating whether data loading was successful.</p> Notes <p>This class uses <code>QThread</code> to manage the process asynchronously.</p> Source code in <code>src/cellects/core/cellects_threads.py</code> <pre><code>class LoadFirstFolderIfSeveralThread(QtCore.QThread):\n    \"\"\"\n    Thread for loading data from the first folder if there are several folders.\n\n    Signals\n    -------\n    message_when_thread_finished : Signal(bool)\n        Emitted when the thread finishes execution, indicating whether data loading was successful.\n\n    Notes\n    -----\n    This class uses `QThread` to manage the process asynchronously.\n    \"\"\"\n    message_when_thread_finished = QtCore.Signal(bool)\n    def __init__(self, parent=None):\n        \"\"\"\n        Initialize the worker thread for loading data and parameters to run Cellects when analyzing several folders.\n\n        Parameters\n        ----------\n        parent : QObject, optional\n            The parent object of this thread instance. In use, an instance of CellectsMainWidget class. Default is None.\n        \"\"\"\n        super(LoadFirstFolderIfSeveralThread, self).__init__(parent)\n        self.setParent(parent)\n\n    def run(self):\n        \"\"\"\n        Run the data lookup process.\n        \"\"\"\n        self.parent().po.load_data_to_run_cellects_quickly()\n        if not self.parent().po.first_exp_ready_to_run:\n            self.parent().po.get_first_image()\n        self.message_when_thread_finished.emit(self.parent().po.first_exp_ready_to_run)\n</code></pre>"},{"location":"api/cellects/core/cellects_threads/#cellects.core.cellects_threads.LoadFirstFolderIfSeveralThread.__init__","title":"<code>__init__(parent=None)</code>","text":"<p>Initialize the worker thread for loading data and parameters to run Cellects when analyzing several folders.</p> <p>Parameters:</p> Name Type Description Default <code>parent</code> <code>QObject</code> <p>The parent object of this thread instance. In use, an instance of CellectsMainWidget class. Default is None.</p> <code>None</code> Source code in <code>src/cellects/core/cellects_threads.py</code> <pre><code>def __init__(self, parent=None):\n    \"\"\"\n    Initialize the worker thread for loading data and parameters to run Cellects when analyzing several folders.\n\n    Parameters\n    ----------\n    parent : QObject, optional\n        The parent object of this thread instance. In use, an instance of CellectsMainWidget class. Default is None.\n    \"\"\"\n    super(LoadFirstFolderIfSeveralThread, self).__init__(parent)\n    self.setParent(parent)\n</code></pre>"},{"location":"api/cellects/core/cellects_threads/#cellects.core.cellects_threads.LoadFirstFolderIfSeveralThread.run","title":"<code>run()</code>","text":"<p>Run the data lookup process.</p> Source code in <code>src/cellects/core/cellects_threads.py</code> <pre><code>def run(self):\n    \"\"\"\n    Run the data lookup process.\n    \"\"\"\n    self.parent().po.load_data_to_run_cellects_quickly()\n    if not self.parent().po.first_exp_ready_to_run:\n        self.parent().po.get_first_image()\n    self.message_when_thread_finished.emit(self.parent().po.first_exp_ready_to_run)\n</code></pre>"},{"location":"api/cellects/core/cellects_threads/#cellects.core.cellects_threads.LookForDataThreadInFirstW","title":"<code>LookForDataThreadInFirstW</code>","text":"<p>               Bases: <code>QThread</code></p> <p>Find and process data in a separate thread.</p> Notes <p>This class uses <code>QThread</code> to manage the process asynchronously.</p> Source code in <code>src/cellects/core/cellects_threads.py</code> <pre><code>class LookForDataThreadInFirstW(QtCore.QThread):\n    \"\"\"\n    Find and process data in a separate thread.\n\n    Notes\n    -----\n    This class uses `QThread` to manage the process asynchronously.\n    \"\"\"\n    def __init__(self, parent=None):\n\n        \"\"\"\n        Initialize the worker thread for finding data to run Cellects.\n\n        Parameters\n        ----------\n        parent : QObject, optional\n            The parent object of this thread instance. In use, an instance of CellectsMainWidget class. Default is None.\n        \"\"\"\n        super(LookForDataThreadInFirstW, self).__init__(parent)\n        self.setParent(parent)\n\n    def run(self):\n        \"\"\"\n        Run the data lookup process.\n        \"\"\"\n        self.parent().po.look_for_data()\n</code></pre>"},{"location":"api/cellects/core/cellects_threads/#cellects.core.cellects_threads.LookForDataThreadInFirstW.__init__","title":"<code>__init__(parent=None)</code>","text":"<p>Initialize the worker thread for finding data to run Cellects.</p> <p>Parameters:</p> Name Type Description Default <code>parent</code> <code>QObject</code> <p>The parent object of this thread instance. In use, an instance of CellectsMainWidget class. Default is None.</p> <code>None</code> Source code in <code>src/cellects/core/cellects_threads.py</code> <pre><code>def __init__(self, parent=None):\n\n    \"\"\"\n    Initialize the worker thread for finding data to run Cellects.\n\n    Parameters\n    ----------\n    parent : QObject, optional\n        The parent object of this thread instance. In use, an instance of CellectsMainWidget class. Default is None.\n    \"\"\"\n    super(LookForDataThreadInFirstW, self).__init__(parent)\n    self.setParent(parent)\n</code></pre>"},{"location":"api/cellects/core/cellects_threads/#cellects.core.cellects_threads.LookForDataThreadInFirstW.run","title":"<code>run()</code>","text":"<p>Run the data lookup process.</p> Source code in <code>src/cellects/core/cellects_threads.py</code> <pre><code>def run(self):\n    \"\"\"\n    Run the data lookup process.\n    \"\"\"\n    self.parent().po.look_for_data()\n</code></pre>"},{"location":"api/cellects/core/cellects_threads/#cellects.core.cellects_threads.OneArenaThread","title":"<code>OneArenaThread</code>","text":"<p>               Bases: <code>QThread</code></p> <p>Thread for completing the analysis of one particular arena in the current folder.</p> Signals <p>message_from_thread_starting : Signal(str)     Signal emitted when the thread successfully starts. image_from_thread : Signal(dict)     Signal emitted during the video reading or analysis to display images of the current status to the GUI. when_loading_finished : Signal(bool)     Signal emitted when the video is completely loaded. when_detection_finished : Signal(str)     Signal emitted when the video analysis is finished.</p> Notes <p>This class uses <code>QThread</code> to manage the process asynchronously.</p> Source code in <code>src/cellects/core/cellects_threads.py</code> <pre><code>class OneArenaThread(QtCore.QThread):\n    \"\"\"\n    Thread for completing the analysis of one particular arena in the current folder.\n\n    Signals\n    -------\n    message_from_thread_starting : Signal(str)\n        Signal emitted when the thread successfully starts.\n    image_from_thread : Signal(dict)\n        Signal emitted during the video reading or analysis to display images of the current status to the GUI.\n    when_loading_finished : Signal(bool)\n        Signal emitted when the video is completely loaded.\n    when_detection_finished : Signal(str)\n        Signal emitted when the video analysis is finished.\n\n    Notes\n    -----\n    This class uses `QThread` to manage the process asynchronously.\n    \"\"\"\n    message_from_thread_starting = QtCore.Signal(str)\n    image_from_thread = QtCore.Signal(dict)\n    when_loading_finished = QtCore.Signal(bool)\n    when_detection_finished = QtCore.Signal(str)\n\n    def __init__(self, parent=None):\n        \"\"\"\n        Initialize the worker thread for saving the analyzing one arena entirely\n\n        Parameters\n        ----------\n        parent : QObject, optional\n            The parent object of this thread instance. In use, an instance of CellectsMainWidget class. Default is None.\n        \"\"\"\n        super(OneArenaThread, self).__init__(parent)\n        self.setParent(parent)\n        self._isRunning = False\n\n    def run(self):\n        \"\"\"\n\n        Run analysis on one arena.\n\n        This method prepares and initiates the analysis process for a video by setting up required folders,\n        loading necessary data, and performing pre-processing steps. It manages the state of running analysis and\n        handles memory allocation for efficient processing.\n\n        Notes\n        -----\n        - This method uses threading to handle long-running operations without blocking the main UI.\n        - The memory allocation is dynamically adjusted based on available system resources.\n\n        Attributes\n        ----------\n        self.parent().po.vars['convert_for_motion'] : dict\n            Dictionary containing variables related to motion conversion.\n        self.parent().po.first_exp_ready_to_run : bool\n            Boolean indicating if the first experiment is ready to run.\n        self.parent().po.cores : int\n            Number of cores available for processing.\n        self.parent().po.motion : object\n            Object containing motion-related data and methods.\n        self.parent().po.load_quick_full : int\n            Number of arenas to load quickly for full detection.\n        \"\"\"\n        continue_analysis = True\n        self._isRunning = True\n        self.message_from_thread_starting.emit(\"Video loading, wait...\")\n\n        self.set_current_folder()\n        if not self.parent().po.first_exp_ready_to_run:\n            self.parent().po.load_data_to_run_cellects_quickly()\n            if not self.parent().po.first_exp_ready_to_run:\n                #Need a look for data when Data to run Cellects quickly.pkl and 1 folder selected amon several\n                continue_analysis = self.pre_processing()\n        if continue_analysis:\n            memory_diff = self.parent().po.update_available_core_nb()\n            if self.parent().po.cores == 0:\n                self.message_from_thread_starting.emit(f\"Analyzing one arena requires {memory_diff}GB of additional RAM to run\")\n            else:\n                if self.parent().po.motion is None or self.parent().po.load_quick_full == 0:\n                    self.load_one_arena()\n                if self.parent().po.load_quick_full &gt; 0:\n                    if self.parent().po.motion.start is not None:\n                        logging.info(\"One arena detection has started\")\n                        self.detection()\n                        if self.parent().po.load_quick_full &gt; 1:\n                            logging.info(\"One arena post-processing has started\")\n                            self.post_processing()\n                        else:\n                            self.when_detection_finished.emit(\"Detection done, read to see the result\")\n                    else:\n                        self.message_from_thread_starting.emit(f\"The current parameters failed to detect the cell(s) motion\")\n\n    def stop(self):\n        \"\"\"\n        Stops the running process.\n\n        This method is used to safely halt the current process.\n        \"\"\"\n        self._isRunning = False\n\n    def set_current_folder(self):\n        \"\"\"\n\n        Sets the current folder based on conditions.\n\n        This method determines which folder to use and updates the current\n        folder ID accordingly. If there are multiple folders, it uses the first folder\n        from the list; otherwise, it uses a reduced global pathway as the current.\n        \"\"\"\n        if self.parent().po.all['folder_number'] &gt; 1:\n            logging.info(f\"Use {self.parent().po.all['folder_list'][0]} folder\")\n            self.parent().po.update_folder_id(self.parent().po.all['sample_number_per_folder'][0],\n                                              self.parent().po.all['folder_list'][0])\n        else:\n            curr_path = reduce_path_len(self.parent().po.all['global_pathway'], 6, 10)\n            logging.info(f\"Use {curr_path} folder\")\n            self.parent().po.update_folder_id(self.parent().po.all['first_folder_sample_number'])\n\n    def pre_processing(self):\n        \"\"\"\n        Pre-processes the data for running Cellects on one arena.\n\n        This function logs various stages of preprocessing, validates specimen numbers,\n        performs necessary segmentations and data saving operations. It handles the\n        initialization, image analysis, and background extraction processes to prepare\n        the folder for further analysis.\n\n        Returns\n        -------\n        bool\n            Returns True if pre-processing completed successfully; False otherwise.\n        \"\"\"\n        logging.info(\"Pre-processing has started\")\n        analysis_status = {\"continue\": True, \"message\": \"\"}\n\n        self.parent().po.get_first_image()\n        self.parent().po.fast_first_image_segmentation()\n        if len(self.parent().po.vars['analyzed_individuals']) != self.parent().po.first_image.shape_number:\n            self.message_from_thread_starting.emit(f\"Wrong specimen number: (re)do the complete analysis.\")\n            analysis_status[\"continue\"] = False\n        else:\n            self.parent().po.cropping(is_first_image=True)\n            self.parent().po.get_average_pixel_size()\n            analysis_status = self.parent().po.delineate_each_arena()\n            if not analysis_status[\"continue\"]:\n                self.message_from_thread_starting.emit(analysis_status[\"message\"])\n                logging.error(analysis_status['message'])\n            else:\n                self.parent().po.data_to_save['exif'] = True\n                self.parent().po.save_data_to_run_cellects_quickly()\n                self.parent().po.data_to_save['exif'] = False\n                self.parent().po.get_background_to_subtract()\n                if len(self.parent().po.vars['analyzed_individuals']) != len(self.parent().po.top):\n                    self.message_from_thread_starting.emit(f\"Wrong specimen number: (re)do the complete analysis.\")\n                    analysis_status[\"continue\"] = False\n                else:\n                    self.parent().po.get_origins_and_backgrounds_lists()\n                    self.parent().po.get_last_image()\n                    self.parent().po.fast_last_image_segmentation()\n                    self.parent().po.find_if_lighter_backgnp.round()\n                    logging.info(\"The current (or the first) folder is ready to run\")\n                    self.parent().po.first_exp_ready_to_run = True\n        return analysis_status[\"continue\"]\n\n    def load_one_arena(self):\n        \"\"\"\n        Load a single arena from images or video to perform motion analysis.\n        \"\"\"\n        arena = self.parent().po.all['arena']\n        i = np.nonzero(self.parent().po.vars['analyzed_individuals'] == arena)[0][0]\n        true_frame_width = self.parent().po.right[i] - self.parent().po.left[i]# self.parent().po.vars['origin_list'][i].shape[1]\n        if self.parent().po.all['overwrite_unaltered_videos'] and os.path.isfile(f'ind_{arena}.npy'):\n            os.remove(f'ind_{arena}.npy')\n        background = None\n        background2 = None\n        if self.parent().po.vars['subtract_background']:\n            background = self.parent().po.vars['background_list'][i]\n            if self.parent().po.vars['convert_for_motion']['logical'] != 'None':\n                background2 = self.parent().po.vars['background_list2'][i]\n        vid_name = None\n        if self.parent().po.vars['video_list'] is not None:\n            vid_name = self.parent().po.vars['video_list'][i]\n        visu, converted_video, converted_video2 = read_one_arena(self.parent().po.all['arena'],\n            self.parent().po.vars['already_greyscale'], self.parent().po.vars['convert_for_motion'],\n            None, true_frame_width, vid_name, background, background2)\n\n        save_loaded_video: bool = False\n        if visu is None or (self.parent().po.vars['already_greyscale'] and converted_video is None):\n            cr = [self.parent().po.top[i], self.parent().po.bot[i],\n                  self.parent().po.left[i], self.parent().po.right[i]]\n            vids = create_empty_videos(self.parent().po.data_list, cr,\n                self.parent().po.vars['lose_accuracy_to_save_memory'], self.parent().po.vars['already_greyscale'],\n                self.parent().po.vars['convert_for_motion'])\n            self.parent().po.visu, self.parent().po.converted_video, self.parent().po.converted_video2 = vids\n            logging.info(f\"Starting to load arena n\u00b0{arena} from images\")\n\n            prev_img = None\n            pat_tracker = PercentAndTimeTracker(self.parent().po.vars['img_number'])\n            is_landscape = self.parent().po.first_image.image.shape[0] &lt; self.parent().po.first_image.image.shape[1]\n            for image_i, image_name in enumerate(self.parent().po.data_list):\n                current_percentage, eta = pat_tracker.get_progress()\n                reduce_image_dim = self.parent().po.vars['already_greyscale'] and self.parent().po.reduce_image_dim\n                img, prev_img = read_rotate_crop_and_reduce_image(image_name, prev_img,\n                    self.parent().po.first_image.crop_coord, cr, self.parent().po.all['raw_images'], is_landscape,\n                    reduce_image_dim)\n                self.image_from_thread.emit(\n                    {\"message\": f\"Video loading: {current_percentage}%{eta}\", \"current_image\": img})\n                if self.parent().po.vars['already_greyscale']:\n                    self.parent().po.converted_video[image_i, ...] = img\n                else:\n                    self.parent().po.visu[image_i, ...] = img\n\n            if not self.parent().po.vars['already_greyscale']:\n                msg = \"Video conversion\"\n                if background is not None :\n                    msg += \", background subtraction\"\n                if self.parent().po.vars['filter_spec'] is not None:\n                    msg += \", filtering\"\n                msg += \", wait...\"\n                self.image_from_thread.emit({\"message\": msg, \"current_image\": img})\n                converted_videos = convert_subtract_and_filter_video(self.parent().po.visu,\n                                                                        self.parent().po.vars['convert_for_motion'],\n                                                                        background, background2,\n                                                                        self.parent().po.vars['lose_accuracy_to_save_memory'],\n                                                                        self.parent().po.vars['filter_spec'])\n                self.parent().po.converted_video, self.parent().po.converted_video2 = converted_videos\n\n            save_loaded_video = True\n            if self.parent().po.vars['already_greyscale']:\n                self.videos_in_ram = self.parent().po.converted_video\n            else:\n                if self.parent().po.vars['convert_for_motion']['logical'] == 'None':\n                    self.videos_in_ram = [self.parent().po.visu, deepcopy(self.parent().po.converted_video)]\n                else:\n                    self.videos_in_ram = [self.parent().po.visu, deepcopy(self.parent().po.converted_video),\n                                          deepcopy(self.parent().po.converted_video2)]\n        else:\n            logging.info(f\"Starting to load arena n\u00b0{arena} from .npy saved file\")\n            self.videos_in_ram = None\n        l = [i, arena, self.parent().po.vars, False, False, False, self.videos_in_ram]\n        self.parent().po.motion = MotionAnalysis(l)\n        r = weakref.ref(self.parent().po.motion)\n\n        if self.videos_in_ram is None:\n            self.parent().po.converted_video = deepcopy(self.parent().po.motion.converted_video)\n            if self.parent().po.vars['convert_for_motion']['logical'] != 'None':\n                self.parent().po.converted_video2 = deepcopy(self.parent().po.motion.converted_video2)\n        self.parent().po.motion.assess_motion_detection()\n        self.when_loading_finished.emit(save_loaded_video)\n\n        if self.parent().po.motion.visu is None:\n            visu = bracket_to_uint8_image_contrast(self.parent().po.motion.converted_video)\n            if len(visu.shape) == 3:\n                visu = np.stack((visu, visu, visu), axis=3)\n            self.parent().po.motion.visu = visu\n\n    def detection(self):\n        \"\"\"\n        Perform quick video segmentation and update motion detection parameters.\n\n        This method is responsible for initiating a quick video segmentation process and updating the motion detection\n        parameters accordingly. It handles duplicate video conversion based on certain logical conditions and computes\n        video options.\n        \"\"\"\n        self.message_from_thread_starting.emit(f\"Quick video segmentation\")\n        self.parent().po.motion.converted_video = deepcopy(self.parent().po.converted_video)\n        if self.parent().po.vars['convert_for_motion']['logical'] != 'None':\n            self.parent().po.motion.converted_video2 = deepcopy(self.parent().po.converted_video2)\n        self.parent().po.motion.detection(compute_all_possibilities=self.parent().po.all['compute_all_options'])\n        if self.parent().po.all['compute_all_options']:\n            self.parent().po.computed_video_options = np.ones(5, bool)\n        else:\n            self.parent().po.computed_video_options = np.zeros(5, bool)\n            self.parent().po.computed_video_options[self.parent().po.all['video_option']] = True\n\n    def post_processing(self):\n        \"\"\"\n        Handle post-processing operations for motion analysis and video processing.\n\n        Extended Description\n        --------------------\n        This method is responsible for managing various post-processing steps,\n        including video segmentation, contour detection, and updating motion analysis\n        parameters. It processes different video options based on the configuration\n        settings and handles motion detection failures by emitting appropriate signals.\n\n        Notes\n        -----\n        This method performs a series of operations that are computationally intensive.\n        It leverages NumPy and OpenCV for image processing tasks. The method assumes\n        that the parent object has been properly initialized with all required attributes\n        and configurations.\n\n        Attributes\n        ----------\n        self.parent().po.motion.smoothed_video : NoneType\n            A placeholder for the smoothed video data.\n        self.parent().po.vars['already_greyscale'] : bool\n            Indicates if the video is already in greyscale format.\n        self.parent().po.vars['convert_for_motion']['logical'] : str\n            Indicates the logical conversion method for motion analysis.\n        self.parent().po.converted_video : ndarray\n            The converted video data for motion analysis.\n        self.parent().po.converted_video2 : ndarray\n            Another converted video data for motion analysis.\n        self.parent().po.visu : ndarray\n            The visual representation of the video data.\n        self.videos_in_ram : list or tuple\n            The videos currently in RAM, either a single video or multiple.\n        self.parent().po.vars['color_number'] : int\n            The number of colors in the video.\n        self.parent().po.all['compute_all_options'] : bool\n            Indicates if all options should be computed.\n        self.parent().po.all['video_option'] : int\n            The current video option to be processed.\n        self.parent().po.newly_explored_area : ndarray\n            The area newly explored during motion detection.\n        self.parent().po.motion.start : int\n            The start frame for motion analysis.\n        self.parent().po.motion.step : int\n            The step interval in frames for motion analysis.\n        self.parent().po.motion.lost_frames : int\n            The number of lost frames during motion analysis.\n        self.parent().po.motion.substantial_growth : int\n            The substantial growth threshold for motion detection.\n        self.parent().po.all['arena'] : int\n            The arena identifier used in motion analysis.\n        self.parent().po.vars['do_fading'] : bool\n            Indicates if fading effects should be applied.\n        self.parent().po.motion.dims : tuple\n            The dimensions of the motion data.\n        analyses_to_compute : list or ndarray\n            List of analysis options to compute based on configuration settings.\n        args : list\n            Arguments used for initializing the MotionAnalysis object.\n        analysis_i : MotionAnalysis\n            An instance of MotionAnalysis for each segment to be processed.\n        mask : tuple or NoneType\n            The mask used for different segmentation options.\n\n        \"\"\"\n        self.parent().po.motion.smoothed_video = None\n        if self.parent().po.vars['color_number'] &gt; 2:\n            analyses_to_compute = [0]\n        else:\n            if self.parent().po.all['compute_all_options']:\n                analyses_to_compute = np.arange(5)\n            else:\n                logging.info(f\"option: {self.parent().po.all['video_option']}\")\n                analyses_to_compute = [self.parent().po.all['video_option']]\n        time_parameters = [self.parent().po.motion.start, self.parent().po.motion.step,\n                           self.parent().po.motion.lost_frames, self.parent().po.motion.substantial_growth]\n\n        args = [self.parent().po.all['arena'] - 1, self.parent().po.all['arena'], self.parent().po.vars,\n                False, False, False, self.videos_in_ram]\n        if self.parent().po.vars['do_fading']:\n            self.parent().po.newly_explored_area = np.zeros((self.parent().po.motion.dims[0], 5), np.int64)\n        for seg_i in analyses_to_compute:\n            analysis_i = MotionAnalysis(args)\n            r = weakref.ref(analysis_i)\n            analysis_i.segmented = np.zeros(analysis_i.converted_video.shape[:3], dtype=np.uint8)\n            if self.parent().po.all['compute_all_options']:\n                if seg_i == 0:\n                    analysis_i.segmented = self.parent().po.motion.segmented\n                else:\n                    if seg_i == 1:\n                        mask = self.parent().po.motion.luminosity_segmentation\n                    elif seg_i == 2:\n                        mask = self.parent().po.motion.gradient_segmentation\n                    elif seg_i == 3:\n                        mask = self.parent().po.motion.logical_and\n                    elif seg_i == 4:\n                        mask = self.parent().po.motion.logical_or\n                    analysis_i.segmented[mask[0], mask[1], mask[2]] = 1\n            else:\n                if self.parent().po.computed_video_options[self.parent().po.all['video_option']]:\n                    analysis_i.segmented = self.parent().po.motion.segmented\n\n            analysis_i.start = time_parameters[0]\n            analysis_i.step = time_parameters[1]\n            analysis_i.lost_frames = time_parameters[2]\n            analysis_i.substantial_growth = time_parameters[3]\n            analysis_i.origin_idx = self.parent().po.motion.origin_idx\n            analysis_i.initialize_post_processing()\n            analysis_i.t = analysis_i.start\n\n            while self._isRunning and analysis_i.t &lt; analysis_i.binary.shape[0]:\n                analysis_i.update_shape(False)\n                contours = np.nonzero(get_contours(analysis_i.binary[analysis_i.t - 1, :, :]))\n                current_image = deepcopy(self.parent().po.motion.visu[analysis_i.t - 1, :, :, :])\n                current_image[contours[0], contours[1], :] = self.parent().po.vars['contour_color']\n                self.image_from_thread.emit(\n                    {\"message\": f\"Tracking option n\u00b0{seg_i + 1}. Image number: {analysis_i.t - 1}\",\n                     \"current_image\": current_image})\n            if analysis_i.start is None:\n                analysis_i.binary = np.repeat(np.expand_dims(analysis_i.origin, 0),\n                                           analysis_i.converted_video.shape[0], axis=0)\n                if self.parent().po.vars['color_number'] &gt; 2:\n                    self.message_from_thread_starting.emit(\n                        f\"Failed to detect motion. Redo image analysis (with only 2 colors?)\")\n                else:\n                    self.message_from_thread_starting.emit(f\"Tracking option n\u00b0{seg_i + 1} failed to detect motion\")\n\n            if self.parent().po.all['compute_all_options']:\n                if seg_i == 0:\n                    self.parent().po.motion.segmented = analysis_i.binary\n                elif seg_i == 1:\n                    self.parent().po.motion.luminosity_segmentation = np.nonzero(analysis_i.binary)\n                elif seg_i == 2:\n                    self.parent().po.motion.gradient_segmentation = np.nonzero(analysis_i.binary)\n                elif seg_i == 3:\n                    self.parent().po.motion.logical_and = np.nonzero(analysis_i.binary)\n                elif seg_i == 4:\n                    self.parent().po.motion.logical_or = np.nonzero(analysis_i.binary)\n            else:\n                self.parent().po.motion.segmented = analysis_i.binary\n        self.when_detection_finished.emit(\"Post processing done, read to see the result\")\n</code></pre>"},{"location":"api/cellects/core/cellects_threads/#cellects.core.cellects_threads.OneArenaThread.__init__","title":"<code>__init__(parent=None)</code>","text":"<p>Initialize the worker thread for saving the analyzing one arena entirely</p> <p>Parameters:</p> Name Type Description Default <code>parent</code> <code>QObject</code> <p>The parent object of this thread instance. In use, an instance of CellectsMainWidget class. Default is None.</p> <code>None</code> Source code in <code>src/cellects/core/cellects_threads.py</code> <pre><code>def __init__(self, parent=None):\n    \"\"\"\n    Initialize the worker thread for saving the analyzing one arena entirely\n\n    Parameters\n    ----------\n    parent : QObject, optional\n        The parent object of this thread instance. In use, an instance of CellectsMainWidget class. Default is None.\n    \"\"\"\n    super(OneArenaThread, self).__init__(parent)\n    self.setParent(parent)\n    self._isRunning = False\n</code></pre>"},{"location":"api/cellects/core/cellects_threads/#cellects.core.cellects_threads.OneArenaThread.detection","title":"<code>detection()</code>","text":"<p>Perform quick video segmentation and update motion detection parameters.</p> <p>This method is responsible for initiating a quick video segmentation process and updating the motion detection parameters accordingly. It handles duplicate video conversion based on certain logical conditions and computes video options.</p> Source code in <code>src/cellects/core/cellects_threads.py</code> <pre><code>def detection(self):\n    \"\"\"\n    Perform quick video segmentation and update motion detection parameters.\n\n    This method is responsible for initiating a quick video segmentation process and updating the motion detection\n    parameters accordingly. It handles duplicate video conversion based on certain logical conditions and computes\n    video options.\n    \"\"\"\n    self.message_from_thread_starting.emit(f\"Quick video segmentation\")\n    self.parent().po.motion.converted_video = deepcopy(self.parent().po.converted_video)\n    if self.parent().po.vars['convert_for_motion']['logical'] != 'None':\n        self.parent().po.motion.converted_video2 = deepcopy(self.parent().po.converted_video2)\n    self.parent().po.motion.detection(compute_all_possibilities=self.parent().po.all['compute_all_options'])\n    if self.parent().po.all['compute_all_options']:\n        self.parent().po.computed_video_options = np.ones(5, bool)\n    else:\n        self.parent().po.computed_video_options = np.zeros(5, bool)\n        self.parent().po.computed_video_options[self.parent().po.all['video_option']] = True\n</code></pre>"},{"location":"api/cellects/core/cellects_threads/#cellects.core.cellects_threads.OneArenaThread.load_one_arena","title":"<code>load_one_arena()</code>","text":"<p>Load a single arena from images or video to perform motion analysis.</p> Source code in <code>src/cellects/core/cellects_threads.py</code> <pre><code>def load_one_arena(self):\n    \"\"\"\n    Load a single arena from images or video to perform motion analysis.\n    \"\"\"\n    arena = self.parent().po.all['arena']\n    i = np.nonzero(self.parent().po.vars['analyzed_individuals'] == arena)[0][0]\n    true_frame_width = self.parent().po.right[i] - self.parent().po.left[i]# self.parent().po.vars['origin_list'][i].shape[1]\n    if self.parent().po.all['overwrite_unaltered_videos'] and os.path.isfile(f'ind_{arena}.npy'):\n        os.remove(f'ind_{arena}.npy')\n    background = None\n    background2 = None\n    if self.parent().po.vars['subtract_background']:\n        background = self.parent().po.vars['background_list'][i]\n        if self.parent().po.vars['convert_for_motion']['logical'] != 'None':\n            background2 = self.parent().po.vars['background_list2'][i]\n    vid_name = None\n    if self.parent().po.vars['video_list'] is not None:\n        vid_name = self.parent().po.vars['video_list'][i]\n    visu, converted_video, converted_video2 = read_one_arena(self.parent().po.all['arena'],\n        self.parent().po.vars['already_greyscale'], self.parent().po.vars['convert_for_motion'],\n        None, true_frame_width, vid_name, background, background2)\n\n    save_loaded_video: bool = False\n    if visu is None or (self.parent().po.vars['already_greyscale'] and converted_video is None):\n        cr = [self.parent().po.top[i], self.parent().po.bot[i],\n              self.parent().po.left[i], self.parent().po.right[i]]\n        vids = create_empty_videos(self.parent().po.data_list, cr,\n            self.parent().po.vars['lose_accuracy_to_save_memory'], self.parent().po.vars['already_greyscale'],\n            self.parent().po.vars['convert_for_motion'])\n        self.parent().po.visu, self.parent().po.converted_video, self.parent().po.converted_video2 = vids\n        logging.info(f\"Starting to load arena n\u00b0{arena} from images\")\n\n        prev_img = None\n        pat_tracker = PercentAndTimeTracker(self.parent().po.vars['img_number'])\n        is_landscape = self.parent().po.first_image.image.shape[0] &lt; self.parent().po.first_image.image.shape[1]\n        for image_i, image_name in enumerate(self.parent().po.data_list):\n            current_percentage, eta = pat_tracker.get_progress()\n            reduce_image_dim = self.parent().po.vars['already_greyscale'] and self.parent().po.reduce_image_dim\n            img, prev_img = read_rotate_crop_and_reduce_image(image_name, prev_img,\n                self.parent().po.first_image.crop_coord, cr, self.parent().po.all['raw_images'], is_landscape,\n                reduce_image_dim)\n            self.image_from_thread.emit(\n                {\"message\": f\"Video loading: {current_percentage}%{eta}\", \"current_image\": img})\n            if self.parent().po.vars['already_greyscale']:\n                self.parent().po.converted_video[image_i, ...] = img\n            else:\n                self.parent().po.visu[image_i, ...] = img\n\n        if not self.parent().po.vars['already_greyscale']:\n            msg = \"Video conversion\"\n            if background is not None :\n                msg += \", background subtraction\"\n            if self.parent().po.vars['filter_spec'] is not None:\n                msg += \", filtering\"\n            msg += \", wait...\"\n            self.image_from_thread.emit({\"message\": msg, \"current_image\": img})\n            converted_videos = convert_subtract_and_filter_video(self.parent().po.visu,\n                                                                    self.parent().po.vars['convert_for_motion'],\n                                                                    background, background2,\n                                                                    self.parent().po.vars['lose_accuracy_to_save_memory'],\n                                                                    self.parent().po.vars['filter_spec'])\n            self.parent().po.converted_video, self.parent().po.converted_video2 = converted_videos\n\n        save_loaded_video = True\n        if self.parent().po.vars['already_greyscale']:\n            self.videos_in_ram = self.parent().po.converted_video\n        else:\n            if self.parent().po.vars['convert_for_motion']['logical'] == 'None':\n                self.videos_in_ram = [self.parent().po.visu, deepcopy(self.parent().po.converted_video)]\n            else:\n                self.videos_in_ram = [self.parent().po.visu, deepcopy(self.parent().po.converted_video),\n                                      deepcopy(self.parent().po.converted_video2)]\n    else:\n        logging.info(f\"Starting to load arena n\u00b0{arena} from .npy saved file\")\n        self.videos_in_ram = None\n    l = [i, arena, self.parent().po.vars, False, False, False, self.videos_in_ram]\n    self.parent().po.motion = MotionAnalysis(l)\n    r = weakref.ref(self.parent().po.motion)\n\n    if self.videos_in_ram is None:\n        self.parent().po.converted_video = deepcopy(self.parent().po.motion.converted_video)\n        if self.parent().po.vars['convert_for_motion']['logical'] != 'None':\n            self.parent().po.converted_video2 = deepcopy(self.parent().po.motion.converted_video2)\n    self.parent().po.motion.assess_motion_detection()\n    self.when_loading_finished.emit(save_loaded_video)\n\n    if self.parent().po.motion.visu is None:\n        visu = bracket_to_uint8_image_contrast(self.parent().po.motion.converted_video)\n        if len(visu.shape) == 3:\n            visu = np.stack((visu, visu, visu), axis=3)\n        self.parent().po.motion.visu = visu\n</code></pre>"},{"location":"api/cellects/core/cellects_threads/#cellects.core.cellects_threads.OneArenaThread.post_processing","title":"<code>post_processing()</code>","text":"<p>Handle post-processing operations for motion analysis and video processing.</p> Extended Description <p>This method is responsible for managing various post-processing steps, including video segmentation, contour detection, and updating motion analysis parameters. It processes different video options based on the configuration settings and handles motion detection failures by emitting appropriate signals.</p> Notes <p>This method performs a series of operations that are computationally intensive. It leverages NumPy and OpenCV for image processing tasks. The method assumes that the parent object has been properly initialized with all required attributes and configurations.</p> <p>Attributes:</p> Name Type Description <code>self.parent().po.motion.smoothed_video</code> <code>NoneType</code> <p>A placeholder for the smoothed video data.</p> <code>self.parent().po.vars['already_greyscale']</code> <code>bool</code> <p>Indicates if the video is already in greyscale format.</p> <code>self.parent().po.vars['convert_for_motion']['logical']</code> <code>str</code> <p>Indicates the logical conversion method for motion analysis.</p> <code>self.parent().po.converted_video</code> <code>ndarray</code> <p>The converted video data for motion analysis.</p> <code>self.parent().po.converted_video2</code> <code>ndarray</code> <p>Another converted video data for motion analysis.</p> <code>self.parent().po.visu</code> <code>ndarray</code> <p>The visual representation of the video data.</p> <code>self.videos_in_ram</code> <code>list or tuple</code> <p>The videos currently in RAM, either a single video or multiple.</p> <code>self.parent().po.vars['color_number']</code> <code>int</code> <p>The number of colors in the video.</p> <code>self.parent().po.all['compute_all_options']</code> <code>bool</code> <p>Indicates if all options should be computed.</p> <code>self.parent().po.all['video_option']</code> <code>int</code> <p>The current video option to be processed.</p> <code>self.parent().po.newly_explored_area</code> <code>ndarray</code> <p>The area newly explored during motion detection.</p> <code>self.parent().po.motion.start</code> <code>int</code> <p>The start frame for motion analysis.</p> <code>self.parent().po.motion.step</code> <code>int</code> <p>The step interval in frames for motion analysis.</p> <code>self.parent().po.motion.lost_frames</code> <code>int</code> <p>The number of lost frames during motion analysis.</p> <code>self.parent().po.motion.substantial_growth</code> <code>int</code> <p>The substantial growth threshold for motion detection.</p> <code>self.parent().po.all['arena']</code> <code>int</code> <p>The arena identifier used in motion analysis.</p> <code>self.parent().po.vars['do_fading']</code> <code>bool</code> <p>Indicates if fading effects should be applied.</p> <code>self.parent().po.motion.dims</code> <code>tuple</code> <p>The dimensions of the motion data.</p> <code>analyses_to_compute</code> <code>list or ndarray</code> <p>List of analysis options to compute based on configuration settings.</p> <code>args</code> <code>list</code> <p>Arguments used for initializing the MotionAnalysis object.</p> <code>analysis_i</code> <code>MotionAnalysis</code> <p>An instance of MotionAnalysis for each segment to be processed.</p> <code>mask</code> <code>tuple or NoneType</code> <p>The mask used for different segmentation options.</p> Source code in <code>src/cellects/core/cellects_threads.py</code> <pre><code>def post_processing(self):\n    \"\"\"\n    Handle post-processing operations for motion analysis and video processing.\n\n    Extended Description\n    --------------------\n    This method is responsible for managing various post-processing steps,\n    including video segmentation, contour detection, and updating motion analysis\n    parameters. It processes different video options based on the configuration\n    settings and handles motion detection failures by emitting appropriate signals.\n\n    Notes\n    -----\n    This method performs a series of operations that are computationally intensive.\n    It leverages NumPy and OpenCV for image processing tasks. The method assumes\n    that the parent object has been properly initialized with all required attributes\n    and configurations.\n\n    Attributes\n    ----------\n    self.parent().po.motion.smoothed_video : NoneType\n        A placeholder for the smoothed video data.\n    self.parent().po.vars['already_greyscale'] : bool\n        Indicates if the video is already in greyscale format.\n    self.parent().po.vars['convert_for_motion']['logical'] : str\n        Indicates the logical conversion method for motion analysis.\n    self.parent().po.converted_video : ndarray\n        The converted video data for motion analysis.\n    self.parent().po.converted_video2 : ndarray\n        Another converted video data for motion analysis.\n    self.parent().po.visu : ndarray\n        The visual representation of the video data.\n    self.videos_in_ram : list or tuple\n        The videos currently in RAM, either a single video or multiple.\n    self.parent().po.vars['color_number'] : int\n        The number of colors in the video.\n    self.parent().po.all['compute_all_options'] : bool\n        Indicates if all options should be computed.\n    self.parent().po.all['video_option'] : int\n        The current video option to be processed.\n    self.parent().po.newly_explored_area : ndarray\n        The area newly explored during motion detection.\n    self.parent().po.motion.start : int\n        The start frame for motion analysis.\n    self.parent().po.motion.step : int\n        The step interval in frames for motion analysis.\n    self.parent().po.motion.lost_frames : int\n        The number of lost frames during motion analysis.\n    self.parent().po.motion.substantial_growth : int\n        The substantial growth threshold for motion detection.\n    self.parent().po.all['arena'] : int\n        The arena identifier used in motion analysis.\n    self.parent().po.vars['do_fading'] : bool\n        Indicates if fading effects should be applied.\n    self.parent().po.motion.dims : tuple\n        The dimensions of the motion data.\n    analyses_to_compute : list or ndarray\n        List of analysis options to compute based on configuration settings.\n    args : list\n        Arguments used for initializing the MotionAnalysis object.\n    analysis_i : MotionAnalysis\n        An instance of MotionAnalysis for each segment to be processed.\n    mask : tuple or NoneType\n        The mask used for different segmentation options.\n\n    \"\"\"\n    self.parent().po.motion.smoothed_video = None\n    if self.parent().po.vars['color_number'] &gt; 2:\n        analyses_to_compute = [0]\n    else:\n        if self.parent().po.all['compute_all_options']:\n            analyses_to_compute = np.arange(5)\n        else:\n            logging.info(f\"option: {self.parent().po.all['video_option']}\")\n            analyses_to_compute = [self.parent().po.all['video_option']]\n    time_parameters = [self.parent().po.motion.start, self.parent().po.motion.step,\n                       self.parent().po.motion.lost_frames, self.parent().po.motion.substantial_growth]\n\n    args = [self.parent().po.all['arena'] - 1, self.parent().po.all['arena'], self.parent().po.vars,\n            False, False, False, self.videos_in_ram]\n    if self.parent().po.vars['do_fading']:\n        self.parent().po.newly_explored_area = np.zeros((self.parent().po.motion.dims[0], 5), np.int64)\n    for seg_i in analyses_to_compute:\n        analysis_i = MotionAnalysis(args)\n        r = weakref.ref(analysis_i)\n        analysis_i.segmented = np.zeros(analysis_i.converted_video.shape[:3], dtype=np.uint8)\n        if self.parent().po.all['compute_all_options']:\n            if seg_i == 0:\n                analysis_i.segmented = self.parent().po.motion.segmented\n            else:\n                if seg_i == 1:\n                    mask = self.parent().po.motion.luminosity_segmentation\n                elif seg_i == 2:\n                    mask = self.parent().po.motion.gradient_segmentation\n                elif seg_i == 3:\n                    mask = self.parent().po.motion.logical_and\n                elif seg_i == 4:\n                    mask = self.parent().po.motion.logical_or\n                analysis_i.segmented[mask[0], mask[1], mask[2]] = 1\n        else:\n            if self.parent().po.computed_video_options[self.parent().po.all['video_option']]:\n                analysis_i.segmented = self.parent().po.motion.segmented\n\n        analysis_i.start = time_parameters[0]\n        analysis_i.step = time_parameters[1]\n        analysis_i.lost_frames = time_parameters[2]\n        analysis_i.substantial_growth = time_parameters[3]\n        analysis_i.origin_idx = self.parent().po.motion.origin_idx\n        analysis_i.initialize_post_processing()\n        analysis_i.t = analysis_i.start\n\n        while self._isRunning and analysis_i.t &lt; analysis_i.binary.shape[0]:\n            analysis_i.update_shape(False)\n            contours = np.nonzero(get_contours(analysis_i.binary[analysis_i.t - 1, :, :]))\n            current_image = deepcopy(self.parent().po.motion.visu[analysis_i.t - 1, :, :, :])\n            current_image[contours[0], contours[1], :] = self.parent().po.vars['contour_color']\n            self.image_from_thread.emit(\n                {\"message\": f\"Tracking option n\u00b0{seg_i + 1}. Image number: {analysis_i.t - 1}\",\n                 \"current_image\": current_image})\n        if analysis_i.start is None:\n            analysis_i.binary = np.repeat(np.expand_dims(analysis_i.origin, 0),\n                                       analysis_i.converted_video.shape[0], axis=0)\n            if self.parent().po.vars['color_number'] &gt; 2:\n                self.message_from_thread_starting.emit(\n                    f\"Failed to detect motion. Redo image analysis (with only 2 colors?)\")\n            else:\n                self.message_from_thread_starting.emit(f\"Tracking option n\u00b0{seg_i + 1} failed to detect motion\")\n\n        if self.parent().po.all['compute_all_options']:\n            if seg_i == 0:\n                self.parent().po.motion.segmented = analysis_i.binary\n            elif seg_i == 1:\n                self.parent().po.motion.luminosity_segmentation = np.nonzero(analysis_i.binary)\n            elif seg_i == 2:\n                self.parent().po.motion.gradient_segmentation = np.nonzero(analysis_i.binary)\n            elif seg_i == 3:\n                self.parent().po.motion.logical_and = np.nonzero(analysis_i.binary)\n            elif seg_i == 4:\n                self.parent().po.motion.logical_or = np.nonzero(analysis_i.binary)\n        else:\n            self.parent().po.motion.segmented = analysis_i.binary\n    self.when_detection_finished.emit(\"Post processing done, read to see the result\")\n</code></pre>"},{"location":"api/cellects/core/cellects_threads/#cellects.core.cellects_threads.OneArenaThread.pre_processing","title":"<code>pre_processing()</code>","text":"<p>Pre-processes the data for running Cellects on one arena.</p> <p>This function logs various stages of preprocessing, validates specimen numbers, performs necessary segmentations and data saving operations. It handles the initialization, image analysis, and background extraction processes to prepare the folder for further analysis.</p> <p>Returns:</p> Type Description <code>bool</code> <p>Returns True if pre-processing completed successfully; False otherwise.</p> Source code in <code>src/cellects/core/cellects_threads.py</code> <pre><code>def pre_processing(self):\n    \"\"\"\n    Pre-processes the data for running Cellects on one arena.\n\n    This function logs various stages of preprocessing, validates specimen numbers,\n    performs necessary segmentations and data saving operations. It handles the\n    initialization, image analysis, and background extraction processes to prepare\n    the folder for further analysis.\n\n    Returns\n    -------\n    bool\n        Returns True if pre-processing completed successfully; False otherwise.\n    \"\"\"\n    logging.info(\"Pre-processing has started\")\n    analysis_status = {\"continue\": True, \"message\": \"\"}\n\n    self.parent().po.get_first_image()\n    self.parent().po.fast_first_image_segmentation()\n    if len(self.parent().po.vars['analyzed_individuals']) != self.parent().po.first_image.shape_number:\n        self.message_from_thread_starting.emit(f\"Wrong specimen number: (re)do the complete analysis.\")\n        analysis_status[\"continue\"] = False\n    else:\n        self.parent().po.cropping(is_first_image=True)\n        self.parent().po.get_average_pixel_size()\n        analysis_status = self.parent().po.delineate_each_arena()\n        if not analysis_status[\"continue\"]:\n            self.message_from_thread_starting.emit(analysis_status[\"message\"])\n            logging.error(analysis_status['message'])\n        else:\n            self.parent().po.data_to_save['exif'] = True\n            self.parent().po.save_data_to_run_cellects_quickly()\n            self.parent().po.data_to_save['exif'] = False\n            self.parent().po.get_background_to_subtract()\n            if len(self.parent().po.vars['analyzed_individuals']) != len(self.parent().po.top):\n                self.message_from_thread_starting.emit(f\"Wrong specimen number: (re)do the complete analysis.\")\n                analysis_status[\"continue\"] = False\n            else:\n                self.parent().po.get_origins_and_backgrounds_lists()\n                self.parent().po.get_last_image()\n                self.parent().po.fast_last_image_segmentation()\n                self.parent().po.find_if_lighter_backgnp.round()\n                logging.info(\"The current (or the first) folder is ready to run\")\n                self.parent().po.first_exp_ready_to_run = True\n    return analysis_status[\"continue\"]\n</code></pre>"},{"location":"api/cellects/core/cellects_threads/#cellects.core.cellects_threads.OneArenaThread.run","title":"<code>run()</code>","text":"<p>Run analysis on one arena.</p> <p>This method prepares and initiates the analysis process for a video by setting up required folders, loading necessary data, and performing pre-processing steps. It manages the state of running analysis and handles memory allocation for efficient processing.</p> Notes <ul> <li>This method uses threading to handle long-running operations without blocking the main UI.</li> <li>The memory allocation is dynamically adjusted based on available system resources.</li> </ul> <p>Attributes:</p> Name Type Description <code>self.parent().po.vars['convert_for_motion']</code> <code>dict</code> <p>Dictionary containing variables related to motion conversion.</p> <code>self.parent().po.first_exp_ready_to_run</code> <code>bool</code> <p>Boolean indicating if the first experiment is ready to run.</p> <code>self.parent().po.cores</code> <code>int</code> <p>Number of cores available for processing.</p> <code>self.parent().po.motion</code> <code>object</code> <p>Object containing motion-related data and methods.</p> <code>self.parent().po.load_quick_full</code> <code>int</code> <p>Number of arenas to load quickly for full detection.</p> Source code in <code>src/cellects/core/cellects_threads.py</code> <pre><code>def run(self):\n    \"\"\"\n\n    Run analysis on one arena.\n\n    This method prepares and initiates the analysis process for a video by setting up required folders,\n    loading necessary data, and performing pre-processing steps. It manages the state of running analysis and\n    handles memory allocation for efficient processing.\n\n    Notes\n    -----\n    - This method uses threading to handle long-running operations without blocking the main UI.\n    - The memory allocation is dynamically adjusted based on available system resources.\n\n    Attributes\n    ----------\n    self.parent().po.vars['convert_for_motion'] : dict\n        Dictionary containing variables related to motion conversion.\n    self.parent().po.first_exp_ready_to_run : bool\n        Boolean indicating if the first experiment is ready to run.\n    self.parent().po.cores : int\n        Number of cores available for processing.\n    self.parent().po.motion : object\n        Object containing motion-related data and methods.\n    self.parent().po.load_quick_full : int\n        Number of arenas to load quickly for full detection.\n    \"\"\"\n    continue_analysis = True\n    self._isRunning = True\n    self.message_from_thread_starting.emit(\"Video loading, wait...\")\n\n    self.set_current_folder()\n    if not self.parent().po.first_exp_ready_to_run:\n        self.parent().po.load_data_to_run_cellects_quickly()\n        if not self.parent().po.first_exp_ready_to_run:\n            #Need a look for data when Data to run Cellects quickly.pkl and 1 folder selected amon several\n            continue_analysis = self.pre_processing()\n    if continue_analysis:\n        memory_diff = self.parent().po.update_available_core_nb()\n        if self.parent().po.cores == 0:\n            self.message_from_thread_starting.emit(f\"Analyzing one arena requires {memory_diff}GB of additional RAM to run\")\n        else:\n            if self.parent().po.motion is None or self.parent().po.load_quick_full == 0:\n                self.load_one_arena()\n            if self.parent().po.load_quick_full &gt; 0:\n                if self.parent().po.motion.start is not None:\n                    logging.info(\"One arena detection has started\")\n                    self.detection()\n                    if self.parent().po.load_quick_full &gt; 1:\n                        logging.info(\"One arena post-processing has started\")\n                        self.post_processing()\n                    else:\n                        self.when_detection_finished.emit(\"Detection done, read to see the result\")\n                else:\n                    self.message_from_thread_starting.emit(f\"The current parameters failed to detect the cell(s) motion\")\n</code></pre>"},{"location":"api/cellects/core/cellects_threads/#cellects.core.cellects_threads.OneArenaThread.set_current_folder","title":"<code>set_current_folder()</code>","text":"<p>Sets the current folder based on conditions.</p> <p>This method determines which folder to use and updates the current folder ID accordingly. If there are multiple folders, it uses the first folder from the list; otherwise, it uses a reduced global pathway as the current.</p> Source code in <code>src/cellects/core/cellects_threads.py</code> <pre><code>def set_current_folder(self):\n    \"\"\"\n\n    Sets the current folder based on conditions.\n\n    This method determines which folder to use and updates the current\n    folder ID accordingly. If there are multiple folders, it uses the first folder\n    from the list; otherwise, it uses a reduced global pathway as the current.\n    \"\"\"\n    if self.parent().po.all['folder_number'] &gt; 1:\n        logging.info(f\"Use {self.parent().po.all['folder_list'][0]} folder\")\n        self.parent().po.update_folder_id(self.parent().po.all['sample_number_per_folder'][0],\n                                          self.parent().po.all['folder_list'][0])\n    else:\n        curr_path = reduce_path_len(self.parent().po.all['global_pathway'], 6, 10)\n        logging.info(f\"Use {curr_path} folder\")\n        self.parent().po.update_folder_id(self.parent().po.all['first_folder_sample_number'])\n</code></pre>"},{"location":"api/cellects/core/cellects_threads/#cellects.core.cellects_threads.OneArenaThread.stop","title":"<code>stop()</code>","text":"<p>Stops the running process.</p> <p>This method is used to safely halt the current process.</p> Source code in <code>src/cellects/core/cellects_threads.py</code> <pre><code>def stop(self):\n    \"\"\"\n    Stops the running process.\n\n    This method is used to safely halt the current process.\n    \"\"\"\n    self._isRunning = False\n</code></pre>"},{"location":"api/cellects/core/cellects_threads/#cellects.core.cellects_threads.PrepareVideoAnalysisThread","title":"<code>PrepareVideoAnalysisThread</code>","text":"<p>               Bases: <code>QThread</code></p> <p>Thread for preparing video analysis.</p> Notes <p>This class uses <code>QThread</code> to manage the process asynchronously.</p> Source code in <code>src/cellects/core/cellects_threads.py</code> <pre><code>class PrepareVideoAnalysisThread(QtCore.QThread):\n    \"\"\"\n    Thread for preparing video analysis.\n\n    Notes\n    -----\n    This class uses `QThread` to manage the process asynchronously.\n    \"\"\"\n\n    def __init__(self, parent=None):\n        \"\"\"\n        Initialize the worker thread for ending up the last image analysis and preparing video analysis.\n\n        Parameters\n        ----------\n        parent : QObject, optional\n            The parent object of this thread instance. In use, an instance of CellectsMainWidget class. Default is None.\n        \"\"\"\n        super(PrepareVideoAnalysisThread, self).__init__(parent)\n        self.setParent(parent)\n\n    def run(self):\n        \"\"\"\n        Run the image processing pipeline for the last image of the current folder.\n\n        This method handles background subtraction,\n        image segmentation, and data saving.\n        \"\"\"\n        self.parent().po.get_background_to_subtract()\n\n        self.parent().po.get_origins_and_backgrounds_lists()\n\n        if self.parent().po.last_image is None:\n            self.parent().po.get_last_image()\n            self.parent().po.fast_last_image_segmentation()\n        self.parent().po.find_if_lighter_background()\n        logging.info(\"The current (or the first) folder is ready to run\")\n        self.parent().po.first_exp_ready_to_run = True\n        self.parent().po.data_to_save['exif'] = True\n        self.parent().po.save_data_to_run_cellects_quickly()\n        self.parent().po.data_to_save['exif'] = False\n</code></pre>"},{"location":"api/cellects/core/cellects_threads/#cellects.core.cellects_threads.PrepareVideoAnalysisThread.__init__","title":"<code>__init__(parent=None)</code>","text":"<p>Initialize the worker thread for ending up the last image analysis and preparing video analysis.</p> <p>Parameters:</p> Name Type Description Default <code>parent</code> <code>QObject</code> <p>The parent object of this thread instance. In use, an instance of CellectsMainWidget class. Default is None.</p> <code>None</code> Source code in <code>src/cellects/core/cellects_threads.py</code> <pre><code>def __init__(self, parent=None):\n    \"\"\"\n    Initialize the worker thread for ending up the last image analysis and preparing video analysis.\n\n    Parameters\n    ----------\n    parent : QObject, optional\n        The parent object of this thread instance. In use, an instance of CellectsMainWidget class. Default is None.\n    \"\"\"\n    super(PrepareVideoAnalysisThread, self).__init__(parent)\n    self.setParent(parent)\n</code></pre>"},{"location":"api/cellects/core/cellects_threads/#cellects.core.cellects_threads.PrepareVideoAnalysisThread.run","title":"<code>run()</code>","text":"<p>Run the image processing pipeline for the last image of the current folder.</p> <p>This method handles background subtraction, image segmentation, and data saving.</p> Source code in <code>src/cellects/core/cellects_threads.py</code> <pre><code>def run(self):\n    \"\"\"\n    Run the image processing pipeline for the last image of the current folder.\n\n    This method handles background subtraction,\n    image segmentation, and data saving.\n    \"\"\"\n    self.parent().po.get_background_to_subtract()\n\n    self.parent().po.get_origins_and_backgrounds_lists()\n\n    if self.parent().po.last_image is None:\n        self.parent().po.get_last_image()\n        self.parent().po.fast_last_image_segmentation()\n    self.parent().po.find_if_lighter_background()\n    logging.info(\"The current (or the first) folder is ready to run\")\n    self.parent().po.first_exp_ready_to_run = True\n    self.parent().po.data_to_save['exif'] = True\n    self.parent().po.save_data_to_run_cellects_quickly()\n    self.parent().po.data_to_save['exif'] = False\n</code></pre>"},{"location":"api/cellects/core/cellects_threads/#cellects.core.cellects_threads.RunAllThread","title":"<code>RunAllThread</code>","text":"<p>               Bases: <code>QThread</code></p> <p>Thread for running the analysis on all arenas of the current folder.</p> Signals <p>image_from_thread : Signal(str)     Signal emitted to send information to the user through the GUI message_from_thread : Signal(str)     Signal emitted to send images showing the current status of the analysis to the GUI.</p> Notes <p>This class uses <code>QThread</code> to manage the process asynchronously.</p> Source code in <code>src/cellects/core/cellects_threads.py</code> <pre><code>class RunAllThread(QtCore.QThread):\n    \"\"\"\n    Thread for running the analysis on all arenas of the current folder.\n\n    Signals\n    --------\n    image_from_thread : Signal(str)\n        Signal emitted to send information to the user through the GUI\n    message_from_thread : Signal(str)\n        Signal emitted to send images showing the current status of the analysis to the GUI.\n\n    Notes\n    -----\n    This class uses `QThread` to manage the process asynchronously.\n    \"\"\"\n    message_from_thread = QtCore.Signal(str)\n    image_from_thread = QtCore.Signal(dict)\n\n    def __init__(self, parent=None):\n        \"\"\"\n        Initialize the worker thread for running a complete analysis on one folder or a folder containing several\n        folders.\n\n        Parameters\n        ----------\n        parent : QObject, optional\n            The parent object of this thread instance. In use, an instance of CellectsMainWidget class. Default is None.\n        \"\"\"\n        super(RunAllThread, self).__init__(parent)\n        self.setParent(parent)\n\n    def run(self):\n        \"\"\"\n        Run the analysis process for video writing and motion analysis.\n\n        This method manages the overall flow of the analysis including setting up\n        folders, loading data, writing videos from images, and performing motion\n        analysis. It handles various conditions like checking if the specimen number\n        matches expectations or if multiple experiments are ready to run.\n\n        Returns\n        -------\n        dict\n            A dictionary containing:\n            - 'continue': bool indicating if the analysis should continue.\n            - 'message': str with a relevant message about the current status.\n        Notes\n        -----\n        This method uses several internal methods like `set_current_folder`,\n        `run_video_writing`, and `run_motion_analysis` to perform the analysis steps.\n        It also checks various conditions based on parent object attributes.\n        \"\"\"\n        analysis_status = {\"continue\": True, \"message\": \"\"}\n        message = self.set_current_folder(0)\n\n        if self.parent().po.first_exp_ready_to_run:\n            self.message_from_thread.emit(message + \": Write videos...\")\n            if not self.parent().po.vars['several_blob_per_arena'] and self.parent().po.sample_number != len(self.parent().po.bot):\n                analysis_status[\"continue\"] = False\n                analysis_status[\"message\"] = f\"Wrong specimen number: redo the first image analysis.\"\n                self.message_from_thread.emit(f\"Wrong specimen number: restart Cellects and do another analysis.\")\n            else:\n                analysis_status = self.run_video_writing(message)\n                if analysis_status[\"continue\"]:\n                    self.message_from_thread.emit(message + \": Analyse all videos...\")\n                    analysis_status = self.run_motion_analysis(message)\n                if analysis_status[\"continue\"]:\n                    if self.parent().po.all['folder_number'] &gt; 1:\n                        self.parent().po.all['folder_list'] = self.parent().po.all['folder_list'][1:]\n                        self.parent().po.all['sample_number_per_folder'] = self.parent().po.all['sample_number_per_folder'][1:]\n        else:\n            self.parent().po.look_for_data()\n\n        if analysis_status[\"continue\"] and (not self.parent().po.first_exp_ready_to_run or self.parent().po.all['folder_number'] &gt; 1):\n            folder_number = np.max((len(self.parent().po.all['folder_list']), 1))\n\n            for exp_i in np.arange(folder_number):\n                if len(self.parent().po.all['folder_list']) &gt; 0:\n                    logging.info(self.parent().po.all['folder_list'][exp_i])\n                self.parent().po.first_im = None\n                self.parent().po.first_image = None\n                self.parent().po.last_im = None\n                self.parent().po.last_image = None\n                self.parent().po.top = None\n\n                message = self.set_current_folder(exp_i)\n                self.message_from_thread.emit(f'{message}, pre-processing...')\n                self.parent().po.load_data_to_run_cellects_quickly()\n                if not self.parent().po.first_exp_ready_to_run:\n                    analysis_status = self.pre_processing()\n                if analysis_status[\"continue\"]:\n                    self.message_from_thread.emit(message + \": Write videos from images before analysis...\")\n                    if not self.parent().po.vars['several_blob_per_arena'] and self.parent().po.sample_number != len(self.parent().po.bot):\n                        self.message_from_thread.emit(f\"Wrong specimen number: first image analysis is mandatory.\")\n                        analysis_status[\"continue\"] = False\n                        analysis_status[\"message\"] = f\"Wrong specimen number: first image analysis is mandatory.\"\n                    else:\n                        analysis_status = self.run_video_writing(message)\n                        if analysis_status[\"continue\"]:\n                            self.message_from_thread.emit(message + \": Starting analysis...\")\n                            analysis_status = self.run_motion_analysis(message)\n\n                if not analysis_status[\"continue\"]:\n                    break\n                print(self.parent().po.vars['convert_for_motion'])\n        if analysis_status[\"continue\"]:\n            if self.parent().po.all['folder_number'] &gt; 1:\n                self.message_from_thread.emit(f\"Exp {self.parent().po.all['folder_list'][0]} to {self.parent().po.all['folder_list'][-1]} analyzed.\")\n            else:\n                curr_path = reduce_path_len(self.parent().po.all['global_pathway'], 6, 10)\n                self.message_from_thread.emit(f'Exp {curr_path}, analyzed.')\n        else:\n            logging.error(message + \" \" + analysis_status[\"message\"])\n            self.message_from_thread.emit(message + \" \" + analysis_status[\"message\"])\n\n    def set_current_folder(self, exp_i):\n        if self.parent().po.all['folder_number'] &gt; 1:\n            logging.info(f\"Use {self.parent().po.all['folder_list'][exp_i]} folder\")\n\n            message = f\"{str(self.parent().po.all['global_pathway'])[:6]} ... {self.parent().po.all['folder_list'][exp_i]}\"\n            self.parent().po.update_folder_id(self.parent().po.all['sample_number_per_folder'][exp_i],\n                                              self.parent().po.all['folder_list'][exp_i])\n        else:\n            message = reduce_path_len(self.parent().po.all['global_pathway'], 6, 10)\n            logging.info(f\"Use {message} folder\")\n            self.parent().po.update_folder_id(self.parent().po.all['first_folder_sample_number'])\n        return message\n\n    def pre_processing(self) -&gt; dict:\n        \"\"\"\n        Pre-processes the video data for further analysis.\n\n        Extended Description\n        ---------------------\n        This method performs several preprocessing steps on the video data, including image segmentation,\n        cropping, background subtraction, and origin detection. It also handles errors related to image analysis\n        and manual delineation.\n\n        Returns\n        -------\n        dict\n            A dictionary containing `continue` (bool) and `message` (str). If analysis can continue, `continue`\n            is True; otherwise, it's False and a descriptive message is provided.\n\n        Raises\n        ------\n        **ValueError**\n            When the correct number of cells cannot be detected in the first image.\n\n        Notes\n        -----\n        * The method logs important preprocessing steps using `logging.info`.\n        * Assumes that parent object (`self.parent().po`) has methods and attributes required for preprocessing.\n        \"\"\"\n        analysis_status = {\"continue\": True, \"message\": \"\"}\n        logging.info(\"Pre-processing has started\")\n        if len(self.parent().po.data_list) &gt; 0:\n            self.parent().po.get_first_image()\n            self.parent().po.fast_first_image_segmentation()\n            self.parent().po.cropping(is_first_image=True)\n            self.parent().po.get_average_pixel_size()\n            try:\n                analysis_status = self.parent().po.delineate_each_arena()\n            except ValueError:\n                analysis_status[\n                    \"message\"] = f\"Failed to detect the right cell(s) number: the first image analysis is mandatory.\"\n                analysis_status[\"continue\"] = False\n\n            if analysis_status[\"continue\"]:\n                self.parent().po.data_to_save['exif'] = True\n                self.parent().po.save_data_to_run_cellects_quickly()\n                self.parent().po.data_to_save['exif'] = False\n                # self.parent().po.extract_exif()\n                self.parent().po.get_background_to_subtract()\n                if len(self.parent().po.vars['analyzed_individuals']) != len(self.parent().po.top):\n                    analysis_status[\"message\"] = f\"Failed to detect the right cell(s) number: the first image analysis is mandatory.\"\n                    analysis_status[\"continue\"] = False\n                elif self.parent().po.top is None and self.parent().imageanalysiswindow.manual_delineation_flag:\n                    analysis_status[\"message\"] = f\"Auto video delineation failed, use manual delineation tool\"\n                    analysis_status[\"continue\"] = False\n                else:\n                    self.parent().po.get_origins_and_backgrounds_lists()\n                    self.parent().po.get_last_image()\n                    self.parent().po.fast_last_image_segmentation()\n                    self.parent().po.find_if_lighter_backgnp.round()\n            return analysis_status\n        else:\n            analysis_status[\"message\"] = f\"Wrong folder or parameters\"\n            analysis_status[\"continue\"] = False\n            return analysis_status\n\n    def run_video_writing(self, message: str) -&gt; dict:\n        \"\"\"\n        Initiate the process of writing videos from image data.\n\n        Parameters\n        ----------\n        message : str\n            A string to emit as a status update during video writing.\n\n        Returns\n        -------\n        dict\n            A dictionary containing the analysis status with keys:\n            - \"continue\": bool indicating whether to continue video writing\n            - \"message\": str providing a status or error message\n\n        Raises\n        ------\n        FileNotFoundError\n            If an image file specified in `data_list` does not exist.\n        OSError\n            If there is an issue writing to disk, such as when the disk is full.\n\n        Notes\n        -----\n        This function manages video writing in batches, checking available memory\n        and handling errors related to file sizes or missing images\n        \"\"\"\n        analysis_status = {\"continue\": True, \"message\": \"\"}\n        look_for_existing_videos = glob('ind_' + '*' + '.npy')\n        there_already_are_videos = len(look_for_existing_videos) == len(self.parent().po.vars['analyzed_individuals'])\n        logging.info(f\"{len(look_for_existing_videos)} .npy video files found for {len(self.parent().po.vars['analyzed_individuals'])} arenas to analyze\")\n        do_write_videos = not self.parent().po.all['im_or_vid'] and (not there_already_are_videos or (there_already_are_videos and self.parent().po.all['overwrite_unaltered_videos']))\n        if do_write_videos:\n            logging.info(f\"Starting video writing\")\n            # self.videos.write_videos_as_np_arrays(self.data_list, self.vars['convert_for_motion'], in_colors=self.vars['save_in_colors'])\n            in_colors = not self.parent().po.vars['already_greyscale']\n            self.parent().po.first_image.shape_number = self.parent().po.sample_number\n            bunch_nb, video_nb_per_bunch, sizes, video_bunch, vid_names, rom_memory_required, analysis_status, remaining, use_list_of_vid, is_landscape = self.parent().po.prepare_video_writing(\n                self.parent().po.data_list, self.parent().po.vars['min_ram_free'], in_colors)\n            if analysis_status[\"continue\"]:\n                # Check that there is enough available RAM for one video par bunch and ROM for all videos\n                if video_nb_per_bunch &gt; 0 and rom_memory_required is None:\n                    pat_tracker1 = PercentAndTimeTracker(bunch_nb * self.parent().po.vars['img_number'])\n                    pat_tracker2 = PercentAndTimeTracker(len(self.parent().po.vars['analyzed_individuals']))\n                    arena_percentage = 0\n                    for bunch in np.arange(bunch_nb):\n                        # Update the labels of arenas and the video_bunch to write\n                        if bunch == (bunch_nb - 1) and remaining &gt; 0:\n                            arena = np.arange(bunch * video_nb_per_bunch, bunch * video_nb_per_bunch + remaining)\n                        else:\n                            arena = np.arange(bunch * video_nb_per_bunch, (bunch + 1) * video_nb_per_bunch)\n                        if use_list_of_vid:\n                            video_bunch = [np.zeros(sizes[i, :], dtype=np.uint8) for i in arena]\n                        else:\n                            video_bunch = np.zeros(np.append(sizes[0, :], len(arena)), dtype=np.uint8)\n                        prev_img = None\n                        images_done = bunch * self.parent().po.vars['img_number']\n                        for image_i, image_name in enumerate(self.parent().po.data_list):\n                            image_percentage, remaining_time = pat_tracker1.get_progress(image_i + images_done)\n                            self.message_from_thread.emit(message + f\" Step 1/2: Video writing ({np.round((image_percentage + arena_percentage) / 2, 2)}%)\")\n                            if not os.path.exists(image_name):\n                                raise FileNotFoundError(image_name)\n                            img = read_and_rotate(image_name, prev_img, self.parent().po.all['raw_images'], is_landscape, self.parent().po.first_image.crop_coord)\n                            prev_img = deepcopy(img)\n                            if self.parent().po.vars['already_greyscale'] and self.parent().po.reduce_image_dim:\n                                img = img[:, :, 0]\n\n                            for arena_i, arena_name in enumerate(arena):\n                                try:\n                                    sub_img = img[self.parent().po.top[arena_name]: self.parent().po.bot[arena_name],\n                                              self.parent().po.left[arena_name]: self.parent().po.right[arena_name], ...]\n                                    if use_list_of_vid:\n                                        video_bunch[arena_i][image_i, ...] = sub_img\n                                    else:\n                                        if len(video_bunch.shape) == 5:\n                                            video_bunch[image_i, :, :, :, arena_i] = sub_img\n                                        else:\n                                            video_bunch[image_i, :, :, arena_i] = sub_img\n                                except ValueError:\n                                    analysis_status[\"message\"] = f\"Some images have incorrect size, reset all settings in advanced parameters\"\n                                    analysis_status[\"continue\"] = False\n                                    logging.info(f\"Reset all settings in advanced parameters\")\n                                    break\n                            if not analysis_status[\"continue\"]:\n                                break\n                        if not analysis_status[\"continue\"]:\n                            break\n                        if analysis_status[\"continue\"]:\n                            for arena_i, arena_name in enumerate(arena):\n                                try:\n                                    arena_percentage, eta = pat_tracker2.get_progress()\n                                    self.message_from_thread.emit(message + f\" Step 1/2: Video writing ({np.round((image_percentage + arena_percentage) / 2, 2)}%)\")# , ETA {remaining_time}\n                                    if use_list_of_vid:\n                                        np.save(vid_names[arena_name], video_bunch[arena_i])\n                                    else:\n                                        if len(video_bunch.shape) == 5:\n                                            np.save(vid_names[arena_name], video_bunch[:, :, :, :, arena_i])\n                                        else:\n                                            np.save(vid_names[arena_name], video_bunch[:, :, :, arena_i])\n                                except OSError:\n                                    self.message_from_thread.emit(message + f\"full disk memory, clear space and retry\")\n                        logging.info(f\"Bunch n\u00b0{bunch + 1} over {bunch_nb} saved.\")\n                    logging.info(\"When they exist, do not overwrite unaltered video\")\n                    self.parent().po.all['overwrite_unaltered_videos'] = False\n                    self.parent().po.save_variable_dict()\n                    self.parent().po.save_data_to_run_cellects_quickly()\n                    if analysis_status[\"continue\"]:\n                        analysis_status[\"message\"] = f\"Video writing complete.\"\n                    return analysis_status\n                else:\n                    analysis_status[\"continue\"] = False\n                    if video_nb_per_bunch == 0:\n                        memory_diff = self.parent().po.update_available_core_nb()\n                        ram_message = f\"{memory_diff}GB of additional RAM\"\n                    if rom_memory_required is not None:\n                        rom_message = f\"at least {rom_memory_required}GB of free ROM\"\n\n                    if video_nb_per_bunch == 0 and rom_memory_required is not None:\n                        analysis_status[\"message\"] = f\"Requires {ram_message} and {rom_message} to run\"\n                        # self.message_from_thread.emit(f\"Analyzing {message} requires {ram_message} and {rom_message} to run\")\n                    elif video_nb_per_bunch == 0:\n                        analysis_status[\"message\"] = f\"Requires {ram_message} to run\"\n                        # self.message_from_thread.emit(f\"Analyzing {message} requires {ram_message} to run\")\n                    elif rom_memory_required is not None:\n                        analysis_status[\"message\"] = f\"Requires {rom_message} to run\"\n                        # self.message_from_thread.emit(f\"Analyzing {message} requires {rom_message} to run\")\n                    logging.info(f\"Cellects is not writing videos: insufficient memory\")\n                    return analysis_status\n            else:\n                return analysis_status\n\n\n        else:\n            logging.info(f\"Cellects is not writing videos: unnecessary\")\n            analysis_status[\"message\"] = f\"Cellects is not writing videos: unnecessary\"\n            return analysis_status\n\n    def run_motion_analysis(self, message: str) -&gt; dict:\n        \"\"\"\n        Run motion analysis on analyzed individuals with optional multiprocessing.\n\n        This method processes video frames to analyze motion attributes of individuals.\n        It can operate in either sequential or parallel mode based on available system\n        resources and configuration settings. Analysis results are saved in multiple\n        output formats.\n\n        Parameters\n        ----------\n        message : str\n            A status message to be displayed during the analysis process.\n\n        Returns\n        -------\n        dict\n            A dictionary containing the status of the motion analysis.\n\n        Raises\n        ------\n        MemoryError\n            If there is insufficient memory to perform the analysis in parallel.\n\n        Notes\n        -----\n        Sequential mode is used when multiprocessing is disabled or only one core\n        is available. Parallel mode utilizes multiple CPU cores for faster processing.\n        \"\"\"\n        analysis_status = {\"continue\": True, \"message\": \"\"}\n        logging.info(f\"Starting motion analysis with the detection method n\u00b0{self.parent().po.all['video_option']}\")\n        self.parent().po.instantiate_tables()\n        try:\n            memory_diff = self.parent().po.update_available_core_nb()\n            if self.parent().po.cores &gt; 0: # i.e. enough memory\n                if not self.parent().po.all['do_multiprocessing'] or self.parent().po.cores == 1:\n                    self.message_from_thread.emit(f\"{message} Step 2/2: Video analysis\")\n                    logging.info(\"fStarting sequential analysis\")\n                    tiii = default_timer()\n                    pat_tracker = PercentAndTimeTracker(len(self.parent().po.vars['analyzed_individuals']))\n                    for i, arena in enumerate(self.parent().po.vars['analyzed_individuals']):\n\n                        l = [i, arena, self.parent().po.vars, True, True, False, None]\n                        analysis_i = MotionAnalysis(l)\n                        r = weakref.ref(analysis_i)\n                        if not self.parent().po.vars['several_blob_per_arena']:\n                            # Save basic statistics\n                            self.parent().po.update_one_row_per_arena(i, analysis_i.one_descriptor_per_arena)\n\n\n                            # Save descriptors in long_format\n                            self.parent().po.update_one_row_per_frame(i * self.parent().po.vars['img_number'], arena * self.parent().po.vars['img_number'], analysis_i.one_row_per_frame)\n\n                        # Save efficiency visualization\n                        self.parent().po.add_analysis_visualization_to_first_and_last_images(i, analysis_i.efficiency_test_1,\n                                                                                 analysis_i.efficiency_test_2)\n                        # Emit message to the interface\n                        current_percentage, eta = pat_tracker.get_progress()\n                        self.image_from_thread.emit({\"current_image\": self.parent().po.last_image.bgr,\n                                                     \"message\": f\"{message} Step 2/2: analyzed {arena} out of {len(self.parent().po.vars['analyzed_individuals'])} arenas ({current_percentage}%){eta}\"})\n                        del analysis_i\n                    logging.info(f\"Sequential analysis lasted {(default_timer() - tiii)/ 60} minutes\")\n                else:\n                    self.message_from_thread.emit(\n                        f\"{message}, Step 2/2:  Analyse all videos using {self.parent().po.cores} cores...\")\n\n                    logging.info(\"fStarting analysis in parallel\")\n\n                    tiii = default_timer()\n                    arena_number = len(self.parent().po.vars['analyzed_individuals'])\n                    self.advance = 0\n                    self.pat_tracker = PercentAndTimeTracker(len(self.parent().po.vars['analyzed_individuals']),\n                                                        core_number=self.parent().po.cores)\n\n                    fair_core_workload = arena_number // self.parent().po.cores\n                    cores_with_1_more = arena_number % self.parent().po.cores\n                    EXTENTS_OF_SUBRANGES = []\n                    bound = 0\n                    parallel_organization = [fair_core_workload + 1 for _ in range(cores_with_1_more)] + [fair_core_workload for _ in range(self.parent().po.cores - cores_with_1_more)]\n                    # Emit message to the interface\n                    self.image_from_thread.emit({\"current_image\": self.parent().po.last_image.bgr,\n                                                 \"message\": f\"{message} Step 2/2: Analysis running on {self.parent().po.cores} CPU cores\"})\n                    for i, extent_size in enumerate(parallel_organization):\n                        EXTENTS_OF_SUBRANGES.append((bound, bound := bound + extent_size))\n\n                    try:\n                        PROCESSES = []\n                        subtotals = Manager().Queue()# Queue()\n                        for extent in EXTENTS_OF_SUBRANGES:\n                            # print(extent)\n                            p = Process(target=motion_analysis_process, args=(extent[0], extent[1], self.parent().po.vars, subtotals))\n                            p.start()\n                            PROCESSES.append(p)\n\n                        for p in PROCESSES:\n                            p.join()\n\n                        self.message_from_thread.emit(f\"{message}, Step 2/2:  Saving all results...\")\n                        for i in range(subtotals.qsize()):\n                            grouped_results = subtotals.get()\n                            for j, results_i in enumerate(grouped_results):\n                                if not self.parent().po.vars['several_blob_per_arena']:\n                                    # Save basic statistics\n                                    self.parent().po.update_one_row_per_arena(results_i['i'], results_i['one_row_per_arena'])\n                                    # Save descriptors in long_format\n                                    self.parent().po.update_one_row_per_frame(results_i['i'] * self.parent().po.vars['img_number'],\n                                                                              (results_i['i'] + 1) * self.parent().po.vars['img_number'],\n                                                                              results_i['one_row_per_frame'])\n\n                                self.parent().po.add_analysis_visualization_to_first_and_last_images(results_i['i'], results_i['efficiency_test_1'],\n                                                                                         results_i['efficiency_test_2'])\n                        self.image_from_thread.emit(\n                            {\"current_image\": self.parent().po.last_image.bgr,\n                             \"message\": f\"{message} Step 2/2: analyzed {len(self.parent().po.vars['analyzed_individuals'])} out of {len(self.parent().po.vars['analyzed_individuals'])} arenas ({100}%)\"})\n\n                        logging.info(f\"Parallel analysis lasted {(default_timer() - tiii)/ 60} minutes\")\n                    except MemoryError:\n                        analysis_status[\"continue\"] = False\n                        analysis_status[\"message\"] = f\"Not enough memory, reduce the core number for parallel analysis\"\n                        self.message_from_thread.emit(f\"Analyzing {message} requires to reduce the core number for parallel analysis\")\n                        return analysis_status\n                self.parent().po.save_tables()\n                return analysis_status\n            else:\n                analysis_status[\"continue\"] = False\n                analysis_status[\"message\"] = f\"Requires an additional {memory_diff}GB of RAM to run\"\n                self.message_from_thread.emit(f\"Analyzing {message} requires an additional {memory_diff}GB of RAM to run\")\n                return analysis_status\n        except MemoryError:\n            analysis_status[\"continue\"] = False\n            analysis_status[\"message\"] = f\"Requires additional memory to run\"\n            self.message_from_thread.emit(f\"Analyzing {message} requires additional memory to run\")\n            return analysis_status\n</code></pre>"},{"location":"api/cellects/core/cellects_threads/#cellects.core.cellects_threads.RunAllThread.__init__","title":"<code>__init__(parent=None)</code>","text":"<p>Initialize the worker thread for running a complete analysis on one folder or a folder containing several folders.</p> <p>Parameters:</p> Name Type Description Default <code>parent</code> <code>QObject</code> <p>The parent object of this thread instance. In use, an instance of CellectsMainWidget class. Default is None.</p> <code>None</code> Source code in <code>src/cellects/core/cellects_threads.py</code> <pre><code>def __init__(self, parent=None):\n    \"\"\"\n    Initialize the worker thread for running a complete analysis on one folder or a folder containing several\n    folders.\n\n    Parameters\n    ----------\n    parent : QObject, optional\n        The parent object of this thread instance. In use, an instance of CellectsMainWidget class. Default is None.\n    \"\"\"\n    super(RunAllThread, self).__init__(parent)\n    self.setParent(parent)\n</code></pre>"},{"location":"api/cellects/core/cellects_threads/#cellects.core.cellects_threads.RunAllThread.pre_processing","title":"<code>pre_processing()</code>","text":"<p>Pre-processes the video data for further analysis.</p> Extended Description <p>This method performs several preprocessing steps on the video data, including image segmentation, cropping, background subtraction, and origin detection. It also handles errors related to image analysis and manual delineation.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing <code>continue</code> (bool) and <code>message</code> (str). If analysis can continue, <code>continue</code> is True; otherwise, it's False and a descriptive message is provided.</p> <p>Raises:</p> Type Description <code>**ValueError**</code> <p>When the correct number of cells cannot be detected in the first image.</p> Notes <ul> <li>The method logs important preprocessing steps using <code>logging.info</code>.</li> <li>Assumes that parent object (<code>self.parent().po</code>) has methods and attributes required for preprocessing.</li> </ul> Source code in <code>src/cellects/core/cellects_threads.py</code> <pre><code>def pre_processing(self) -&gt; dict:\n    \"\"\"\n    Pre-processes the video data for further analysis.\n\n    Extended Description\n    ---------------------\n    This method performs several preprocessing steps on the video data, including image segmentation,\n    cropping, background subtraction, and origin detection. It also handles errors related to image analysis\n    and manual delineation.\n\n    Returns\n    -------\n    dict\n        A dictionary containing `continue` (bool) and `message` (str). If analysis can continue, `continue`\n        is True; otherwise, it's False and a descriptive message is provided.\n\n    Raises\n    ------\n    **ValueError**\n        When the correct number of cells cannot be detected in the first image.\n\n    Notes\n    -----\n    * The method logs important preprocessing steps using `logging.info`.\n    * Assumes that parent object (`self.parent().po`) has methods and attributes required for preprocessing.\n    \"\"\"\n    analysis_status = {\"continue\": True, \"message\": \"\"}\n    logging.info(\"Pre-processing has started\")\n    if len(self.parent().po.data_list) &gt; 0:\n        self.parent().po.get_first_image()\n        self.parent().po.fast_first_image_segmentation()\n        self.parent().po.cropping(is_first_image=True)\n        self.parent().po.get_average_pixel_size()\n        try:\n            analysis_status = self.parent().po.delineate_each_arena()\n        except ValueError:\n            analysis_status[\n                \"message\"] = f\"Failed to detect the right cell(s) number: the first image analysis is mandatory.\"\n            analysis_status[\"continue\"] = False\n\n        if analysis_status[\"continue\"]:\n            self.parent().po.data_to_save['exif'] = True\n            self.parent().po.save_data_to_run_cellects_quickly()\n            self.parent().po.data_to_save['exif'] = False\n            # self.parent().po.extract_exif()\n            self.parent().po.get_background_to_subtract()\n            if len(self.parent().po.vars['analyzed_individuals']) != len(self.parent().po.top):\n                analysis_status[\"message\"] = f\"Failed to detect the right cell(s) number: the first image analysis is mandatory.\"\n                analysis_status[\"continue\"] = False\n            elif self.parent().po.top is None and self.parent().imageanalysiswindow.manual_delineation_flag:\n                analysis_status[\"message\"] = f\"Auto video delineation failed, use manual delineation tool\"\n                analysis_status[\"continue\"] = False\n            else:\n                self.parent().po.get_origins_and_backgrounds_lists()\n                self.parent().po.get_last_image()\n                self.parent().po.fast_last_image_segmentation()\n                self.parent().po.find_if_lighter_backgnp.round()\n        return analysis_status\n    else:\n        analysis_status[\"message\"] = f\"Wrong folder or parameters\"\n        analysis_status[\"continue\"] = False\n        return analysis_status\n</code></pre>"},{"location":"api/cellects/core/cellects_threads/#cellects.core.cellects_threads.RunAllThread.run","title":"<code>run()</code>","text":"<p>Run the analysis process for video writing and motion analysis.</p> <p>This method manages the overall flow of the analysis including setting up folders, loading data, writing videos from images, and performing motion analysis. It handles various conditions like checking if the specimen number matches expectations or if multiple experiments are ready to run.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing: - 'continue': bool indicating if the analysis should continue. - 'message': str with a relevant message about the current status.</p> Notes <p>This method uses several internal methods like <code>set_current_folder</code>, <code>run_video_writing</code>, and <code>run_motion_analysis</code> to perform the analysis steps. It also checks various conditions based on parent object attributes.</p> Source code in <code>src/cellects/core/cellects_threads.py</code> <pre><code>def run(self):\n    \"\"\"\n    Run the analysis process for video writing and motion analysis.\n\n    This method manages the overall flow of the analysis including setting up\n    folders, loading data, writing videos from images, and performing motion\n    analysis. It handles various conditions like checking if the specimen number\n    matches expectations or if multiple experiments are ready to run.\n\n    Returns\n    -------\n    dict\n        A dictionary containing:\n        - 'continue': bool indicating if the analysis should continue.\n        - 'message': str with a relevant message about the current status.\n    Notes\n    -----\n    This method uses several internal methods like `set_current_folder`,\n    `run_video_writing`, and `run_motion_analysis` to perform the analysis steps.\n    It also checks various conditions based on parent object attributes.\n    \"\"\"\n    analysis_status = {\"continue\": True, \"message\": \"\"}\n    message = self.set_current_folder(0)\n\n    if self.parent().po.first_exp_ready_to_run:\n        self.message_from_thread.emit(message + \": Write videos...\")\n        if not self.parent().po.vars['several_blob_per_arena'] and self.parent().po.sample_number != len(self.parent().po.bot):\n            analysis_status[\"continue\"] = False\n            analysis_status[\"message\"] = f\"Wrong specimen number: redo the first image analysis.\"\n            self.message_from_thread.emit(f\"Wrong specimen number: restart Cellects and do another analysis.\")\n        else:\n            analysis_status = self.run_video_writing(message)\n            if analysis_status[\"continue\"]:\n                self.message_from_thread.emit(message + \": Analyse all videos...\")\n                analysis_status = self.run_motion_analysis(message)\n            if analysis_status[\"continue\"]:\n                if self.parent().po.all['folder_number'] &gt; 1:\n                    self.parent().po.all['folder_list'] = self.parent().po.all['folder_list'][1:]\n                    self.parent().po.all['sample_number_per_folder'] = self.parent().po.all['sample_number_per_folder'][1:]\n    else:\n        self.parent().po.look_for_data()\n\n    if analysis_status[\"continue\"] and (not self.parent().po.first_exp_ready_to_run or self.parent().po.all['folder_number'] &gt; 1):\n        folder_number = np.max((len(self.parent().po.all['folder_list']), 1))\n\n        for exp_i in np.arange(folder_number):\n            if len(self.parent().po.all['folder_list']) &gt; 0:\n                logging.info(self.parent().po.all['folder_list'][exp_i])\n            self.parent().po.first_im = None\n            self.parent().po.first_image = None\n            self.parent().po.last_im = None\n            self.parent().po.last_image = None\n            self.parent().po.top = None\n\n            message = self.set_current_folder(exp_i)\n            self.message_from_thread.emit(f'{message}, pre-processing...')\n            self.parent().po.load_data_to_run_cellects_quickly()\n            if not self.parent().po.first_exp_ready_to_run:\n                analysis_status = self.pre_processing()\n            if analysis_status[\"continue\"]:\n                self.message_from_thread.emit(message + \": Write videos from images before analysis...\")\n                if not self.parent().po.vars['several_blob_per_arena'] and self.parent().po.sample_number != len(self.parent().po.bot):\n                    self.message_from_thread.emit(f\"Wrong specimen number: first image analysis is mandatory.\")\n                    analysis_status[\"continue\"] = False\n                    analysis_status[\"message\"] = f\"Wrong specimen number: first image analysis is mandatory.\"\n                else:\n                    analysis_status = self.run_video_writing(message)\n                    if analysis_status[\"continue\"]:\n                        self.message_from_thread.emit(message + \": Starting analysis...\")\n                        analysis_status = self.run_motion_analysis(message)\n\n            if not analysis_status[\"continue\"]:\n                break\n            print(self.parent().po.vars['convert_for_motion'])\n    if analysis_status[\"continue\"]:\n        if self.parent().po.all['folder_number'] &gt; 1:\n            self.message_from_thread.emit(f\"Exp {self.parent().po.all['folder_list'][0]} to {self.parent().po.all['folder_list'][-1]} analyzed.\")\n        else:\n            curr_path = reduce_path_len(self.parent().po.all['global_pathway'], 6, 10)\n            self.message_from_thread.emit(f'Exp {curr_path}, analyzed.')\n    else:\n        logging.error(message + \" \" + analysis_status[\"message\"])\n        self.message_from_thread.emit(message + \" \" + analysis_status[\"message\"])\n</code></pre>"},{"location":"api/cellects/core/cellects_threads/#cellects.core.cellects_threads.RunAllThread.run_motion_analysis","title":"<code>run_motion_analysis(message)</code>","text":"<p>Run motion analysis on analyzed individuals with optional multiprocessing.</p> <p>This method processes video frames to analyze motion attributes of individuals. It can operate in either sequential or parallel mode based on available system resources and configuration settings. Analysis results are saved in multiple output formats.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>A status message to be displayed during the analysis process.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the status of the motion analysis.</p> <p>Raises:</p> Type Description <code>MemoryError</code> <p>If there is insufficient memory to perform the analysis in parallel.</p> Notes <p>Sequential mode is used when multiprocessing is disabled or only one core is available. Parallel mode utilizes multiple CPU cores for faster processing.</p> Source code in <code>src/cellects/core/cellects_threads.py</code> <pre><code>def run_motion_analysis(self, message: str) -&gt; dict:\n    \"\"\"\n    Run motion analysis on analyzed individuals with optional multiprocessing.\n\n    This method processes video frames to analyze motion attributes of individuals.\n    It can operate in either sequential or parallel mode based on available system\n    resources and configuration settings. Analysis results are saved in multiple\n    output formats.\n\n    Parameters\n    ----------\n    message : str\n        A status message to be displayed during the analysis process.\n\n    Returns\n    -------\n    dict\n        A dictionary containing the status of the motion analysis.\n\n    Raises\n    ------\n    MemoryError\n        If there is insufficient memory to perform the analysis in parallel.\n\n    Notes\n    -----\n    Sequential mode is used when multiprocessing is disabled or only one core\n    is available. Parallel mode utilizes multiple CPU cores for faster processing.\n    \"\"\"\n    analysis_status = {\"continue\": True, \"message\": \"\"}\n    logging.info(f\"Starting motion analysis with the detection method n\u00b0{self.parent().po.all['video_option']}\")\n    self.parent().po.instantiate_tables()\n    try:\n        memory_diff = self.parent().po.update_available_core_nb()\n        if self.parent().po.cores &gt; 0: # i.e. enough memory\n            if not self.parent().po.all['do_multiprocessing'] or self.parent().po.cores == 1:\n                self.message_from_thread.emit(f\"{message} Step 2/2: Video analysis\")\n                logging.info(\"fStarting sequential analysis\")\n                tiii = default_timer()\n                pat_tracker = PercentAndTimeTracker(len(self.parent().po.vars['analyzed_individuals']))\n                for i, arena in enumerate(self.parent().po.vars['analyzed_individuals']):\n\n                    l = [i, arena, self.parent().po.vars, True, True, False, None]\n                    analysis_i = MotionAnalysis(l)\n                    r = weakref.ref(analysis_i)\n                    if not self.parent().po.vars['several_blob_per_arena']:\n                        # Save basic statistics\n                        self.parent().po.update_one_row_per_arena(i, analysis_i.one_descriptor_per_arena)\n\n\n                        # Save descriptors in long_format\n                        self.parent().po.update_one_row_per_frame(i * self.parent().po.vars['img_number'], arena * self.parent().po.vars['img_number'], analysis_i.one_row_per_frame)\n\n                    # Save efficiency visualization\n                    self.parent().po.add_analysis_visualization_to_first_and_last_images(i, analysis_i.efficiency_test_1,\n                                                                             analysis_i.efficiency_test_2)\n                    # Emit message to the interface\n                    current_percentage, eta = pat_tracker.get_progress()\n                    self.image_from_thread.emit({\"current_image\": self.parent().po.last_image.bgr,\n                                                 \"message\": f\"{message} Step 2/2: analyzed {arena} out of {len(self.parent().po.vars['analyzed_individuals'])} arenas ({current_percentage}%){eta}\"})\n                    del analysis_i\n                logging.info(f\"Sequential analysis lasted {(default_timer() - tiii)/ 60} minutes\")\n            else:\n                self.message_from_thread.emit(\n                    f\"{message}, Step 2/2:  Analyse all videos using {self.parent().po.cores} cores...\")\n\n                logging.info(\"fStarting analysis in parallel\")\n\n                tiii = default_timer()\n                arena_number = len(self.parent().po.vars['analyzed_individuals'])\n                self.advance = 0\n                self.pat_tracker = PercentAndTimeTracker(len(self.parent().po.vars['analyzed_individuals']),\n                                                    core_number=self.parent().po.cores)\n\n                fair_core_workload = arena_number // self.parent().po.cores\n                cores_with_1_more = arena_number % self.parent().po.cores\n                EXTENTS_OF_SUBRANGES = []\n                bound = 0\n                parallel_organization = [fair_core_workload + 1 for _ in range(cores_with_1_more)] + [fair_core_workload for _ in range(self.parent().po.cores - cores_with_1_more)]\n                # Emit message to the interface\n                self.image_from_thread.emit({\"current_image\": self.parent().po.last_image.bgr,\n                                             \"message\": f\"{message} Step 2/2: Analysis running on {self.parent().po.cores} CPU cores\"})\n                for i, extent_size in enumerate(parallel_organization):\n                    EXTENTS_OF_SUBRANGES.append((bound, bound := bound + extent_size))\n\n                try:\n                    PROCESSES = []\n                    subtotals = Manager().Queue()# Queue()\n                    for extent in EXTENTS_OF_SUBRANGES:\n                        # print(extent)\n                        p = Process(target=motion_analysis_process, args=(extent[0], extent[1], self.parent().po.vars, subtotals))\n                        p.start()\n                        PROCESSES.append(p)\n\n                    for p in PROCESSES:\n                        p.join()\n\n                    self.message_from_thread.emit(f\"{message}, Step 2/2:  Saving all results...\")\n                    for i in range(subtotals.qsize()):\n                        grouped_results = subtotals.get()\n                        for j, results_i in enumerate(grouped_results):\n                            if not self.parent().po.vars['several_blob_per_arena']:\n                                # Save basic statistics\n                                self.parent().po.update_one_row_per_arena(results_i['i'], results_i['one_row_per_arena'])\n                                # Save descriptors in long_format\n                                self.parent().po.update_one_row_per_frame(results_i['i'] * self.parent().po.vars['img_number'],\n                                                                          (results_i['i'] + 1) * self.parent().po.vars['img_number'],\n                                                                          results_i['one_row_per_frame'])\n\n                            self.parent().po.add_analysis_visualization_to_first_and_last_images(results_i['i'], results_i['efficiency_test_1'],\n                                                                                     results_i['efficiency_test_2'])\n                    self.image_from_thread.emit(\n                        {\"current_image\": self.parent().po.last_image.bgr,\n                         \"message\": f\"{message} Step 2/2: analyzed {len(self.parent().po.vars['analyzed_individuals'])} out of {len(self.parent().po.vars['analyzed_individuals'])} arenas ({100}%)\"})\n\n                    logging.info(f\"Parallel analysis lasted {(default_timer() - tiii)/ 60} minutes\")\n                except MemoryError:\n                    analysis_status[\"continue\"] = False\n                    analysis_status[\"message\"] = f\"Not enough memory, reduce the core number for parallel analysis\"\n                    self.message_from_thread.emit(f\"Analyzing {message} requires to reduce the core number for parallel analysis\")\n                    return analysis_status\n            self.parent().po.save_tables()\n            return analysis_status\n        else:\n            analysis_status[\"continue\"] = False\n            analysis_status[\"message\"] = f\"Requires an additional {memory_diff}GB of RAM to run\"\n            self.message_from_thread.emit(f\"Analyzing {message} requires an additional {memory_diff}GB of RAM to run\")\n            return analysis_status\n    except MemoryError:\n        analysis_status[\"continue\"] = False\n        analysis_status[\"message\"] = f\"Requires additional memory to run\"\n        self.message_from_thread.emit(f\"Analyzing {message} requires additional memory to run\")\n        return analysis_status\n</code></pre>"},{"location":"api/cellects/core/cellects_threads/#cellects.core.cellects_threads.RunAllThread.run_video_writing","title":"<code>run_video_writing(message)</code>","text":"<p>Initiate the process of writing videos from image data.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>A string to emit as a status update during video writing.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the analysis status with keys: - \"continue\": bool indicating whether to continue video writing - \"message\": str providing a status or error message</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If an image file specified in <code>data_list</code> does not exist.</p> <code>OSError</code> <p>If there is an issue writing to disk, such as when the disk is full.</p> Notes <p>This function manages video writing in batches, checking available memory and handling errors related to file sizes or missing images</p> Source code in <code>src/cellects/core/cellects_threads.py</code> <pre><code>def run_video_writing(self, message: str) -&gt; dict:\n    \"\"\"\n    Initiate the process of writing videos from image data.\n\n    Parameters\n    ----------\n    message : str\n        A string to emit as a status update during video writing.\n\n    Returns\n    -------\n    dict\n        A dictionary containing the analysis status with keys:\n        - \"continue\": bool indicating whether to continue video writing\n        - \"message\": str providing a status or error message\n\n    Raises\n    ------\n    FileNotFoundError\n        If an image file specified in `data_list` does not exist.\n    OSError\n        If there is an issue writing to disk, such as when the disk is full.\n\n    Notes\n    -----\n    This function manages video writing in batches, checking available memory\n    and handling errors related to file sizes or missing images\n    \"\"\"\n    analysis_status = {\"continue\": True, \"message\": \"\"}\n    look_for_existing_videos = glob('ind_' + '*' + '.npy')\n    there_already_are_videos = len(look_for_existing_videos) == len(self.parent().po.vars['analyzed_individuals'])\n    logging.info(f\"{len(look_for_existing_videos)} .npy video files found for {len(self.parent().po.vars['analyzed_individuals'])} arenas to analyze\")\n    do_write_videos = not self.parent().po.all['im_or_vid'] and (not there_already_are_videos or (there_already_are_videos and self.parent().po.all['overwrite_unaltered_videos']))\n    if do_write_videos:\n        logging.info(f\"Starting video writing\")\n        # self.videos.write_videos_as_np_arrays(self.data_list, self.vars['convert_for_motion'], in_colors=self.vars['save_in_colors'])\n        in_colors = not self.parent().po.vars['already_greyscale']\n        self.parent().po.first_image.shape_number = self.parent().po.sample_number\n        bunch_nb, video_nb_per_bunch, sizes, video_bunch, vid_names, rom_memory_required, analysis_status, remaining, use_list_of_vid, is_landscape = self.parent().po.prepare_video_writing(\n            self.parent().po.data_list, self.parent().po.vars['min_ram_free'], in_colors)\n        if analysis_status[\"continue\"]:\n            # Check that there is enough available RAM for one video par bunch and ROM for all videos\n            if video_nb_per_bunch &gt; 0 and rom_memory_required is None:\n                pat_tracker1 = PercentAndTimeTracker(bunch_nb * self.parent().po.vars['img_number'])\n                pat_tracker2 = PercentAndTimeTracker(len(self.parent().po.vars['analyzed_individuals']))\n                arena_percentage = 0\n                for bunch in np.arange(bunch_nb):\n                    # Update the labels of arenas and the video_bunch to write\n                    if bunch == (bunch_nb - 1) and remaining &gt; 0:\n                        arena = np.arange(bunch * video_nb_per_bunch, bunch * video_nb_per_bunch + remaining)\n                    else:\n                        arena = np.arange(bunch * video_nb_per_bunch, (bunch + 1) * video_nb_per_bunch)\n                    if use_list_of_vid:\n                        video_bunch = [np.zeros(sizes[i, :], dtype=np.uint8) for i in arena]\n                    else:\n                        video_bunch = np.zeros(np.append(sizes[0, :], len(arena)), dtype=np.uint8)\n                    prev_img = None\n                    images_done = bunch * self.parent().po.vars['img_number']\n                    for image_i, image_name in enumerate(self.parent().po.data_list):\n                        image_percentage, remaining_time = pat_tracker1.get_progress(image_i + images_done)\n                        self.message_from_thread.emit(message + f\" Step 1/2: Video writing ({np.round((image_percentage + arena_percentage) / 2, 2)}%)\")\n                        if not os.path.exists(image_name):\n                            raise FileNotFoundError(image_name)\n                        img = read_and_rotate(image_name, prev_img, self.parent().po.all['raw_images'], is_landscape, self.parent().po.first_image.crop_coord)\n                        prev_img = deepcopy(img)\n                        if self.parent().po.vars['already_greyscale'] and self.parent().po.reduce_image_dim:\n                            img = img[:, :, 0]\n\n                        for arena_i, arena_name in enumerate(arena):\n                            try:\n                                sub_img = img[self.parent().po.top[arena_name]: self.parent().po.bot[arena_name],\n                                          self.parent().po.left[arena_name]: self.parent().po.right[arena_name], ...]\n                                if use_list_of_vid:\n                                    video_bunch[arena_i][image_i, ...] = sub_img\n                                else:\n                                    if len(video_bunch.shape) == 5:\n                                        video_bunch[image_i, :, :, :, arena_i] = sub_img\n                                    else:\n                                        video_bunch[image_i, :, :, arena_i] = sub_img\n                            except ValueError:\n                                analysis_status[\"message\"] = f\"Some images have incorrect size, reset all settings in advanced parameters\"\n                                analysis_status[\"continue\"] = False\n                                logging.info(f\"Reset all settings in advanced parameters\")\n                                break\n                        if not analysis_status[\"continue\"]:\n                            break\n                    if not analysis_status[\"continue\"]:\n                        break\n                    if analysis_status[\"continue\"]:\n                        for arena_i, arena_name in enumerate(arena):\n                            try:\n                                arena_percentage, eta = pat_tracker2.get_progress()\n                                self.message_from_thread.emit(message + f\" Step 1/2: Video writing ({np.round((image_percentage + arena_percentage) / 2, 2)}%)\")# , ETA {remaining_time}\n                                if use_list_of_vid:\n                                    np.save(vid_names[arena_name], video_bunch[arena_i])\n                                else:\n                                    if len(video_bunch.shape) == 5:\n                                        np.save(vid_names[arena_name], video_bunch[:, :, :, :, arena_i])\n                                    else:\n                                        np.save(vid_names[arena_name], video_bunch[:, :, :, arena_i])\n                            except OSError:\n                                self.message_from_thread.emit(message + f\"full disk memory, clear space and retry\")\n                    logging.info(f\"Bunch n\u00b0{bunch + 1} over {bunch_nb} saved.\")\n                logging.info(\"When they exist, do not overwrite unaltered video\")\n                self.parent().po.all['overwrite_unaltered_videos'] = False\n                self.parent().po.save_variable_dict()\n                self.parent().po.save_data_to_run_cellects_quickly()\n                if analysis_status[\"continue\"]:\n                    analysis_status[\"message\"] = f\"Video writing complete.\"\n                return analysis_status\n            else:\n                analysis_status[\"continue\"] = False\n                if video_nb_per_bunch == 0:\n                    memory_diff = self.parent().po.update_available_core_nb()\n                    ram_message = f\"{memory_diff}GB of additional RAM\"\n                if rom_memory_required is not None:\n                    rom_message = f\"at least {rom_memory_required}GB of free ROM\"\n\n                if video_nb_per_bunch == 0 and rom_memory_required is not None:\n                    analysis_status[\"message\"] = f\"Requires {ram_message} and {rom_message} to run\"\n                    # self.message_from_thread.emit(f\"Analyzing {message} requires {ram_message} and {rom_message} to run\")\n                elif video_nb_per_bunch == 0:\n                    analysis_status[\"message\"] = f\"Requires {ram_message} to run\"\n                    # self.message_from_thread.emit(f\"Analyzing {message} requires {ram_message} to run\")\n                elif rom_memory_required is not None:\n                    analysis_status[\"message\"] = f\"Requires {rom_message} to run\"\n                    # self.message_from_thread.emit(f\"Analyzing {message} requires {rom_message} to run\")\n                logging.info(f\"Cellects is not writing videos: insufficient memory\")\n                return analysis_status\n        else:\n            return analysis_status\n\n\n    else:\n        logging.info(f\"Cellects is not writing videos: unnecessary\")\n        analysis_status[\"message\"] = f\"Cellects is not writing videos: unnecessary\"\n        return analysis_status\n</code></pre>"},{"location":"api/cellects/core/cellects_threads/#cellects.core.cellects_threads.SaveAllVarsThread","title":"<code>SaveAllVarsThread</code>","text":"<p>               Bases: <code>QThread</code></p> <p>Thread for saving the GUI parameters and updating current folder.</p> Notes <p>This class uses <code>QThread</code> to manage the process asynchronously.</p> Source code in <code>src/cellects/core/cellects_threads.py</code> <pre><code>class SaveAllVarsThread(QtCore.QThread):\n    \"\"\"\n    Thread for saving the GUI parameters and updating current folder.\n\n    Notes\n    -----\n    This class uses `QThread` to manage the process asynchronously.\n    \"\"\"\n\n    def __init__(self, parent=None):\n        \"\"\"\n        Initialize the worker thread for saving the GUI parameters and updating current folder\n\n        Parameters\n        ----------\n        parent : QObject, optional\n            The parent object of this thread instance. In use, an instance of CellectsMainWidget class. Default is None.\n        \"\"\"\n        super(SaveAllVarsThread, self).__init__(parent)\n        self.setParent(parent)\n\n    def run(self):\n        \"\"\"\n        Execute a sequence of operations to save data and update the current folder.\n\n        This method performs several steps:\n            1. Save variable dictionary.\n            2. Set the current folder.\n            3. Save data to run Cellects quickly without creating a new one if it doesn't exist.\n        \"\"\"\n        self.parent().po.save_variable_dict()\n        self._set_current_folder()\n        self.parent().po.save_data_to_run_cellects_quickly(new_one_if_does_not_exist=False)\n\n    def _set_current_folder(self):\n        \"\"\"\n        Set the current folder based on conditions.\n\n        Sets the current folder to the first one in the list if there are multiple\n        folders, otherwise sets it to a reduced global pathway.\n        \"\"\"\n        if self.parent().po.all['folder_number'] &gt; 1: # len(self.parent().po.all['folder_list']) &gt; 1:  # len(self.parent().po.all['folder_list']) &gt; 0:\n            logging.info(f\"Use {self.parent().po.all['folder_list'][0]} folder\")\n            self.parent().po.update_folder_id(self.parent().po.all['sample_number_per_folder'][0],\n                                              self.parent().po.all['folder_list'][0])\n        else:\n            curr_path = reduce_path_len(self.parent().po.all['global_pathway'], 6, 10)\n            logging.info(f\"Use {curr_path} folder\")\n            self.parent().po.update_folder_id(self.parent().po.all['first_folder_sample_number'])\n</code></pre>"},{"location":"api/cellects/core/cellects_threads/#cellects.core.cellects_threads.SaveAllVarsThread.__init__","title":"<code>__init__(parent=None)</code>","text":"<p>Initialize the worker thread for saving the GUI parameters and updating current folder</p> <p>Parameters:</p> Name Type Description Default <code>parent</code> <code>QObject</code> <p>The parent object of this thread instance. In use, an instance of CellectsMainWidget class. Default is None.</p> <code>None</code> Source code in <code>src/cellects/core/cellects_threads.py</code> <pre><code>def __init__(self, parent=None):\n    \"\"\"\n    Initialize the worker thread for saving the GUI parameters and updating current folder\n\n    Parameters\n    ----------\n    parent : QObject, optional\n        The parent object of this thread instance. In use, an instance of CellectsMainWidget class. Default is None.\n    \"\"\"\n    super(SaveAllVarsThread, self).__init__(parent)\n    self.setParent(parent)\n</code></pre>"},{"location":"api/cellects/core/cellects_threads/#cellects.core.cellects_threads.SaveAllVarsThread.run","title":"<code>run()</code>","text":"<p>Execute a sequence of operations to save data and update the current folder.</p> <p>This method performs several steps:     1. Save variable dictionary.     2. Set the current folder.     3. Save data to run Cellects quickly without creating a new one if it doesn't exist.</p> Source code in <code>src/cellects/core/cellects_threads.py</code> <pre><code>def run(self):\n    \"\"\"\n    Execute a sequence of operations to save data and update the current folder.\n\n    This method performs several steps:\n        1. Save variable dictionary.\n        2. Set the current folder.\n        3. Save data to run Cellects quickly without creating a new one if it doesn't exist.\n    \"\"\"\n    self.parent().po.save_variable_dict()\n    self._set_current_folder()\n    self.parent().po.save_data_to_run_cellects_quickly(new_one_if_does_not_exist=False)\n</code></pre>"},{"location":"api/cellects/core/cellects_threads/#cellects.core.cellects_threads.SaveManualDelineationThread","title":"<code>SaveManualDelineationThread</code>","text":"<p>               Bases: <code>QThread</code></p> <p>Thread for saving user's defined arena delineation through the GUI.</p> Notes <p>This class uses <code>QThread</code> to manage the process asynchronously.</p> Source code in <code>src/cellects/core/cellects_threads.py</code> <pre><code>class SaveManualDelineationThread(QtCore.QThread):\n    \"\"\"\n    Thread for saving user's defined arena delineation through the GUI.\n\n    Notes\n    -----\n    This class uses `QThread` to manage the process asynchronously.\n    \"\"\"\n    def __init__(self, parent=None):\n        \"\"\"\n        Initialize the worker thread for saving the arena coordinates when the user draw them manually\n\n        Parameters\n        ----------\n        parent : QObject, optional\n            The parent object of this thread instance. In use, an instance of CellectsMainWidget class. Default is None.\n        \"\"\"\n        super(SaveManualDelineationThread, self).__init__(parent)\n        self.setParent(parent)\n\n    def run(self):\n        \"\"\"\n        Do save the coordinates.\n        \"\"\"\n        self.parent().po.left = np.zeros(self.parent().po.sample_number)\n        self.parent().po.right = np.zeros(self.parent().po.sample_number)\n        self.parent().po.top = np.zeros(self.parent().po.sample_number)\n        self.parent().po.bot = np.zeros(self.parent().po.sample_number)\n        for arena_i in np.arange(self.parent().po.sample_number):\n            y, x = np.nonzero(self.parent().imageanalysiswindow.arena_mask == arena_i + 1)\n            self.parent().po.left[arena_i] = np.min(x)\n            self.parent().po.right[arena_i] = np.max(x)\n            self.parent().po.top[arena_i] = np.min(y)\n            self.parent().po.bot[arena_i] = np.max(y)\n        self.parent().po.list_coordinates()\n        self.parent().po.save_data_to_run_cellects_quickly()\n\n        logging.info(\"Save manual video delineation\")\n        self.parent().po.vars['analyzed_individuals'] = np.arange(self.parent().po.sample_number) + 1\n</code></pre>"},{"location":"api/cellects/core/cellects_threads/#cellects.core.cellects_threads.SaveManualDelineationThread.__init__","title":"<code>__init__(parent=None)</code>","text":"<p>Initialize the worker thread for saving the arena coordinates when the user draw them manually</p> <p>Parameters:</p> Name Type Description Default <code>parent</code> <code>QObject</code> <p>The parent object of this thread instance. In use, an instance of CellectsMainWidget class. Default is None.</p> <code>None</code> Source code in <code>src/cellects/core/cellects_threads.py</code> <pre><code>def __init__(self, parent=None):\n    \"\"\"\n    Initialize the worker thread for saving the arena coordinates when the user draw them manually\n\n    Parameters\n    ----------\n    parent : QObject, optional\n        The parent object of this thread instance. In use, an instance of CellectsMainWidget class. Default is None.\n    \"\"\"\n    super(SaveManualDelineationThread, self).__init__(parent)\n    self.setParent(parent)\n</code></pre>"},{"location":"api/cellects/core/cellects_threads/#cellects.core.cellects_threads.SaveManualDelineationThread.run","title":"<code>run()</code>","text":"<p>Do save the coordinates.</p> Source code in <code>src/cellects/core/cellects_threads.py</code> <pre><code>def run(self):\n    \"\"\"\n    Do save the coordinates.\n    \"\"\"\n    self.parent().po.left = np.zeros(self.parent().po.sample_number)\n    self.parent().po.right = np.zeros(self.parent().po.sample_number)\n    self.parent().po.top = np.zeros(self.parent().po.sample_number)\n    self.parent().po.bot = np.zeros(self.parent().po.sample_number)\n    for arena_i in np.arange(self.parent().po.sample_number):\n        y, x = np.nonzero(self.parent().imageanalysiswindow.arena_mask == arena_i + 1)\n        self.parent().po.left[arena_i] = np.min(x)\n        self.parent().po.right[arena_i] = np.max(x)\n        self.parent().po.top[arena_i] = np.min(y)\n        self.parent().po.bot[arena_i] = np.max(y)\n    self.parent().po.list_coordinates()\n    self.parent().po.save_data_to_run_cellects_quickly()\n\n    logging.info(\"Save manual video delineation\")\n    self.parent().po.vars['analyzed_individuals'] = np.arange(self.parent().po.sample_number) + 1\n</code></pre>"},{"location":"api/cellects/core/cellects_threads/#cellects.core.cellects_threads.UpdateImageThread","title":"<code>UpdateImageThread</code>","text":"<p>               Bases: <code>QThread</code></p> <p>Thread for updating GUI image.</p> Signals <p>message_when_thread_finished : Signal(bool)     Emitted when the thread finishes execution, indicating whether image displaying was successful.</p> Notes <p>This class uses <code>QThread</code> to manage the process asynchronously.</p> Source code in <code>src/cellects/core/cellects_threads.py</code> <pre><code>class UpdateImageThread(QtCore.QThread):\n    \"\"\"\n    Thread for updating GUI image.\n\n    Signals\n    -------\n    message_when_thread_finished : Signal(bool)\n        Emitted when the thread finishes execution, indicating whether image displaying was successful.\n\n    Notes\n    -----\n    This class uses `QThread` to manage the process asynchronously.\n    \"\"\"\n    message_when_thread_finished = QtCore.Signal(bool)\n\n    def __init__(self, parent=None):\n        \"\"\"\n        Initialize the worker thread for updating the image displayed in GUI\n\n        Parameters\n        ----------\n        parent : QObject, optional\n            The parent object of this thread instance. In use, an instance of CellectsMainWidget class. Default is None.\n        \"\"\"\n        super(UpdateImageThread, self).__init__(parent)\n        self.setParent(parent)\n\n    def run(self):\n        \"\"\"\n        Execute the image display process, including user input handling and mask application.\n\n        This method performs several steps to analyze an image based on user input\n        and saved mask coordinates. It updates the drawn image with segmentation masks,\n        back masks, bio masks, and video contours.\n\n        Other Parameters\n        ----------------\n        user_input : bool, optional\n            Flag indicating whether user input is available.\n        idx : list or numpy.ndarray, optional\n            Coordinates of the user- defined region of interest.\n        temp_mask_coord : list, optional\n            Temporary mask coordinates.\n        saved_coord : list, optional\n            Saved mask coordinates.\n\n        Notes\n        -----\n        - This function updates several attributes of `self.parent().imageanalysiswindow`.\n        - Performance considerations include handling large images efficiently.\n        - Important behavioral caveats: Ensure coordinates are within image bounds.\n        \"\"\"\n        # I/ If this thread runs from user input, get the right coordinates\n        # and convert them to fit the displayed image size\n        user_input = len(self.parent().imageanalysiswindow.saved_coord) &gt; 0 or len(self.parent().imageanalysiswindow.temporary_mask_coord) &gt; 0\n        dims = self.parent().imageanalysiswindow.drawn_image.shape\n        if user_input:\n            if len(self.parent().imageanalysiswindow.temporary_mask_coord) &gt; 0:\n                idx = self.parent().imageanalysiswindow.temporary_mask_coord\n            else:\n                idx = self.parent().imageanalysiswindow.saved_coord\n            if len(idx) &lt; 2:\n                user_input = False\n            else:\n                # Convert coordinates:\n                self.parent().imageanalysiswindow.display_image.update_image_scaling_factors()\n                sf = self.parent().imageanalysiswindow.display_image.scaling_factors\n                idx, min_y, max_y, min_x, max_x = scale_coordinates(coord=idx, scale=sf, dims=dims)\n                minmax = min_y, max_y, min_x, max_x\n\n        if len(self.parent().imageanalysiswindow.temporary_mask_coord) == 0:\n            # not_load\n            # II/ If this thread aims at saving the last user input and displaying all user inputs:\n            # Update the drawn_image according to every saved masks\n            # 1) The segmentation mask\n            # 2) The back_mask and bio_mask\n            # 3) The automatically detected video contours\n            # (re-)Initialize drawn image\n            self.parent().imageanalysiswindow.drawn_image = deepcopy(self.parent().po.current_image)\n            contour_width = get_contour_width_from_im_shape(dims)\n            # 1) Add the segmentation mask to the image\n            if self.parent().imageanalysiswindow.is_first_image_flag:\n                im_combinations = self.parent().po.first_image.im_combinations\n                im_mean = self.parent().po.first_image.image.mean()\n            else:\n                im_combinations = self.parent().po.last_image.im_combinations\n                im_mean = self.parent().po.last_image.image.mean()\n            # If there are image combinations, get the current corresponding binary image\n            if im_combinations is not None and len(im_combinations) != 0:\n                binary_idx = im_combinations[self.parent().po.current_combination_id][\"binary_image\"]\n                # If it concerns the last image, only keep the contour coordinates\n                binary_idx = cv2.dilate(get_contours(binary_idx), kernel=cross_33, iterations=contour_width)\n                binary_idx = np.nonzero(binary_idx)\n                # Color these coordinates in magenta on bright images, and in pink on dark images\n                if im_mean &gt; 126:\n                    # Color the segmentation mask in magenta\n                    self.parent().imageanalysiswindow.drawn_image[binary_idx[0], binary_idx[1], :] = np.array((20, 0, 150), dtype=np.uint8)\n                else:\n                    # Color the segmentation mask in pink\n                    self.parent().imageanalysiswindow.drawn_image[binary_idx[0], binary_idx[1], :] = np.array((94, 0, 213), dtype=np.uint8)\n            if user_input:# save\n                if self.parent().imageanalysiswindow.back1_bio2 == 0:\n                    mask_shape = self.parent().po.vars['arena_shape']\n                elif self.parent().imageanalysiswindow.back1_bio2 == 1:\n                    mask_shape = \"rectangle\"\n                elif self.parent().imageanalysiswindow.back1_bio2 == 2:\n                    mask_shape = self.parent().po.all['starting_blob_shape']\n                    if mask_shape is None:\n                        mask_shape = 'circle'\n                # Save the user drawn mask\n                mask = create_mask(dims, minmax, mask_shape)\n                mask = np.nonzero(mask)\n\n                if self.parent().imageanalysiswindow.back1_bio2 == 1:\n                    self.parent().imageanalysiswindow.back_masks_number += 1\n                    self.parent().imageanalysiswindow.back_mask[mask[0], mask[1]] = self.parent().imageanalysiswindow.available_back_names[0]\n                elif self.parent().imageanalysiswindow.back1_bio2 == 2:\n                    self.parent().imageanalysiswindow.bio_masks_number += 1\n                    self.parent().imageanalysiswindow.bio_mask[mask[0], mask[1]] = self.parent().imageanalysiswindow.available_bio_names[0]\n                elif self.parent().imageanalysiswindow.manual_delineation_flag:\n                    self.parent().imageanalysiswindow.arena_masks_number += 1\n                    self.parent().imageanalysiswindow.arena_mask[mask[0], mask[1]] = self.parent().imageanalysiswindow.available_arena_names[0]\n                # 2)a) Apply all these masks to the drawn image:\n\n            back_coord = np.nonzero(self.parent().imageanalysiswindow.back_mask)\n\n            bio_coord = np.nonzero(self.parent().imageanalysiswindow.bio_mask)\n\n            if self.parent().imageanalysiswindow.arena_mask is not None:\n                arena_coord = np.nonzero(self.parent().imageanalysiswindow.arena_mask)\n                self.parent().imageanalysiswindow.drawn_image[arena_coord[0], arena_coord[1], :] = np.repeat(self.parent().po.vars['contour_color'], 3).astype(np.uint8)\n\n            self.parent().imageanalysiswindow.drawn_image[back_coord[0], back_coord[1], :] = np.array((224, 160, 81), dtype=np.uint8)\n\n            self.parent().imageanalysiswindow.drawn_image[bio_coord[0], bio_coord[1], :] = np.array((17, 160, 212), dtype=np.uint8)\n\n            image = self.parent().imageanalysiswindow.drawn_image.copy()\n            # 3) The automatically detected video contours\n            if self.parent().imageanalysiswindow.delineation_done:  # add a mask of the video contour\n                if self.parent().po.vars['contour_color'] == 255:\n                    arena_contour_col = (240, 232, 202)\n                else:\n                    arena_contour_col = (138, 95, 18)\n                # Draw the delineation mask of each arena\n                for _i, (min_cy, max_cy, min_cx, max_cx) in enumerate(zip(self.parent().po.top, self.parent().po.bot, self.parent().po.left, self.parent().po.right)):\n                    position = (min_cx + 25, min_cy + (max_cy - min_cy) // 2)\n                    image = cv2.putText(image, f\"{_i + 1}\", position, cv2.FONT_HERSHEY_SIMPLEX, 1,  arena_contour_col + (255,),2)\n                    if (max_cy - min_cy) &lt; 0 or (max_cx - min_cx) &lt; 0:\n                        self.parent().imageanalysiswindow.message.setText(\"Error: the shape number or the detection is wrong\")\n                    image = draw_img_with_mask(image, dims, (min_cy, max_cy - 1, min_cx, max_cx - 1),\n                                               self.parent().po.vars['arena_shape'], arena_contour_col, True, contour_width)\n        else: #load\n            if user_input:\n                # III/ If this thread runs from user input: update the drawn_image according to the current user input\n                # Just add the mask to drawn_image as quick as possible\n                # Add user defined masks\n                # Take the drawn image and add the temporary mask to it\n                image = self.parent().imageanalysiswindow.drawn_image.copy()\n                if self.parent().imageanalysiswindow.back1_bio2 == 2:\n                    color = (17, 160, 212)\n                    mask_shape = self.parent().po.all['starting_blob_shape']\n                    if mask_shape is None:\n                        mask_shape = 'circle'\n                elif self.parent().imageanalysiswindow.back1_bio2 == 1:\n                    color = (224, 160, 81)\n                    mask_shape = \"rectangle\"\n                else:\n                    color = (0, 0, 0)\n                    mask_shape = self.parent().po.vars['arena_shape']\n                image = draw_img_with_mask(image, dims, minmax, mask_shape, color)\n        self.parent().imageanalysiswindow.display_image.update_image(image)\n        self.message_when_thread_finished.emit(True)\n</code></pre>"},{"location":"api/cellects/core/cellects_threads/#cellects.core.cellects_threads.UpdateImageThread.__init__","title":"<code>__init__(parent=None)</code>","text":"<p>Initialize the worker thread for updating the image displayed in GUI</p> <p>Parameters:</p> Name Type Description Default <code>parent</code> <code>QObject</code> <p>The parent object of this thread instance. In use, an instance of CellectsMainWidget class. Default is None.</p> <code>None</code> Source code in <code>src/cellects/core/cellects_threads.py</code> <pre><code>def __init__(self, parent=None):\n    \"\"\"\n    Initialize the worker thread for updating the image displayed in GUI\n\n    Parameters\n    ----------\n    parent : QObject, optional\n        The parent object of this thread instance. In use, an instance of CellectsMainWidget class. Default is None.\n    \"\"\"\n    super(UpdateImageThread, self).__init__(parent)\n    self.setParent(parent)\n</code></pre>"},{"location":"api/cellects/core/cellects_threads/#cellects.core.cellects_threads.UpdateImageThread.run","title":"<code>run()</code>","text":"<p>Execute the image display process, including user input handling and mask application.</p> <p>This method performs several steps to analyze an image based on user input and saved mask coordinates. It updates the drawn image with segmentation masks, back masks, bio masks, and video contours.</p> <p>Other Parameters:</p> Name Type Description <code>user_input</code> <code>bool</code> <p>Flag indicating whether user input is available.</p> <code>idx</code> <code>list or ndarray</code> <p>Coordinates of the user- defined region of interest.</p> <code>temp_mask_coord</code> <code>list</code> <p>Temporary mask coordinates.</p> <code>saved_coord</code> <code>list</code> <p>Saved mask coordinates.</p> Notes <ul> <li>This function updates several attributes of <code>self.parent().imageanalysiswindow</code>.</li> <li>Performance considerations include handling large images efficiently.</li> <li>Important behavioral caveats: Ensure coordinates are within image bounds.</li> </ul> Source code in <code>src/cellects/core/cellects_threads.py</code> <pre><code>def run(self):\n    \"\"\"\n    Execute the image display process, including user input handling and mask application.\n\n    This method performs several steps to analyze an image based on user input\n    and saved mask coordinates. It updates the drawn image with segmentation masks,\n    back masks, bio masks, and video contours.\n\n    Other Parameters\n    ----------------\n    user_input : bool, optional\n        Flag indicating whether user input is available.\n    idx : list or numpy.ndarray, optional\n        Coordinates of the user- defined region of interest.\n    temp_mask_coord : list, optional\n        Temporary mask coordinates.\n    saved_coord : list, optional\n        Saved mask coordinates.\n\n    Notes\n    -----\n    - This function updates several attributes of `self.parent().imageanalysiswindow`.\n    - Performance considerations include handling large images efficiently.\n    - Important behavioral caveats: Ensure coordinates are within image bounds.\n    \"\"\"\n    # I/ If this thread runs from user input, get the right coordinates\n    # and convert them to fit the displayed image size\n    user_input = len(self.parent().imageanalysiswindow.saved_coord) &gt; 0 or len(self.parent().imageanalysiswindow.temporary_mask_coord) &gt; 0\n    dims = self.parent().imageanalysiswindow.drawn_image.shape\n    if user_input:\n        if len(self.parent().imageanalysiswindow.temporary_mask_coord) &gt; 0:\n            idx = self.parent().imageanalysiswindow.temporary_mask_coord\n        else:\n            idx = self.parent().imageanalysiswindow.saved_coord\n        if len(idx) &lt; 2:\n            user_input = False\n        else:\n            # Convert coordinates:\n            self.parent().imageanalysiswindow.display_image.update_image_scaling_factors()\n            sf = self.parent().imageanalysiswindow.display_image.scaling_factors\n            idx, min_y, max_y, min_x, max_x = scale_coordinates(coord=idx, scale=sf, dims=dims)\n            minmax = min_y, max_y, min_x, max_x\n\n    if len(self.parent().imageanalysiswindow.temporary_mask_coord) == 0:\n        # not_load\n        # II/ If this thread aims at saving the last user input and displaying all user inputs:\n        # Update the drawn_image according to every saved masks\n        # 1) The segmentation mask\n        # 2) The back_mask and bio_mask\n        # 3) The automatically detected video contours\n        # (re-)Initialize drawn image\n        self.parent().imageanalysiswindow.drawn_image = deepcopy(self.parent().po.current_image)\n        contour_width = get_contour_width_from_im_shape(dims)\n        # 1) Add the segmentation mask to the image\n        if self.parent().imageanalysiswindow.is_first_image_flag:\n            im_combinations = self.parent().po.first_image.im_combinations\n            im_mean = self.parent().po.first_image.image.mean()\n        else:\n            im_combinations = self.parent().po.last_image.im_combinations\n            im_mean = self.parent().po.last_image.image.mean()\n        # If there are image combinations, get the current corresponding binary image\n        if im_combinations is not None and len(im_combinations) != 0:\n            binary_idx = im_combinations[self.parent().po.current_combination_id][\"binary_image\"]\n            # If it concerns the last image, only keep the contour coordinates\n            binary_idx = cv2.dilate(get_contours(binary_idx), kernel=cross_33, iterations=contour_width)\n            binary_idx = np.nonzero(binary_idx)\n            # Color these coordinates in magenta on bright images, and in pink on dark images\n            if im_mean &gt; 126:\n                # Color the segmentation mask in magenta\n                self.parent().imageanalysiswindow.drawn_image[binary_idx[0], binary_idx[1], :] = np.array((20, 0, 150), dtype=np.uint8)\n            else:\n                # Color the segmentation mask in pink\n                self.parent().imageanalysiswindow.drawn_image[binary_idx[0], binary_idx[1], :] = np.array((94, 0, 213), dtype=np.uint8)\n        if user_input:# save\n            if self.parent().imageanalysiswindow.back1_bio2 == 0:\n                mask_shape = self.parent().po.vars['arena_shape']\n            elif self.parent().imageanalysiswindow.back1_bio2 == 1:\n                mask_shape = \"rectangle\"\n            elif self.parent().imageanalysiswindow.back1_bio2 == 2:\n                mask_shape = self.parent().po.all['starting_blob_shape']\n                if mask_shape is None:\n                    mask_shape = 'circle'\n            # Save the user drawn mask\n            mask = create_mask(dims, minmax, mask_shape)\n            mask = np.nonzero(mask)\n\n            if self.parent().imageanalysiswindow.back1_bio2 == 1:\n                self.parent().imageanalysiswindow.back_masks_number += 1\n                self.parent().imageanalysiswindow.back_mask[mask[0], mask[1]] = self.parent().imageanalysiswindow.available_back_names[0]\n            elif self.parent().imageanalysiswindow.back1_bio2 == 2:\n                self.parent().imageanalysiswindow.bio_masks_number += 1\n                self.parent().imageanalysiswindow.bio_mask[mask[0], mask[1]] = self.parent().imageanalysiswindow.available_bio_names[0]\n            elif self.parent().imageanalysiswindow.manual_delineation_flag:\n                self.parent().imageanalysiswindow.arena_masks_number += 1\n                self.parent().imageanalysiswindow.arena_mask[mask[0], mask[1]] = self.parent().imageanalysiswindow.available_arena_names[0]\n            # 2)a) Apply all these masks to the drawn image:\n\n        back_coord = np.nonzero(self.parent().imageanalysiswindow.back_mask)\n\n        bio_coord = np.nonzero(self.parent().imageanalysiswindow.bio_mask)\n\n        if self.parent().imageanalysiswindow.arena_mask is not None:\n            arena_coord = np.nonzero(self.parent().imageanalysiswindow.arena_mask)\n            self.parent().imageanalysiswindow.drawn_image[arena_coord[0], arena_coord[1], :] = np.repeat(self.parent().po.vars['contour_color'], 3).astype(np.uint8)\n\n        self.parent().imageanalysiswindow.drawn_image[back_coord[0], back_coord[1], :] = np.array((224, 160, 81), dtype=np.uint8)\n\n        self.parent().imageanalysiswindow.drawn_image[bio_coord[0], bio_coord[1], :] = np.array((17, 160, 212), dtype=np.uint8)\n\n        image = self.parent().imageanalysiswindow.drawn_image.copy()\n        # 3) The automatically detected video contours\n        if self.parent().imageanalysiswindow.delineation_done:  # add a mask of the video contour\n            if self.parent().po.vars['contour_color'] == 255:\n                arena_contour_col = (240, 232, 202)\n            else:\n                arena_contour_col = (138, 95, 18)\n            # Draw the delineation mask of each arena\n            for _i, (min_cy, max_cy, min_cx, max_cx) in enumerate(zip(self.parent().po.top, self.parent().po.bot, self.parent().po.left, self.parent().po.right)):\n                position = (min_cx + 25, min_cy + (max_cy - min_cy) // 2)\n                image = cv2.putText(image, f\"{_i + 1}\", position, cv2.FONT_HERSHEY_SIMPLEX, 1,  arena_contour_col + (255,),2)\n                if (max_cy - min_cy) &lt; 0 or (max_cx - min_cx) &lt; 0:\n                    self.parent().imageanalysiswindow.message.setText(\"Error: the shape number or the detection is wrong\")\n                image = draw_img_with_mask(image, dims, (min_cy, max_cy - 1, min_cx, max_cx - 1),\n                                           self.parent().po.vars['arena_shape'], arena_contour_col, True, contour_width)\n    else: #load\n        if user_input:\n            # III/ If this thread runs from user input: update the drawn_image according to the current user input\n            # Just add the mask to drawn_image as quick as possible\n            # Add user defined masks\n            # Take the drawn image and add the temporary mask to it\n            image = self.parent().imageanalysiswindow.drawn_image.copy()\n            if self.parent().imageanalysiswindow.back1_bio2 == 2:\n                color = (17, 160, 212)\n                mask_shape = self.parent().po.all['starting_blob_shape']\n                if mask_shape is None:\n                    mask_shape = 'circle'\n            elif self.parent().imageanalysiswindow.back1_bio2 == 1:\n                color = (224, 160, 81)\n                mask_shape = \"rectangle\"\n            else:\n                color = (0, 0, 0)\n                mask_shape = self.parent().po.vars['arena_shape']\n            image = draw_img_with_mask(image, dims, minmax, mask_shape, color)\n    self.parent().imageanalysiswindow.display_image.update_image(image)\n    self.message_when_thread_finished.emit(True)\n</code></pre>"},{"location":"api/cellects/core/cellects_threads/#cellects.core.cellects_threads.VideoReaderThread","title":"<code>VideoReaderThread</code>","text":"<p>               Bases: <code>QThread</code></p> <p>Thread for reading a video in the GUI.</p> Signals <p>message_from_thread : Signal(dict)     Signal emitted during the video reading to display images to the GUI.</p> Notes <p>This class uses <code>QThread</code> to manage the process asynchronously.</p> Source code in <code>src/cellects/core/cellects_threads.py</code> <pre><code>class VideoReaderThread(QtCore.QThread):\n    \"\"\"\n    Thread for reading a video in the GUI.\n\n    Signals\n    --------\n    message_from_thread : Signal(dict)\n        Signal emitted during the video reading to display images to the GUI.\n\n    Notes\n    -----\n    This class uses `QThread` to manage the process asynchronously.\n    \"\"\"\n    message_from_thread = QtCore.Signal(dict)\n\n    def __init__(self, parent=None):\n        \"\"\"\n        Initialize the worker thread for reading a video in the GUI\n\n        Parameters\n        ----------\n        parent : QObject, optional\n            The parent object of this thread instance. In use, an instance of CellectsMainWidget class. Default is None.\n        \"\"\"\n        super(VideoReaderThread, self).__init__(parent)\n        self.setParent(parent)\n\n    def run(self):\n        \"\"\"\n        Summary\n        -------\n        Run the video analysis process, applying segmentation and contouring to each frame.\n\n        Extended Description\n        --------------------\n        This method performs video analysis by segmenting frames based on selected options and overlaying contours.\n        It also updates the UI with progress messages.\n\n        Notes\n        -----\n        This method emits signals to update the UI with progress messages and current images.\n        It uses OpenCV for morphological operations on video frames.\n        \"\"\"\n        video_analysis = deepcopy(self.parent().po.motion.visu)\n        self.message_from_thread.emit(\n            {\"current_image\": video_analysis[0, ...], \"message\": f\"Video preparation, wait...\"})\n        if self.parent().po.load_quick_full &gt; 0:\n\n            if self.parent().po.all['compute_all_options']:\n                if self.parent().po.all['video_option'] == 0:\n                    video_mask = self.parent().po.motion.segmented\n                else:\n                    if self.parent().po.all['video_option'] == 1:\n                        mask = self.parent().po.motion.luminosity_segmentation\n                    elif self.parent().po.all['video_option'] == 2:\n                        mask = self.parent().po.motion.gradient_segmentation\n                    elif self.parent().po.all['video_option'] == 3:\n                        mask = self.parent().po.motion.logical_and\n                    elif self.parent().po.all['video_option'] == 4:\n                        mask = self.parent().po.motion.logical_or\n                    video_mask = np.zeros(self.parent().po.motion.dims[:3], dtype=np.uint8)\n                    video_mask[mask[0], mask[1], mask[2]] = 1\n            else:\n                video_mask = np.zeros(self.parent().po.motion.dims[:3], dtype=np.uint8)\n                if self.parent().po.computed_video_options[self.parent().po.all['video_option']]:\n                    video_mask = self.parent().po.motion.segmented\n\n            if self.parent().po.load_quick_full == 1:\n                video_mask = np.cumsum(video_mask.astype(np.uint32), axis=0)\n                video_mask[video_mask &gt; 0] = 1\n                video_mask = video_mask.astype(np.uint8)\n        frame_delay = (8 + np.log10(self.parent().po.motion.dims[0])) / self.parent().po.motion.dims[0]\n        for t in np.arange(self.parent().po.motion.dims[0]):\n            mask = cv2.morphologyEx(video_mask[t, ...], cv2.MORPH_GRADIENT, cross_33)\n            mask = np.stack((mask, mask, mask), axis=2)\n            current_image = deepcopy(video_analysis[t, ...])\n            current_image[mask &gt; 0] = self.parent().po.vars['contour_color']\n            self.message_from_thread.emit(\n                {\"current_image\": current_image, \"message\": f\"Reading in progress... Image number: {t}\"}) #, \"time\": timings[t]\n            time.sleep(frame_delay)\n        self.message_from_thread.emit({\"current_image\": current_image, \"message\": \"\"})#, \"time\": timings[t]\n</code></pre>"},{"location":"api/cellects/core/cellects_threads/#cellects.core.cellects_threads.VideoReaderThread.__init__","title":"<code>__init__(parent=None)</code>","text":"<p>Initialize the worker thread for reading a video in the GUI</p> <p>Parameters:</p> Name Type Description Default <code>parent</code> <code>QObject</code> <p>The parent object of this thread instance. In use, an instance of CellectsMainWidget class. Default is None.</p> <code>None</code> Source code in <code>src/cellects/core/cellects_threads.py</code> <pre><code>def __init__(self, parent=None):\n    \"\"\"\n    Initialize the worker thread for reading a video in the GUI\n\n    Parameters\n    ----------\n    parent : QObject, optional\n        The parent object of this thread instance. In use, an instance of CellectsMainWidget class. Default is None.\n    \"\"\"\n    super(VideoReaderThread, self).__init__(parent)\n    self.setParent(parent)\n</code></pre>"},{"location":"api/cellects/core/cellects_threads/#cellects.core.cellects_threads.VideoReaderThread.run","title":"<code>run()</code>","text":"Summary <p>Run the video analysis process, applying segmentation and contouring to each frame.</p> Extended Description <p>This method performs video analysis by segmenting frames based on selected options and overlaying contours. It also updates the UI with progress messages.</p> Notes <p>This method emits signals to update the UI with progress messages and current images. It uses OpenCV for morphological operations on video frames.</p> Source code in <code>src/cellects/core/cellects_threads.py</code> <pre><code>def run(self):\n    \"\"\"\n    Summary\n    -------\n    Run the video analysis process, applying segmentation and contouring to each frame.\n\n    Extended Description\n    --------------------\n    This method performs video analysis by segmenting frames based on selected options and overlaying contours.\n    It also updates the UI with progress messages.\n\n    Notes\n    -----\n    This method emits signals to update the UI with progress messages and current images.\n    It uses OpenCV for morphological operations on video frames.\n    \"\"\"\n    video_analysis = deepcopy(self.parent().po.motion.visu)\n    self.message_from_thread.emit(\n        {\"current_image\": video_analysis[0, ...], \"message\": f\"Video preparation, wait...\"})\n    if self.parent().po.load_quick_full &gt; 0:\n\n        if self.parent().po.all['compute_all_options']:\n            if self.parent().po.all['video_option'] == 0:\n                video_mask = self.parent().po.motion.segmented\n            else:\n                if self.parent().po.all['video_option'] == 1:\n                    mask = self.parent().po.motion.luminosity_segmentation\n                elif self.parent().po.all['video_option'] == 2:\n                    mask = self.parent().po.motion.gradient_segmentation\n                elif self.parent().po.all['video_option'] == 3:\n                    mask = self.parent().po.motion.logical_and\n                elif self.parent().po.all['video_option'] == 4:\n                    mask = self.parent().po.motion.logical_or\n                video_mask = np.zeros(self.parent().po.motion.dims[:3], dtype=np.uint8)\n                video_mask[mask[0], mask[1], mask[2]] = 1\n        else:\n            video_mask = np.zeros(self.parent().po.motion.dims[:3], dtype=np.uint8)\n            if self.parent().po.computed_video_options[self.parent().po.all['video_option']]:\n                video_mask = self.parent().po.motion.segmented\n\n        if self.parent().po.load_quick_full == 1:\n            video_mask = np.cumsum(video_mask.astype(np.uint32), axis=0)\n            video_mask[video_mask &gt; 0] = 1\n            video_mask = video_mask.astype(np.uint8)\n    frame_delay = (8 + np.log10(self.parent().po.motion.dims[0])) / self.parent().po.motion.dims[0]\n    for t in np.arange(self.parent().po.motion.dims[0]):\n        mask = cv2.morphologyEx(video_mask[t, ...], cv2.MORPH_GRADIENT, cross_33)\n        mask = np.stack((mask, mask, mask), axis=2)\n        current_image = deepcopy(video_analysis[t, ...])\n        current_image[mask &gt; 0] = self.parent().po.vars['contour_color']\n        self.message_from_thread.emit(\n            {\"current_image\": current_image, \"message\": f\"Reading in progress... Image number: {t}\"}) #, \"time\": timings[t]\n        time.sleep(frame_delay)\n    self.message_from_thread.emit({\"current_image\": current_image, \"message\": \"\"})#, \"time\": timings[t]\n</code></pre>"},{"location":"api/cellects/core/cellects_threads/#cellects.core.cellects_threads.WriteVideoThread","title":"<code>WriteVideoThread</code>","text":"<p>               Bases: <code>QThread</code></p> <p>Thread for writing one video per arena in the current folder.</p> Notes <p>This class uses <code>QThread</code> to manage the process asynchronously.</p> Source code in <code>src/cellects/core/cellects_threads.py</code> <pre><code>class WriteVideoThread(QtCore.QThread):\n    \"\"\"\n    Thread for writing one video per arena in the current folder.\n\n    Notes\n    -----\n    This class uses `QThread` to manage the process asynchronously.\n    \"\"\"\n    def __init__(self, parent=None):\n        \"\"\"\n        Initialize the worker thread for writing the video corresponding to the current arena\n\n        Parameters\n        ----------\n        parent : QObject, optional\n            The parent object of this thread instance. In use, an instance of CellectsMainWidget class. Default is None.\n        \"\"\"\n        super(WriteVideoThread, self).__init__(parent)\n        self.setParent(parent)\n\n    def run(self):\n        \"\"\"\n        Run the visualization or converted video for a specific arena and save it as an .npy file.\n\n        Parameters\n        ----------\n        self : object\n            The instance of the class containing this method.\n\n        Other Parameters\n        ----------------\n        arena : str\n            Name of the arena.\n\n        already_greyscale : bool\n            Flag indicating if the video is already in greyscale format.\n\n        Raises\n        ------\n        FileNotFoundError\n            When the path to write the video is not specified.\n        \"\"\"\n        arena = self.parent().po.all['arena']\n        if not self.parent().po.vars['already_greyscale']:\n            write_video(self.parent().po.visu, f'ind_{arena}.npy')\n        else:\n            write_video(self.parent().po.converted_video, f'ind_{arena}.npy')\n</code></pre>"},{"location":"api/cellects/core/cellects_threads/#cellects.core.cellects_threads.WriteVideoThread.__init__","title":"<code>__init__(parent=None)</code>","text":"<p>Initialize the worker thread for writing the video corresponding to the current arena</p> <p>Parameters:</p> Name Type Description Default <code>parent</code> <code>QObject</code> <p>The parent object of this thread instance. In use, an instance of CellectsMainWidget class. Default is None.</p> <code>None</code> Source code in <code>src/cellects/core/cellects_threads.py</code> <pre><code>def __init__(self, parent=None):\n    \"\"\"\n    Initialize the worker thread for writing the video corresponding to the current arena\n\n    Parameters\n    ----------\n    parent : QObject, optional\n        The parent object of this thread instance. In use, an instance of CellectsMainWidget class. Default is None.\n    \"\"\"\n    super(WriteVideoThread, self).__init__(parent)\n    self.setParent(parent)\n</code></pre>"},{"location":"api/cellects/core/cellects_threads/#cellects.core.cellects_threads.WriteVideoThread.run","title":"<code>run()</code>","text":"<p>Run the visualization or converted video for a specific arena and save it as an .npy file.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>object</code> <p>The instance of the class containing this method.</p> required <p>Other Parameters:</p> Name Type Description <code>arena</code> <code>str</code> <p>Name of the arena.</p> <code>already_greyscale</code> <code>bool</code> <p>Flag indicating if the video is already in greyscale format.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>When the path to write the video is not specified.</p> Source code in <code>src/cellects/core/cellects_threads.py</code> <pre><code>def run(self):\n    \"\"\"\n    Run the visualization or converted video for a specific arena and save it as an .npy file.\n\n    Parameters\n    ----------\n    self : object\n        The instance of the class containing this method.\n\n    Other Parameters\n    ----------------\n    arena : str\n        Name of the arena.\n\n    already_greyscale : bool\n        Flag indicating if the video is already in greyscale format.\n\n    Raises\n    ------\n    FileNotFoundError\n        When the path to write the video is not specified.\n    \"\"\"\n    arena = self.parent().po.all['arena']\n    if not self.parent().po.vars['already_greyscale']:\n        write_video(self.parent().po.visu, f'ind_{arena}.npy')\n    else:\n        write_video(self.parent().po.converted_video, f'ind_{arena}.npy')\n</code></pre>"},{"location":"api/cellects/core/cellects_threads/#cellects.core.cellects_threads.motion_analysis_process","title":"<code>motion_analysis_process(lower_bound, upper_bound, vars, subtotals)</code>","text":"<p>Motion Analysis Process for parallel computing</p> <p>Process a group of motion analysis results and store them in a queue.</p> <p>Parameters:</p> Name Type Description Default <code>lower_bound</code> <code>int</code> <p>The lower bound index for the range of analysis.</p> required <code>upper_bound</code> <code>int</code> <p>The upper bound index (exclusive) for the range of analysis.</p> required <code>vars</code> <code>dict</code> <p>Dictionary containing variables and configurations for the motion analysis process.</p> required <code>subtotals</code> <code>Queue</code> <p>A queue to store intermediate results.</p> required Notes <p>This function processes a range of motion analysis results based on the provided configuration variables and stores the intermediate results in a queue.</p> Source code in <code>src/cellects/core/cellects_threads.py</code> <pre><code>def motion_analysis_process(lower_bound: int, upper_bound: int, vars: dict, subtotals: Queue) -&gt; None:\n    \"\"\"\n    Motion Analysis Process for parallel computing\n\n    Process a group of motion analysis results and store them in a queue.\n\n    Parameters\n    ----------\n    lower_bound : int\n        The lower bound index for the range of analysis.\n    upper_bound : int\n        The upper bound index (exclusive) for the range of analysis.\n    vars : dict\n        Dictionary containing variables and configurations for the motion analysis process.\n    subtotals : Queue\n        A queue to store intermediate results.\n    Notes\n    -----\n    This function processes a range of motion analysis results based on the provided configuration variables and\n    stores the intermediate results in a queue.\n    \"\"\"\n    grouped_results = []\n    for i in range(lower_bound, upper_bound):\n        analysis_i = MotionAnalysis([i, i + 1, vars, True, True, False, None])\n        r = weakref.ref(analysis_i)\n        results_i = dict()\n        results_i['arena'] = analysis_i.one_descriptor_per_arena['arena']\n        results_i['i'] = analysis_i.one_descriptor_per_arena['arena'] - 1\n        arena = results_i['arena']\n        i = arena - 1\n        if not vars['several_blob_per_arena']:\n            # Save basic statistics\n            results_i['one_row_per_arena'] = analysis_i.one_descriptor_per_arena\n            # Save descriptors in long_format\n            results_i['one_row_per_frame'] = analysis_i.one_row_per_frame\n\n        results_i['first_move'] = analysis_i.one_descriptor_per_arena[\"first_move\"]\n        # Save efficiency visualization\n        results_i['efficiency_test_1'] = analysis_i.efficiency_test_1\n        results_i['efficiency_test_2'] = analysis_i.efficiency_test_2\n        grouped_results.append(results_i)\n\n    subtotals.put(grouped_results)\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/","title":"<code>cellects.core.motion_analysis</code>","text":""},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis","title":"<code>cellects.core.motion_analysis</code>","text":"<p>Module for analyzing motion, growth patterns, and structural properties of biological specimens in video data.</p> <p>This module provides comprehensive tools to analyze videos of biological samples (e.g., cell colonies) by: 1. Loading and converting RGB videos to grayscale using configurable color space combinations 2. Performing multi-strategy segmentation (frame-by-frame, intensity thresholding, derivative-based detection) 3. Applying post-processing steps including error correction algorithms for shape continuity 4. Computing morphological descriptors over time (area, perimeter, fractal dimension, etc.) 5. Detecting network structures and oscillatory behavior in dynamic biological systems</p> <p>Classes:</p> Name Description <code>MotionAnalysis : Processes video data to analyze specimen motion, growth patterns, and structural properties.</code> <p>Provides methods for loading videos, performing segmentation using multiple algorithms, post-processing results with error correction, extracting morphological descriptors, detecting network structures, analyzing oscillations, and saving processed outputs.</p> <p>Functions:</p> Name Description <code>load_images_and_videos : Loads and converts video files to appropriate format for analysis.</code> <code>get_converted_video : Converts RGB video to grayscale based on specified color space parameters.</code> <code>detection : Performs multi-strategy segmentation of the specimen across all frames.</code> <code>update_shape : Updates segmented shape with post-processing steps like noise filtering and hole filling.</code> <code>save_results : Saves processed data, efficiency tests, and annotated videos.</code> Notes <p>The features of this module include: - Processes large video datasets with memory optimization strategies including typed arrays (NumPy)   and progressive processing techniques. - The module supports both single-specimen and multi-specimen analysis through configurable parameters. - Segmentation strategies include intensity-based thresholding, gradient detection, and combinations thereof. - Post-processing includes morphological operations to refine segmented regions and error correction for specific use cases (e.g., Physarum polycephalum). - Biological network detection and graph extraction is available to represent network structures as vertex-edge tables. - Biological oscillatory pattern detection - Fractal dimension calculation</p>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.CompareNeighborsWithValue","title":"<code>CompareNeighborsWithValue</code>","text":"<p>CompareNeighborsWithValue class to summarize each pixel by comparing its neighbors to a value.</p> <p>This class analyzes pixels in a 2D array, comparing each pixel's neighbors to a specified value. The comparison can be equality, superiority, or inferiority, and neighbors can be the 4 or 8 nearest pixels based on the connectivity parameter.</p> Source code in <code>src/cellects/image_analysis/morphological_operations.py</code> <pre><code>class CompareNeighborsWithValue:\n    \"\"\"\n    CompareNeighborsWithValue class to summarize each pixel by comparing its neighbors to a value.\n\n    This class analyzes pixels in a 2D array, comparing each pixel's neighbors\n    to a specified value. The comparison can be equality, superiority,\n    or inferiority, and neighbors can be the 4 or 8 nearest pixels based on\n    the connectivity parameter.\n    \"\"\"\n    def __init__(self, array: np.ndarray, connectivity: int=None, data_type: np.dtype=np.int8):\n        \"\"\"\n        Initialize a class for array connectivity processing.\n\n        This class processes arrays based on given connectivities, creating\n        windows around the original data for both 1D and 2D arrays. Depending on\n        the connectivity value (4 or 8), it creates different windows with borders.\n\n        Parameters\n        ----------\n        array : ndarray\n            Input array to process, can be 1D or 2D.\n        connectivity : int, optional\n            Connectivity type for processing (4 or 8), by default None.\n        data_type : dtype, optional\n            Data type for the array elements, by default np.int8.\n\n        Attributes\n        ----------\n        array : ndarray\n            The processed array based on the given data type.\n        connectivity : int\n            Connectivity value used for processing.\n        on_the_right : ndarray\n            Array with shifted elements to the right.\n        on_the_left : ndarray\n            Array with shifted elements to the left.\n        on_the_bot : ndarray, optional\n            Array with shifted elements to the bottom (for 2D arrays).\n        on_the_top : ndarray, optional\n            Array with shifted elements to the top (for 2D arrays).\n        on_the_topleft : ndarray, optional\n            Array with shifted elements to the top left (for 2D arrays).\n        on_the_topright : ndarray, optional\n            Array with shifted elements to the top right (for 2D arrays).\n        on_the_botleft : ndarray, optional\n            Array with shifted elements to the bottom left (for 2D arrays).\n        on_the_botright : ndarray, optional\n            Array with shifted elements to the bottom right (for 2D arrays).\n        \"\"\"\n        array = array.astype(data_type)\n        self.array = array\n        self.connectivity = connectivity\n        if len(self.array.shape) == 1:\n            self.on_the_right = np.append(array[1:], array[-1])\n            self.on_the_left = np.append(array[0], array[:-1])\n        else:\n            # Build 4 window of the original array, each missing one of the four borders\n            # Grow each window with a copy of the last border at the opposite of the side a border have been deleted\n            if self.connectivity == 4 or self.connectivity == 8:\n                self.on_the_right = np.column_stack((array[:, 1:], array[:, -1]))\n                self.on_the_left = np.column_stack((array[:, 0], array[:, :-1]))\n                self.on_the_bot = np.vstack((array[1:, :], array[-1, :]))\n                self.on_the_top = np.vstack((array[0, :], array[:-1, :]))\n            if self.connectivity != 4:\n                self.on_the_topleft = array[:-1, :-1]\n                self.on_the_topright = array[:-1, 1:]\n                self.on_the_botleft = array[1:, :-1]\n                self.on_the_botright = array[1:, 1:]\n\n                self.on_the_topleft = np.vstack((self.on_the_topleft[0, :], self.on_the_topleft))\n                self.on_the_topleft = np.column_stack((self.on_the_topleft[:, 0], self.on_the_topleft))\n\n                self.on_the_topright = np.vstack((self.on_the_topright[0, :], self.on_the_topright))\n                self.on_the_topright = np.column_stack((self.on_the_topright, self.on_the_topright[:, -1]))\n\n                self.on_the_botleft = np.vstack((self.on_the_botleft, self.on_the_botleft[-1, :]))\n                self.on_the_botleft = np.column_stack((self.on_the_botleft[:, 0], self.on_the_botleft))\n\n                self.on_the_botright = np.vstack((self.on_the_botright, self.on_the_botright[-1, :]))\n                self.on_the_botright = np.column_stack((self.on_the_botright, self.on_the_botright[:, -1]))\n\n    def is_equal(self, value, and_itself: bool=False):\n        \"\"\"\n        Check equality of neighboring values in an array.\n\n        This method compares the neighbors of each element in `self.array` to a given value.\n        Depending on the dimensionality and connectivity settings, it checks different neighboring\n        elements.\n\n        Parameters\n        ----------\n        value : int or float\n            The value to check equality with neighboring elements.\n        and_itself : bool, optional\n            If True, also check equality with the element itself. Defaults to False.\n\n        Returns\n        -------\n        None\n\n        Attributes (not standard Qt properties)\n        --------------------------------------\n        equal_neighbor_nb : ndarray of uint8\n            Array that holds the number of equal neighbors for each element.\n\n        Examples\n        --------\n        &gt;&gt;&gt; matrix = np.array([[9, 0, 4, 6], [4, 9, 1, 3], [7, 2, 1, 4], [9, 0, 8, 5]], dtype=np.int8)\n        &gt;&gt;&gt; compare = CompareNeighborsWithValue(matrix, connectivity=4)\n        &gt;&gt;&gt; compare.is_equal(1)\n        &gt;&gt;&gt; print(compare.equal_neighbor_nb)\n        [[0 0 1 0]\n        [0 1 1 1]\n        [0 1 1 1]\n        [0 0 1 0]]\n        \"\"\"\n\n        if len(self.array.shape) == 1:\n            self.equal_neighbor_nb = np.sum((np.equal(self.on_the_right, value), np.equal(self.on_the_left, value)), axis=0)\n        else:\n            if self.connectivity == 4:\n                self.equal_neighbor_nb =  np.dstack((np.equal(self.on_the_right, value), np.equal(self.on_the_left, value),\n                                                 np.equal(self.on_the_bot, value), np.equal(self.on_the_top, value)))\n            elif self.connectivity == 8:\n                self.equal_neighbor_nb =  np.dstack(\n                    (np.equal(self.on_the_right, value), np.equal(self.on_the_left, value),\n                     np.equal(self.on_the_bot, value), np.equal(self.on_the_top, value),\n                     np.equal(self.on_the_topleft, value), np.equal(self.on_the_topright, value),\n                     np.equal(self.on_the_botleft, value), np.equal(self.on_the_botright, value)))\n            else:\n                self.equal_neighbor_nb =  np.dstack(\n                    (np.equal(self.on_the_topleft, value), np.equal(self.on_the_topright, value),\n                     np.equal(self.on_the_botleft, value), np.equal(self.on_the_botright, value)))\n            self.equal_neighbor_nb = np.sum(self.equal_neighbor_nb, 2, dtype=np.uint8)\n\n        if and_itself:\n            self.equal_neighbor_nb[np.not_equal(self.array, value)] = 0\n\n    def is_sup(self, value, and_itself=False):\n        \"\"\"\n        Determine if pixels have more neighbors with higher values than a given threshold.\n\n        This method computes the number of neighboring pixels that have values greater\n        than a specified `value` for each pixel in the array. Optionally, it can exclude\n        the pixel itself if its value is less than or equal to `value`.\n\n        Parameters\n        ----------\n        value : int\n            The threshold value used to determine if a neighboring pixel's value is greater.\n        and_itself : bool, optional\n            If True, exclude the pixel itself if its value is less than or equal to `value`.\n            Defaults to False.\n\n        Examples\n        --------\n        &gt;&gt;&gt; matrix = np.array([[9, 0, 4, 6], [4, 9, 1, 3], [7, 2, 1, 4], [9, 0, 8, 5]], dtype=np.int8)\n        &gt;&gt;&gt; compare = CompareNeighborsWithValue(matrix, connectivity=4)\n        &gt;&gt;&gt; compare.is_sup(1)\n        &gt;&gt;&gt; print(compare.sup_neighbor_nb)\n        [[3 3 2 4]\n         [4 2 3 3]\n         [4 2 3 3]\n         [3 3 2 4]]\n        \"\"\"\n        if len(self.array.shape) == 1:\n            self.sup_neighbor_nb = (self.on_the_right &gt; value).astype(self.array.dtype) + (self.on_the_left &gt; value).astype(self.array.dtype)\n        else:\n            if self.connectivity == 4:\n                self.sup_neighbor_nb =  np.dstack((self.on_the_right &gt; value, self.on_the_left &gt; value,\n                                               self.on_the_bot &gt; value, self.on_the_top &gt; value))\n            elif self.connectivity == 8:\n                self.sup_neighbor_nb =  np.dstack((self.on_the_right &gt; value, self.on_the_left &gt; value,\n                                               self.on_the_bot &gt; value, self.on_the_top &gt; value,\n                                               self.on_the_topleft &gt; value, self.on_the_topright &gt; value,\n                                               self.on_the_botleft &gt; value, self.on_the_botright &gt; value))\n            else:\n                self.sup_neighbor_nb =  np.dstack((self.on_the_topleft &gt; value, self.on_the_topright &gt; value,\n                                               self.on_the_botleft &gt; value, self.on_the_botright &gt; value))\n\n            self.sup_neighbor_nb = np.sum(self.sup_neighbor_nb, 2, dtype=np.uint8)\n        if and_itself:\n            self.sup_neighbor_nb[np.less_equal(self.array, value)] = 0\n\n    def is_inf(self, value, and_itself=False):\n        \"\"\"\n        is_inf(value and_itself=False)\n\n        Determine the number of neighbors that are infinitely small relative to a given value,\n        considering optional connectivity and exclusion of the element itself.\n\n        Parameters\n        ----------\n        value : numeric\n            The value to compare neighbor elements against.\n        and_itself : bool, optional\n            If True, excludes the element itself from being counted. Default is False.\n\n        Examples\n        --------\n        &gt;&gt;&gt; matrix = np.array([[9, 0, 4, 6], [4, 9, 1, 3], [7, 2, 1, 4], [9, 0, 8, 5]], dtype=np.int8)\n        &gt;&gt;&gt; compare = CompareNeighborsWithValue(matrix, connectivity=4)\n        &gt;&gt;&gt; compare.is_inf(1)\n        &gt;&gt;&gt; print(compare.inf_neighbor_nb)\n        [[1 1 1 0]\n         [0 1 0 0]\n         [0 1 0 0]\n         [1 1 1 0]]\n        \"\"\"\n        if len(self.array.shape) == 1:\n            self.inf_neighbor_nb = (self.on_the_right &lt; value).astype(self.array.dtype) + (self.on_the_left &lt; value).astype(self.array.dtype)\n        else:\n            if self.connectivity == 4:\n                self.inf_neighbor_nb =  np.dstack((self.on_the_right &lt; value, self.on_the_left &lt; value,\n                                               self.on_the_bot &lt; value, self.on_the_top &lt; value))\n            elif self.connectivity == 8:\n                self.inf_neighbor_nb =  np.dstack((self.on_the_right &lt; value, self.on_the_left &lt; value,\n                                               self.on_the_bot &lt; value, self.on_the_top &lt; value,\n                                               self.on_the_topleft &lt; value, self.on_the_topright &lt; value,\n                                               self.on_the_botleft &lt; value, self.on_the_botright &lt; value))\n            else:\n                self.inf_neighbor_nb =  np.dstack((self.on_the_topleft &lt; value, self.on_the_topright &lt; value,\n                                               self.on_the_botleft &lt; value, self.on_the_botright &lt; value))\n\n            self.inf_neighbor_nb = np.sum(self.inf_neighbor_nb, 2, dtype=np.uint8)\n        if and_itself:\n            self.inf_neighbor_nb[np.greater_equal(self.array, value)] = 0\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.CompareNeighborsWithValue.__init__","title":"<code>__init__(array, connectivity=None, data_type=np.int8)</code>","text":"<p>Initialize a class for array connectivity processing.</p> <p>This class processes arrays based on given connectivities, creating windows around the original data for both 1D and 2D arrays. Depending on the connectivity value (4 or 8), it creates different windows with borders.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>ndarray</code> <p>Input array to process, can be 1D or 2D.</p> required <code>connectivity</code> <code>int</code> <p>Connectivity type for processing (4 or 8), by default None.</p> <code>None</code> <code>data_type</code> <code>dtype</code> <p>Data type for the array elements, by default np.int8.</p> <code>int8</code> <p>Attributes:</p> Name Type Description <code>array</code> <code>ndarray</code> <p>The processed array based on the given data type.</p> <code>connectivity</code> <code>int</code> <p>Connectivity value used for processing.</p> <code>on_the_right</code> <code>ndarray</code> <p>Array with shifted elements to the right.</p> <code>on_the_left</code> <code>ndarray</code> <p>Array with shifted elements to the left.</p> <code>on_the_bot</code> <code>(ndarray, optional)</code> <p>Array with shifted elements to the bottom (for 2D arrays).</p> <code>on_the_top</code> <code>(ndarray, optional)</code> <p>Array with shifted elements to the top (for 2D arrays).</p> <code>on_the_topleft</code> <code>(ndarray, optional)</code> <p>Array with shifted elements to the top left (for 2D arrays).</p> <code>on_the_topright</code> <code>(ndarray, optional)</code> <p>Array with shifted elements to the top right (for 2D arrays).</p> <code>on_the_botleft</code> <code>(ndarray, optional)</code> <p>Array with shifted elements to the bottom left (for 2D arrays).</p> <code>on_the_botright</code> <code>(ndarray, optional)</code> <p>Array with shifted elements to the bottom right (for 2D arrays).</p> Source code in <code>src/cellects/image_analysis/morphological_operations.py</code> <pre><code>def __init__(self, array: np.ndarray, connectivity: int=None, data_type: np.dtype=np.int8):\n    \"\"\"\n    Initialize a class for array connectivity processing.\n\n    This class processes arrays based on given connectivities, creating\n    windows around the original data for both 1D and 2D arrays. Depending on\n    the connectivity value (4 or 8), it creates different windows with borders.\n\n    Parameters\n    ----------\n    array : ndarray\n        Input array to process, can be 1D or 2D.\n    connectivity : int, optional\n        Connectivity type for processing (4 or 8), by default None.\n    data_type : dtype, optional\n        Data type for the array elements, by default np.int8.\n\n    Attributes\n    ----------\n    array : ndarray\n        The processed array based on the given data type.\n    connectivity : int\n        Connectivity value used for processing.\n    on_the_right : ndarray\n        Array with shifted elements to the right.\n    on_the_left : ndarray\n        Array with shifted elements to the left.\n    on_the_bot : ndarray, optional\n        Array with shifted elements to the bottom (for 2D arrays).\n    on_the_top : ndarray, optional\n        Array with shifted elements to the top (for 2D arrays).\n    on_the_topleft : ndarray, optional\n        Array with shifted elements to the top left (for 2D arrays).\n    on_the_topright : ndarray, optional\n        Array with shifted elements to the top right (for 2D arrays).\n    on_the_botleft : ndarray, optional\n        Array with shifted elements to the bottom left (for 2D arrays).\n    on_the_botright : ndarray, optional\n        Array with shifted elements to the bottom right (for 2D arrays).\n    \"\"\"\n    array = array.astype(data_type)\n    self.array = array\n    self.connectivity = connectivity\n    if len(self.array.shape) == 1:\n        self.on_the_right = np.append(array[1:], array[-1])\n        self.on_the_left = np.append(array[0], array[:-1])\n    else:\n        # Build 4 window of the original array, each missing one of the four borders\n        # Grow each window with a copy of the last border at the opposite of the side a border have been deleted\n        if self.connectivity == 4 or self.connectivity == 8:\n            self.on_the_right = np.column_stack((array[:, 1:], array[:, -1]))\n            self.on_the_left = np.column_stack((array[:, 0], array[:, :-1]))\n            self.on_the_bot = np.vstack((array[1:, :], array[-1, :]))\n            self.on_the_top = np.vstack((array[0, :], array[:-1, :]))\n        if self.connectivity != 4:\n            self.on_the_topleft = array[:-1, :-1]\n            self.on_the_topright = array[:-1, 1:]\n            self.on_the_botleft = array[1:, :-1]\n            self.on_the_botright = array[1:, 1:]\n\n            self.on_the_topleft = np.vstack((self.on_the_topleft[0, :], self.on_the_topleft))\n            self.on_the_topleft = np.column_stack((self.on_the_topleft[:, 0], self.on_the_topleft))\n\n            self.on_the_topright = np.vstack((self.on_the_topright[0, :], self.on_the_topright))\n            self.on_the_topright = np.column_stack((self.on_the_topright, self.on_the_topright[:, -1]))\n\n            self.on_the_botleft = np.vstack((self.on_the_botleft, self.on_the_botleft[-1, :]))\n            self.on_the_botleft = np.column_stack((self.on_the_botleft[:, 0], self.on_the_botleft))\n\n            self.on_the_botright = np.vstack((self.on_the_botright, self.on_the_botright[-1, :]))\n            self.on_the_botright = np.column_stack((self.on_the_botright, self.on_the_botright[:, -1]))\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.CompareNeighborsWithValue.is_equal","title":"<code>is_equal(value, and_itself=False)</code>","text":"<p>Check equality of neighboring values in an array.</p> <p>This method compares the neighbors of each element in <code>self.array</code> to a given value. Depending on the dimensionality and connectivity settings, it checks different neighboring elements.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>int or float</code> <p>The value to check equality with neighboring elements.</p> required <code>and_itself</code> <code>bool</code> <p>If True, also check equality with the element itself. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> Attributes (not standard Qt properties) <p>equal_neighbor_nb : ndarray of uint8     Array that holds the number of equal neighbors for each element.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; matrix = np.array([[9, 0, 4, 6], [4, 9, 1, 3], [7, 2, 1, 4], [9, 0, 8, 5]], dtype=np.int8)\n&gt;&gt;&gt; compare = CompareNeighborsWithValue(matrix, connectivity=4)\n&gt;&gt;&gt; compare.is_equal(1)\n&gt;&gt;&gt; print(compare.equal_neighbor_nb)\n[[0 0 1 0]\n[0 1 1 1]\n[0 1 1 1]\n[0 0 1 0]]\n</code></pre> Source code in <code>src/cellects/image_analysis/morphological_operations.py</code> <pre><code>def is_equal(self, value, and_itself: bool=False):\n    \"\"\"\n    Check equality of neighboring values in an array.\n\n    This method compares the neighbors of each element in `self.array` to a given value.\n    Depending on the dimensionality and connectivity settings, it checks different neighboring\n    elements.\n\n    Parameters\n    ----------\n    value : int or float\n        The value to check equality with neighboring elements.\n    and_itself : bool, optional\n        If True, also check equality with the element itself. Defaults to False.\n\n    Returns\n    -------\n    None\n\n    Attributes (not standard Qt properties)\n    --------------------------------------\n    equal_neighbor_nb : ndarray of uint8\n        Array that holds the number of equal neighbors for each element.\n\n    Examples\n    --------\n    &gt;&gt;&gt; matrix = np.array([[9, 0, 4, 6], [4, 9, 1, 3], [7, 2, 1, 4], [9, 0, 8, 5]], dtype=np.int8)\n    &gt;&gt;&gt; compare = CompareNeighborsWithValue(matrix, connectivity=4)\n    &gt;&gt;&gt; compare.is_equal(1)\n    &gt;&gt;&gt; print(compare.equal_neighbor_nb)\n    [[0 0 1 0]\n    [0 1 1 1]\n    [0 1 1 1]\n    [0 0 1 0]]\n    \"\"\"\n\n    if len(self.array.shape) == 1:\n        self.equal_neighbor_nb = np.sum((np.equal(self.on_the_right, value), np.equal(self.on_the_left, value)), axis=0)\n    else:\n        if self.connectivity == 4:\n            self.equal_neighbor_nb =  np.dstack((np.equal(self.on_the_right, value), np.equal(self.on_the_left, value),\n                                             np.equal(self.on_the_bot, value), np.equal(self.on_the_top, value)))\n        elif self.connectivity == 8:\n            self.equal_neighbor_nb =  np.dstack(\n                (np.equal(self.on_the_right, value), np.equal(self.on_the_left, value),\n                 np.equal(self.on_the_bot, value), np.equal(self.on_the_top, value),\n                 np.equal(self.on_the_topleft, value), np.equal(self.on_the_topright, value),\n                 np.equal(self.on_the_botleft, value), np.equal(self.on_the_botright, value)))\n        else:\n            self.equal_neighbor_nb =  np.dstack(\n                (np.equal(self.on_the_topleft, value), np.equal(self.on_the_topright, value),\n                 np.equal(self.on_the_botleft, value), np.equal(self.on_the_botright, value)))\n        self.equal_neighbor_nb = np.sum(self.equal_neighbor_nb, 2, dtype=np.uint8)\n\n    if and_itself:\n        self.equal_neighbor_nb[np.not_equal(self.array, value)] = 0\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.CompareNeighborsWithValue.is_inf","title":"<code>is_inf(value, and_itself=False)</code>","text":"<p>is_inf(value and_itself=False)</p> <p>Determine the number of neighbors that are infinitely small relative to a given value, considering optional connectivity and exclusion of the element itself.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>numeric</code> <p>The value to compare neighbor elements against.</p> required <code>and_itself</code> <code>bool</code> <p>If True, excludes the element itself from being counted. Default is False.</p> <code>False</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; matrix = np.array([[9, 0, 4, 6], [4, 9, 1, 3], [7, 2, 1, 4], [9, 0, 8, 5]], dtype=np.int8)\n&gt;&gt;&gt; compare = CompareNeighborsWithValue(matrix, connectivity=4)\n&gt;&gt;&gt; compare.is_inf(1)\n&gt;&gt;&gt; print(compare.inf_neighbor_nb)\n[[1 1 1 0]\n [0 1 0 0]\n [0 1 0 0]\n [1 1 1 0]]\n</code></pre> Source code in <code>src/cellects/image_analysis/morphological_operations.py</code> <pre><code>def is_inf(self, value, and_itself=False):\n    \"\"\"\n    is_inf(value and_itself=False)\n\n    Determine the number of neighbors that are infinitely small relative to a given value,\n    considering optional connectivity and exclusion of the element itself.\n\n    Parameters\n    ----------\n    value : numeric\n        The value to compare neighbor elements against.\n    and_itself : bool, optional\n        If True, excludes the element itself from being counted. Default is False.\n\n    Examples\n    --------\n    &gt;&gt;&gt; matrix = np.array([[9, 0, 4, 6], [4, 9, 1, 3], [7, 2, 1, 4], [9, 0, 8, 5]], dtype=np.int8)\n    &gt;&gt;&gt; compare = CompareNeighborsWithValue(matrix, connectivity=4)\n    &gt;&gt;&gt; compare.is_inf(1)\n    &gt;&gt;&gt; print(compare.inf_neighbor_nb)\n    [[1 1 1 0]\n     [0 1 0 0]\n     [0 1 0 0]\n     [1 1 1 0]]\n    \"\"\"\n    if len(self.array.shape) == 1:\n        self.inf_neighbor_nb = (self.on_the_right &lt; value).astype(self.array.dtype) + (self.on_the_left &lt; value).astype(self.array.dtype)\n    else:\n        if self.connectivity == 4:\n            self.inf_neighbor_nb =  np.dstack((self.on_the_right &lt; value, self.on_the_left &lt; value,\n                                           self.on_the_bot &lt; value, self.on_the_top &lt; value))\n        elif self.connectivity == 8:\n            self.inf_neighbor_nb =  np.dstack((self.on_the_right &lt; value, self.on_the_left &lt; value,\n                                           self.on_the_bot &lt; value, self.on_the_top &lt; value,\n                                           self.on_the_topleft &lt; value, self.on_the_topright &lt; value,\n                                           self.on_the_botleft &lt; value, self.on_the_botright &lt; value))\n        else:\n            self.inf_neighbor_nb =  np.dstack((self.on_the_topleft &lt; value, self.on_the_topright &lt; value,\n                                           self.on_the_botleft &lt; value, self.on_the_botright &lt; value))\n\n        self.inf_neighbor_nb = np.sum(self.inf_neighbor_nb, 2, dtype=np.uint8)\n    if and_itself:\n        self.inf_neighbor_nb[np.greater_equal(self.array, value)] = 0\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.CompareNeighborsWithValue.is_sup","title":"<code>is_sup(value, and_itself=False)</code>","text":"<p>Determine if pixels have more neighbors with higher values than a given threshold.</p> <p>This method computes the number of neighboring pixels that have values greater than a specified <code>value</code> for each pixel in the array. Optionally, it can exclude the pixel itself if its value is less than or equal to <code>value</code>.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>int</code> <p>The threshold value used to determine if a neighboring pixel's value is greater.</p> required <code>and_itself</code> <code>bool</code> <p>If True, exclude the pixel itself if its value is less than or equal to <code>value</code>. Defaults to False.</p> <code>False</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; matrix = np.array([[9, 0, 4, 6], [4, 9, 1, 3], [7, 2, 1, 4], [9, 0, 8, 5]], dtype=np.int8)\n&gt;&gt;&gt; compare = CompareNeighborsWithValue(matrix, connectivity=4)\n&gt;&gt;&gt; compare.is_sup(1)\n&gt;&gt;&gt; print(compare.sup_neighbor_nb)\n[[3 3 2 4]\n [4 2 3 3]\n [4 2 3 3]\n [3 3 2 4]]\n</code></pre> Source code in <code>src/cellects/image_analysis/morphological_operations.py</code> <pre><code>def is_sup(self, value, and_itself=False):\n    \"\"\"\n    Determine if pixels have more neighbors with higher values than a given threshold.\n\n    This method computes the number of neighboring pixels that have values greater\n    than a specified `value` for each pixel in the array. Optionally, it can exclude\n    the pixel itself if its value is less than or equal to `value`.\n\n    Parameters\n    ----------\n    value : int\n        The threshold value used to determine if a neighboring pixel's value is greater.\n    and_itself : bool, optional\n        If True, exclude the pixel itself if its value is less than or equal to `value`.\n        Defaults to False.\n\n    Examples\n    --------\n    &gt;&gt;&gt; matrix = np.array([[9, 0, 4, 6], [4, 9, 1, 3], [7, 2, 1, 4], [9, 0, 8, 5]], dtype=np.int8)\n    &gt;&gt;&gt; compare = CompareNeighborsWithValue(matrix, connectivity=4)\n    &gt;&gt;&gt; compare.is_sup(1)\n    &gt;&gt;&gt; print(compare.sup_neighbor_nb)\n    [[3 3 2 4]\n     [4 2 3 3]\n     [4 2 3 3]\n     [3 3 2 4]]\n    \"\"\"\n    if len(self.array.shape) == 1:\n        self.sup_neighbor_nb = (self.on_the_right &gt; value).astype(self.array.dtype) + (self.on_the_left &gt; value).astype(self.array.dtype)\n    else:\n        if self.connectivity == 4:\n            self.sup_neighbor_nb =  np.dstack((self.on_the_right &gt; value, self.on_the_left &gt; value,\n                                           self.on_the_bot &gt; value, self.on_the_top &gt; value))\n        elif self.connectivity == 8:\n            self.sup_neighbor_nb =  np.dstack((self.on_the_right &gt; value, self.on_the_left &gt; value,\n                                           self.on_the_bot &gt; value, self.on_the_top &gt; value,\n                                           self.on_the_topleft &gt; value, self.on_the_topright &gt; value,\n                                           self.on_the_botleft &gt; value, self.on_the_botright &gt; value))\n        else:\n            self.sup_neighbor_nb =  np.dstack((self.on_the_topleft &gt; value, self.on_the_topright &gt; value,\n                                           self.on_the_botleft &gt; value, self.on_the_botright &gt; value))\n\n        self.sup_neighbor_nb = np.sum(self.sup_neighbor_nb, 2, dtype=np.uint8)\n    if and_itself:\n        self.sup_neighbor_nb[np.less_equal(self.array, value)] = 0\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.EdgeIdentification","title":"<code>EdgeIdentification</code>","text":"<p>Initialize the class with skeleton and distance arrays.</p> <p>This class is used to identify edges within a skeleton structure based on provided skeleton and distance arrays. It performs various operations to refine and label edges, ultimately producing a fully identified network.</p> Source code in <code>src/cellects/image_analysis/network_functions.py</code> <pre><code>class EdgeIdentification:\n    \"\"\"Initialize the class with skeleton and distance arrays.\n\n    This class is used to identify edges within a skeleton structure based on\n    provided skeleton and distance arrays. It performs various operations to\n    refine and label edges, ultimately producing a fully identified network.\n    \"\"\"\n    def __init__(self, pad_skeleton: NDArray[np.uint8], pad_distances: NDArray[np.float64], t: int=0):\n        \"\"\"\n        Initialize the class with skeleton and distance arrays.\n\n        Parameters\n        ----------\n        pad_skeleton : ndarray of uint8\n            Array representing the skeleton to pad.\n        pad_distances : ndarray of float64\n            Array representing distances corresponding to the skeleton.\n\n        Attributes\n        ----------\n        remaining_vertices : None\n            Remaining vertices. Initialized as `None`.\n        vertices : None\n            Vertices. Initialized as `None`.\n        growing_vertices : None\n            Growing vertices. Initialized as `None`.\n        im_shape : tuple of ints\n            Shape of the skeleton array.\n        \"\"\"\n        self.pad_skeleton = pad_skeleton\n        self.pad_distances = pad_distances\n        self.t = t\n        self.remaining_vertices = None\n        self.vertices = None\n        self.growing_vertices = None\n        self.im_shape = pad_skeleton.shape\n\n    def run_edge_identification(self):\n        \"\"\"\n        Run the edge identification process.\n\n        This method orchestrates a series of steps to identify and label edges\n        within the graph structure. Each step handles a specific aspect of edge\n        identification, ultimately leading to a clearer and more refined edge network.\n\n        Steps involved:\n        1. Get vertices and tips coordinates.\n        2. Identify tipped edges.\n        3. Remove tipped edges smaller than branch width.\n        4. Label tipped edges and their vertices.\n        5. Label edges connected with vertex clusters.\n        6. Label edges connecting vertex clusters.\n        7. Label edges from known vertices iteratively.\n        8. Label edges looping on 1 vertex.\n        9. Clear areas with 1 or 2 unidentified pixels.\n        10. Clear edge duplicates.\n        11. Clear vertices connecting 2 edges.\n        \"\"\"\n        self.get_vertices_and_tips_coord()\n        self.get_tipped_edges()\n        self.remove_tipped_edge_smaller_than_branch_width()\n        self.label_tipped_edges_and_their_vertices()\n        self.check_vertex_existence()\n        self.label_edges_connected_with_vertex_clusters()\n        self.label_edges_connecting_vertex_clusters()\n        self.label_edges_from_known_vertices_iteratively()\n        self.label_edges_looping_on_1_vertex()\n        self.clear_areas_of_1_or_2_unidentified_pixels()\n        self.clear_edge_duplicates()\n        self.clear_vertices_connecting_2_edges()\n\n    def get_vertices_and_tips_coord(self):\n        \"\"\"Process skeleton data to extract non-tip vertices and tip coordinates.\n\n        This method processes the skeleton stored in `self.pad_skeleton` by first\n        extracting all vertices and tips. It then separates these into branch points\n        (non-tip vertices) and specific tip coordinates using internal processing.\n\n        Attributes\n        ----------\n        self.non_tip_vertices : array-like\n            Coordinates of non-tip (branch) vertices.\n        self.tips_coord : array-like\n            Coordinates of identified tips in the skeleton.\n        \"\"\"\n        pad_vertices, pad_tips = get_vertices_and_tips_from_skeleton(self.pad_skeleton)\n        self.non_tip_vertices, self.tips_coord = get_branches_and_tips_coord(pad_vertices, pad_tips)\n\n    def get_tipped_edges(self):\n        \"\"\"\n        get_tipped_edges : method to extract skeleton edges connecting branching points and tips.\n\n        Makes sure that there is only one connected component constituting the skeleton of the network and\n        identifies all edges that are connected to a tip.\n\n        Attributes\n        ----------\n        pad_skeleton : ndarray of bool, modified\n            Boolean mask representing the pruned skeleton after isolating the largest connected component.\n        vertices_branching_tips : ndarray of int, shape (N, 2)\n            Coordinates of branching points that connect to tips in the skeleton structure.\n        edge_lengths : ndarray of float, shape (M,)\n            Lengths of edges connecting non-tip vertices to identified tip locations.\n        edge_pix_coord : list of array of int\n            Pixel coordinates for each edge path between connected skeleton elements.\n\n        \"\"\"\n        self.pad_skeleton = keep_one_connected_component(self.pad_skeleton)\n        self.vertices_branching_tips, self.edge_lengths, self.edge_pix_coord = _find_closest_vertices(self.pad_skeleton,\n                                                                                        self.non_tip_vertices,\n                                                                                        self.tips_coord[:, :2])\n\n    def remove_tipped_edge_smaller_than_branch_width(self):\n        \"\"\"Remove very short edges from the skeleton.\n\n        This method focuses on edges connecting tips. When too short, they are considered are noise and\n        removed from the skeleton and distances matrices. These edges are considered too short when their length is\n        smaller than the width of the nearest network branch (an information included in pad_distances).\n        This method also updates internal data structures (skeleton, edge coordinates, vertex/tip positions)\n        accordingly through pixel-wise analysis and connectivity checks.\n        \"\"\"\n        # Identify edges that are smaller than the width of the branch it is attached to\n        tipped_edges_to_remove = np.zeros(self.edge_lengths.shape[0], dtype=bool)\n        # connecting_vertices_to_remove = np.zeros(self.vertices_branching_tips.shape[0], dtype=bool)\n        branches_to_remove = np.zeros(self.non_tip_vertices.shape[0], dtype=bool)\n        new_edge_pix_coord = []\n        remaining_tipped_edges_nb = 0\n        for i in range(len(self.edge_lengths)): # i = 3142 #1096 # 974 # 222\n            Y, X = self.vertices_branching_tips[i, 0], self.vertices_branching_tips[i, 1]\n            edge_bool = self.edge_pix_coord[:, 2] == i + 1\n            eY, eX = self.edge_pix_coord[edge_bool, 0], self.edge_pix_coord[edge_bool, 1]\n            if np.nanmax(self.pad_distances[(Y - 1): (Y + 2), (X - 1): (X + 2)]) &gt;= self.edge_lengths[i]:\n                tipped_edges_to_remove[i] = True\n                # Remove the edge\n                self.pad_skeleton[eY, eX] = 0\n                # Remove the tip\n                self.pad_skeleton[self.tips_coord[i, 0], self.tips_coord[i, 1]] = 0\n\n                # Remove the coordinates corresponding to that edge\n                self.edge_pix_coord = np.delete(self.edge_pix_coord, edge_bool, 0)\n\n                # check whether the connecting vertex remains a vertex of not\n                pad_sub_skeleton = np.pad(self.pad_skeleton[(Y - 2): (Y + 3), (X - 2): (X + 3)], [(1,), (1,)],\n                                          mode='constant')\n                sub_vertices, sub_tips = get_vertices_and_tips_from_skeleton(pad_sub_skeleton)\n                # If the vertex does not connect at least 3 edges anymore, remove its vertex label\n                if sub_vertices[3, 3] == 0:\n                    vertex_to_remove = np.nonzero(np.logical_and(self.non_tip_vertices[:, 0] == Y, self.non_tip_vertices[:, 1] == X))[0]\n                    branches_to_remove[vertex_to_remove] = True\n                # If that pixel became a tip connected to another vertex remove it from the skeleton\n                if sub_tips[3, 3]:\n                    if sub_vertices[2:5, 2:5].sum() &gt; 1:\n                        self.pad_skeleton[Y, X] = 0\n                        self.edge_pix_coord = np.delete(self.edge_pix_coord, np.all(self.edge_pix_coord[:, :2] == [Y, X], axis=1), 0)\n                        vertex_to_remove = np.nonzero(np.logical_and(self.non_tip_vertices[:, 0] == Y, self.non_tip_vertices[:, 1] == X))[0]\n                        branches_to_remove[vertex_to_remove] = True\n            else:\n                remaining_tipped_edges_nb += 1\n                new_edge_pix_coord.append(np.stack((eY, eX, np.repeat(remaining_tipped_edges_nb, len(eY))), axis=1))\n\n        # Check that excedent connected components are 1 pixel size, if so:\n        # It means that they were neighbors to removed tips and not necessary for the skeleton\n        nb, sh = cv2.connectedComponents(self.pad_skeleton)\n        if nb &gt; 2:\n            logging.error(\"Removing small tipped edges split the skeleton\")\n            # for i in range(2, nb):\n            #     excedent = sh == i\n            #     if (excedent).sum() == 1:\n            #         self.pad_skeleton[excedent] = 0\n\n        # Remove in distances the pixels removed in skeleton:\n        self.pad_distances *= self.pad_skeleton\n\n        # update edge_pix_coord\n        if len(new_edge_pix_coord) &gt; 0:\n            self.edge_pix_coord = np.vstack(new_edge_pix_coord)\n\n        # # Remove tips connected to very small edges\n        # self.tips_coord = np.delete(self.tips_coord, tipped_edges_to_remove, 0)\n        # # Add corresponding edge names\n        # self.tips_coord = np.hstack((self.tips_coord, np.arange(1, len(self.tips_coord) + 1)[:, None]))\n\n        # # Within all branching (non-tip) vertices, keep those that did not lose their vertex status because of the edge removal\n        # self.non_tip_vertices = np.delete(self.non_tip_vertices, branches_to_remove, 0)\n\n        # # Get the branching vertices who kept their typped edge\n        # self.vertices_branching_tips = np.delete(self.vertices_branching_tips, tipped_edges_to_remove, 0)\n\n        # Within all branching (non-tip) vertices, keep those that do not connect a tipped edge.\n        # v_branching_tips_in_branching_v = find_common_coord(self.non_tip_vertices, self.vertices_branching_tips[:, :2])\n        # self.remaining_vertices = np.delete(self.non_tip_vertices, v_branching_tips_in_branching_v, 0)\n        # ordered_v_coord = np.vstack((self.tips_coord[:, :2], self.vertices_branching_tips[:, :2], self.remaining_vertices))\n\n        # tips = self.tips_coord\n        # branching_any_edge = self.non_tip_vertices\n        # branching_typped_edges = self.vertices_branching_tips\n        # branching_no_typped_edges = self.remaining_vertices\n\n        self.get_vertices_and_tips_coord()\n        self.get_tipped_edges()\n\n    def label_tipped_edges_and_their_vertices(self):\n        \"\"\"Label edges connecting tip vertices to branching vertices and assign unique labels to all relevant vertices.\n\n        Processes vertex coordinates by stacking tips, vertices branching from tips, and remaining non-tip vertices.\n        Assigns unique sequential identifiers to these vertices in a new array. Constructs an array of edge-label information,\n        where each row contains the edge label (starting at 1), corresponding tip label, and connected vertex label.\n\n        Attributes\n        ----------\n        tip_number : int\n            The number of tip coordinates available in `tips_coord`.\n\n        ordered_v_coord : ndarray of float\n            Stack of unique vertex coordinates ordered by: tips first, vertices branching tips second, non-tip vertices third.\n\n        numbered_vertices : ndarray of uint32\n            2D array where each coordinate position is labeled with a sequential integer (starting at 1) based on the order in `ordered_v_coord`.\n\n        edges_labels : ndarray of uint32\n            Array of shape (n_edges, 3). Each row contains:\n            - Edge label (sequential from 1 to n_edges)\n            - Label of the tip vertex for that edge.\n            - Label of the vertex branching the tip.\n\n        vertices_branching_tips : ndarray of float\n            Unique coordinates of vertices directly connected to tips after removing duplicates.\n        \"\"\"\n        self.tip_number = self.tips_coord.shape[0]\n\n        # Stack vertex coordinates in that order: 1. Tips, 2. Vertices branching tips, 3. All remaining vertices\n        ordered_v_coord = np.vstack((self.tips_coord[:, :2], self.vertices_branching_tips[:, :2], self.non_tip_vertices))\n        ordered_v_coord = np.unique(ordered_v_coord, axis=0)\n\n        # Create arrays to store edges and vertices labels\n        self.numbered_vertices = np.zeros(self.im_shape, dtype=np.uint32)\n        self.numbered_vertices[ordered_v_coord[:, 0], ordered_v_coord[:, 1]] = np.arange(1, ordered_v_coord.shape[0] + 1)\n        self.vertices = None\n        self.vertex_index_map = {}\n        for idx, (y, x) in enumerate(ordered_v_coord):\n            self.vertex_index_map[idx + 1] = tuple((np.uint32(y), np.uint32(x)))\n\n        # Name edges from 1 to the number of edges connecting tips and set the vertices labels from all tips to their connected vertices:\n        self.edges_labels = np.zeros((self.tip_number, 3), dtype=np.uint32)\n        # edge label:\n        self.edges_labels[:, 0] = np.arange(self.tip_number) + 1\n        # tip label:\n        self.edges_labels[:, 1] = self.numbered_vertices[self.tips_coord[:, 0], self.tips_coord[:, 1]]\n        # vertex branching tip label:\n        self.edges_labels[:, 2] = self.numbered_vertices[self.vertices_branching_tips[:, 0], self.vertices_branching_tips[:, 1]]\n\n        # Remove duplicates in vertices_branching_tips\n        self.vertices_branching_tips = np.unique(self.vertices_branching_tips[:, :2], axis=0)\n\n    def check_vertex_existence(self):\n        if self.tips_coord.shape[0] == 0 and self.non_tip_vertices.shape[0] == 0:\n            loop_coord = np.nonzero(self.pad_skeleton)\n            start = 1\n            end = 1\n            vertex_coord = loop_coord[0][0], loop_coord[1][0]\n            self.numbered_vertices[vertex_coord[0], vertex_coord[1]] = 1\n            self.vertex_index_map[1] = vertex_coord\n            self.non_tip_vertices = np.array(vertex_coord)[None, :]\n            new_edge_lengths = len(loop_coord[0]) - 1\n            new_edge_pix_coord = np.transpose(np.vstack(((loop_coord[0][1:], loop_coord[1][1:], np.zeros(new_edge_lengths, dtype=np.int32)))))\n            self.edge_pix_coord = np.zeros((0, 3), dtype=np.int32)\n            self._update_edge_data(start, end, new_edge_lengths, new_edge_pix_coord)\n\n    def label_edges_connected_with_vertex_clusters(self):\n        \"\"\"\n        Identify edges connected to touching vertices by processing vertex clusters.\n\n        This function processes the skeleton to identify edges connecting vertices\n        that are part of touching clusters. It creates a cropped version of the skeleton\n        by removing already detected edges and their tips, then iterates through vertex\n        clusters to explore and identify nearby edges.\n        \"\"\"\n        # I.1. Identify edges connected to touching vertices:\n        # First, create another version of these arrays, where we remove every already detected edge and their tips\n        cropped_skeleton = self.pad_skeleton.copy()\n        cropped_skeleton[self.edge_pix_coord[:, 0], self.edge_pix_coord[:, 1]] = 0\n        cropped_skeleton[self.tips_coord[:, 0], self.tips_coord[:, 1]] = 0\n\n        # non_tip_vertices does not need to be updated yet, because it only contains verified branching vertices\n        cropped_non_tip_vertices = self.non_tip_vertices.copy()\n\n        self.new_level_vertices = None\n        # The problem with vertex_to_vertex_connexion is that since they are not separated by zeros,\n        # they always atract each other instead of exploring other paths.\n        # To fix this, we loop over each vertex group to\n        # 1. Add one edge per inter-vertex connexion inside the group\n        # 2. Remove all except one, and loop as many time as necessary.\n        # Inside that second loop, we explore and identify every edge nearby.\n        # Find every vertex_to_vertex_connexion\n        v_cluster_nb, self.v_cluster_lab, self.v_cluster_stats, vgc = cv2.connectedComponentsWithStats(\n            (self.numbered_vertices &gt; 0).astype(np.uint8), connectivity=8)\n        if v_cluster_nb &gt; 0:\n            max_v_nb = np.max(self.v_cluster_stats[1:, 4])\n            cropped_skeleton_list = []\n            starting_vertices_list = []\n            for v_nb in range(2, max_v_nb + 1):\n                labels = np.nonzero(self.v_cluster_stats[:, 4] == v_nb)[0]\n                coord_list = []\n                for lab in labels:  # lab=labels[0]\n                    coord_list.append(np.nonzero(self.v_cluster_lab == lab))\n                for iter in range(v_nb):\n                    for lab_ in range(labels.shape[0]): # lab=labels[0]\n                        cs = cropped_skeleton.copy()\n                        sv = []\n                        v_c = coord_list[lab_]\n                        # Save the current coordinate in the starting vertices array of this iteration\n                        sv.append([v_c[0][iter], v_c[1][iter]])\n                        # Remove one vertex coordinate to keep it from cs\n                        v_y, v_x = np.delete(v_c[0], iter), np.delete(v_c[1], iter)\n                        cs[v_y, v_x] = 0\n                        cropped_skeleton_list.append(cs)\n                        starting_vertices_list.append(np.array(sv))\n            for cropped_skeleton, starting_vertices in zip(cropped_skeleton_list, starting_vertices_list):\n                _, _ = self._identify_edges_connecting_a_vertex_list(cropped_skeleton, cropped_non_tip_vertices, starting_vertices)\n\n    def label_edges_connecting_vertex_clusters(self):\n        \"\"\"\n        Label edges connecting vertex clusters.\n\n        This method identifies the connections between connected vertices within\n        vertex clusters and labels these edges. It uses the previously found connected\n        vertices, creates an image of the connections, and then identifies\n        and labels the edges between these touching vertices.\n        \"\"\"\n        # I.2. Identify the connexions between connected vertices:\n        all_connected_vertices = np.nonzero(self.v_cluster_stats[:, 4] &gt; 1)[0][1:]\n        all_con_v_im = np.zeros_like(self.pad_skeleton)\n        for v_group in all_connected_vertices:\n            all_con_v_im[self.v_cluster_lab == v_group] = 1\n        cropped_skeleton = all_con_v_im\n        self.vertex_clusters_coord = np.transpose(np.array(np.nonzero(cropped_skeleton)))\n        _, _ = self._identify_edges_connecting_a_vertex_list(cropped_skeleton, self.vertex_clusters_coord, self.vertex_clusters_coord)\n        # self.edges_labels\n        del self.v_cluster_stats\n        del self.v_cluster_lab\n\n    def label_edges_from_known_vertices_iteratively(self):\n        \"\"\"\n        Label edges iteratively from known vertices.\n\n        This method labels edges in an iterative process starting from\n        known vertices. It handles the removal of detected edges and\n        updates the skeleton accordingly, to avoid detecting edges twice.\n        \"\"\"\n        # II/ Identify all remaining edges\n        if self.new_level_vertices is not None:\n            starting_vertices_coord = np.vstack((self.new_level_vertices[:, :2], self.vertices_branching_tips))\n            starting_vertices_coord = np.unique(starting_vertices_coord, axis=0)\n        else:\n            # We start from the vertices connecting tips\n            starting_vertices_coord = self.vertices_branching_tips.copy()\n        # Remove the detected edges from cropped_skeleton:\n        cropped_skeleton = self.pad_skeleton.copy()\n        cropped_skeleton[self.edge_pix_coord[:, 0], self.edge_pix_coord[:, 1]] = 0\n        cropped_skeleton[self.tips_coord[:, 0], self.tips_coord[:, 1]] = 0\n        cropped_skeleton[self.vertex_clusters_coord[:, 0], self.vertex_clusters_coord[:, 1]] = 0\n\n        # Reinitialize cropped_non_tip_vertices to browse all vertices except tips and groups\n        cropped_non_tip_vertices = self.non_tip_vertices.copy()\n        cropped_non_tip_vertices = remove_coordinates(cropped_non_tip_vertices, self.vertex_clusters_coord)\n        del self.vertex_clusters_coord\n        remaining_v = cropped_non_tip_vertices.shape[0] + 1\n        while remaining_v &gt; cropped_non_tip_vertices.shape[0]:\n            remaining_v = cropped_non_tip_vertices.shape[0]\n            cropped_skeleton, cropped_non_tip_vertices = self._identify_edges_connecting_a_vertex_list(cropped_skeleton, cropped_non_tip_vertices, starting_vertices_coord)\n            if self.new_level_vertices is not None:\n                starting_vertices_coord = np.unique(self.new_level_vertices[:, :2], axis=0)\n\n    def label_edges_looping_on_1_vertex(self):\n        \"\"\"\n        Identify and handle edges that form loops around a single vertex.\n        This method processes the skeleton image to find looping edges and updates\n        the edge data structure accordingly.\n        \"\"\"\n        self.identified = np.zeros_like(self.numbered_vertices)\n        self.identified[self.edge_pix_coord[:, 0], self.edge_pix_coord[:, 1]] = 1\n        self.identified[self.non_tip_vertices[:, 0], self.non_tip_vertices[:, 1]] = 1\n        self.identified[self.tips_coord[:, 0], self.tips_coord[:, 1]] = 1\n        unidentified = (1 - self.identified) * self.pad_skeleton\n\n        # Find out the remaining non-identified pixels\n        nb, self.unidentified_shapes, self.unidentified_stats, ce = cv2.connectedComponentsWithStats(unidentified.astype(np.uint8))\n        # Handle the cases where edges are loops over only one vertex\n        looping_edges = np.nonzero(self.unidentified_stats[:, 4 ] &gt; 2)[0][1:]\n        for loop_i in looping_edges: # loop_i = looping_edges[0] loop_i=11 #  zoom_on_nonzero(unique_vertices_im, return_coord=False)\n            edge_i = (self.unidentified_shapes == loop_i).astype(np.uint8)\n            dil_edge_i = cv2.dilate(edge_i, square_33)\n            unique_vertices_im = self.numbered_vertices.copy()\n            unique_vertices_im[self.tips_coord[:, 0], self.tips_coord[:, 1]] = 0\n            unique_vertices_im = dil_edge_i * unique_vertices_im\n            unique_vertices = np.unique(unique_vertices_im)\n            unique_vertices = unique_vertices[unique_vertices &gt; 0]\n            v_nb = len(unique_vertices)\n            new_edge_lengths = edge_i.sum()\n            new_edge_pix_coord = np.transpose(np.vstack((np.nonzero(edge_i))))\n            new_edge_pix_coord = np.hstack((new_edge_pix_coord, np.repeat(1, new_edge_pix_coord.shape[0])[:, None]))\n            if v_nb == 1:\n                start, end = unique_vertices[0], unique_vertices[0]\n                self._update_edge_data(start, end, new_edge_lengths, new_edge_pix_coord)\n            elif v_nb == 2:\n                # The edge loops around a group of connected vertices\n                start, end = unique_vertices[0], unique_vertices[1]\n                self._update_edge_data(start, end, new_edge_lengths, new_edge_pix_coord)\n                # conn_v_nb, conn_v = cv2.connectedComponents((unique_vertices_im &gt; 0).astype(np.uint8))\n                # if len(unique_vertices) == 2 and conn_v_nb == 2:\n            elif v_nb &gt; 2: # The question is: How to choose two vertices so that they link all missing pixels?\n                # 1. Find every edge pixel connected to these vertices\n                vertex_connected_pixels = np.nonzero(cv2.dilate((unique_vertices_im &gt; 0).astype(np.uint8), square_33) * edge_i)\n                # 2. Find the most distant pair of these\n                pix1, pix2 = get_min_or_max_euclidean_pair(vertex_connected_pixels, \"max\")\n                # 3. The two best vertices are the two nearest to these two most distant edge pixels\n                dist_to_pix1 = np.zeros(v_nb, np.float64)\n                dist_to_pix2 = np.zeros(v_nb, np.float64)\n                for _i, v_i in enumerate(unique_vertices):\n                    v_coord = self.vertex_index_map[v_i]\n                    dist_to_pix1[_i] = eudist(pix1, v_coord)\n                    dist_to_pix2[_i] = eudist(pix2, v_coord)\n                start, end = unique_vertices[np.argmin(dist_to_pix1)], unique_vertices[np.argmin(dist_to_pix2)]\n                self._update_edge_data(start, end, new_edge_lengths, new_edge_pix_coord)\n            else:\n                logging.error(f\"t={self.t}, One long edge is not identified: i={loop_i} of length={edge_i.sum()} close to {len(unique_vertices)} vertices.\")\n        self.identified[self.edge_pix_coord[:, 0], self.edge_pix_coord[:, 1]] = 1\n\n    def clear_areas_of_1_or_2_unidentified_pixels(self):\n        \"\"\"Removes 1 or 2 pixel size non-identified areas from the skeleton.\n\n        This function checks whether small non-identified areas (1 or 2 pixels)\n        can be removed without breaking the skeleton structure. It performs\n        a series of operations to ensure only safe removals are made, logging\n        errors if the final skeleton is not fully connected or if some unidentified pixels remain.\n        \"\"\"\n        # Check whether the 1 or 2 pixel size non-identified areas can be removed without breaking the skel\n        one_pix = np.nonzero(self.unidentified_stats[:, 4 ] &lt;= 2)[0] # == 1)[0]\n        cutting_removal = []\n        for pix_i in one_pix: #pix_i=one_pix[0]\n            skel_copy = self.pad_skeleton.copy()\n            y1, y2, x1, x2 = self.unidentified_stats[pix_i, 1], self.unidentified_stats[pix_i, 1] + self.unidentified_stats[pix_i, 3], self.unidentified_stats[pix_i, 0], self.unidentified_stats[pix_i, 0] + self.unidentified_stats[pix_i, 2]\n            skel_copy[y1:y2, x1:x2][self.unidentified_shapes[y1:y2, x1:x2] == pix_i] = 0\n            nb1, sh1 = cv2.connectedComponents(skel_copy.astype(np.uint8), connectivity=8)\n            if nb1 &gt; 2:\n                cutting_removal.append(pix_i)\n            else:\n                self.pad_skeleton[y1:y2, x1:x2][self.unidentified_shapes[y1:y2, x1:x2] == pix_i] = 0\n        if len(cutting_removal) &gt; 0:\n            logging.error(f\"t={self.t}, These pixels break the skeleton when removed: {cutting_removal}\")\n        if (self.identified &gt; 0).sum() != self.pad_skeleton.sum():\n            logging.error(f\"t={self.t}, Proportion of identified pixels in the skeleton: {(self.identified &gt; 0).sum() / self.pad_skeleton.sum()}\")\n        self.pad_distances *= self.pad_skeleton\n        del self.identified\n        del self.unidentified_stats\n        del self.unidentified_shapes\n\n\n    def _identify_edges_connecting_a_vertex_list(self, cropped_skeleton: NDArray[np.uint8], cropped_non_tip_vertices: NDArray, starting_vertices_coord: NDArray) -&gt; Tuple[NDArray[np.uint8], NDArray]:\n        \"\"\"Identify edges connecting a list of vertices within a cropped skeleton.\n\n        This function iteratively connects the closest vertices from starting_vertices_coord to their nearest neighbors,\n        updating the skeleton and removing already connected vertices until no new connections can be made or\n        a maximum number of connections is reached.\n\n        Parameters\n        ----------\n        cropped_skeleton : ndarray of uint8\n            A binary skeleton image where skeletal pixels are marked as 1.\n        cropped_non_tip_vertices : ndarray of int\n            Coordinates of non-tip vertices in the cropped skeleton.\n        starting_vertices_coord : ndarray of int\n            Coordinates of vertices from which to find connections.\n\n        Returns\n        -------\n        cropped_skeleton : ndarray of uint8\n            Updated skeleton with edges marked as 0.\n        cropped_non_tip_vertices : ndarray of int\n            Updated list of non-tip vertices after removing those that have been connected.\n        \"\"\"\n        explored_connexions_per_vertex = 0  # the maximal edge number that can connect a vertex\n        new_connexions = True\n        while new_connexions and explored_connexions_per_vertex &lt; 5 and np.any(cropped_non_tip_vertices) and np.any(starting_vertices_coord):\n\n            explored_connexions_per_vertex += 1\n            # 1. Find the ith closest vertex to each focal vertex\n            ending_vertices_coord, new_edge_lengths, new_edge_pix_coord = _find_closest_vertices(\n                cropped_skeleton, cropped_non_tip_vertices, starting_vertices_coord)\n            if np.isnan(new_edge_lengths).sum() + (new_edge_lengths == 0).sum() == new_edge_lengths.shape[0]:\n                new_connexions = False\n            else:\n                # In new_edge_lengths, zeros are duplicates and nan are lone vertices (from starting_vertices_coord)\n                # Find out which starting_vertices_coord should be kept and which one should be used to save edges\n                no_new_connexion = np.isnan(new_edge_lengths)\n                no_found_connexion = np.logical_or(no_new_connexion, new_edge_lengths == 0)\n                found_connexion = np.logical_not(no_found_connexion)\n\n                # Any vertex_to_vertex_connexions must be analyzed only once. We remove them with the non-connectable vertices\n                vertex_to_vertex_connexions = new_edge_lengths == 1\n\n                # Save edge data\n                start = self.numbered_vertices[\n                    starting_vertices_coord[found_connexion, 0], starting_vertices_coord[found_connexion, 1]]\n                end = self.numbered_vertices[\n                    ending_vertices_coord[found_connexion, 0], ending_vertices_coord[found_connexion, 1]]\n                new_edge_lengths = new_edge_lengths[found_connexion]\n                self._update_edge_data(start, end, new_edge_lengths, new_edge_pix_coord)\n\n                no_new_connexion = np.logical_or(no_new_connexion, vertex_to_vertex_connexions)\n                vertices_to_crop = starting_vertices_coord[no_new_connexion, :]\n\n                # Remove non-connectable and connected_vertices from:\n                cropped_non_tip_vertices = remove_coordinates(cropped_non_tip_vertices, vertices_to_crop)\n                starting_vertices_coord = remove_coordinates(starting_vertices_coord, vertices_to_crop)\n\n                if new_edge_pix_coord.shape[0] &gt; 0:\n                    # Update cropped_skeleton to not identify each edge more than once\n                    cropped_skeleton[new_edge_pix_coord[:, 0], new_edge_pix_coord[:, 1]] = 0\n                # And the starting vertices that cannot connect anymore\n                cropped_skeleton[vertices_to_crop[:, 0], vertices_to_crop[:, 1]] = 0\n\n                if self.new_level_vertices is None:\n                    self.new_level_vertices = ending_vertices_coord[found_connexion, :].copy()\n                else:\n                    self.new_level_vertices = np.vstack((self.new_level_vertices, ending_vertices_coord[found_connexion, :]))\n\n        return cropped_skeleton, cropped_non_tip_vertices\n\n    def _update_edge_data(self, start, end, new_edge_lengths: NDArray, new_edge_pix_coord: NDArray):\n        \"\"\"\n        Update edge data by expanding existing arrays with new edges.\n\n        This method updates the internal edge data structures (lengths,\n        labels, and pixel coordinates) by appending new edges.\n\n        Parameters\n        ----------\n        start : int or ndarray of int\n            The starting vertex label(s) for the new edges.\n        end : int or ndarray of int\n            The ending vertex label(s) for the new edges.\n        new_edge_lengths : ndarray of float\n            The lengths of the new edges to be added.\n        new_edge_pix_coord : ndarray of float\n            The pixel coordinates of the new edges.\n\n        Attributes\n        ----------\n        edge_lengths : ndarray of float\n            The lengths of all edges.\n        edges_labels : ndarray of int\n            The labels for each edge (start and end vertices).\n        edge_pix_coord : ndarray of float\n            The pixel coordinates for all edges.\n        \"\"\"\n        if isinstance(start, np.ndarray):\n            end_idx = len(start)\n            self.edge_lengths = np.concatenate((self.edge_lengths, new_edge_lengths))\n        else:\n            end_idx = 1\n            self.edge_lengths = np.append(self.edge_lengths, new_edge_lengths)\n        start_idx = self.edges_labels.shape[0]\n        new_edges = np.zeros((end_idx, 3), dtype=np.uint32)\n        new_edges[:, 0] = np.arange(start_idx, start_idx + end_idx) + 1  # edge label\n        new_edges[:, 1] = start  # starting vertex label\n        new_edges[:, 2] = end  # ending vertex label\n        self.edges_labels = np.vstack((self.edges_labels, new_edges))\n        # Add the new edge coord\n        if new_edge_pix_coord.shape[0] &gt; 0:\n            # Add the new edge pixel coord\n            new_edge_pix_coord[:, 2] += start_idx\n            self.edge_pix_coord = np.vstack((self.edge_pix_coord, new_edge_pix_coord))\n\n    def clear_edge_duplicates(self):\n        \"\"\"\n        Remove duplicate edges by checking vertices and coordinates.\n\n        This method identifies and removes duplicate edges based on their vertex labels\n        and pixel coordinates. It scans through the edge attributes, compares them,\n        and removes duplicates if they are found.\n        \"\"\"\n        edges_to_remove = []\n        duplicates = find_duplicates_coord(np.vstack((self.edges_labels[:, 1:], self.edges_labels[:, :0:-1])))\n        duplicates = np.logical_or(duplicates[:len(duplicates)//2], duplicates[len(duplicates)//2:])\n        for v in self.edges_labels[duplicates, 1:]: #v = self.edges_labels[duplicates, 1:][4]\n            edge_lab1 = self.edges_labels[np.all(self.edges_labels[:, 1:] == v, axis=1), 0]\n            edge_lab2 = self.edges_labels[np.all(self.edges_labels[:, 1:] == v[::-1], axis=1), 0]\n            edge_labs = np.unique(np.concatenate((edge_lab1, edge_lab2)))\n            for edge_i in range(0, len(edge_labs) - 1):  #  edge_i = 0\n                edge_i_coord = self.edge_pix_coord[self.edge_pix_coord[:, 2] == edge_labs[edge_i], :2]\n                for edge_j in range(edge_i + 1, len(edge_labs)):  #  edge_j = 1\n                    edge_j_coord = self.edge_pix_coord[self.edge_pix_coord[:, 2] == edge_labs[edge_j], :2]\n                    if np.array_equal(edge_i_coord, edge_j_coord):\n                        edges_to_remove.append(edge_labs[edge_j])\n        edges_to_remove = np.unique(edges_to_remove)\n        for edge in edges_to_remove:\n            edge_bool = self.edges_labels[:, 0] != edge\n            self.edges_labels = self.edges_labels[edge_bool, :]\n            self.edge_lengths = self.edge_lengths[edge_bool]\n            self.edge_pix_coord = self.edge_pix_coord[self.edge_pix_coord[:, 2] != edge, :]\n\n\n    def clear_vertices_connecting_2_edges(self):\n        \"\"\"\n        Remove vertices connecting exactly two edges and update edge-related attributes.\n\n        This method identifies vertices that are connected to exactly 2 edges,\n        renames edges, updates edge lengths and vertex coordinates accordingly.\n        It also removes the corresponding vertices from non-tip vertices list.\n        \"\"\"\n        v_labels, v_counts = np.unique(self.edges_labels[:, 1:], return_counts=True)\n        vertices2 = v_labels[v_counts == 2]\n        for vertex2 in vertices2:  # vertex2 = vertices2[0]\n            edge_indices = np.nonzero(self.edges_labels[:, 1:] == vertex2)[0]\n            edge_names = [self.edges_labels[edge_indices[0], 0], self.edges_labels[edge_indices[1], 0]]\n            v_names = np.concatenate((self.edges_labels[edge_indices[0], 1:], self.edges_labels[edge_indices[1], 1:]))\n            v_names = v_names[v_names != vertex2]\n            if len(v_names) &gt; 0: # Otherwise it's a vertex between a normal edge and a loop\n                kept_edge = int(self.edge_lengths[edge_indices[1]] &gt;= self.edge_lengths[edge_indices[0]])\n                # Rename the removed edge by the kept edge name in pix_coord:\n                self.edge_pix_coord[self.edge_pix_coord[:, 2] == edge_names[1 - kept_edge], 2] = edge_names[kept_edge]\n                # Add the removed edge length to the kept edge length (minus 2, corresponding to the removed vertex)\n                self.edge_lengths[self.edges_labels[:, 0] == edge_names[kept_edge]] += self.edge_lengths[self.edges_labels[:, 0] == edge_names[1 - kept_edge]] - 1\n                # Remove the corresponding edge length from the list\n                self.edge_lengths = self.edge_lengths[self.edges_labels[:, 0] != edge_names[1 - kept_edge]]\n                # Rename the vertex of the kept edge in edges_labels\n                self.edges_labels[self.edges_labels[:, 0] == edge_names[kept_edge], 1:] = v_names[1 - kept_edge], v_names[kept_edge]\n                # Remove the removed edge from the edges_labels array\n                self.edges_labels = self.edges_labels[self.edges_labels[:, 0] != edge_names[1 - kept_edge], :]\n                # vY, vX = np.nonzero(self.numbered_vertices == vertex2)\n                # v_idx = np.nonzero(np.all(self.non_tip_vertices == [vY[0], vX[0]], axis=1))\n                vY, vX = self.vertex_index_map[vertex2]\n                v_idx = np.nonzero(np.all(self.non_tip_vertices == [vY, vX], axis=1))\n                self.non_tip_vertices = np.delete(self.non_tip_vertices, v_idx, axis=0)\n        # Sometimes, clearing vertices connecting 2 edges can create edge duplicates, so:\n        self.clear_edge_duplicates()\n\n    def _remove_padding(self):\n        \"\"\"\n        Removes padding from various coordinate arrays.\n\n        This method adjusts the coordinates of edge pixels, tips,\n        and non-tip vertices by subtracting 1 from their x and y values.\n        It also removes padding from the skeleton, distances, and vertices\n        using the `remove_padding` function.\n        \"\"\"\n        self.edge_pix_coord[:, :2] -= 1\n        self.tips_coord[:, :2] -= 1\n        self.non_tip_vertices[:, :2] -= 1\n        del self.vertex_index_map\n        self.skeleton, self.distances, self.vertices = remove_padding(\n            [self.pad_skeleton, self.pad_distances, self.numbered_vertices])\n\n\n    def make_vertex_table(self, origin_contours: NDArray[np.uint8]=None, growing_areas: NDArray=None):\n        \"\"\"\n        Generate a table for the vertices.\n\n        This method constructs and returns a 2D NumPy array holding information\n        about all vertices. Each row corresponds to one vertex identified either\n        by its coordinates in `self.tips_coord` or `self.non_tip_vertices`. The\n        array includes additional information about each vertex, including whether\n        they are food vertices, growing areas, and connected components.\n\n        Parameters\n        ----------\n        origin_contours : ndarray of uint8, optional\n            Binary map to identify food vertices. Default is `None`.\n        growing_areas : ndarray, optional\n            Binary map to identify growing regions. Default is `None`.\n\n        Notes\n        -----\n            The method updates the instance attribute `self.vertex_table` with\n            the generated vertex information.\n        \"\"\"\n        if self.vertices is None:\n            self._remove_padding()\n        self.vertex_table = np.zeros((self.tips_coord.shape[0] + self.non_tip_vertices.shape[0], 6), dtype=self.vertices.dtype)\n        self.vertex_table[:self.tips_coord.shape[0], :2] = self.tips_coord\n        self.vertex_table[self.tips_coord.shape[0]:, :2] = self.non_tip_vertices\n        self.vertex_table[:self.tips_coord.shape[0], 2] = self.vertices[self.tips_coord[:, 0], self.tips_coord[:, 1]]\n        self.vertex_table[self.tips_coord.shape[0]:, 2] = self.vertices[self.non_tip_vertices[:, 0], self.non_tip_vertices[:, 1]]\n        self.vertex_table[:self.tips_coord.shape[0], 3] = 1\n        if origin_contours is not None:\n            food_vertices = self.vertices[origin_contours &gt; 0]\n            food_vertices = food_vertices[food_vertices &gt; 0]\n            self.vertex_table[np.isin(self.vertex_table[:, 2], food_vertices), 4] = 1\n\n        if growing_areas is not None and growing_areas.shape[1] &gt; 0:\n            # growing = np.unique(self.vertices * growing_areas)[1:]\n            growing = np.unique(self.vertices[growing_areas[0], growing_areas[1]])\n            growing = growing[growing &gt; 0]\n            if len(growing) &gt; 0:\n                growing = np.isin(self.vertex_table[:, 2], growing)\n                self.vertex_table[growing, 4] = 2\n\n        nb, sh, stats, cent = cv2.connectedComponentsWithStats((self.vertices &gt; 0).astype(np.uint8))\n        for i, v_i in enumerate(np.nonzero(stats[:, 4] &gt; 1)[0][1:]):\n            v_labs = self.vertices[sh == v_i]\n            for v_lab in v_labs: # v_lab = v_labs[0]\n                self.vertex_table[self.vertex_table[:, 2] == v_lab, 5] = 1\n\n\n    def make_edge_table(self, greyscale: NDArray[np.uint8], compute_BC: bool=False):\n        \"\"\"\n        Generate edge table with length and average intensity information.\n\n        This method processes the vertex coordinates, calculates lengths\n        between vertices for each edge, and computes average width and intensity\n        along the edges. Additionally, it computes edge betweenness centrality\n        for each vertex pair.\n\n        Parameters\n        ----------\n        greyscale : ndarray of uint8\n            Grayscale image.\n        \"\"\"\n        if self.vertices is None:\n            self._remove_padding()\n        self.edge_table = np.zeros((self.edges_labels.shape[0], 7), float) # edge_id, vertex1, vertex2, length, average_width, int, BC\n        self.edge_table[:, :3] = self.edges_labels[:, :]\n        self.edge_table[:, 3] = self.edge_lengths\n        for idx, edge_lab in enumerate(self.edges_labels[:, 0]):\n            edge_coord = self.edge_pix_coord[self.edge_pix_coord[:, 2] == edge_lab, :]\n            pix_widths = self.distances[edge_coord[:, 0], edge_coord[:, 1]]\n            v_id = self.edges_labels[self.edges_labels[:, 0] == edge_lab, 1:][0]\n            v1_coord = self.vertex_table[self.vertex_table[:, 2] == v_id[0], :2][0]#\n            v2_coord = self.vertex_table[self.vertex_table[:, 2] == v_id[1], :2][0]#\n            v1_width, v2_width = self.distances[v1_coord[0], v1_coord[1]], self.distances[v2_coord[0], v2_coord[1]]\n\n            if not np.isnan(v1_width):\n                pix_widths = np.append(pix_widths, v1_width)\n            if not np.isnan(v2_width):\n                pix_widths = np.append(pix_widths, v2_width)\n            if pix_widths.size &gt; 0:\n                self.edge_table[idx, 4] = pix_widths.mean()\n            else:\n                self.edge_table[idx, 4] = np.nan\n            pix_ints = greyscale[edge_coord[:, 0], edge_coord[:, 1]]\n            v1_int, v2_int = greyscale[v1_coord[0], v1_coord[1]], greyscale[v2_coord[0], v2_coord[1]]\n            pix_ints = np.append(pix_ints, (v1_int, v2_int))\n            self.edge_table[idx, 5] = pix_ints.mean()\n\n        if compute_BC:\n            G = nx.from_edgelist(self.edges_labels[:, 1:])\n            e_BC = nx.edge_betweenness_centrality(G, seed=0)\n            self.BC_net = np.zeros_like(self.distances)\n            for v, k in e_BC.items(): # v=(81, 80)\n                v1_coord = self.vertex_table[self.vertex_table[:, 2] == v[0], :2]\n                v2_coord = self.vertex_table[self.vertex_table[:, 2] == v[1], :2]\n                full_coord = np.concatenate((v1_coord, v2_coord))\n                edge_lab1 = self.edges_labels[np.all(self.edges_labels[:, 1:] == v[::-1], axis=1), 0]\n                edge_lab2 = self.edges_labels[np.all(self.edges_labels[:, 1:] == v, axis=1), 0]\n                edge_lab = np.unique(np.concatenate((edge_lab1, edge_lab2)))\n                if len(edge_lab) == 1:\n                    edge_coord = self.edge_pix_coord[self.edge_pix_coord[:, 2] == edge_lab, :2]\n                    full_coord = np.concatenate((full_coord, edge_coord))\n                    self.BC_net[full_coord[:, 0], full_coord[:, 1]] = k\n                    self.edge_table[self.edge_table[:, 0] == edge_lab, 6] = k\n                elif len(edge_lab) &gt; 1:\n                    edge_coord0 = self.edge_pix_coord[self.edge_pix_coord[:, 2] == edge_lab[0], :2]\n                    for edge_i in range(len(edge_lab)): #  edge_i=1\n                        edge_coord = self.edge_pix_coord[self.edge_pix_coord[:, 2] == edge_lab[edge_i], :2]\n                        self.edge_table[self.edge_table[:, 0] == edge_lab[edge_i], 6] = k\n                        full_coord = np.concatenate((full_coord, edge_coord))\n                        self.BC_net[full_coord[:, 0], full_coord[:, 1]] = k\n                        if edge_i &gt; 0 and np.array_equal(edge_coord0, edge_coord):\n                            logging.error(f\"There still is two identical edges: {edge_lab} of len: {len(edge_coord)} linking vertices {v}\")\n                            break\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.EdgeIdentification.__init__","title":"<code>__init__(pad_skeleton, pad_distances, t=0)</code>","text":"<p>Initialize the class with skeleton and distance arrays.</p> <p>Parameters:</p> Name Type Description Default <code>pad_skeleton</code> <code>ndarray of uint8</code> <p>Array representing the skeleton to pad.</p> required <code>pad_distances</code> <code>ndarray of float64</code> <p>Array representing distances corresponding to the skeleton.</p> required <p>Attributes:</p> Name Type Description <code>remaining_vertices</code> <code>None</code> <p>Remaining vertices. Initialized as <code>None</code>.</p> <code>vertices</code> <code>None</code> <p>Vertices. Initialized as <code>None</code>.</p> <code>growing_vertices</code> <code>None</code> <p>Growing vertices. Initialized as <code>None</code>.</p> <code>im_shape</code> <code>tuple of ints</code> <p>Shape of the skeleton array.</p> Source code in <code>src/cellects/image_analysis/network_functions.py</code> <pre><code>def __init__(self, pad_skeleton: NDArray[np.uint8], pad_distances: NDArray[np.float64], t: int=0):\n    \"\"\"\n    Initialize the class with skeleton and distance arrays.\n\n    Parameters\n    ----------\n    pad_skeleton : ndarray of uint8\n        Array representing the skeleton to pad.\n    pad_distances : ndarray of float64\n        Array representing distances corresponding to the skeleton.\n\n    Attributes\n    ----------\n    remaining_vertices : None\n        Remaining vertices. Initialized as `None`.\n    vertices : None\n        Vertices. Initialized as `None`.\n    growing_vertices : None\n        Growing vertices. Initialized as `None`.\n    im_shape : tuple of ints\n        Shape of the skeleton array.\n    \"\"\"\n    self.pad_skeleton = pad_skeleton\n    self.pad_distances = pad_distances\n    self.t = t\n    self.remaining_vertices = None\n    self.vertices = None\n    self.growing_vertices = None\n    self.im_shape = pad_skeleton.shape\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.EdgeIdentification.clear_areas_of_1_or_2_unidentified_pixels","title":"<code>clear_areas_of_1_or_2_unidentified_pixels()</code>","text":"<p>Removes 1 or 2 pixel size non-identified areas from the skeleton.</p> <p>This function checks whether small non-identified areas (1 or 2 pixels) can be removed without breaking the skeleton structure. It performs a series of operations to ensure only safe removals are made, logging errors if the final skeleton is not fully connected or if some unidentified pixels remain.</p> Source code in <code>src/cellects/image_analysis/network_functions.py</code> <pre><code>def clear_areas_of_1_or_2_unidentified_pixels(self):\n    \"\"\"Removes 1 or 2 pixel size non-identified areas from the skeleton.\n\n    This function checks whether small non-identified areas (1 or 2 pixels)\n    can be removed without breaking the skeleton structure. It performs\n    a series of operations to ensure only safe removals are made, logging\n    errors if the final skeleton is not fully connected or if some unidentified pixels remain.\n    \"\"\"\n    # Check whether the 1 or 2 pixel size non-identified areas can be removed without breaking the skel\n    one_pix = np.nonzero(self.unidentified_stats[:, 4 ] &lt;= 2)[0] # == 1)[0]\n    cutting_removal = []\n    for pix_i in one_pix: #pix_i=one_pix[0]\n        skel_copy = self.pad_skeleton.copy()\n        y1, y2, x1, x2 = self.unidentified_stats[pix_i, 1], self.unidentified_stats[pix_i, 1] + self.unidentified_stats[pix_i, 3], self.unidentified_stats[pix_i, 0], self.unidentified_stats[pix_i, 0] + self.unidentified_stats[pix_i, 2]\n        skel_copy[y1:y2, x1:x2][self.unidentified_shapes[y1:y2, x1:x2] == pix_i] = 0\n        nb1, sh1 = cv2.connectedComponents(skel_copy.astype(np.uint8), connectivity=8)\n        if nb1 &gt; 2:\n            cutting_removal.append(pix_i)\n        else:\n            self.pad_skeleton[y1:y2, x1:x2][self.unidentified_shapes[y1:y2, x1:x2] == pix_i] = 0\n    if len(cutting_removal) &gt; 0:\n        logging.error(f\"t={self.t}, These pixels break the skeleton when removed: {cutting_removal}\")\n    if (self.identified &gt; 0).sum() != self.pad_skeleton.sum():\n        logging.error(f\"t={self.t}, Proportion of identified pixels in the skeleton: {(self.identified &gt; 0).sum() / self.pad_skeleton.sum()}\")\n    self.pad_distances *= self.pad_skeleton\n    del self.identified\n    del self.unidentified_stats\n    del self.unidentified_shapes\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.EdgeIdentification.clear_edge_duplicates","title":"<code>clear_edge_duplicates()</code>","text":"<p>Remove duplicate edges by checking vertices and coordinates.</p> <p>This method identifies and removes duplicate edges based on their vertex labels and pixel coordinates. It scans through the edge attributes, compares them, and removes duplicates if they are found.</p> Source code in <code>src/cellects/image_analysis/network_functions.py</code> <pre><code>def clear_edge_duplicates(self):\n    \"\"\"\n    Remove duplicate edges by checking vertices and coordinates.\n\n    This method identifies and removes duplicate edges based on their vertex labels\n    and pixel coordinates. It scans through the edge attributes, compares them,\n    and removes duplicates if they are found.\n    \"\"\"\n    edges_to_remove = []\n    duplicates = find_duplicates_coord(np.vstack((self.edges_labels[:, 1:], self.edges_labels[:, :0:-1])))\n    duplicates = np.logical_or(duplicates[:len(duplicates)//2], duplicates[len(duplicates)//2:])\n    for v in self.edges_labels[duplicates, 1:]: #v = self.edges_labels[duplicates, 1:][4]\n        edge_lab1 = self.edges_labels[np.all(self.edges_labels[:, 1:] == v, axis=1), 0]\n        edge_lab2 = self.edges_labels[np.all(self.edges_labels[:, 1:] == v[::-1], axis=1), 0]\n        edge_labs = np.unique(np.concatenate((edge_lab1, edge_lab2)))\n        for edge_i in range(0, len(edge_labs) - 1):  #  edge_i = 0\n            edge_i_coord = self.edge_pix_coord[self.edge_pix_coord[:, 2] == edge_labs[edge_i], :2]\n            for edge_j in range(edge_i + 1, len(edge_labs)):  #  edge_j = 1\n                edge_j_coord = self.edge_pix_coord[self.edge_pix_coord[:, 2] == edge_labs[edge_j], :2]\n                if np.array_equal(edge_i_coord, edge_j_coord):\n                    edges_to_remove.append(edge_labs[edge_j])\n    edges_to_remove = np.unique(edges_to_remove)\n    for edge in edges_to_remove:\n        edge_bool = self.edges_labels[:, 0] != edge\n        self.edges_labels = self.edges_labels[edge_bool, :]\n        self.edge_lengths = self.edge_lengths[edge_bool]\n        self.edge_pix_coord = self.edge_pix_coord[self.edge_pix_coord[:, 2] != edge, :]\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.EdgeIdentification.clear_vertices_connecting_2_edges","title":"<code>clear_vertices_connecting_2_edges()</code>","text":"<p>Remove vertices connecting exactly two edges and update edge-related attributes.</p> <p>This method identifies vertices that are connected to exactly 2 edges, renames edges, updates edge lengths and vertex coordinates accordingly. It also removes the corresponding vertices from non-tip vertices list.</p> Source code in <code>src/cellects/image_analysis/network_functions.py</code> <pre><code>def clear_vertices_connecting_2_edges(self):\n    \"\"\"\n    Remove vertices connecting exactly two edges and update edge-related attributes.\n\n    This method identifies vertices that are connected to exactly 2 edges,\n    renames edges, updates edge lengths and vertex coordinates accordingly.\n    It also removes the corresponding vertices from non-tip vertices list.\n    \"\"\"\n    v_labels, v_counts = np.unique(self.edges_labels[:, 1:], return_counts=True)\n    vertices2 = v_labels[v_counts == 2]\n    for vertex2 in vertices2:  # vertex2 = vertices2[0]\n        edge_indices = np.nonzero(self.edges_labels[:, 1:] == vertex2)[0]\n        edge_names = [self.edges_labels[edge_indices[0], 0], self.edges_labels[edge_indices[1], 0]]\n        v_names = np.concatenate((self.edges_labels[edge_indices[0], 1:], self.edges_labels[edge_indices[1], 1:]))\n        v_names = v_names[v_names != vertex2]\n        if len(v_names) &gt; 0: # Otherwise it's a vertex between a normal edge and a loop\n            kept_edge = int(self.edge_lengths[edge_indices[1]] &gt;= self.edge_lengths[edge_indices[0]])\n            # Rename the removed edge by the kept edge name in pix_coord:\n            self.edge_pix_coord[self.edge_pix_coord[:, 2] == edge_names[1 - kept_edge], 2] = edge_names[kept_edge]\n            # Add the removed edge length to the kept edge length (minus 2, corresponding to the removed vertex)\n            self.edge_lengths[self.edges_labels[:, 0] == edge_names[kept_edge]] += self.edge_lengths[self.edges_labels[:, 0] == edge_names[1 - kept_edge]] - 1\n            # Remove the corresponding edge length from the list\n            self.edge_lengths = self.edge_lengths[self.edges_labels[:, 0] != edge_names[1 - kept_edge]]\n            # Rename the vertex of the kept edge in edges_labels\n            self.edges_labels[self.edges_labels[:, 0] == edge_names[kept_edge], 1:] = v_names[1 - kept_edge], v_names[kept_edge]\n            # Remove the removed edge from the edges_labels array\n            self.edges_labels = self.edges_labels[self.edges_labels[:, 0] != edge_names[1 - kept_edge], :]\n            # vY, vX = np.nonzero(self.numbered_vertices == vertex2)\n            # v_idx = np.nonzero(np.all(self.non_tip_vertices == [vY[0], vX[0]], axis=1))\n            vY, vX = self.vertex_index_map[vertex2]\n            v_idx = np.nonzero(np.all(self.non_tip_vertices == [vY, vX], axis=1))\n            self.non_tip_vertices = np.delete(self.non_tip_vertices, v_idx, axis=0)\n    # Sometimes, clearing vertices connecting 2 edges can create edge duplicates, so:\n    self.clear_edge_duplicates()\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.EdgeIdentification.get_tipped_edges","title":"<code>get_tipped_edges()</code>","text":"<p>get_tipped_edges : method to extract skeleton edges connecting branching points and tips.</p> <p>Makes sure that there is only one connected component constituting the skeleton of the network and identifies all edges that are connected to a tip.</p> <p>Attributes:</p> Name Type Description <code>pad_skeleton</code> <code>ndarray of bool, modified</code> <p>Boolean mask representing the pruned skeleton after isolating the largest connected component.</p> <code>vertices_branching_tips</code> <code>ndarray of int, shape (N, 2)</code> <p>Coordinates of branching points that connect to tips in the skeleton structure.</p> <code>edge_lengths</code> <code>ndarray of float, shape (M,)</code> <p>Lengths of edges connecting non-tip vertices to identified tip locations.</p> <code>edge_pix_coord</code> <code>list of array of int</code> <p>Pixel coordinates for each edge path between connected skeleton elements.</p> Source code in <code>src/cellects/image_analysis/network_functions.py</code> <pre><code>def get_tipped_edges(self):\n    \"\"\"\n    get_tipped_edges : method to extract skeleton edges connecting branching points and tips.\n\n    Makes sure that there is only one connected component constituting the skeleton of the network and\n    identifies all edges that are connected to a tip.\n\n    Attributes\n    ----------\n    pad_skeleton : ndarray of bool, modified\n        Boolean mask representing the pruned skeleton after isolating the largest connected component.\n    vertices_branching_tips : ndarray of int, shape (N, 2)\n        Coordinates of branching points that connect to tips in the skeleton structure.\n    edge_lengths : ndarray of float, shape (M,)\n        Lengths of edges connecting non-tip vertices to identified tip locations.\n    edge_pix_coord : list of array of int\n        Pixel coordinates for each edge path between connected skeleton elements.\n\n    \"\"\"\n    self.pad_skeleton = keep_one_connected_component(self.pad_skeleton)\n    self.vertices_branching_tips, self.edge_lengths, self.edge_pix_coord = _find_closest_vertices(self.pad_skeleton,\n                                                                                    self.non_tip_vertices,\n                                                                                    self.tips_coord[:, :2])\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.EdgeIdentification.get_vertices_and_tips_coord","title":"<code>get_vertices_and_tips_coord()</code>","text":"<p>Process skeleton data to extract non-tip vertices and tip coordinates.</p> <p>This method processes the skeleton stored in <code>self.pad_skeleton</code> by first extracting all vertices and tips. It then separates these into branch points (non-tip vertices) and specific tip coordinates using internal processing.</p> <p>Attributes:</p> Name Type Description <code>self.non_tip_vertices</code> <code>array - like</code> <p>Coordinates of non-tip (branch) vertices.</p> <code>self.tips_coord</code> <code>array - like</code> <p>Coordinates of identified tips in the skeleton.</p> Source code in <code>src/cellects/image_analysis/network_functions.py</code> <pre><code>def get_vertices_and_tips_coord(self):\n    \"\"\"Process skeleton data to extract non-tip vertices and tip coordinates.\n\n    This method processes the skeleton stored in `self.pad_skeleton` by first\n    extracting all vertices and tips. It then separates these into branch points\n    (non-tip vertices) and specific tip coordinates using internal processing.\n\n    Attributes\n    ----------\n    self.non_tip_vertices : array-like\n        Coordinates of non-tip (branch) vertices.\n    self.tips_coord : array-like\n        Coordinates of identified tips in the skeleton.\n    \"\"\"\n    pad_vertices, pad_tips = get_vertices_and_tips_from_skeleton(self.pad_skeleton)\n    self.non_tip_vertices, self.tips_coord = get_branches_and_tips_coord(pad_vertices, pad_tips)\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.EdgeIdentification.label_edges_connected_with_vertex_clusters","title":"<code>label_edges_connected_with_vertex_clusters()</code>","text":"<p>Identify edges connected to touching vertices by processing vertex clusters.</p> <p>This function processes the skeleton to identify edges connecting vertices that are part of touching clusters. It creates a cropped version of the skeleton by removing already detected edges and their tips, then iterates through vertex clusters to explore and identify nearby edges.</p> Source code in <code>src/cellects/image_analysis/network_functions.py</code> <pre><code>def label_edges_connected_with_vertex_clusters(self):\n    \"\"\"\n    Identify edges connected to touching vertices by processing vertex clusters.\n\n    This function processes the skeleton to identify edges connecting vertices\n    that are part of touching clusters. It creates a cropped version of the skeleton\n    by removing already detected edges and their tips, then iterates through vertex\n    clusters to explore and identify nearby edges.\n    \"\"\"\n    # I.1. Identify edges connected to touching vertices:\n    # First, create another version of these arrays, where we remove every already detected edge and their tips\n    cropped_skeleton = self.pad_skeleton.copy()\n    cropped_skeleton[self.edge_pix_coord[:, 0], self.edge_pix_coord[:, 1]] = 0\n    cropped_skeleton[self.tips_coord[:, 0], self.tips_coord[:, 1]] = 0\n\n    # non_tip_vertices does not need to be updated yet, because it only contains verified branching vertices\n    cropped_non_tip_vertices = self.non_tip_vertices.copy()\n\n    self.new_level_vertices = None\n    # The problem with vertex_to_vertex_connexion is that since they are not separated by zeros,\n    # they always atract each other instead of exploring other paths.\n    # To fix this, we loop over each vertex group to\n    # 1. Add one edge per inter-vertex connexion inside the group\n    # 2. Remove all except one, and loop as many time as necessary.\n    # Inside that second loop, we explore and identify every edge nearby.\n    # Find every vertex_to_vertex_connexion\n    v_cluster_nb, self.v_cluster_lab, self.v_cluster_stats, vgc = cv2.connectedComponentsWithStats(\n        (self.numbered_vertices &gt; 0).astype(np.uint8), connectivity=8)\n    if v_cluster_nb &gt; 0:\n        max_v_nb = np.max(self.v_cluster_stats[1:, 4])\n        cropped_skeleton_list = []\n        starting_vertices_list = []\n        for v_nb in range(2, max_v_nb + 1):\n            labels = np.nonzero(self.v_cluster_stats[:, 4] == v_nb)[0]\n            coord_list = []\n            for lab in labels:  # lab=labels[0]\n                coord_list.append(np.nonzero(self.v_cluster_lab == lab))\n            for iter in range(v_nb):\n                for lab_ in range(labels.shape[0]): # lab=labels[0]\n                    cs = cropped_skeleton.copy()\n                    sv = []\n                    v_c = coord_list[lab_]\n                    # Save the current coordinate in the starting vertices array of this iteration\n                    sv.append([v_c[0][iter], v_c[1][iter]])\n                    # Remove one vertex coordinate to keep it from cs\n                    v_y, v_x = np.delete(v_c[0], iter), np.delete(v_c[1], iter)\n                    cs[v_y, v_x] = 0\n                    cropped_skeleton_list.append(cs)\n                    starting_vertices_list.append(np.array(sv))\n        for cropped_skeleton, starting_vertices in zip(cropped_skeleton_list, starting_vertices_list):\n            _, _ = self._identify_edges_connecting_a_vertex_list(cropped_skeleton, cropped_non_tip_vertices, starting_vertices)\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.EdgeIdentification.label_edges_connecting_vertex_clusters","title":"<code>label_edges_connecting_vertex_clusters()</code>","text":"<p>Label edges connecting vertex clusters.</p> <p>This method identifies the connections between connected vertices within vertex clusters and labels these edges. It uses the previously found connected vertices, creates an image of the connections, and then identifies and labels the edges between these touching vertices.</p> Source code in <code>src/cellects/image_analysis/network_functions.py</code> <pre><code>def label_edges_connecting_vertex_clusters(self):\n    \"\"\"\n    Label edges connecting vertex clusters.\n\n    This method identifies the connections between connected vertices within\n    vertex clusters and labels these edges. It uses the previously found connected\n    vertices, creates an image of the connections, and then identifies\n    and labels the edges between these touching vertices.\n    \"\"\"\n    # I.2. Identify the connexions between connected vertices:\n    all_connected_vertices = np.nonzero(self.v_cluster_stats[:, 4] &gt; 1)[0][1:]\n    all_con_v_im = np.zeros_like(self.pad_skeleton)\n    for v_group in all_connected_vertices:\n        all_con_v_im[self.v_cluster_lab == v_group] = 1\n    cropped_skeleton = all_con_v_im\n    self.vertex_clusters_coord = np.transpose(np.array(np.nonzero(cropped_skeleton)))\n    _, _ = self._identify_edges_connecting_a_vertex_list(cropped_skeleton, self.vertex_clusters_coord, self.vertex_clusters_coord)\n    # self.edges_labels\n    del self.v_cluster_stats\n    del self.v_cluster_lab\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.EdgeIdentification.label_edges_from_known_vertices_iteratively","title":"<code>label_edges_from_known_vertices_iteratively()</code>","text":"<p>Label edges iteratively from known vertices.</p> <p>This method labels edges in an iterative process starting from known vertices. It handles the removal of detected edges and updates the skeleton accordingly, to avoid detecting edges twice.</p> Source code in <code>src/cellects/image_analysis/network_functions.py</code> <pre><code>def label_edges_from_known_vertices_iteratively(self):\n    \"\"\"\n    Label edges iteratively from known vertices.\n\n    This method labels edges in an iterative process starting from\n    known vertices. It handles the removal of detected edges and\n    updates the skeleton accordingly, to avoid detecting edges twice.\n    \"\"\"\n    # II/ Identify all remaining edges\n    if self.new_level_vertices is not None:\n        starting_vertices_coord = np.vstack((self.new_level_vertices[:, :2], self.vertices_branching_tips))\n        starting_vertices_coord = np.unique(starting_vertices_coord, axis=0)\n    else:\n        # We start from the vertices connecting tips\n        starting_vertices_coord = self.vertices_branching_tips.copy()\n    # Remove the detected edges from cropped_skeleton:\n    cropped_skeleton = self.pad_skeleton.copy()\n    cropped_skeleton[self.edge_pix_coord[:, 0], self.edge_pix_coord[:, 1]] = 0\n    cropped_skeleton[self.tips_coord[:, 0], self.tips_coord[:, 1]] = 0\n    cropped_skeleton[self.vertex_clusters_coord[:, 0], self.vertex_clusters_coord[:, 1]] = 0\n\n    # Reinitialize cropped_non_tip_vertices to browse all vertices except tips and groups\n    cropped_non_tip_vertices = self.non_tip_vertices.copy()\n    cropped_non_tip_vertices = remove_coordinates(cropped_non_tip_vertices, self.vertex_clusters_coord)\n    del self.vertex_clusters_coord\n    remaining_v = cropped_non_tip_vertices.shape[0] + 1\n    while remaining_v &gt; cropped_non_tip_vertices.shape[0]:\n        remaining_v = cropped_non_tip_vertices.shape[0]\n        cropped_skeleton, cropped_non_tip_vertices = self._identify_edges_connecting_a_vertex_list(cropped_skeleton, cropped_non_tip_vertices, starting_vertices_coord)\n        if self.new_level_vertices is not None:\n            starting_vertices_coord = np.unique(self.new_level_vertices[:, :2], axis=0)\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.EdgeIdentification.label_edges_looping_on_1_vertex","title":"<code>label_edges_looping_on_1_vertex()</code>","text":"<p>Identify and handle edges that form loops around a single vertex. This method processes the skeleton image to find looping edges and updates the edge data structure accordingly.</p> Source code in <code>src/cellects/image_analysis/network_functions.py</code> <pre><code>def label_edges_looping_on_1_vertex(self):\n    \"\"\"\n    Identify and handle edges that form loops around a single vertex.\n    This method processes the skeleton image to find looping edges and updates\n    the edge data structure accordingly.\n    \"\"\"\n    self.identified = np.zeros_like(self.numbered_vertices)\n    self.identified[self.edge_pix_coord[:, 0], self.edge_pix_coord[:, 1]] = 1\n    self.identified[self.non_tip_vertices[:, 0], self.non_tip_vertices[:, 1]] = 1\n    self.identified[self.tips_coord[:, 0], self.tips_coord[:, 1]] = 1\n    unidentified = (1 - self.identified) * self.pad_skeleton\n\n    # Find out the remaining non-identified pixels\n    nb, self.unidentified_shapes, self.unidentified_stats, ce = cv2.connectedComponentsWithStats(unidentified.astype(np.uint8))\n    # Handle the cases where edges are loops over only one vertex\n    looping_edges = np.nonzero(self.unidentified_stats[:, 4 ] &gt; 2)[0][1:]\n    for loop_i in looping_edges: # loop_i = looping_edges[0] loop_i=11 #  zoom_on_nonzero(unique_vertices_im, return_coord=False)\n        edge_i = (self.unidentified_shapes == loop_i).astype(np.uint8)\n        dil_edge_i = cv2.dilate(edge_i, square_33)\n        unique_vertices_im = self.numbered_vertices.copy()\n        unique_vertices_im[self.tips_coord[:, 0], self.tips_coord[:, 1]] = 0\n        unique_vertices_im = dil_edge_i * unique_vertices_im\n        unique_vertices = np.unique(unique_vertices_im)\n        unique_vertices = unique_vertices[unique_vertices &gt; 0]\n        v_nb = len(unique_vertices)\n        new_edge_lengths = edge_i.sum()\n        new_edge_pix_coord = np.transpose(np.vstack((np.nonzero(edge_i))))\n        new_edge_pix_coord = np.hstack((new_edge_pix_coord, np.repeat(1, new_edge_pix_coord.shape[0])[:, None]))\n        if v_nb == 1:\n            start, end = unique_vertices[0], unique_vertices[0]\n            self._update_edge_data(start, end, new_edge_lengths, new_edge_pix_coord)\n        elif v_nb == 2:\n            # The edge loops around a group of connected vertices\n            start, end = unique_vertices[0], unique_vertices[1]\n            self._update_edge_data(start, end, new_edge_lengths, new_edge_pix_coord)\n            # conn_v_nb, conn_v = cv2.connectedComponents((unique_vertices_im &gt; 0).astype(np.uint8))\n            # if len(unique_vertices) == 2 and conn_v_nb == 2:\n        elif v_nb &gt; 2: # The question is: How to choose two vertices so that they link all missing pixels?\n            # 1. Find every edge pixel connected to these vertices\n            vertex_connected_pixels = np.nonzero(cv2.dilate((unique_vertices_im &gt; 0).astype(np.uint8), square_33) * edge_i)\n            # 2. Find the most distant pair of these\n            pix1, pix2 = get_min_or_max_euclidean_pair(vertex_connected_pixels, \"max\")\n            # 3. The two best vertices are the two nearest to these two most distant edge pixels\n            dist_to_pix1 = np.zeros(v_nb, np.float64)\n            dist_to_pix2 = np.zeros(v_nb, np.float64)\n            for _i, v_i in enumerate(unique_vertices):\n                v_coord = self.vertex_index_map[v_i]\n                dist_to_pix1[_i] = eudist(pix1, v_coord)\n                dist_to_pix2[_i] = eudist(pix2, v_coord)\n            start, end = unique_vertices[np.argmin(dist_to_pix1)], unique_vertices[np.argmin(dist_to_pix2)]\n            self._update_edge_data(start, end, new_edge_lengths, new_edge_pix_coord)\n        else:\n            logging.error(f\"t={self.t}, One long edge is not identified: i={loop_i} of length={edge_i.sum()} close to {len(unique_vertices)} vertices.\")\n    self.identified[self.edge_pix_coord[:, 0], self.edge_pix_coord[:, 1]] = 1\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.EdgeIdentification.label_tipped_edges_and_their_vertices","title":"<code>label_tipped_edges_and_their_vertices()</code>","text":"<p>Label edges connecting tip vertices to branching vertices and assign unique labels to all relevant vertices.</p> <p>Processes vertex coordinates by stacking tips, vertices branching from tips, and remaining non-tip vertices. Assigns unique sequential identifiers to these vertices in a new array. Constructs an array of edge-label information, where each row contains the edge label (starting at 1), corresponding tip label, and connected vertex label.</p> <p>Attributes:</p> Name Type Description <code>tip_number</code> <code>int</code> <p>The number of tip coordinates available in <code>tips_coord</code>.</p> <code>ordered_v_coord</code> <code>ndarray of float</code> <p>Stack of unique vertex coordinates ordered by: tips first, vertices branching tips second, non-tip vertices third.</p> <code>numbered_vertices</code> <code>ndarray of uint32</code> <p>2D array where each coordinate position is labeled with a sequential integer (starting at 1) based on the order in <code>ordered_v_coord</code>.</p> <code>edges_labels</code> <code>ndarray of uint32</code> <p>Array of shape (n_edges, 3). Each row contains: - Edge label (sequential from 1 to n_edges) - Label of the tip vertex for that edge. - Label of the vertex branching the tip.</p> <code>vertices_branching_tips</code> <code>ndarray of float</code> <p>Unique coordinates of vertices directly connected to tips after removing duplicates.</p> Source code in <code>src/cellects/image_analysis/network_functions.py</code> <pre><code>def label_tipped_edges_and_their_vertices(self):\n    \"\"\"Label edges connecting tip vertices to branching vertices and assign unique labels to all relevant vertices.\n\n    Processes vertex coordinates by stacking tips, vertices branching from tips, and remaining non-tip vertices.\n    Assigns unique sequential identifiers to these vertices in a new array. Constructs an array of edge-label information,\n    where each row contains the edge label (starting at 1), corresponding tip label, and connected vertex label.\n\n    Attributes\n    ----------\n    tip_number : int\n        The number of tip coordinates available in `tips_coord`.\n\n    ordered_v_coord : ndarray of float\n        Stack of unique vertex coordinates ordered by: tips first, vertices branching tips second, non-tip vertices third.\n\n    numbered_vertices : ndarray of uint32\n        2D array where each coordinate position is labeled with a sequential integer (starting at 1) based on the order in `ordered_v_coord`.\n\n    edges_labels : ndarray of uint32\n        Array of shape (n_edges, 3). Each row contains:\n        - Edge label (sequential from 1 to n_edges)\n        - Label of the tip vertex for that edge.\n        - Label of the vertex branching the tip.\n\n    vertices_branching_tips : ndarray of float\n        Unique coordinates of vertices directly connected to tips after removing duplicates.\n    \"\"\"\n    self.tip_number = self.tips_coord.shape[0]\n\n    # Stack vertex coordinates in that order: 1. Tips, 2. Vertices branching tips, 3. All remaining vertices\n    ordered_v_coord = np.vstack((self.tips_coord[:, :2], self.vertices_branching_tips[:, :2], self.non_tip_vertices))\n    ordered_v_coord = np.unique(ordered_v_coord, axis=0)\n\n    # Create arrays to store edges and vertices labels\n    self.numbered_vertices = np.zeros(self.im_shape, dtype=np.uint32)\n    self.numbered_vertices[ordered_v_coord[:, 0], ordered_v_coord[:, 1]] = np.arange(1, ordered_v_coord.shape[0] + 1)\n    self.vertices = None\n    self.vertex_index_map = {}\n    for idx, (y, x) in enumerate(ordered_v_coord):\n        self.vertex_index_map[idx + 1] = tuple((np.uint32(y), np.uint32(x)))\n\n    # Name edges from 1 to the number of edges connecting tips and set the vertices labels from all tips to their connected vertices:\n    self.edges_labels = np.zeros((self.tip_number, 3), dtype=np.uint32)\n    # edge label:\n    self.edges_labels[:, 0] = np.arange(self.tip_number) + 1\n    # tip label:\n    self.edges_labels[:, 1] = self.numbered_vertices[self.tips_coord[:, 0], self.tips_coord[:, 1]]\n    # vertex branching tip label:\n    self.edges_labels[:, 2] = self.numbered_vertices[self.vertices_branching_tips[:, 0], self.vertices_branching_tips[:, 1]]\n\n    # Remove duplicates in vertices_branching_tips\n    self.vertices_branching_tips = np.unique(self.vertices_branching_tips[:, :2], axis=0)\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.EdgeIdentification.make_edge_table","title":"<code>make_edge_table(greyscale, compute_BC=False)</code>","text":"<p>Generate edge table with length and average intensity information.</p> <p>This method processes the vertex coordinates, calculates lengths between vertices for each edge, and computes average width and intensity along the edges. Additionally, it computes edge betweenness centrality for each vertex pair.</p> <p>Parameters:</p> Name Type Description Default <code>greyscale</code> <code>ndarray of uint8</code> <p>Grayscale image.</p> required Source code in <code>src/cellects/image_analysis/network_functions.py</code> <pre><code>def make_edge_table(self, greyscale: NDArray[np.uint8], compute_BC: bool=False):\n    \"\"\"\n    Generate edge table with length and average intensity information.\n\n    This method processes the vertex coordinates, calculates lengths\n    between vertices for each edge, and computes average width and intensity\n    along the edges. Additionally, it computes edge betweenness centrality\n    for each vertex pair.\n\n    Parameters\n    ----------\n    greyscale : ndarray of uint8\n        Grayscale image.\n    \"\"\"\n    if self.vertices is None:\n        self._remove_padding()\n    self.edge_table = np.zeros((self.edges_labels.shape[0], 7), float) # edge_id, vertex1, vertex2, length, average_width, int, BC\n    self.edge_table[:, :3] = self.edges_labels[:, :]\n    self.edge_table[:, 3] = self.edge_lengths\n    for idx, edge_lab in enumerate(self.edges_labels[:, 0]):\n        edge_coord = self.edge_pix_coord[self.edge_pix_coord[:, 2] == edge_lab, :]\n        pix_widths = self.distances[edge_coord[:, 0], edge_coord[:, 1]]\n        v_id = self.edges_labels[self.edges_labels[:, 0] == edge_lab, 1:][0]\n        v1_coord = self.vertex_table[self.vertex_table[:, 2] == v_id[0], :2][0]#\n        v2_coord = self.vertex_table[self.vertex_table[:, 2] == v_id[1], :2][0]#\n        v1_width, v2_width = self.distances[v1_coord[0], v1_coord[1]], self.distances[v2_coord[0], v2_coord[1]]\n\n        if not np.isnan(v1_width):\n            pix_widths = np.append(pix_widths, v1_width)\n        if not np.isnan(v2_width):\n            pix_widths = np.append(pix_widths, v2_width)\n        if pix_widths.size &gt; 0:\n            self.edge_table[idx, 4] = pix_widths.mean()\n        else:\n            self.edge_table[idx, 4] = np.nan\n        pix_ints = greyscale[edge_coord[:, 0], edge_coord[:, 1]]\n        v1_int, v2_int = greyscale[v1_coord[0], v1_coord[1]], greyscale[v2_coord[0], v2_coord[1]]\n        pix_ints = np.append(pix_ints, (v1_int, v2_int))\n        self.edge_table[idx, 5] = pix_ints.mean()\n\n    if compute_BC:\n        G = nx.from_edgelist(self.edges_labels[:, 1:])\n        e_BC = nx.edge_betweenness_centrality(G, seed=0)\n        self.BC_net = np.zeros_like(self.distances)\n        for v, k in e_BC.items(): # v=(81, 80)\n            v1_coord = self.vertex_table[self.vertex_table[:, 2] == v[0], :2]\n            v2_coord = self.vertex_table[self.vertex_table[:, 2] == v[1], :2]\n            full_coord = np.concatenate((v1_coord, v2_coord))\n            edge_lab1 = self.edges_labels[np.all(self.edges_labels[:, 1:] == v[::-1], axis=1), 0]\n            edge_lab2 = self.edges_labels[np.all(self.edges_labels[:, 1:] == v, axis=1), 0]\n            edge_lab = np.unique(np.concatenate((edge_lab1, edge_lab2)))\n            if len(edge_lab) == 1:\n                edge_coord = self.edge_pix_coord[self.edge_pix_coord[:, 2] == edge_lab, :2]\n                full_coord = np.concatenate((full_coord, edge_coord))\n                self.BC_net[full_coord[:, 0], full_coord[:, 1]] = k\n                self.edge_table[self.edge_table[:, 0] == edge_lab, 6] = k\n            elif len(edge_lab) &gt; 1:\n                edge_coord0 = self.edge_pix_coord[self.edge_pix_coord[:, 2] == edge_lab[0], :2]\n                for edge_i in range(len(edge_lab)): #  edge_i=1\n                    edge_coord = self.edge_pix_coord[self.edge_pix_coord[:, 2] == edge_lab[edge_i], :2]\n                    self.edge_table[self.edge_table[:, 0] == edge_lab[edge_i], 6] = k\n                    full_coord = np.concatenate((full_coord, edge_coord))\n                    self.BC_net[full_coord[:, 0], full_coord[:, 1]] = k\n                    if edge_i &gt; 0 and np.array_equal(edge_coord0, edge_coord):\n                        logging.error(f\"There still is two identical edges: {edge_lab} of len: {len(edge_coord)} linking vertices {v}\")\n                        break\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.EdgeIdentification.make_vertex_table","title":"<code>make_vertex_table(origin_contours=None, growing_areas=None)</code>","text":"<p>Generate a table for the vertices.</p> <p>This method constructs and returns a 2D NumPy array holding information about all vertices. Each row corresponds to one vertex identified either by its coordinates in <code>self.tips_coord</code> or <code>self.non_tip_vertices</code>. The array includes additional information about each vertex, including whether they are food vertices, growing areas, and connected components.</p> <p>Parameters:</p> Name Type Description Default <code>origin_contours</code> <code>ndarray of uint8</code> <p>Binary map to identify food vertices. Default is <code>None</code>.</p> <code>None</code> <code>growing_areas</code> <code>ndarray</code> <p>Binary map to identify growing regions. Default is <code>None</code>.</p> <code>None</code> Notes <pre><code>The method updates the instance attribute `self.vertex_table` with\nthe generated vertex information.\n</code></pre> Source code in <code>src/cellects/image_analysis/network_functions.py</code> <pre><code>def make_vertex_table(self, origin_contours: NDArray[np.uint8]=None, growing_areas: NDArray=None):\n    \"\"\"\n    Generate a table for the vertices.\n\n    This method constructs and returns a 2D NumPy array holding information\n    about all vertices. Each row corresponds to one vertex identified either\n    by its coordinates in `self.tips_coord` or `self.non_tip_vertices`. The\n    array includes additional information about each vertex, including whether\n    they are food vertices, growing areas, and connected components.\n\n    Parameters\n    ----------\n    origin_contours : ndarray of uint8, optional\n        Binary map to identify food vertices. Default is `None`.\n    growing_areas : ndarray, optional\n        Binary map to identify growing regions. Default is `None`.\n\n    Notes\n    -----\n        The method updates the instance attribute `self.vertex_table` with\n        the generated vertex information.\n    \"\"\"\n    if self.vertices is None:\n        self._remove_padding()\n    self.vertex_table = np.zeros((self.tips_coord.shape[0] + self.non_tip_vertices.shape[0], 6), dtype=self.vertices.dtype)\n    self.vertex_table[:self.tips_coord.shape[0], :2] = self.tips_coord\n    self.vertex_table[self.tips_coord.shape[0]:, :2] = self.non_tip_vertices\n    self.vertex_table[:self.tips_coord.shape[0], 2] = self.vertices[self.tips_coord[:, 0], self.tips_coord[:, 1]]\n    self.vertex_table[self.tips_coord.shape[0]:, 2] = self.vertices[self.non_tip_vertices[:, 0], self.non_tip_vertices[:, 1]]\n    self.vertex_table[:self.tips_coord.shape[0], 3] = 1\n    if origin_contours is not None:\n        food_vertices = self.vertices[origin_contours &gt; 0]\n        food_vertices = food_vertices[food_vertices &gt; 0]\n        self.vertex_table[np.isin(self.vertex_table[:, 2], food_vertices), 4] = 1\n\n    if growing_areas is not None and growing_areas.shape[1] &gt; 0:\n        # growing = np.unique(self.vertices * growing_areas)[1:]\n        growing = np.unique(self.vertices[growing_areas[0], growing_areas[1]])\n        growing = growing[growing &gt; 0]\n        if len(growing) &gt; 0:\n            growing = np.isin(self.vertex_table[:, 2], growing)\n            self.vertex_table[growing, 4] = 2\n\n    nb, sh, stats, cent = cv2.connectedComponentsWithStats((self.vertices &gt; 0).astype(np.uint8))\n    for i, v_i in enumerate(np.nonzero(stats[:, 4] &gt; 1)[0][1:]):\n        v_labs = self.vertices[sh == v_i]\n        for v_lab in v_labs: # v_lab = v_labs[0]\n            self.vertex_table[self.vertex_table[:, 2] == v_lab, 5] = 1\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.EdgeIdentification.remove_tipped_edge_smaller_than_branch_width","title":"<code>remove_tipped_edge_smaller_than_branch_width()</code>","text":"<p>Remove very short edges from the skeleton.</p> <p>This method focuses on edges connecting tips. When too short, they are considered are noise and removed from the skeleton and distances matrices. These edges are considered too short when their length is smaller than the width of the nearest network branch (an information included in pad_distances). This method also updates internal data structures (skeleton, edge coordinates, vertex/tip positions) accordingly through pixel-wise analysis and connectivity checks.</p> Source code in <code>src/cellects/image_analysis/network_functions.py</code> <pre><code>def remove_tipped_edge_smaller_than_branch_width(self):\n    \"\"\"Remove very short edges from the skeleton.\n\n    This method focuses on edges connecting tips. When too short, they are considered are noise and\n    removed from the skeleton and distances matrices. These edges are considered too short when their length is\n    smaller than the width of the nearest network branch (an information included in pad_distances).\n    This method also updates internal data structures (skeleton, edge coordinates, vertex/tip positions)\n    accordingly through pixel-wise analysis and connectivity checks.\n    \"\"\"\n    # Identify edges that are smaller than the width of the branch it is attached to\n    tipped_edges_to_remove = np.zeros(self.edge_lengths.shape[0], dtype=bool)\n    # connecting_vertices_to_remove = np.zeros(self.vertices_branching_tips.shape[0], dtype=bool)\n    branches_to_remove = np.zeros(self.non_tip_vertices.shape[0], dtype=bool)\n    new_edge_pix_coord = []\n    remaining_tipped_edges_nb = 0\n    for i in range(len(self.edge_lengths)): # i = 3142 #1096 # 974 # 222\n        Y, X = self.vertices_branching_tips[i, 0], self.vertices_branching_tips[i, 1]\n        edge_bool = self.edge_pix_coord[:, 2] == i + 1\n        eY, eX = self.edge_pix_coord[edge_bool, 0], self.edge_pix_coord[edge_bool, 1]\n        if np.nanmax(self.pad_distances[(Y - 1): (Y + 2), (X - 1): (X + 2)]) &gt;= self.edge_lengths[i]:\n            tipped_edges_to_remove[i] = True\n            # Remove the edge\n            self.pad_skeleton[eY, eX] = 0\n            # Remove the tip\n            self.pad_skeleton[self.tips_coord[i, 0], self.tips_coord[i, 1]] = 0\n\n            # Remove the coordinates corresponding to that edge\n            self.edge_pix_coord = np.delete(self.edge_pix_coord, edge_bool, 0)\n\n            # check whether the connecting vertex remains a vertex of not\n            pad_sub_skeleton = np.pad(self.pad_skeleton[(Y - 2): (Y + 3), (X - 2): (X + 3)], [(1,), (1,)],\n                                      mode='constant')\n            sub_vertices, sub_tips = get_vertices_and_tips_from_skeleton(pad_sub_skeleton)\n            # If the vertex does not connect at least 3 edges anymore, remove its vertex label\n            if sub_vertices[3, 3] == 0:\n                vertex_to_remove = np.nonzero(np.logical_and(self.non_tip_vertices[:, 0] == Y, self.non_tip_vertices[:, 1] == X))[0]\n                branches_to_remove[vertex_to_remove] = True\n            # If that pixel became a tip connected to another vertex remove it from the skeleton\n            if sub_tips[3, 3]:\n                if sub_vertices[2:5, 2:5].sum() &gt; 1:\n                    self.pad_skeleton[Y, X] = 0\n                    self.edge_pix_coord = np.delete(self.edge_pix_coord, np.all(self.edge_pix_coord[:, :2] == [Y, X], axis=1), 0)\n                    vertex_to_remove = np.nonzero(np.logical_and(self.non_tip_vertices[:, 0] == Y, self.non_tip_vertices[:, 1] == X))[0]\n                    branches_to_remove[vertex_to_remove] = True\n        else:\n            remaining_tipped_edges_nb += 1\n            new_edge_pix_coord.append(np.stack((eY, eX, np.repeat(remaining_tipped_edges_nb, len(eY))), axis=1))\n\n    # Check that excedent connected components are 1 pixel size, if so:\n    # It means that they were neighbors to removed tips and not necessary for the skeleton\n    nb, sh = cv2.connectedComponents(self.pad_skeleton)\n    if nb &gt; 2:\n        logging.error(\"Removing small tipped edges split the skeleton\")\n        # for i in range(2, nb):\n        #     excedent = sh == i\n        #     if (excedent).sum() == 1:\n        #         self.pad_skeleton[excedent] = 0\n\n    # Remove in distances the pixels removed in skeleton:\n    self.pad_distances *= self.pad_skeleton\n\n    # update edge_pix_coord\n    if len(new_edge_pix_coord) &gt; 0:\n        self.edge_pix_coord = np.vstack(new_edge_pix_coord)\n\n    # # Remove tips connected to very small edges\n    # self.tips_coord = np.delete(self.tips_coord, tipped_edges_to_remove, 0)\n    # # Add corresponding edge names\n    # self.tips_coord = np.hstack((self.tips_coord, np.arange(1, len(self.tips_coord) + 1)[:, None]))\n\n    # # Within all branching (non-tip) vertices, keep those that did not lose their vertex status because of the edge removal\n    # self.non_tip_vertices = np.delete(self.non_tip_vertices, branches_to_remove, 0)\n\n    # # Get the branching vertices who kept their typped edge\n    # self.vertices_branching_tips = np.delete(self.vertices_branching_tips, tipped_edges_to_remove, 0)\n\n    # Within all branching (non-tip) vertices, keep those that do not connect a tipped edge.\n    # v_branching_tips_in_branching_v = find_common_coord(self.non_tip_vertices, self.vertices_branching_tips[:, :2])\n    # self.remaining_vertices = np.delete(self.non_tip_vertices, v_branching_tips_in_branching_v, 0)\n    # ordered_v_coord = np.vstack((self.tips_coord[:, :2], self.vertices_branching_tips[:, :2], self.remaining_vertices))\n\n    # tips = self.tips_coord\n    # branching_any_edge = self.non_tip_vertices\n    # branching_typped_edges = self.vertices_branching_tips\n    # branching_no_typped_edges = self.remaining_vertices\n\n    self.get_vertices_and_tips_coord()\n    self.get_tipped_edges()\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.EdgeIdentification.run_edge_identification","title":"<code>run_edge_identification()</code>","text":"<p>Run the edge identification process.</p> <p>This method orchestrates a series of steps to identify and label edges within the graph structure. Each step handles a specific aspect of edge identification, ultimately leading to a clearer and more refined edge network.</p> <p>Steps involved: 1. Get vertices and tips coordinates. 2. Identify tipped edges. 3. Remove tipped edges smaller than branch width. 4. Label tipped edges and their vertices. 5. Label edges connected with vertex clusters. 6. Label edges connecting vertex clusters. 7. Label edges from known vertices iteratively. 8. Label edges looping on 1 vertex. 9. Clear areas with 1 or 2 unidentified pixels. 10. Clear edge duplicates. 11. Clear vertices connecting 2 edges.</p> Source code in <code>src/cellects/image_analysis/network_functions.py</code> <pre><code>def run_edge_identification(self):\n    \"\"\"\n    Run the edge identification process.\n\n    This method orchestrates a series of steps to identify and label edges\n    within the graph structure. Each step handles a specific aspect of edge\n    identification, ultimately leading to a clearer and more refined edge network.\n\n    Steps involved:\n    1. Get vertices and tips coordinates.\n    2. Identify tipped edges.\n    3. Remove tipped edges smaller than branch width.\n    4. Label tipped edges and their vertices.\n    5. Label edges connected with vertex clusters.\n    6. Label edges connecting vertex clusters.\n    7. Label edges from known vertices iteratively.\n    8. Label edges looping on 1 vertex.\n    9. Clear areas with 1 or 2 unidentified pixels.\n    10. Clear edge duplicates.\n    11. Clear vertices connecting 2 edges.\n    \"\"\"\n    self.get_vertices_and_tips_coord()\n    self.get_tipped_edges()\n    self.remove_tipped_edge_smaller_than_branch_width()\n    self.label_tipped_edges_and_their_vertices()\n    self.check_vertex_existence()\n    self.label_edges_connected_with_vertex_clusters()\n    self.label_edges_connecting_vertex_clusters()\n    self.label_edges_from_known_vertices_iteratively()\n    self.label_edges_looping_on_1_vertex()\n    self.clear_areas_of_1_or_2_unidentified_pixels()\n    self.clear_edge_duplicates()\n    self.clear_vertices_connecting_2_edges()\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.MotionAnalysis","title":"<code>MotionAnalysis</code>","text":"Source code in <code>src/cellects/core/motion_analysis.py</code> <pre><code>class MotionAnalysis:\n\n    def __init__(self, l: list):\n\n        \"\"\"\n        Analyzes motion in a given arena using video data.\n\n        This class processes video frames to analyze motion within a specified area,\n        detecting shapes, covering durations, and generating descriptors for further\n        analysis.\n\n        Args:\n            l (list): A list containing various parameters and flags necessary for the motion\n                analysis.\n\n        Args:\n            l[0] (int): Arena index.\n            l[1] (str): Arena identifier or name, stored in one_descriptor_per_arena['arena'].\n            l[2] (dict): Variables required for the analysis, stored in vars.\n            l[3] (bool): Flag to detect shape.\n            l[4] (bool): Flag to analyze shape.\n            l[5] (bool): Flag to show segmentation.\n            l[6] (None or list): Videos already in RAM.\n\n        Attributes:\n            vars (dict): Variables required for the analysis.\n            visu (None): Placeholder for visualization data.\n            binary (None): Placeholder for binary segmentation data.\n            origin_idx (None): Placeholder for the index of the first frame.\n            smoothing_flag (bool): Flag to indicate if smoothing should be applied.\n            dims (tuple): Dimensions of the converted video.\n            segmentation (ndarray): Array to store segmentation data.\n            covering_intensity (ndarray): Intensity values for covering analysis.\n            mean_intensity_per_frame (ndarray): Mean intensity per frame.\n            borders (object): Borders of the arena.\n            pixel_ring_depth (int): Depth of the pixel ring for analysis, default is 9.\n            step (int): Step size for processing, default is 10.\n            lost_frames (int): Number of lost frames to account for, default is 10.\n            start (None or int): Starting frame index for the analysis.\n\n        Methods:\n            load_images_and_videos(videos_already_in_ram, arena_idx): Loads images and videos\n                for the specified arena index.\n            update_ring_width(): Updates the width of the pixel ring for analysis.\n            get_origin_shape(): Detects the origin shape in the video frames.\n            get_covering_duration(step): Calculates the covering duration based on a step size.\n            detection(): Performs motion detection within the arena.\n            initialize_post_processing(): Initializes post-processing steps.\n            update_shape(show_seg): Updates the shape based on segmentation and visualization flags.\n            get_descriptors_from_binary(): Extracts descriptors from binary data.\n            detect_growth_transitions(): Detects growth transitions in the data.\n            networks_analysis(show_seg): Detected networks within the arena based on segmentation\n                visualization.\n            study_cytoscillations(show_seg): Studies cytoscillations within the arena with\n                segmentation visualization.\n            fractal_descriptions(): Generates fractal descriptions of the analyzed data.\n            get_descriptors_summary(): Summarizes the descriptors obtained from the analysis.\n            save_results(): Saves the results of the analysis.\n\n        \"\"\"\n        self.one_descriptor_per_arena = {}\n        self.one_descriptor_per_arena['arena'] = l[1]\n        vars = l[2]\n        detect_shape = l[3]\n        analyse_shape = l[4]\n        show_seg = l[5]\n        videos_already_in_ram = l[6]\n        self.visu = None\n        self.binary = None\n        self.origin_idx = None\n        self.smoothing_flag: bool = False\n        self.drift_mask_coord = None\n        self.coord_network = None\n        logging.info(f\"Start the motion analysis of the arena n\u00b0{self.one_descriptor_per_arena['arena']}\")\n\n        self.vars = vars\n        if not 'contour_color' in self.vars:\n            self.vars['contour_color']: np.uint8 = 0\n        if not 'background_list' in self.vars:\n            self.vars['background_list'] = []\n        self.load_images_and_videos(videos_already_in_ram, l[0])\n\n        self.dims = self.converted_video.shape\n        self.segmented = np.zeros(self.dims, dtype=np.uint8)\n\n        self.covering_intensity = np.zeros(self.dims[1:], dtype=np.float64)\n        self.mean_intensity_per_frame = np.mean(self.converted_video, (1, 2))\n\n        self.borders = image_borders(self.dims[1:], shape=self.vars['arena_shape'])\n        self.pixel_ring_depth = 9\n        self.step: int = 10\n        self.lost_frames = 10\n        self.update_ring_width()\n\n        self.start = None\n        if detect_shape:\n            self.assess_motion_detection()\n            if self.start is not None:\n                self.detection()\n                self.initialize_post_processing()\n                self.t = self.start\n                while self.t &lt; self.dims[0]:  #200:\n                    self.update_shape(show_seg)\n                #\n\n            if analyse_shape:\n                self.get_descriptors_from_binary()\n                self.detect_growth_transitions()\n                self.networks_analysis(show_seg)\n                self.study_cytoscillations(show_seg)\n                self.fractal_descriptions()\n                if videos_already_in_ram is None:\n                    self.save_results()\n\n    def load_images_and_videos(self, videos_already_in_ram, i: int):\n        \"\"\"\n\n        Load images and videos from disk or RAM.\n\n        Parameters\n        ----------\n        videos_already_in_ram : numpy.ndarray or None\n            Video data that is already loaded into RAM. If `None`, videos will be\n            loaded from disk.\n        i : int\n            Index used to select the origin and background data.\n\n        Notes\n        -----\n        This method logs information about the arena number and loads necessary data\n        from disk or RAM based on whether videos are already in memory. It sets various\n        attributes like `self.origin`, `self.background`, and `self.converted_video`.\n\n        \"\"\"\n        logging.info(f\"Arena n\u00b0{self.one_descriptor_per_arena['arena']}. Load images and videos\")\n        if 'bb_coord' in self.vars:\n            crop_top, crop_bot, crop_left, crop_right, top, bot, left, right = self.vars['bb_coord']\n        elif videos_already_in_ram is not None:\n            if isinstance(videos_already_in_ram, list):\n                crop_bot, crop_right = videos_already_in_ram[0].shape[1], videos_already_in_ram[0].shape[2]\n            else:\n                crop_bot, crop_right = videos_already_in_ram.shape[1], videos_already_in_ram.shape[2]\n            crop_top, crop_left, top, bot, left, right = 0, 0, [0], [crop_bot], [0], [crop_right]\n        if isinstance(self.vars['origin_list'][i], Tuple):\n            self.origin_idx = self.vars['origin_list'][i]\n            frame_height = bot[i] - top[i]\n            true_frame_width = right[i] - left[i]\n            self.origin = np.zeros((frame_height, true_frame_width), dtype=np.uint8)\n            self.origin[self.origin_idx[0], self.origin_idx[1]] = 1\n        else:\n            self.origin = self.vars['origin_list'][i]\n            frame_height = self.origin.shape[0]\n            true_frame_width = self.origin.shape[1]\n\n        vid_name = None\n        if self.vars['video_list'] is not None:\n            vid_name = self.vars['video_list'][i]\n        self.background = None\n        if len(self.vars['background_list']) &gt; 0:\n            self.background = self.vars['background_list'][i]\n        self.background2 = None\n        if 'background_list2' in self.vars and len(self.vars['background_list2']) &gt; 0:\n            self.background2 = self.vars['background_list2'][i]\n        vids = read_one_arena(self.one_descriptor_per_arena['arena'], self.vars['already_greyscale'],\n                              self.vars['convert_for_motion'], videos_already_in_ram, true_frame_width, vid_name,\n                              self.background, self.background2)\n        self.visu, self.converted_video, self.converted_video2 = vids\n        # When the video(s) already exists (not just written as .pny), they need to be sliced:\n        if self.visu is not None:\n            if self.visu.shape[1] != frame_height or self.visu.shape[2] != true_frame_width:\n                self.visu = self.visu[:, crop_top:crop_bot, crop_left:crop_right, ...]\n                self.visu = self.visu[:, top[i]:bot[i], left[i]:right[i], ...]\n                if self.converted_video is not None:\n                    self.converted_video = self.converted_video[:, crop_top:crop_bot, crop_left:crop_right]\n                    self.converted_video = self.converted_video[:, top[i]:bot[i], left[i]:right[i]]\n                    if self.converted_video2 is not None:\n                        self.converted_video2 = self.converted_video2[:, crop_top:crop_bot, crop_left:crop_right]\n                        self.converted_video2 = self.converted_video2[:, top[i]:bot[i], left[i]:right[i]]\n\n        if self.converted_video is None:\n            logging.info(\n                f\"Arena n\u00b0{self.one_descriptor_per_arena['arena']}. Convert the RGB visu video into a greyscale image using the color space combination: {self.vars['convert_for_motion']}\")\n            vids = convert_subtract_and_filter_video(self.visu, self.vars['convert_for_motion'],\n                                                     self.background, self.background2,\n                                                     self.vars['lose_accuracy_to_save_memory'],\n                                                     self.vars['filter_spec'])\n            self.converted_video, self.converted_video2 = vids\n\n    def assess_motion_detection(self):\n        \"\"\"\n        Assess if a motion can be detected using the current parameters.\n\n        Validate the specimen(s) detected in the first frame and evaluate roughly how growth occurs during the video.\n        \"\"\"\n        # Here to conditional layers allow to detect if an expansion/exploration occured\n        self.get_origin_shape()\n        # The first, user-defined is the 'first_move_threshold' and the second is the detection of the\n        # substantial image: if any of them is not detected, the program considers there is no motion.\n        if self.dims[0] &gt;= 40:\n            step = self.dims[0] // 20\n        else:\n            step = 1\n        if self.dims[0] == 1 or self.start &gt;= (self.dims[0] - step - 1):\n            self.start = None\n            self.binary = np.repeat(np.expand_dims(self.origin, 0), self.converted_video.shape[0], axis=0)\n        else:\n            self.get_covering_duration(step)\n\n    def get_origin_shape(self):\n        \"\"\"\n        Determine the origin shape and initialize variables based on the state of the current analysis.\n\n        This method analyzes the initial frame or frames to determine the origin shape\n        of an object in a video, initializing necessary variables and matrices for\n        further processing.\n\n\n        Attributes Modified:\n            start: (int) Indicates the starting frame index.\n            origin_idx: (np.ndarray) The indices of non-zero values in the origin matrix.\n            covering_intensity: (np.ndarray) Matrix used for pixel fading intensity.\n            substantial_growth: (int) Represents a significant growth measure based on the origin.\n\n        Notes:\n            - The method behavior varies if 'origin_state' is set to \"constant\" or not.\n            - If the background is lighter, 'covering_intensity' matrix is initialized.\n            - Uses connected components to determine which shape is closest to the center\n              or largest, based on 'appearance_detection_method'.\n        \"\"\"\n        logging.info(f\"Arena n\u00b0{self.one_descriptor_per_arena['arena']}. Make sure of origin shape\")\n        if self.vars['drift_already_corrected']:\n            self.drift_mask_coord = np.zeros((self.dims[0], 4), dtype=np.uint32)\n            for frame_i in np.arange(self.dims[0]):  # 100):#\n                true_pixels = np.nonzero(self.converted_video[frame_i, ...])\n                self.drift_mask_coord[frame_i, :] = np.min(true_pixels[0]), np.max(true_pixels[0]) + 1, np.min(true_pixels[1]), np.max(true_pixels[1]) + 1\n            if np.all(self.drift_mask_coord[:, 0] == 0) and np.all(self.drift_mask_coord[:, 1] == self.dims[1] - 1) and np.all(\n                    self.drift_mask_coord[:, 2] == 0) and np.all(self.drift_mask_coord[:, 3] == self.dims[2] - 1):\n                logging.error(f\"Drift correction has been wrongly detected. Images do not contain zero-valued pixels\")\n                self.vars['drift_already_corrected'] = False\n        self.start = 1\n        if self.vars['origin_state'] == \"invisible\":\n            self.start += self.vars['first_detection_frame']\n            analysisi = self.frame_by_frame_segmentation(self.start, self.origin)\n            # Use connected components to find which shape is the nearest from the image center.\n            if self.vars['several_blob_per_arena']:\n                self.origin = analysisi.binary_image\n            else:\n                if self.vars['appearance_detection_method'] == 'largest':\n                    self.origin = keep_one_connected_component(analysisi.binary_image)\n                elif self.vars['appearance_detection_method'] == 'most_central':\n                    nb_components, output, stats, centroids = cv2.connectedComponentsWithStats(analysisi.binary_image,\n                                                                                               connectivity=8)\n                    center = np.array((self.dims[2] // 2, self.dims[1] // 2))\n                    stats = np.zeros(nb_components - 1)\n                    for shape_i in np.arange(1, nb_components):\n                        stats[shape_i - 1] = eudist(center, centroids[shape_i, :])\n                    # The shape having the minimal euclidean distance from the center will be the original shape\n                    self.origin = np.zeros((self.dims[1], self.dims[2]), dtype=np.uint8)\n                    self.origin[output == (np.argmin(stats) + 1)] = 1\n        self.origin_idx = np.nonzero(self.origin)\n        if self.vars['origin_state'] == \"constant\":\n            if self.vars['lighter_background']:\n                # Initialize the covering_intensity matrix as a reference for pixel fading\n                self.covering_intensity[self.origin_idx[0], self.origin_idx[1]] = 200\n        self.substantial_growth = np.min((1.2 * self.origin.sum(), self.origin.sum() + 250))\n\n    def get_covering_duration(self, step: int):\n        \"\"\"\n        Determine the number of frames necessary for a pixel to get covered.\n\n        This function identifies the time when significant growth or motion occurs\n        in a video and calculates the number of frames needed for a pixel to be\n        completely covered. It also handles noise and ensures that the calculated\n        step value is reasonable.\n\n        Parameters\n        ----------\n        step : int\n            The initial step size for frame analysis.\n\n        Raises\n        ------\n        Exception\n            If an error occurs during the calculation process.\n\n        Notes\n        -----\n        This function may modify several instance attributes including\n        `substantial_time`, `step`, and `start`.\n        \"\"\"\n        logging.info(f\"Arena n\u00b0{self.one_descriptor_per_arena['arena']}. Find a frame with a significant growth/motion and determine the number of frames necessary for a pixel to get covered\")\n        ## Find the time at which growth reached a substantial growth.\n        self.substantial_time = self.start\n        # To avoid noisy images to have deleterious effects, make sure that area area reaches the threshold thrice.\n        occurrence = 0\n        allowed_window = None\n        if self.vars['drift_already_corrected']:\n            allowed_window = self.drift_mask_coord[:, 0].max(), self.drift_mask_coord[:, 1].min(), self.drift_mask_coord[:, 2].max(), self.drift_mask_coord[:, 3].min()\n        prev_bin_im = self.origin\n        while np.logical_and(occurrence &lt; 3, self.substantial_time &lt; (self.dims[0] - step - 1)):\n            self.substantial_time += step\n            growth_vision = self.frame_by_frame_segmentation(self.substantial_time, prev_bin_im)\n            prev_bin_im = growth_vision.binary_image * self.borders\n            surfarea = np.sum(prev_bin_im)\n            prev_bin_im = np.logical_or(prev_bin_im, self.origin).astype(np.uint8)\n            if surfarea &gt; self.substantial_growth:\n                occurrence += 1\n        # get a rough idea of the area covered during this time\n        if (self.substantial_time - self.start) &gt; 20:\n            if self.vars['lighter_background']:\n                growth = (np.sum(self.converted_video[self.start:(self.start + 10), :, :], 0) / 10) - (np.sum(self.converted_video[(self.substantial_time - 10):self.substantial_time, :, :], 0) / 10)\n            else:\n                growth = (np.sum(self.converted_video[(self.substantial_time - 10):self.substantial_time, :, :], 0) / 10) - (\n                            np.sum(self.converted_video[self.start:(self.start + 10), :, :], 0) / 10)\n        else:\n            if self.vars['lighter_background']:\n                growth = self.converted_video[self.start, ...] - self.converted_video[self.substantial_time, ...]\n            else:\n                growth = self.converted_video[self.substantial_time, ...] - self.converted_video[self.start, ...]\n        intensity_extent = np.ptp(self.converted_video[self.start:self.substantial_time, :, :], axis=0)\n        growth[np.logical_or(growth &lt; 0, intensity_extent &lt; np.median(intensity_extent))] = 0\n        growth = bracket_to_uint8_image_contrast(growth)\n        growth *= self.borders\n        growth_vision = OneImageAnalysis(growth)\n        growth_vision.segmentation(allowed_window=allowed_window)\n        if self.vars['several_blob_per_arena']:\n            _, _, stats, _ = cv2.connectedComponentsWithStats(self.origin)\n            do_erode = np.any(stats[1:, 4] &gt; 50)\n        else:\n            do_erode = self.origin.sum() &gt; 50\n        if do_erode:\n            self.substantial_image = cv2.erode(growth_vision.binary_image, cross_33, iterations=2)\n        else:\n            self.substantial_image = growth_vision.binary_image\n\n        if np.any(self.substantial_image):\n            natural_noise = np.nonzero(intensity_extent == np.min(intensity_extent))\n            natural_noise = self.converted_video[self.start:self.substantial_time, natural_noise[0][0], natural_noise[1][0]]\n            natural_noise = moving_average(natural_noise, 5)\n            natural_noise = np.ptp(natural_noise)\n            subst_idx = np.nonzero(self.substantial_image)\n            cover_lengths = np.zeros(len(subst_idx[0]), dtype=np.uint32)\n            for index in np.arange(len(subst_idx[0])):\n                vector = self.converted_video[self.start:self.substantial_time, subst_idx[0][index], subst_idx[1][index]]\n                left, right = find_major_incline(vector, natural_noise)\n                # If find_major_incline did find a major incline: (otherwise it put 0 to left and 1 to right)\n                if not np.logical_and(left == 0, right == 1):\n                    cover_lengths[index] = len(vector[left:-right])\n            # If this analysis fails put a deterministic step\n            if len(cover_lengths[cover_lengths &gt; 0]) &gt; 0:\n                self.step = (np.round(np.mean(cover_lengths[cover_lengths &gt; 0])).astype(int) // 2) + 1\n                logging.info(f\"Arena n\u00b0{self.one_descriptor_per_arena['arena']}. Pre-processing detection: the time for a pixel to get covered is set to {self.step}\")\n            else:\n                logging.info(f\"Arena n\u00b0{self.one_descriptor_per_arena['arena']}. Pre-processing detection: could not automatically find the time for a pixel to get covered. Default value is 1 for video length &lt; 40 and 10 otherwise\")\n\n            # Make sure to avoid a step overestimation\n            if self.step &gt; self.dims[0] // 20:\n                self.step: int = self.dims[0] // 20\n\n            if self.step == 0:\n                self.step: int = 1\n        # When the first_move_threshold is not stringent enough the program may detect a movement due to noise\n        # In that case, the substantial_image is empty and there is no reason to proceed further\n        else:\n            self.start = None\n\n    def detection(self, compute_all_possibilities: bool=False):\n        \"\"\"\n\n            Perform frame-by-frame or luminosity-based segmentation on video data to detect cell motion and growth.\n\n            This function processes video frames using either frame-by-frame segmentation or luminosity-based\n            segmentation algorithms to detect cell motion and growth. It handles drift correction, adjusts parameters\n            based on configuration settings, and applies logical operations to combine results from different segmentation\n            methods.\n\n            Parameters\n            ----------\n            compute_all_possibilities : bool, optional\n                Flag to determine if all segmentation possibilities should be computed, by default False\n\n            Returns\n            -------\n            None\n\n            Notes\n            -----\n            This function modifies the instance variables `self.segmented`, `self.converted_video`,\n            and potentially `self.luminosity_segmentation` and `self.gradient_segmentation`.\n            Depending on the configuration settings, it performs various segmentation algorithms and updates\n            the instance variables accordingly.\n\n        \"\"\"\n        if self.start is None:\n            self.start = 1\n        else:\n            self.start = np.max((self.start, 1))\n        self.lost_frames = np.min((self.step, self.dims[0] // 4))\n        # I/ Image by image segmentation algorithms\n        # If images contain a drift correction (zeros at borders of the image,\n        # Replace these 0 by normal background values before segmenting\n        if self.vars['frame_by_frame_segmentation'] or compute_all_possibilities:\n            logging.info(f\"Arena n\u00b0{self.one_descriptor_per_arena['arena']}. Detect cell motion and growth using the frame by frame segmentation algorithm\")\n            self.segmented = np.zeros(self.dims, dtype=np.uint8)\n            for t in np.arange(self.dims[0]):#20):#\n                analysisi = self.frame_by_frame_segmentation(t, self.segmented[t - 1, ...])\n                self.segmented[t, ...] = analysisi.binary_image\n\n                if self.vars['lose_accuracy_to_save_memory']:\n                    self.converted_video[t, ...] = bracket_to_uint8_image_contrast(analysisi.image)\n                else:\n                    self.converted_video[t, ...] = analysisi.image\n                if self.vars['convert_for_motion']['logical'] != 'None':\n                    if self.vars['lose_accuracy_to_save_memory']:\n                        self.converted_video2[t, ...] = bracket_to_uint8_image_contrast(analysisi.image2)\n                    else:\n                        self.converted_video2[t, ...] = analysisi.image2\n\n        if self.vars['color_number'] == 2:\n            luminosity_segmentation, l_threshold_over_time = self.lum_value_segmentation(self.converted_video, do_threshold_segmentation=self.vars['do_threshold_segmentation'] or compute_all_possibilities)\n            self.converted_video = self.smooth_pixel_slopes(self.converted_video)\n            gradient_segmentation = None\n            if self.vars['do_slope_segmentation'] or compute_all_possibilities:\n                gradient_segmentation = self.lum_slope_segmentation(self.converted_video)\n                if gradient_segmentation is not None:\n                    gradient_segmentation[-self.lost_frames:, ...] = np.repeat(gradient_segmentation[-self.lost_frames, :, :][np.newaxis, :, :], self.lost_frames, axis=0)\n            if self.vars['convert_for_motion']['logical'] != 'None':\n                if self.vars['do_threshold_segmentation'] or compute_all_possibilities:\n                    luminosity_segmentation2, l_threshold_over_time2 = self.lum_value_segmentation(self.converted_video2, do_threshold_segmentation=True)\n                    if luminosity_segmentation is None:\n                        luminosity_segmentation = luminosity_segmentation2\n                    if luminosity_segmentation is not None:\n                        if self.vars['convert_for_motion']['logical'] == 'Or':\n                            luminosity_segmentation = np.logical_or(luminosity_segmentation, luminosity_segmentation2)\n                        elif self.vars['convert_for_motion']['logical'] == 'And':\n                            luminosity_segmentation = np.logical_and(luminosity_segmentation, luminosity_segmentation2)\n                        elif self.vars['convert_for_motion']['logical'] == 'Xor':\n                            luminosity_segmentation = np.logical_xor(luminosity_segmentation, luminosity_segmentation2)\n                self.converted_video2 = self.smooth_pixel_slopes(self.converted_video2)\n                if self.vars['do_slope_segmentation'] or compute_all_possibilities:\n                    gradient_segmentation2 = self.lum_slope_segmentation(self.converted_video2)\n                    if gradient_segmentation2 is not None:\n                        gradient_segmentation2[-self.lost_frames:, ...] = np.repeat(gradient_segmentation2[-self.lost_frames, :, :][np.newaxis, :, :], self.lost_frames, axis=0)\n                    if gradient_segmentation is None:\n                        gradient_segmentation = gradient_segmentation2\n                    if gradient_segmentation is not None:\n                        if self.vars['convert_for_motion']['logical'] == 'Or':\n                            gradient_segmentation = np.logical_or(gradient_segmentation, gradient_segmentation2)\n                        elif self.vars['convert_for_motion']['logical'] == 'And':\n                            gradient_segmentation = np.logical_and(gradient_segmentation, gradient_segmentation2)\n                        elif self.vars['convert_for_motion']['logical'] == 'Xor':\n                            gradient_segmentation = np.logical_xor(gradient_segmentation, gradient_segmentation2)\n\n            if compute_all_possibilities:\n                logging.info(f\"Arena n\u00b0{self.one_descriptor_per_arena['arena']}. Compute all options to detect cell motion and growth. Maximal growth per frame: {self.vars['maximal_growth_factor']}\")\n                if luminosity_segmentation is not None:\n                    self.luminosity_segmentation = np.nonzero(luminosity_segmentation)\n                if gradient_segmentation is not None:\n                    self.gradient_segmentation = np.nonzero(gradient_segmentation)\n                if luminosity_segmentation is not None and gradient_segmentation is not None:\n                    self.logical_and = np.nonzero(np.logical_and(luminosity_segmentation, gradient_segmentation))\n                    self.logical_or = np.nonzero(np.logical_or(luminosity_segmentation, gradient_segmentation))\n            elif not self.vars['frame_by_frame_segmentation']:\n                if self.vars['do_threshold_segmentation'] and not self.vars['do_slope_segmentation']:\n                    logging.info(f\"Arena n\u00b0{self.one_descriptor_per_arena['arena']}. Detect with luminosity threshold segmentation algorithm\")\n                    self.segmented = luminosity_segmentation\n                if self.vars['do_slope_segmentation']:\n                    logging.info(f\"Arena n\u00b0{self.one_descriptor_per_arena['arena']}. Detect with luminosity slope segmentation algorithm\")\n                    self.segmented = gradient_segmentation\n                if np.logical_and(self.vars['do_threshold_segmentation'], self.vars['do_slope_segmentation']):\n                    if self.vars['true_if_use_light_AND_slope_else_OR']:\n                        logging.info(f\"Arena n\u00b0{self.one_descriptor_per_arena['arena']}. Detection resuts from threshold AND slope segmentation algorithms\")\n                        if luminosity_segmentation is not None and gradient_segmentation is not None:\n                            self.segmented = np.logical_and(luminosity_segmentation, gradient_segmentation)\n                    else:\n                        logging.info(f\"Arena n\u00b0{self.one_descriptor_per_arena['arena']}. Detection resuts from threshold OR slope segmentation algorithms\")\n                        if luminosity_segmentation is not None and gradient_segmentation is not None:\n                            self.segmented = np.logical_or(luminosity_segmentation, gradient_segmentation)\n                self.segmented = self.segmented.astype(np.uint8)\n\n\n    def frame_by_frame_segmentation(self, t: int, previous_binary_image: NDArray=None):\n        \"\"\"\n\n        Frame-by-frame segmentation of a video.\n\n        Parameters\n        ----------\n        t : int\n            The time index of the frame to process.\n        previous_binary_image : NDArray, optional\n            The binary image from the previous frame. Default is `None`.\n\n        Returns\n        -------\n        OneImageAnalysis\n            An object containing the analysis of the current frame.\n        \"\"\"\n        contrasted_im = bracket_to_uint8_image_contrast(self.converted_video[t, :, :])\n        # 1. Get the mask valid for a number of images around it (step).\n        allowed_window = None\n        if self.vars['drift_already_corrected']:\n            half_step = np.ceil(self.step / 2).astype(int)\n            t_start = t - half_step\n            t_end = t + half_step\n            t_start = np.max((t_start, 0))\n            t_end = np.min((t_end, self.dims[0]))\n            min_y, max_y = np.max(self.drift_mask_coord[t_start:t_end, 0]), np.min(self.drift_mask_coord[t_start:t_end, 1])\n            min_x, max_x = np.max(self.drift_mask_coord[t_start:t_end, 2]), np.min(self.drift_mask_coord[t_start:t_end, 3])\n            allowed_window = min_y, max_y, min_x, max_x\n\n        analysisi = OneImageAnalysis(contrasted_im)\n        if self.vars['convert_for_motion']['logical'] != 'None':\n            contrasted_im2 = bracket_to_uint8_image_contrast(self.converted_video2[t, :, :])\n            analysisi.image2 = contrasted_im2\n\n        if previous_binary_image is None or t == 0:\n            analysisi.previous_binary_image = self.origin\n        else:\n            analysisi.previous_binary_image = previous_binary_image\n\n        analysisi.segmentation(self.vars['convert_for_motion']['logical'], self.vars['color_number'],\n                               bio_label=self.vars[\"bio_label\"], bio_label2=self.vars[\"bio_label2\"],\n                               rolling_window_segmentation=self.vars['rolling_window_segmentation'],\n                               lighter_background=self.vars['lighter_background'],\n                               allowed_window=allowed_window, filter_spec=self.vars['filter_spec']) # filtering already done when creating converted_video\n\n        return analysisi\n\n    def lum_value_segmentation(self, converted_video: NDArray, do_threshold_segmentation: bool) -&gt; Tuple[NDArray, NDArray]:\n        \"\"\"\n        Perform segmentation based on luminosity values from a video.\n\n        Parameters\n        ----------\n        converted_video : NDArray\n            The input video data in a NumPy array format.\n        do_threshold_segmentation : bool\n            Flag to determine whether threshold segmentation should be applied.\n\n        Returns\n        -------\n        Tuple[NDArray, NDArray]\n            A tuple containing two NumPy arrays:\n            - The first array is the luminosity segmentation of the video.\n            - The second array represents the luminosity threshold over time.\n\n        Notes\n        -----\n        This function operates under the assumption that there is sufficient motion in the video data.\n        If no valid thresholds are found for segmentation, the function returns None for\n        `luminosity_segmentation`.\n        \"\"\"\n        shape_motion_failed: bool = False\n        if self.vars['lighter_background']:\n            covering_l_values = np.min(converted_video[:self.substantial_time, :, :],\n                                             0) * self.substantial_image\n        else:\n            covering_l_values = np.max(converted_video[:self.substantial_time, :, :],\n                                             0) * self.substantial_image\n        # Avoid errors by checking whether the covering values are nonzero\n        covering_l_values = covering_l_values[covering_l_values != 0]\n        if len(covering_l_values) == 0:\n            shape_motion_failed = True\n\n        luminosity_segmentation = None\n        l_threshold_over_time = None\n        if not shape_motion_failed:\n            value_segmentation_thresholds = np.arange(0.8, -0.7, -0.1)\n            validated_thresholds = np.zeros(value_segmentation_thresholds.shape, dtype=bool)\n            counter = 0\n            while_condition = True\n            max_motion_per_frame = (self.dims[1] * self.dims[2]) * self.vars['maximal_growth_factor'] * 2\n            if self.vars['lighter_background']:\n                basic_bckgrnd_values = np.quantile(converted_video[:(self.lost_frames + 1), ...], 0.9, axis=(1, 2))\n            else:\n                basic_bckgrnd_values = np.quantile(converted_video[:(self.lost_frames + 1), ...], 0.1, axis=(1, 2))\n            # Try different values of do_threshold_segmentation and keep the one that does not\n            # segment more than x percent of the image\n            while counter &lt;= 14:\n                value_threshold = value_segmentation_thresholds[counter]\n                if self.vars['lighter_background']:\n                    l_threshold = (1 + value_threshold) * np.max(covering_l_values)\n                else:\n                    l_threshold = (1 - value_threshold) * np.min(covering_l_values)\n                starting_segmentation, l_threshold_over_time = segment_with_lum_value(converted_video[:(self.lost_frames + 1), ...],\n                                                               basic_bckgrnd_values, l_threshold,\n                                                               self.vars['lighter_background'])\n\n                changing_pixel_number = np.sum(np.absolute(np.diff(starting_segmentation.astype(np.int8), 1, 0)), (1, 2))\n                validation = np.max(np.sum(starting_segmentation, (1, 2))) &lt; max_motion_per_frame and (\n                        np.max(changing_pixel_number) &lt; max_motion_per_frame)\n                validated_thresholds[counter] = validation\n                if np.any(validated_thresholds):\n                    if not validation:\n                        break\n                counter += 1\n            # If any threshold is accepted, use their average to proceed the final thresholding\n            valid_number = validated_thresholds.sum()\n            if valid_number &gt; 0:\n                if valid_number &gt; 2:\n                    index_to_keep = 2\n                else:\n                    index_to_keep = valid_number - 1\n                value_threshold = value_segmentation_thresholds[\n                    np.uint8(np.floor(np.mean(np.nonzero(validated_thresholds)[0][index_to_keep])))]\n            else:\n                value_threshold = 0\n\n            if self.vars['lighter_background']:\n                l_threshold = (1 + value_threshold) * np.max(covering_l_values)\n            else:\n                l_threshold = (1 - value_threshold) * np.min(covering_l_values)\n            if do_threshold_segmentation:\n                if self.vars['lighter_background']:\n                    basic_bckgrnd_values = np.quantile(converted_video, 0.9, axis=(1, 2))\n                else:\n                    basic_bckgrnd_values = np.quantile(converted_video, 0.1, axis=(1, 2))\n                luminosity_segmentation, l_threshold_over_time = segment_with_lum_value(converted_video, basic_bckgrnd_values,\n                                                                 l_threshold, self.vars['lighter_background'])\n            else:\n                luminosity_segmentation, l_threshold_over_time = segment_with_lum_value(converted_video[:(self.lost_frames + 1), ...],\n                                                               basic_bckgrnd_values, l_threshold,\n                                                               self.vars['lighter_background'])\n        return luminosity_segmentation, l_threshold_over_time\n\n    def smooth_pixel_slopes(self, converted_video: NDArray) -&gt; NDArray:\n        \"\"\"\n        Apply smoothing to pixel slopes in a video by convolving with a moving average kernel.\n\n        Parameters\n        ----------\n        converted_video : NDArray\n            The input video array to be smoothed.\n\n        Returns\n        -------\n        NDArray\n            Smoothed video array with pixel slopes averaged using a moving average kernel.\n\n        Raises\n        ------\n        MemoryError\n            If there is not enough RAM available to perform the smoothing operation.\n\n        Notes\n        -----\n        This function applies a moving average kernel to each pixel across the frames of\n        the input video. The smoothing operation can be repeated based on user-defined settings.\n        The precision of the output array is controlled by a flag that determines whether to\n        save memory at the cost of accuracy.\n\n        Examples\n        --------\n        &gt;&gt;&gt; smoothed = smooth_pixel_slopes(converted_video)\n        &gt;&gt;&gt; print(smoothed.shape)  # Expected output will vary depending on the input video shape\n        \"\"\"\n        try:\n            if self.vars['lose_accuracy_to_save_memory']:\n                smoothed_video = np.zeros(self.dims, dtype=np.float32)\n                smooth_kernel = np.ones(self.step, dtype=np.float64) / self.step\n                for i in np.arange(converted_video.shape[1]):\n                    for j in np.arange(converted_video.shape[2]):\n                        padded = np.pad(converted_video[:, i, j] / self.mean_intensity_per_frame,\n                                     (self.step // 2, self.step - 1 - self.step // 2), mode='edge')\n                        moving_average = np.convolve(padded, smooth_kernel, mode='valid')\n                        if self.vars['repeat_video_smoothing'] &gt; 1:\n                            for it in np.arange(1, self.vars['repeat_video_smoothing']):\n                                padded = np.pad(moving_average,\n                                             (self.step // 2, self.step - 1 - self.step // 2), mode='edge')\n                                moving_average = np.convolve(padded, smooth_kernel, mode='valid')\n                        smoothed_video[:, i, j] = moving_average.astype(np.float32)\n            else:\n                smoothed_video = np.zeros(self.dims, dtype=np.float64)\n                smooth_kernel = np.ones(self.step) / self.step\n                for i in np.arange(converted_video.shape[1]):\n                    for j in np.arange(converted_video.shape[2]):\n                        padded = np.pad(converted_video[:, i, j] / self.mean_intensity_per_frame,\n                                     (self.step // 2, self.step - 1 - self.step // 2), mode='edge')\n                        moving_average = np.convolve(padded, smooth_kernel, mode='valid')\n                        if self.vars['repeat_video_smoothing'] &gt; 1:\n                            for it in np.arange(1, self.vars['repeat_video_smoothing']):\n                                padded = np.pad(moving_average,\n                                             (self.step // 2, self.step - 1 - self.step // 2), mode='edge')\n                                moving_average = np.convolve(padded, smooth_kernel, mode='valid')\n                        smoothed_video[:, i, j] = moving_average\n            return smoothed_video\n\n        except MemoryError:\n            logging.error(\"Not enough RAM available to smooth pixel curves. Detection may fail.\")\n            smoothed_video = converted_video\n            return smoothed_video\n\n    def lum_slope_segmentation(self, converted_video: NDArray) -&gt; NDArray:\n        \"\"\"\n        Perform lum slope segmentation on the given video.\n\n        Parameters\n        ----------\n        converted_video : NDArray\n            The input video array for segmentation processing.\n\n        Returns\n        -------\n        NDArray\n            Segmented gradient array of the video. If segmentation fails,\n            returns `None` for the corresponding frames.\n\n        Notes\n        -----\n        This function may consume significant memory and adjusts\n        data types (float32 or float64) based on available RAM.\n\n        Examples\n        --------\n        &gt;&gt;&gt; result = lum_slope_segmentation(converted_video)\n        \"\"\"\n        shape_motion_failed : bool = False\n        # 2) Contrast increase\n        oridx = np.nonzero(self.origin)\n        notoridx = np.nonzero(1 - self.origin)\n        do_increase_contrast = np.mean(converted_video[0, oridx[0], oridx[1]]) * 10 &gt; np.mean(\n                converted_video[0, notoridx[0], notoridx[1]])\n        necessary_memory = self.dims[0] * self.dims[1] * self.dims[2] * 64 * 2 * 1.16415e-10\n        available_memory = (virtual_memory().available &gt;&gt; 30) - self.vars['min_ram_free']\n        if self.vars['lose_accuracy_to_save_memory']:\n            derive = converted_video.astype(np.float32)\n        else:\n            derive = converted_video.astype(np.float64)\n        if necessary_memory &gt; available_memory:\n            converted_video = None\n\n        if do_increase_contrast:\n            derive = np.square(derive)\n\n        # 3) Get the gradient\n        necessary_memory = derive.size * 64 * 4 * 1.16415e-10\n        available_memory = (virtual_memory().available &gt;&gt; 30) - self.vars['min_ram_free']\n        if necessary_memory &gt; available_memory:\n            for cy in np.arange(self.dims[1]):\n                for cx in np.arange(self.dims[2]):\n                    if self.vars['lose_accuracy_to_save_memory']:\n                        derive[:, cy, cx] = np.gradient(derive[:, cy, cx], self.step).astype(np.float32)\n                    else:\n                        derive[:, cy, cx] = np.gradient(derive[:, cy, cx], self.step)\n        else:\n            if self.vars['lose_accuracy_to_save_memory']:\n                derive = np.gradient(derive, self.step, axis=0).astype(np.float32)\n            else:\n                derive = np.gradient(derive, self.step, axis=0)\n\n        # 4) Segment\n        if self.vars['lighter_background']:\n            covering_slopes = np.min(derive[:self.substantial_time, :, :], 0) * self.substantial_image\n        else:\n            covering_slopes = np.max(derive[:self.substantial_time, :, :], 0) * self.substantial_image\n        covering_slopes = covering_slopes[covering_slopes != 0]\n        if len(covering_slopes) == 0:\n            shape_motion_failed = True\n\n        gradient_segmentation = None\n        if not shape_motion_failed:\n            gradient_segmentation = np.zeros(self.dims, np.uint8)\n            ####\n            # ease_slope_segmentation = 0.8\n            value_segmentation_thresholds = np.arange(0.8, -0.7, -0.1)\n            validated_thresholds = np.zeros(value_segmentation_thresholds.shape, dtype=bool)\n            counter = 0\n            while_condition = True\n            max_motion_per_frame = (self.dims[1] * self.dims[2]) * self.vars['maximal_growth_factor']\n            # Try different values of do_slope_segmentation and keep the one that does not\n            # segment more than x percent of the image\n            while counter &lt; value_segmentation_thresholds.shape[0]:\n                ease_slope_segmentation = value_segmentation_thresholds[counter]\n                if self.vars['lighter_background']:\n                    gradient_threshold = (1 + ease_slope_segmentation) * np.max(covering_slopes)\n                    sample = np.less(derive[:self.substantial_time], gradient_threshold)\n                else:\n                    gradient_threshold = (1 - ease_slope_segmentation) * np.min(covering_slopes)\n                    sample = np.greater(derive[:self.substantial_time], gradient_threshold)\n                changing_pixel_number = np.sum(np.absolute(np.diff(sample.astype(np.int8), 1, 0)), (1, 2))\n                validation = np.max(np.sum(sample, (1, 2))) &lt; max_motion_per_frame and (\n                        np.max(changing_pixel_number) &lt; max_motion_per_frame)\n                validated_thresholds[counter] = validation\n                if np.any(validated_thresholds):\n                    if not validation:\n                        break\n                counter += 1\n                # If any threshold is accepted, use their average to proceed the final thresholding\n            valid_number = validated_thresholds.sum()\n            if valid_number &gt; 0:\n                if valid_number &gt; 2:\n                    index_to_keep = 2\n                else:\n                    index_to_keep = valid_number - 1\n                ease_slope_segmentation = value_segmentation_thresholds[\n                    np.uint8(np.floor(np.mean(np.nonzero(validated_thresholds)[0][index_to_keep])))]\n\n                if self.vars['lighter_background']:\n                    gradient_threshold = (1 - ease_slope_segmentation) * np.max(covering_slopes)\n                    gradient_segmentation[:-self.lost_frames, :, :] = np.less(derive, gradient_threshold)[\n                        self.lost_frames:, :, :]\n                else:\n                    gradient_threshold = (1 - ease_slope_segmentation) * np.min(covering_slopes)\n                    gradient_segmentation[:-self.lost_frames, :, :] = np.greater(derive, gradient_threshold)[\n                        self.lost_frames:, :, :]\n            else:\n                if self.vars['lighter_background']:\n                    gradient_segmentation[:-self.lost_frames, :, :] = (derive &lt; (np.min(derive, (1, 2)) * 1.1)[:, None, None])[self.lost_frames:, :, :]\n                else:\n                    gradient_segmentation[:-self.lost_frames, :, :] = (derive &gt; (np.max(derive, (1, 2)) * 0.1)[:, None, None])[self.lost_frames:, :, :]\n        return gradient_segmentation\n\n    def update_ring_width(self):\n        \"\"\"\n\n        Update the `pixel_ring_depth` and create an erodila disk.\n\n        This method ensures that the pixel ring depth is odd and at least 3,\n        then creates an erodila disk of that size.\n        \"\"\"\n        # Make sure that self.pixels_depths are odd and greater than 3\n        if self.pixel_ring_depth &lt;= 3:\n            self.pixel_ring_depth = 3\n        if self.pixel_ring_depth % 2 == 0:\n            self.pixel_ring_depth = self.pixel_ring_depth + 1\n        self.erodila_disk = create_ellipse(self.pixel_ring_depth, self.pixel_ring_depth, min_size=3).astype(np.uint8)\n        self.max_distance = self.pixel_ring_depth * self.vars['detection_range_factor']\n\n    def initialize_post_processing(self):\n        \"\"\"\n\n        Initialize post-processing for video analysis.\n\n        This function initializes various parameters and prepares the binary\n        representation used in post-processing of video data. It logs information about\n        the settings, handles initial origin states, sets up segmentation data,\n        calculates surface areas, and optionally corrects errors around the initial\n        shape or prevents fast growth near the periphery.\n\n        Notes\n        -----\n        This function performs several initialization steps and logs relevant information,\n        including handling different origin states, updating segmentation data, and\n        calculating the gravity field based on binary representation.\n\n        \"\"\"\n        ## Initialization\n        logging.info(f\"Arena n\u00b0{self.one_descriptor_per_arena['arena']}. Starting Post_processing. Fading detection: {self.vars['do_fading']}: {self.vars['fading']}, Subtract background: {self.vars['subtract_background']}, Correct errors around initial shape: {self.vars['correct_errors_around_initial']}, Connect distant shapes: {self.vars['detection_range_factor'] &gt; 0}, How to select appearing cell(s): {self.vars['appearance_detection_method']}\")\n        self.binary = np.zeros(self.dims[:3], dtype=np.uint8)\n        if self.origin.shape[0] != self.binary[self.start - 1, :, :].shape[0] or self.origin.shape[1] != self.binary[self.start - 1, :, :].shape[1]:\n            logging.error(\"Unaltered videos deprecated, they have been created with different settings.\\nDelete .npy videos and Data to run Cellects quickly.pkl and re-run\")\n\n        if self.vars['origin_state'] == \"invisible\":\n            self.binary[self.start - 1, :, :] = deepcopy(self.origin)\n            self.covering_intensity[self.origin_idx[0], self.origin_idx[1]] = self.converted_video[self.start, self.origin_idx[0], self.origin_idx[1]]\n        else:\n            if self.vars['origin_state'] == \"fluctuating\":\n                self.covering_intensity[self.origin_idx[0], self.origin_idx[1]] = np.median(self.converted_video[:self.start, self.origin_idx[0], self.origin_idx[1]], axis=0)\n\n            self.binary[:self.start, :, :] = np.repeat(np.expand_dims(self.origin, 0), self.start, axis=0)\n            if self.start &lt; self.step:\n                frames_to_assess = self.step\n                self.segmented[self.start - 1, ...] = self.binary[self.start - 1, :, :]\n                for t in np.arange(self.start, self.lost_frames):\n                    # Only keep pixels that are always detected\n                    always_found = np.sum(self.segmented[t:(t + frames_to_assess), ...], 0)\n                    always_found = always_found == frames_to_assess\n                    # Remove too small shapes\n                    without_small, stats, centro = cc(always_found.astype(np.uint8))\n                    large_enough = np.nonzero(stats[1:, 4] &gt; ((self.vars['first_move_threshold'] + 1) // 2))[0]\n                    if len(large_enough) &gt; 0:\n                        always_found *= np.isin(always_found, large_enough + 1)\n                        always_found = np.logical_or(always_found, self.segmented[t - 1, ...])\n                        self.segmented[t, ...] *= always_found\n                    else:\n                        self.segmented[t, ...] = 0\n                    self.segmented[t, ...] = np.logical_or(self.segmented[t - 1, ...], self.segmented[t, ...])\n        self.mean_distance_per_frame = None\n        self.surfarea = np.zeros(self.dims[0], dtype =np.uint64)\n        self.surfarea[:self.start] = np.sum(self.binary[:self.start, :, :], (1, 2))\n        self.gravity_field = inverted_distance_transform(self.binary[(self.start - 1), :, :],\n                                           np.sqrt(np.sum(self.binary[(self.start - 1), :, :])))\n        if self.vars['correct_errors_around_initial']:\n            self.rays, self.sun = draw_me_a_sun(self.binary[(self.start - 1), :, :], ray_length_coef=1)  # plt.imshow(sun)\n            self.holes = np.zeros(self.dims[1:], dtype=np.uint8)\n            self.pixel_ring_depth += 2\n            self.update_ring_width()\n\n        if self.vars['prevent_fast_growth_near_periphery']:\n            self.near_periphery = np.zeros(self.dims[1:])\n            if self.vars['arena_shape'] == 'circle':\n                periphery_width = self.vars['periphery_width'] * 2\n                elliperiphery = create_ellipse(self.dims[1] - periphery_width, self.dims[2] - periphery_width, min_size=3)\n                half_width = periphery_width // 2\n                if periphery_width % 2 == 0:\n                    self.near_periphery[half_width:-half_width, half_width:-half_width] = elliperiphery\n                else:\n                    self.near_periphery[half_width:-half_width - 1, half_width:-half_width - 1] = elliperiphery\n                self.near_periphery = 1 - self.near_periphery\n            else:\n                self.near_periphery[:self.vars['periphery_width'], :] = 1\n                self.near_periphery[-self.vars['periphery_width']:, :] = 1\n                self.near_periphery[:, :self.vars['periphery_width']] = 1\n                self.near_periphery[:, -self.vars['periphery_width']:] = 1\n            self.near_periphery = np.nonzero(self.near_periphery)\n\n    def update_shape(self, show_seg: bool):\n        \"\"\"\n        Update the shape of detected objects in the current frame by analyzing\n        segmentation potentials and applying morphological operations.\n\n        Parameters\n        ----------\n        show_seg : bool\n            Flag indicating whether to display segmentation results.\n\n        Notes\n        -----\n        This function performs several operations to update the shape of detected objects:\n        - Analyzes segmentation potentials from previous frames.\n        - Applies morphological operations to refine the shape.\n        - Updates internal state variables such as `binary` and `covering_intensity`.\n\n        \"\"\"\n        # Get from gradients, a 2D matrix of potentially covered pixels\n        # I/ dilate the shape made with covered pixels to assess for covering\n\n        # I/ 1) Only keep pixels that have been detected at least two times in the three previous frames\n        if self.dims[0] &lt; 100:\n            new_potentials = self.segmented[self.t, :, :]\n        else:\n            if self.t &gt; 1:\n                new_potentials = np.sum(self.segmented[(self.t - 2): (self.t + 1), :, :], 0, dtype=np.uint8)\n            else:\n                new_potentials = np.sum(self.segmented[: (self.t + 1), :, :], 0, dtype=np.uint8)\n            new_potentials[new_potentials == 1] = 0\n            new_potentials[new_potentials &gt; 1] = 1\n\n        # I/ 2) If an image displays more new potential pixels than 50% of image pixels,\n        # one of these images is considered noisy and we try taking only one.\n        frame_counter = -1\n        maximal_size = 0.5 * new_potentials.size\n        if (self.vars[\"do_threshold_segmentation\"] or self.vars[\"frame_by_frame_segmentation\"]) and self.t &gt; np.max((self.start + self.step, 6)):\n           maximal_size = np.min((np.max(self.binary[:self.t].sum((1, 2))) * (1 + self.vars['maximal_growth_factor']), self.borders.sum()))\n        while np.logical_and(np.sum(new_potentials) &gt; maximal_size,\n                             frame_counter &lt;= 5):  # np.logical_and(np.sum(new_potentials &gt; 0) &gt; 5 * np.sum(dila_ring), frame_counter &lt;= 5):\n            frame_counter += 1\n            if frame_counter &gt; self.t:\n                break\n            else:\n                if frame_counter &lt; 5:\n                    new_potentials = self.segmented[self.t - frame_counter, :, :]\n                else:\n                # If taking only one image is not enough, use the inverse of the fadinged matrix as new_potentials\n                # Given it haven't been processed by any slope calculation, it should be less noisy\n                    new_potentials = np.sum(self.segmented[(self.t - 5): (self.t + 1), :, :], 0, dtype=np.uint8)\n                    new_potentials[new_potentials &lt; 6] = 0\n                    new_potentials[new_potentials == 6] = 1\n\n\n        new_shape = deepcopy(self.binary[self.t - 1, :, :])\n        new_potentials = cv2.morphologyEx(new_potentials, cv2.MORPH_CLOSE, cross_33)\n        new_potentials = cv2.morphologyEx(new_potentials, cv2.MORPH_OPEN, cross_33) * self.borders\n        new_shape = np.logical_or(new_shape, new_potentials).astype(np.uint8)\n        # Add distant shapes within a radius, score every added pixels according to their distance\n        if not self.vars['several_blob_per_arena']:\n            if new_shape.sum() == 0:\n                new_shape = deepcopy(new_potentials)\n            else:\n                pads = ProgressivelyAddDistantShapes(new_potentials, new_shape, self.max_distance)\n                r = weakref.ref(pads)\n                # If max_distance is non nul look for distant shapes\n                pads.consider_shapes_sizes(self.vars['min_size_for_connection'],\n                                                     self.vars['max_size_for_connection'])\n                pads.connect_shapes(only_keep_connected_shapes=True, rank_connecting_pixels=True)\n\n                new_shape = deepcopy(pads.expanded_shape)\n                new_shape[new_shape &gt; 1] = 1\n                if np.logical_and(self.t &gt; self.step, self.t &lt; self.dims[0]):\n                    if np.any(pads.expanded_shape &gt; 5):\n                        # Add distant shapes back in time at the covering speed of neighbors\n                        self.binary[self.t][np.nonzero(new_shape)] = 1\n                        self.binary[(self.step):(self.t + 1), :, :] = \\\n                            pads.modify_past_analysis(self.binary[(self.step):(self.t + 1), :, :],\n                                                      self.segmented[(self.step):(self.t + 1), :, :])\n                        new_shape = deepcopy(self.binary[self.t, :, :])\n                pads = None\n\n            # Fill holes\n            new_shape = cv2.morphologyEx(new_shape, cv2.MORPH_CLOSE, cross_33)\n\n        if self.vars['do_fading'] and (self.t &gt; self.step + self.lost_frames):\n            # Shape Erosion\n            # I/ After a substantial growth, erode the shape made with covered pixels to assess for fading\n            # Use the newly covered pixels to calculate their mean covering intensity\n            new_idx = np.nonzero(np.logical_xor(new_shape, self.binary[self.t - 1, :, :]))\n            start_intensity_monitoring = self.t - self.lost_frames - self.step\n            end_intensity_monitoring = self.t - self.lost_frames\n            self.covering_intensity[new_idx[0], new_idx[1]] = np.median(self.converted_video[start_intensity_monitoring:end_intensity_monitoring, new_idx[0], new_idx[1]], axis=0)\n            previous_binary = self.binary[self.t - 1, :, :]\n            greyscale_image = self.converted_video[self.t - self.lost_frames, :, :]\n            protect_from_fading = None\n            if self.vars['origin_state'] == 'constant':\n                protect_from_fading = self.origin\n            new_shape, self.covering_intensity = cell_leaving_detection(new_shape, self.covering_intensity, previous_binary, greyscale_image, self.vars['fading'], self.vars['lighter_background'], self.vars['several_blob_per_arena'], self.erodila_disk, protect_from_fading)\n\n        self.covering_intensity *= new_shape\n        self.binary[self.t, :, :] = new_shape * self.borders\n        self.surfarea[self.t] = np.sum(self.binary[self.t, :, :])\n\n        # Calculate the mean distance covered per frame and correct for a ring of not really fading pixels\n        if self.mean_distance_per_frame is None:\n            if self.vars['correct_errors_around_initial'] and not self.vars['several_blob_per_arena']:\n                if np.logical_and((self.t % 20) == 0,\n                                  np.logical_and(self.surfarea[self.t] &gt; self.substantial_growth,\n                                                 self.surfarea[self.t] &lt; self.substantial_growth * 2)):\n                    shape = self.binary[self.t, :, :] * self.sun\n                    back = (1 - self.binary[self.t, :, :]) * self.sun\n                    for ray in self.rays:\n                        # For each sun's ray, see how they cross the shape/back and\n                        # store the gravity_field value of these pixels (distance to the original shape).\n                        ray_through_shape = (shape == ray) * self.gravity_field\n                        ray_through_back = (back == ray) * self.gravity_field\n                        if np.any(ray_through_shape):\n                            if np.any(ray_through_back):\n                                # If at least one back pixel is nearer to the original shape than a shape pixel,\n                                # there is a hole to fill.\n                                if np.any(ray_through_back &gt; np.min(ray_through_shape[ray_through_shape &gt; 0])):\n                                    # Check if the nearest pixels are shape, if so, supress them until the nearest pixel\n                                    # becomes back\n                                    while np.max(ray_through_back) &lt;= np.max(ray_through_shape):\n                                        ray_through_shape[ray_through_shape == np.max(ray_through_shape)] = 0\n                                    # Now, all back pixels that are nearer than the closest shape pixel should get filled\n                                    # To do so, replace back pixels further than the nearest shape pixel by 0\n                                    ray_through_back[ray_through_back &lt; np.max(ray_through_shape)] = 0\n                                    self.holes[np.nonzero(ray_through_back)] = 1\n                            else:\n                                self.rays = np.concatenate((self.rays[:(ray - 2)], self.rays[(ray - 1):]))\n                        ray_through_shape = None\n                        ray_through_back = None\n            if np.any(self.surfarea[:self.t] &gt; self.substantial_growth * 2):\n\n                if self.vars['correct_errors_around_initial'] and not self.vars['several_blob_per_arena']:\n                    # Apply the hole correction\n                    self.holes = cv2.morphologyEx(self.holes, cv2.MORPH_CLOSE, cross_33, iterations=10)\n                    # If some holes are not covered by now\n                    if np.any(self.holes * (1 - self.binary[self.t, :, :])):\n                        self.binary[:(self.t + 1), :, :], holes_time_end, distance_against_time = \\\n                            dynamically_expand_to_fill_holes(self.binary[:(self.t + 1), :, :], self.holes)\n                        if holes_time_end is not None:\n                            self.binary[holes_time_end:(self.t + 1), :, :] += self.binary[holes_time_end, :, :]\n                            self.binary[holes_time_end:(self.t + 1), :, :][\n                                self.binary[holes_time_end:(self.t + 1), :, :] &gt; 1] = 1\n                            self.surfarea[:(self.t + 1)] = np.sum(self.binary[:(self.t + 1), :, :], (1, 2))\n\n                    else:\n                        distance_against_time = [1, 2]\n                else:\n                    distance_against_time = [1, 2]\n                distance_against_time = np.diff(distance_against_time)\n                if len(distance_against_time) &gt; 0:\n                    self.mean_distance_per_frame = np.mean(- distance_against_time)\n                else:\n                    self.mean_distance_per_frame = 1\n\n        if self.vars['prevent_fast_growth_near_periphery']:\n            # growth_near_periphery = np.diff(self.binary[self.t-1:self.t+1, :, :] * self.near_periphery, axis=0)\n            growth_near_periphery = np.diff(self.binary[self.t-1:self.t+1, self.near_periphery[0], self.near_periphery[1]], axis=0)\n            if (growth_near_periphery == 1).sum() &gt; self.vars['max_periphery_growth']:\n                # self.binary[self.t, self.near_periphery[0], self.near_periphery[1]] = self.binary[self.t - 1, self.near_periphery[0], self.near_periphery[1]]\n                periphery_to_remove = np.zeros(self.dims[1:], dtype=np.uint8)\n                periphery_to_remove[self.near_periphery[0], self.near_periphery[1]] = self.binary[self.t, self.near_periphery[0], self.near_periphery[1]]\n                shapes, stats, centers = cc(periphery_to_remove)\n                periphery_to_remove = np.nonzero(np.isin(shapes, np.nonzero(stats[:, 4] &gt; self.vars['max_periphery_growth'])[0][1:]))\n                self.binary[self.t, periphery_to_remove[0], periphery_to_remove[1]] = self.binary[self.t - 1, periphery_to_remove[0], periphery_to_remove[1]]\n                if not self.vars['several_blob_per_arena']:\n                    shapes, stats, centers = cc(self.binary[self.t, ...])\n                    shapes[shapes != 1] = 0\n                    self.binary[self.t, ...] = shapes\n\n        # Display\n        if show_seg:\n            if self.visu is not None:\n                im_to_display = deepcopy(self.visu[self.t, ...])\n                contours = np.nonzero(cv2.morphologyEx(self.binary[self.t, :, :], cv2.MORPH_GRADIENT, cross_33))\n                if self.vars['lighter_background']:\n                    im_to_display[contours[0], contours[1]] = 0\n                else:\n                    im_to_display[contours[0], contours[1]] = 255\n            else:\n                im_to_display = self.binary[self.t, :, :] * 255\n            imtoshow = cv2.resize(im_to_display, (540, 540))\n            cv2.imshow(\"shape_motion\", imtoshow)\n            cv2.waitKey(1)\n        self.t += 1\n\n    def get_descriptors_from_binary(self, release_memory: bool=True):\n        \"\"\"\n\n        Methods: get_descriptors_from_binary\n\n        Summary\n        -------\n        Generates shape descriptors for binary images, computes these descriptors for each frame and handles colony\n        tracking. This method can optionally release memory to reduce usage, apply scaling factors to descriptors\n        in millimeters and computes solidity separately if requested.\n\n        Parameters\n        ----------\n        release_memory : bool, optional\n            Flag to determine whether memory should be released after computation. Default is True.\n\n        Other Parameters\n        ----------------\n        **self.one_row_per_frame**\n            DataFrame to store one row of descriptors per frame.\n            - **'arena'**: Arena identifier, repeated for each frame.\n            - **'time'**: Array of time values corresponding to frames.\n\n        **self.binary**\n            3D array representing binary images over time.\n            - **t,x,y**: Time index, x-coordinate, and y-coordinate.\n\n        **self.dims**\n            Tuple containing image dimensions.\n            - **0**: Number of time frames.\n            - **1,2**: Image width and height respectively.\n\n        **self.surfarea**\n            Array containing surface areas for each frame.\n\n        **self.time_interval**\n            Time interval between frames, calculated only if provided timings are non-zero.\n\n        Notes\n        -----\n        This method uses various helper methods and classes like `ShapeDescriptors` for computing shape descriptors,\n        `PercentAndTimeTracker` for progress tracking, and other image processing techniques such as connected components analysis.\n\n        \"\"\"\n        logging.info(f\"Arena n\u00b0{self.one_descriptor_per_arena['arena']}. Computing and saving specimen(s) coordinates and required descriptors\")\n        if release_memory:\n            self.substantial_image = None\n            self.covering_intensity = None\n            self.segmented = None\n            self.gravity_field = None\n            self.sun = None\n            self.rays = None\n            self.holes = None\n            collect()\n        self.surfarea = self.binary.sum((1, 2))\n        timings = self.vars['exif']\n        if len(timings) &lt; self.dims[0]:\n            timings = np.arange(self.dims[0])\n        if np.any(timings &gt; 0):\n            self.time_interval = np.mean(np.diff(timings))\n        else:\n            self.time_interval = 1.\n        timings = timings[:self.dims[0]]\n\n        # Detect first motion\n        self.one_descriptor_per_arena['first_move'] = detect_first_move(self.surfarea, self.vars['first_move_threshold'])\n\n        self.compute_solidity_separately: bool = self.vars['iso_digi_analysis'] and not self.vars['several_blob_per_arena'] and not self.vars['descriptors']['solidity']\n        if self.compute_solidity_separately:\n            self.solidity = np.zeros(self.dims[0], dtype=np.float64)\n        if not self.vars['several_blob_per_arena']:\n            # solidity must be added if detect growth transition is computed\n            if self.compute_solidity_separately:\n                for t in np.arange(self.dims[0]):\n                    solidity = ShapeDescriptors(self.binary[t, :, :], [\"solidity\"])\n                    self.solidity[t] = solidity.descriptors[\"solidity\"]\n            self.one_row_per_frame = compute_one_descriptor_per_frame(self.binary,\n                                                                      self.one_descriptor_per_arena['arena'], timings,\n                                                                      self.vars['descriptors'],\n                                                                      self.vars['output_in_mm'],\n                                                                      self.vars['average_pixel_size'],\n                                                                      self.vars['do_fading'],\n                                                                       self.vars['save_coord_specimen'])\n        else:\n            self.one_row_per_frame = compute_one_descriptor_per_colony(self.binary,\n                                                                       self.one_descriptor_per_arena['arena'], timings,\n                                                                       self.vars['descriptors'],\n                                                                       self.vars['output_in_mm'],\n                                                                       self.vars['average_pixel_size'],\n                                                                       self.vars['do_fading'],\n                                                                       self.vars['first_move_threshold'],\n                                                                       self.vars['save_coord_specimen'])\n        self.one_descriptor_per_arena[\"final_area\"] = self.binary[-1, :, :].sum()\n        if self.vars['output_in_mm']:\n            self.one_descriptor_per_arena = scale_descriptors(self.one_descriptor_per_arena, self.vars['average_pixel_size'])\n\n    def detect_growth_transitions(self):\n        \"\"\"\n        Detect growth transitions in a biological image processing context.\n\n        Analyzes the growth transitions of a shape within an arena, determining\n        whether growth is isotropic and identifying any breaking points.\n\n        Notes:\n            This method modifies the `one_descriptor_per_arena` dictionary in place\n            to include growth transition information.\n\n        \"\"\"\n        if self.vars['iso_digi_analysis'] and not self.vars['several_blob_per_arena']:\n            self.one_descriptor_per_arena['iso_digi_transi'] = pd.NA\n            if not pd.isna(self.one_descriptor_per_arena['first_move']):\n                logging.info(f\"Arena n\u00b0{self.one_descriptor_per_arena['arena']}. Starting growth transition analysis.\")\n\n                # II) Once a pseudopod is deployed, look for a disk/ around the original shape\n                growth_begining = self.surfarea &lt; ((self.surfarea[0] * 1.2) + ((self.dims[1] / 4) * (self.dims[2] / 4)))\n                dilated_origin = cv2.dilate(self.binary[self.one_descriptor_per_arena['first_move'], :, :], kernel=cross_33, iterations=10, borderType=cv2.BORDER_CONSTANT, borderValue=0)\n                isisotropic = np.sum(self.binary[:, :, :] * dilated_origin, (1, 2))\n                isisotropic *= growth_begining\n                # Ask if the dilated origin area is 90% covered during the growth beginning\n                isisotropic = isisotropic &gt; 0.9 * dilated_origin.sum()\n                if np.any(isisotropic):\n                    self.one_descriptor_per_arena['is_growth_isotropic'] = 1\n                    # Determine a solidity reference to look for a potential breaking of the isotropic growth\n                    if self.compute_solidity_separately:\n                        solidity_reference = np.mean(self.solidity[:self.one_descriptor_per_arena['first_move']])\n                        different_solidity = self.solidity &lt; (0.9 * solidity_reference)\n                        del self.solidity\n                    else:\n                        solidity_reference = np.mean(\n                            self.one_row_per_frame.iloc[:(self.one_descriptor_per_arena['first_move']), :][\"solidity\"])\n                        different_solidity = self.one_row_per_frame[\"solidity\"].values &lt; (0.9 * solidity_reference)\n                    # Make sure that isotropic breaking not occur before isotropic growth\n                    if np.any(different_solidity):\n                        self.one_descriptor_per_arena[\"iso_digi_transi\"] = np.nonzero(different_solidity)[0][0] * self.time_interval\n                else:\n                    self.one_descriptor_per_arena['is_growth_isotropic'] = 0\n            else:\n                self.one_descriptor_per_arena['is_growth_isotropic'] = pd.NA\n\n\n    def check_converted_video_type(self):\n        \"\"\"\n        Check if the converted video type is uint8 and normalize it if necessary.\n        \"\"\"\n        if self.converted_video.dtype != \"uint8\":\n            self.converted_video = bracket_to_uint8_image_contrast(self.converted_video)\n\n    def networks_analysis(self, show_seg: bool=False):\n        \"\"\"\n        Perform network detection within a given arena.\n\n        This function carries out the task of detecting networks in an arena\n        based on several parameters and variables. It involves checking video\n        type, performing network detection over time, potentially detecting\n        pseudopods, and smoothing segmentation. The results can be visualized or saved.\nExtract and analyze graphs from a binary representation of network dynamics, producing vertex\n        and edge tables that represent the graph structure over time.\n\n        Args:\n            None\n\n        Attributes:\n            vars (dict): Dictionary of variables that control the graph extraction process.\n                - 'save_graph': Boolean indicating if graph extraction should be performed.\n                - 'save_coord_network': Boolean indicating if the coordinate network should be saved.\n\n            one_descriptor_per_arena (dict): Dictionary containing descriptors for each arena.\n\n            dims (tuple): Tuple containing dimension information.\n                - [0]: Integer representing the number of time steps.\n                - [1]: Integer representing the y-dimension size.\n                - [2]: Integer representing the x-dimension size.\n\n            origin (np.ndarray): Binary image representing the origin of the network.\n\n            binary (np.ndarray): Binary representation of network dynamics over time.\n                Shape: (time_steps, y_dimension, x_dimension).\n\n            converted_video (np.ndarray): Converted video data.\n                Shape: (y_dimension, x_dimension, time_steps).\n\n            network_dynamics (np.ndarray): Network dynamics representation.\n                Shape: (time_steps, y_dimension, x_dimension).\n\n        Notes:\n            - This method performs graph extraction and saves the vertex and edge tables to CSV files.\n            - The CSV files are named according to the arena, time steps, and dimensions.\n\n        Args:\n            show_seg: bool = False\n                A flag that determines whether to display the segmentation visually.\n        \"\"\"\n        coord_pseudopods = None\n        if not self.vars['several_blob_per_arena'] and self.vars['save_coord_network']:\n            self.check_converted_video_type()\n\n            if self.vars['origin_state'] == \"constant\":\n                self.coord_network, coord_pseudopods = detect_network_dynamics(self.converted_video, self.binary,\n                                                           self.one_descriptor_per_arena['arena'], 0,\n                                                           self.visu, self.origin, True, True,\n                                                           self.vars['save_coord_network'], show_seg)\n            else:\n                self.coord_network, coord_pseudopods = detect_network_dynamics(self.converted_video, self.binary,\n                                                           self.one_descriptor_per_arena['arena'], 0,\n                                                           self.visu, None, True, True,\n                                                           self.vars['save_coord_network'], show_seg)\n\n        if not self.vars['several_blob_per_arena'] and self.vars['save_graph']:\n            if self.coord_network is None:\n                self.coord_network = np.array(np.nonzero(self.binary))\n            if self.vars['origin_state'] == \"constant\":\n                extract_graph_dynamics(self.converted_video, self.coord_network, self.one_descriptor_per_arena['arena'],\n                                       0, self.origin, coord_pseudopods)\n            else:\n                extract_graph_dynamics(self.converted_video, self.coord_network, self.one_descriptor_per_arena['arena'],\n                                       0, None, coord_pseudopods)\n\n    def study_cytoscillations(self, show_seg: bool=False):\n        \"\"\"\n\n            Study the cytoskeletal oscillations within a video frame by frame.\n\n            This method performs an analysis of cytoskeletal oscillations in the video,\n            identifying regions of influx and efflux based on pixel connectivity.\n            It also handles memory allocation for the oscillations video, computes\n            connected components, and optionally displays the segmented regions.\n\n            Args:\n                show_seg (bool): If True, display the segmentation results.\n        \"\"\"\n        if self.vars['save_coord_thickening_slimming'] or self.vars['oscilacyto_analysis']:\n            oscillations_video = detect_oscillations_dynamics(self.converted_video, self.binary,\n                                                              self.one_descriptor_per_arena['arena'], self.start,\n                                                              self.vars['expected_oscillation_period'],\n                                                              self.time_interval,\n                                                              self.vars['minimal_oscillating_cluster_size'],\n                                                              self.vars['min_ram_free'],\n                                                              self.vars['lose_accuracy_to_save_memory'],\n                                                              self.vars['save_coord_thickening_slimming'])\n            del oscillations_video\n\n\n    def fractal_descriptions(self):\n        \"\"\"\n\n        Method for analyzing fractal patterns in binary data.\n\n        Fractal analysis is performed on the binary representation of the data,\n        optionally considering network dynamics if specified. The results\n        include fractal dimensions, R-values, and box counts for the data.\n\n        If network analysis is enabled, additional fractal dimensions,\n        R-values, and box counts are calculated for the inner network.\n        If 'output_in_mm' is True, then values in mm can be obtained.\n\n        \"\"\"\n        if self.vars['fractal_analysis']:\n            logging.info(f\"Arena n\u00b0{self.one_descriptor_per_arena['arena']}. Starting fractal analysis.\")\n\n            if self.vars['save_coord_network']:\n                box_counting_dimensions = np.zeros((self.dims[0], 7), dtype=np.float64)\n            else:\n                box_counting_dimensions = np.zeros((self.dims[0], 3), dtype=np.float64)\n\n            for t in np.arange(self.dims[0]):\n                if self.vars['save_coord_network']:\n                    current_network = np.zeros(self.dims[1:], dtype=np.uint8)\n                    net_t = self.coord_network[1:, self.coord_network[0, :] == t]\n                    current_network[net_t[0], net_t[1]] = 1\n                    box_counting_dimensions[t, 0] = current_network.sum()\n                    zoomed_binary, side_lengths = prepare_box_counting(self.binary[t, ...], min_mesh_side=self.vars[\n                        'fractal_box_side_threshold'], zoom_step=self.vars['fractal_zoom_step'], contours=True)\n                    box_counting_dimensions[t, 1], box_counting_dimensions[t, 2], box_counting_dimensions[\n                        t, 3] = box_counting_dimension(zoomed_binary, side_lengths)\n                    zoomed_binary, side_lengths = prepare_box_counting(current_network,\n                                                                       min_mesh_side=self.vars[\n                                                                           'fractal_box_side_threshold'],\n                                                                       zoom_step=self.vars['fractal_zoom_step'],\n                                                                       contours=False)\n                    box_counting_dimensions[t, 4], box_counting_dimensions[t, 5], box_counting_dimensions[\n                        t, 6] = box_counting_dimension(zoomed_binary, side_lengths)\n                else:\n                    zoomed_binary, side_lengths = prepare_box_counting(self.binary[t, ...],\n                                                                       min_mesh_side=self.vars['fractal_box_side_threshold'],\n                                                                       zoom_step=self.vars['fractal_zoom_step'], contours=True)\n                    box_counting_dimensions[t, :] = box_counting_dimension(zoomed_binary, side_lengths)\n\n            if self.vars['save_coord_network']:\n                self.one_row_per_frame[\"inner_network_size\"] = box_counting_dimensions[:, 0]\n                self.one_row_per_frame[\"fractal_dimension\"] = box_counting_dimensions[:, 1]\n                self.one_row_per_frame[\"fractal_r_value\"] = box_counting_dimensions[:, 2]\n                self.one_row_per_frame[\"fractal_box_nb\"] = box_counting_dimensions[:, 3]\n                self.one_row_per_frame[\"inner_network_fractal_dimension\"] = box_counting_dimensions[:, 4]\n                self.one_row_per_frame[\"inner_network_fractal_r_value\"] = box_counting_dimensions[:, 5]\n                self.one_row_per_frame[\"inner_network_fractal_box_nb\"] = box_counting_dimensions[:, 6]\n                if self.vars['output_in_mm']:\n                    self.one_row_per_frame[\"inner_network_size\"] *= self.vars['average_pixel_size']\n            else:\n                self.one_row_per_frame[\"fractal_dimension\"] = box_counting_dimensions[:, 0]\n                self.one_row_per_frame[\"fractal_box_nb\"] = box_counting_dimensions[:, 1]\n                self.one_row_per_frame[\"fractal_r_value\"] = box_counting_dimensions[:, 2]\n\n            if self.vars['save_coord_network']:\n                del self.coord_network\n\n    def save_efficiency_tests(self):\n        \"\"\"\n        Provide images allowing to assess the analysis efficiency\n\n        This method generates two test images used for assessing\n        the efficiency of the analysis. It performs various operations on\n        video frames to create these images, including copying and manipulating\n        frames from the video, detecting contours on binary images,\n        and drawing the arena label on the left of the frames.\n        \"\"\"\n        # Provide images allowing to assess the analysis efficiency\n        if self.dims[0] &gt; 1:\n            after_one_tenth_of_time = np.ceil(self.dims[0] / 10).astype(np.uint64)\n        else:\n            after_one_tenth_of_time = 0\n\n        last_good_detection = self.dims[0] - 1\n        if self.dims[0] &gt; self.lost_frames:\n            if self.vars['do_threshold_segmentation']:\n                last_good_detection -= self.lost_frames\n        else:\n            last_good_detection = 0\n        if self.visu is None:\n            if len(self.converted_video.shape) == 3:\n                self.converted_video = np.stack((self.converted_video, self.converted_video, self.converted_video),\n                                             axis=3)\n            self.efficiency_test_1 = deepcopy(self.converted_video[after_one_tenth_of_time, ...])\n            self.efficiency_test_2 = deepcopy(self.converted_video[last_good_detection, ...])\n        else:\n            self.efficiency_test_1 = deepcopy(self.visu[after_one_tenth_of_time, :, :, :])\n            self.efficiency_test_2 = deepcopy(self.visu[last_good_detection, :, :, :])\n\n        position = (25, self.dims[1] // 2)\n        text = str(self.one_descriptor_per_arena['arena'])\n        contours = np.nonzero(get_contours(self.binary[after_one_tenth_of_time, :, :]))\n        self.efficiency_test_1[contours[0], contours[1], :] = self.vars['contour_color']\n        self.efficiency_test_1 = cv2.putText(self.efficiency_test_1, text, position, cv2.FONT_HERSHEY_SIMPLEX, 1,\n                                        (self.vars[\"contour_color\"], self.vars[\"contour_color\"],\n                                         self.vars[\"contour_color\"], 255), 3)\n\n        eroded_binary = cv2.erode(self.binary[last_good_detection, :, :], cross_33)\n        contours = np.nonzero(self.binary[last_good_detection, :, :] - eroded_binary)\n        self.efficiency_test_2[contours[0], contours[1], :] = self.vars['contour_color']\n        self.efficiency_test_2 = cv2.putText(self.efficiency_test_2, text, position, cv2.FONT_HERSHEY_SIMPLEX, 1,\n                                         (self.vars[\"contour_color\"], self.vars[\"contour_color\"],\n                                          self.vars[\"contour_color\"], 255), 3)\n\n    def save_video(self):\n        \"\"\"\n        Save processed video with contours and other annotations.\n\n        This method processes the binary image to extract contours, overlay them\n        on a video, and save the resulting video file.\n\n        Notes:\n            - This method uses OpenCV for image processing and contour extraction.\n            - The processed video includes contours colored according to the\n              `contour_color` specified in the variables.\n            - Additional annotations such as time in minutes are added to each\n              frame if applicable.\n\n        \"\"\"\n        if self.vars['save_processed_videos']:\n            self.check_converted_video_type()\n            if len(self.converted_video.shape) == 3:\n                self.converted_video = np.stack((self.converted_video, self.converted_video, self.converted_video),\n                                                axis=3)\n            for t in np.arange(self.dims[0]):\n\n                eroded_binary = cv2.erode(self.binary[t, :, :], cross_33)\n                contours = np.nonzero(self.binary[t, :, :] - eroded_binary)\n                self.converted_video[t, contours[0], contours[1], :] = self.vars['contour_color']\n                if \"iso_digi_transi\" in self.one_descriptor_per_arena.keys():\n                    if self.vars['iso_digi_analysis']  and not self.vars['several_blob_per_arena'] and not pd.isna(self.one_descriptor_per_arena[\"iso_digi_transi\"]):\n                        if self.one_descriptor_per_arena['is_growth_isotropic'] == 1:\n                            if t &lt; self.one_descriptor_per_arena[\"iso_digi_transi\"]:\n                                self.converted_video[t, contours[0], contours[1], :] = 0, 0, 255\n            del self.binary\n            del self.surfarea\n            del self.borders\n            del self.origin\n            del self.origin_idx\n            del self.mean_intensity_per_frame\n            del self.erodila_disk\n            collect()\n            if self.visu is None:\n                true_frame_width = self.dims[2]\n                if len(self.vars['background_list']) == 0:\n                    self.background = None\n                else:\n                    self.background = self.vars['background_list'][self.one_descriptor_per_arena['arena'] - 1]\n                if os.path.isfile(f\"ind_{self.one_descriptor_per_arena['arena']}.npy\"):\n                    self.visu = video2numpy(f\"ind_{self.one_descriptor_per_arena['arena']}.npy\",\n                              None, true_frame_width=true_frame_width)\n                else:\n                    self.visu = self.converted_video\n                if len(self.visu.shape) == 3:\n                    self.visu = np.stack((self.visu, self.visu, self.visu), axis=3)\n            self.converted_video = np.concatenate((self.visu, self.converted_video), axis=2)\n\n            if np.any(self.one_row_per_frame['time'] &gt; 0):\n                position = (5, self.dims[1] - 5)\n                if self.vars['time_step_is_arbitrary']:\n                    time_unit = \"\"\n                else:\n                    time_unit = \" min\"\n                for t in np.arange(self.dims[0]):\n                    image = self.converted_video[t, ...]\n                    text = str(self.one_row_per_frame['time'][t]) + time_unit\n                    image = cv2.putText(image,  # numpy array on which text is written\n                                    text,  # text\n                                    position,  # position at which writing has to start\n                                    cv2.FONT_HERSHEY_SIMPLEX,  # font family\n                                    1,  # font size\n                                    (self.vars[\"contour_color\"], self.vars[\"contour_color\"], self.vars[\"contour_color\"], 255),  #(209, 80, 0, 255),  \n                                    2)  # font stroke\n                    self.converted_video[t, ...] = image\n            vid_name = f\"ind_{self.one_descriptor_per_arena['arena']}{self.vars['videos_extension']}\"\n            write_video(self.converted_video, vid_name, is_color=True, fps=self.vars['video_fps'])\n\n    def save_results(self):\n        \"\"\"\n        Save the results of testing and video processing.\n\n        This method handles the saving of efficiency tests, video files,\n        and CSV data related to test results. It checks for existing files before writing new data.\n        Additionally, it cleans up temporary files if configured to do so.\n        \"\"\"\n        self.save_efficiency_tests()\n        self.save_video()\n        if self.vars['several_blob_per_arena']:\n            try:\n                with open(f\"one_row_per_frame_arena{self.one_descriptor_per_arena['arena']}.csv\", 'w') as file:\n                    self.one_row_per_frame.to_csv(file, sep=';', index=False, lineterminator='\\n')\n            except PermissionError:\n                logging.error(f\"Never let one_row_per_frame_arena{self.one_descriptor_per_arena['arena']}.csv open when Cellects runs\")\n\n            create_new_csv: bool = False\n            if os.path.isfile(\"one_row_per_arena.csv\"):\n                try:\n                    with open(f\"one_row_per_arena.csv\", 'r') as file:\n                        stats = pd.read_csv(file, header=0, sep=\";\")\n                except PermissionError:\n                    logging.error(\"Never let one_row_per_arena.csv open when Cellects runs\")\n\n                if len(self.one_descriptor_per_arena) == len(stats.columns) - 1:\n                    try:\n                        with open(f\"one_row_per_arena.csv\", 'w') as file:\n                            stats.iloc[(self.one_descriptor_per_arena['arena'] - 1), 1:] = self.one_descriptor_per_arena.values()\n                            stats.to_csv(file, sep=';', index=False, lineterminator='\\n')\n                    except PermissionError:\n                        logging.error(\"Never let one_row_per_arena.csv open when Cellects runs\")\n                else:\n                    create_new_csv = True\n            else:\n                create_new_csv = True\n            if create_new_csv:\n                with open(f\"one_row_per_arena.csv\", 'w') as file:\n                    stats = pd.DataFrame(np.zeros((len(self.vars['analyzed_individuals']), len(self.one_descriptor_per_arena))),\n                               columns=list(self.one_descriptor_per_arena.keys()))\n                    stats.iloc[(self.one_descriptor_per_arena['arena'] - 1), :] = self.one_descriptor_per_arena.values()\n                    stats.to_csv(file, sep=';', index=False, lineterminator='\\n')\n        if not self.vars['keep_unaltered_videos'] and os.path.isfile(f\"ind_{self.one_descriptor_per_arena['arena']}.npy\"):\n            os.remove(f\"ind_{self.one_descriptor_per_arena['arena']}.npy\")\n\n    def change_results_of_one_arena(self, save_video: bool = True):\n        \"\"\"\n        Manages the saving and updating of CSV files based on data extracted from analyzed\n        one arena. Specifically handles three CSV files: \"one_row_per_arena.csv\",\n        \"one_row_per_frame.csv\".\n        Each file is updated or created based on the presence of existing data.\n        The method ensures that each CSV file contains the relevant information for\n        the given arena, frame, and oscillator cluster data.\n        \"\"\"\n        if save_video:\n            self.save_video()\n        # I/ Update/Create one_row_per_arena.csv\n        create_new_csv: bool = False\n        if os.path.isfile(\"one_row_per_arena.csv\"):\n            try:\n                with open(f\"one_row_per_arena.csv\", 'r') as file:\n                    stats = pd.read_csv(file, header=0, sep=\";\")\n                for stat_name, stat_value in self.one_descriptor_per_arena.items():\n                    if stat_name in stats.columns:\n                        stats.loc[(self.one_descriptor_per_arena['arena'] - 1), stat_name] = np.uint32(self.one_descriptor_per_arena[stat_name])\n                with open(f\"one_row_per_arena.csv\", 'w') as file:\n                    stats.to_csv(file, sep=';', index=False, lineterminator='\\n')\n            except PermissionError:\n                logging.error(\"Never let one_row_per_arena.csv open when Cellects runs\")\n            except Exception as e:\n                logging.error(f\"{e}\")\n                create_new_csv = True\n        else:\n            create_new_csv = True\n        if create_new_csv:\n            logging.info(\"Create a new one_row_per_arena.csv file\")\n            try:\n                with open(f\"one_row_per_arena.csv\", 'w') as file:\n                    stats = pd.DataFrame(np.zeros((len(self.vars['analyzed_individuals']), len(self.one_descriptor_per_arena))),\n                               columns=list(self.one_descriptor_per_arena.keys()))\n                    stats.iloc[(self.one_descriptor_per_arena['arena'] - 1), :] = self.one_descriptor_per_arena.values() #  np.array(list(self.one_descriptor_per_arena.values()), dtype=np.uint32)\n                    stats.to_csv(file, sep=';', index=False, lineterminator='\\n')\n            except PermissionError:\n                logging.error(\"Never let one_row_per_arena.csv open when Cellects runs\")\n\n        # II/ Update/Create one_row_per_frame.csv\n        create_new_csv = False\n        if os.path.isfile(\"one_row_per_frame.csv\"):\n            try:\n                with open(f\"one_row_per_frame.csv\", 'r') as file:\n                    descriptors = pd.read_csv(file, header=0, sep=\";\")\n                for stat_name, stat_value in self.one_row_per_frame.items():\n                    if stat_name in descriptors.columns:\n                        descriptors.loc[((self.one_descriptor_per_arena['arena'] - 1) * self.dims[0]):((self.one_descriptor_per_arena['arena']) * self.dims[0] - 1), stat_name] = self.one_row_per_frame.loc[:, stat_name].values[:]\n                with open(f\"one_row_per_frame.csv\", 'w') as file:\n                    descriptors.to_csv(file, sep=';', index=False, lineterminator='\\n')\n            except PermissionError:\n                logging.error(\"Never let one_row_per_frame.csv open when Cellects runs\")\n            except Exception as e:\n                logging.error(f\"{e}\")\n                create_new_csv = True\n        else:\n            create_new_csv = True\n        if create_new_csv:\n            logging.info(\"Create a new one_row_per_frame.csv file\")\n            try:\n                with open(f\"one_row_per_frame.csv\", 'w') as file:\n                    descriptors = pd.DataFrame(np.zeros((len(self.vars['analyzed_individuals']) * self.dims[0], len(self.one_row_per_frame.columns))),\n                               columns=list(self.one_row_per_frame.keys()))\n                    descriptors.iloc[((self.one_descriptor_per_arena['arena'] - 1) * self.dims[0]):((self.one_descriptor_per_arena['arena']) * self.dims[0]), :] = self.one_row_per_frame\n                    descriptors.to_csv(file, sep=';', index=False, lineterminator='\\n')\n            except PermissionError:\n                logging.error(\"Never let one_row_per_frame.csv open when Cellects runs\")\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.MotionAnalysis.__init__","title":"<code>__init__(l)</code>","text":"<p>Analyzes motion in a given arena using video data.</p> <p>This class processes video frames to analyze motion within a specified area, detecting shapes, covering durations, and generating descriptors for further analysis.</p> <p>Args:     l (list): A list containing various parameters and flags necessary for the motion         analysis.</p> <p>Args:     l[0] (int): Arena index.     l[1] (str): Arena identifier or name, stored in one_descriptor_per_arena['arena'].     l[2] (dict): Variables required for the analysis, stored in vars.     l[3] (bool): Flag to detect shape.     l[4] (bool): Flag to analyze shape.     l[5] (bool): Flag to show segmentation.     l[6] (None or list): Videos already in RAM.</p> <p>Attributes:     vars (dict): Variables required for the analysis.     visu (None): Placeholder for visualization data.     binary (None): Placeholder for binary segmentation data.     origin_idx (None): Placeholder for the index of the first frame.     smoothing_flag (bool): Flag to indicate if smoothing should be applied.     dims (tuple): Dimensions of the converted video.     segmentation (ndarray): Array to store segmentation data.     covering_intensity (ndarray): Intensity values for covering analysis.     mean_intensity_per_frame (ndarray): Mean intensity per frame.     borders (object): Borders of the arena.     pixel_ring_depth (int): Depth of the pixel ring for analysis, default is 9.     step (int): Step size for processing, default is 10.     lost_frames (int): Number of lost frames to account for, default is 10.     start (None or int): Starting frame index for the analysis.</p> <p>Methods:     load_images_and_videos(videos_already_in_ram, arena_idx): Loads images and videos         for the specified arena index.     update_ring_width(): Updates the width of the pixel ring for analysis.     get_origin_shape(): Detects the origin shape in the video frames.     get_covering_duration(step): Calculates the covering duration based on a step size.     detection(): Performs motion detection within the arena.     initialize_post_processing(): Initializes post-processing steps.     update_shape(show_seg): Updates the shape based on segmentation and visualization flags.     get_descriptors_from_binary(): Extracts descriptors from binary data.     detect_growth_transitions(): Detects growth transitions in the data.     networks_analysis(show_seg): Detected networks within the arena based on segmentation         visualization.     study_cytoscillations(show_seg): Studies cytoscillations within the arena with         segmentation visualization.     fractal_descriptions(): Generates fractal descriptions of the analyzed data.     get_descriptors_summary(): Summarizes the descriptors obtained from the analysis.     save_results(): Saves the results of the analysis.</p> Source code in <code>src/cellects/core/motion_analysis.py</code> <pre><code>def __init__(self, l: list):\n\n    \"\"\"\n    Analyzes motion in a given arena using video data.\n\n    This class processes video frames to analyze motion within a specified area,\n    detecting shapes, covering durations, and generating descriptors for further\n    analysis.\n\n    Args:\n        l (list): A list containing various parameters and flags necessary for the motion\n            analysis.\n\n    Args:\n        l[0] (int): Arena index.\n        l[1] (str): Arena identifier or name, stored in one_descriptor_per_arena['arena'].\n        l[2] (dict): Variables required for the analysis, stored in vars.\n        l[3] (bool): Flag to detect shape.\n        l[4] (bool): Flag to analyze shape.\n        l[5] (bool): Flag to show segmentation.\n        l[6] (None or list): Videos already in RAM.\n\n    Attributes:\n        vars (dict): Variables required for the analysis.\n        visu (None): Placeholder for visualization data.\n        binary (None): Placeholder for binary segmentation data.\n        origin_idx (None): Placeholder for the index of the first frame.\n        smoothing_flag (bool): Flag to indicate if smoothing should be applied.\n        dims (tuple): Dimensions of the converted video.\n        segmentation (ndarray): Array to store segmentation data.\n        covering_intensity (ndarray): Intensity values for covering analysis.\n        mean_intensity_per_frame (ndarray): Mean intensity per frame.\n        borders (object): Borders of the arena.\n        pixel_ring_depth (int): Depth of the pixel ring for analysis, default is 9.\n        step (int): Step size for processing, default is 10.\n        lost_frames (int): Number of lost frames to account for, default is 10.\n        start (None or int): Starting frame index for the analysis.\n\n    Methods:\n        load_images_and_videos(videos_already_in_ram, arena_idx): Loads images and videos\n            for the specified arena index.\n        update_ring_width(): Updates the width of the pixel ring for analysis.\n        get_origin_shape(): Detects the origin shape in the video frames.\n        get_covering_duration(step): Calculates the covering duration based on a step size.\n        detection(): Performs motion detection within the arena.\n        initialize_post_processing(): Initializes post-processing steps.\n        update_shape(show_seg): Updates the shape based on segmentation and visualization flags.\n        get_descriptors_from_binary(): Extracts descriptors from binary data.\n        detect_growth_transitions(): Detects growth transitions in the data.\n        networks_analysis(show_seg): Detected networks within the arena based on segmentation\n            visualization.\n        study_cytoscillations(show_seg): Studies cytoscillations within the arena with\n            segmentation visualization.\n        fractal_descriptions(): Generates fractal descriptions of the analyzed data.\n        get_descriptors_summary(): Summarizes the descriptors obtained from the analysis.\n        save_results(): Saves the results of the analysis.\n\n    \"\"\"\n    self.one_descriptor_per_arena = {}\n    self.one_descriptor_per_arena['arena'] = l[1]\n    vars = l[2]\n    detect_shape = l[3]\n    analyse_shape = l[4]\n    show_seg = l[5]\n    videos_already_in_ram = l[6]\n    self.visu = None\n    self.binary = None\n    self.origin_idx = None\n    self.smoothing_flag: bool = False\n    self.drift_mask_coord = None\n    self.coord_network = None\n    logging.info(f\"Start the motion analysis of the arena n\u00b0{self.one_descriptor_per_arena['arena']}\")\n\n    self.vars = vars\n    if not 'contour_color' in self.vars:\n        self.vars['contour_color']: np.uint8 = 0\n    if not 'background_list' in self.vars:\n        self.vars['background_list'] = []\n    self.load_images_and_videos(videos_already_in_ram, l[0])\n\n    self.dims = self.converted_video.shape\n    self.segmented = np.zeros(self.dims, dtype=np.uint8)\n\n    self.covering_intensity = np.zeros(self.dims[1:], dtype=np.float64)\n    self.mean_intensity_per_frame = np.mean(self.converted_video, (1, 2))\n\n    self.borders = image_borders(self.dims[1:], shape=self.vars['arena_shape'])\n    self.pixel_ring_depth = 9\n    self.step: int = 10\n    self.lost_frames = 10\n    self.update_ring_width()\n\n    self.start = None\n    if detect_shape:\n        self.assess_motion_detection()\n        if self.start is not None:\n            self.detection()\n            self.initialize_post_processing()\n            self.t = self.start\n            while self.t &lt; self.dims[0]:  #200:\n                self.update_shape(show_seg)\n            #\n\n        if analyse_shape:\n            self.get_descriptors_from_binary()\n            self.detect_growth_transitions()\n            self.networks_analysis(show_seg)\n            self.study_cytoscillations(show_seg)\n            self.fractal_descriptions()\n            if videos_already_in_ram is None:\n                self.save_results()\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.MotionAnalysis.assess_motion_detection","title":"<code>assess_motion_detection()</code>","text":"<p>Assess if a motion can be detected using the current parameters.</p> <p>Validate the specimen(s) detected in the first frame and evaluate roughly how growth occurs during the video.</p> Source code in <code>src/cellects/core/motion_analysis.py</code> <pre><code>def assess_motion_detection(self):\n    \"\"\"\n    Assess if a motion can be detected using the current parameters.\n\n    Validate the specimen(s) detected in the first frame and evaluate roughly how growth occurs during the video.\n    \"\"\"\n    # Here to conditional layers allow to detect if an expansion/exploration occured\n    self.get_origin_shape()\n    # The first, user-defined is the 'first_move_threshold' and the second is the detection of the\n    # substantial image: if any of them is not detected, the program considers there is no motion.\n    if self.dims[0] &gt;= 40:\n        step = self.dims[0] // 20\n    else:\n        step = 1\n    if self.dims[0] == 1 or self.start &gt;= (self.dims[0] - step - 1):\n        self.start = None\n        self.binary = np.repeat(np.expand_dims(self.origin, 0), self.converted_video.shape[0], axis=0)\n    else:\n        self.get_covering_duration(step)\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.MotionAnalysis.change_results_of_one_arena","title":"<code>change_results_of_one_arena(save_video=True)</code>","text":"<p>Manages the saving and updating of CSV files based on data extracted from analyzed one arena. Specifically handles three CSV files: \"one_row_per_arena.csv\", \"one_row_per_frame.csv\". Each file is updated or created based on the presence of existing data. The method ensures that each CSV file contains the relevant information for the given arena, frame, and oscillator cluster data.</p> Source code in <code>src/cellects/core/motion_analysis.py</code> <pre><code>def change_results_of_one_arena(self, save_video: bool = True):\n    \"\"\"\n    Manages the saving and updating of CSV files based on data extracted from analyzed\n    one arena. Specifically handles three CSV files: \"one_row_per_arena.csv\",\n    \"one_row_per_frame.csv\".\n    Each file is updated or created based on the presence of existing data.\n    The method ensures that each CSV file contains the relevant information for\n    the given arena, frame, and oscillator cluster data.\n    \"\"\"\n    if save_video:\n        self.save_video()\n    # I/ Update/Create one_row_per_arena.csv\n    create_new_csv: bool = False\n    if os.path.isfile(\"one_row_per_arena.csv\"):\n        try:\n            with open(f\"one_row_per_arena.csv\", 'r') as file:\n                stats = pd.read_csv(file, header=0, sep=\";\")\n            for stat_name, stat_value in self.one_descriptor_per_arena.items():\n                if stat_name in stats.columns:\n                    stats.loc[(self.one_descriptor_per_arena['arena'] - 1), stat_name] = np.uint32(self.one_descriptor_per_arena[stat_name])\n            with open(f\"one_row_per_arena.csv\", 'w') as file:\n                stats.to_csv(file, sep=';', index=False, lineterminator='\\n')\n        except PermissionError:\n            logging.error(\"Never let one_row_per_arena.csv open when Cellects runs\")\n        except Exception as e:\n            logging.error(f\"{e}\")\n            create_new_csv = True\n    else:\n        create_new_csv = True\n    if create_new_csv:\n        logging.info(\"Create a new one_row_per_arena.csv file\")\n        try:\n            with open(f\"one_row_per_arena.csv\", 'w') as file:\n                stats = pd.DataFrame(np.zeros((len(self.vars['analyzed_individuals']), len(self.one_descriptor_per_arena))),\n                           columns=list(self.one_descriptor_per_arena.keys()))\n                stats.iloc[(self.one_descriptor_per_arena['arena'] - 1), :] = self.one_descriptor_per_arena.values() #  np.array(list(self.one_descriptor_per_arena.values()), dtype=np.uint32)\n                stats.to_csv(file, sep=';', index=False, lineterminator='\\n')\n        except PermissionError:\n            logging.error(\"Never let one_row_per_arena.csv open when Cellects runs\")\n\n    # II/ Update/Create one_row_per_frame.csv\n    create_new_csv = False\n    if os.path.isfile(\"one_row_per_frame.csv\"):\n        try:\n            with open(f\"one_row_per_frame.csv\", 'r') as file:\n                descriptors = pd.read_csv(file, header=0, sep=\";\")\n            for stat_name, stat_value in self.one_row_per_frame.items():\n                if stat_name in descriptors.columns:\n                    descriptors.loc[((self.one_descriptor_per_arena['arena'] - 1) * self.dims[0]):((self.one_descriptor_per_arena['arena']) * self.dims[0] - 1), stat_name] = self.one_row_per_frame.loc[:, stat_name].values[:]\n            with open(f\"one_row_per_frame.csv\", 'w') as file:\n                descriptors.to_csv(file, sep=';', index=False, lineterminator='\\n')\n        except PermissionError:\n            logging.error(\"Never let one_row_per_frame.csv open when Cellects runs\")\n        except Exception as e:\n            logging.error(f\"{e}\")\n            create_new_csv = True\n    else:\n        create_new_csv = True\n    if create_new_csv:\n        logging.info(\"Create a new one_row_per_frame.csv file\")\n        try:\n            with open(f\"one_row_per_frame.csv\", 'w') as file:\n                descriptors = pd.DataFrame(np.zeros((len(self.vars['analyzed_individuals']) * self.dims[0], len(self.one_row_per_frame.columns))),\n                           columns=list(self.one_row_per_frame.keys()))\n                descriptors.iloc[((self.one_descriptor_per_arena['arena'] - 1) * self.dims[0]):((self.one_descriptor_per_arena['arena']) * self.dims[0]), :] = self.one_row_per_frame\n                descriptors.to_csv(file, sep=';', index=False, lineterminator='\\n')\n        except PermissionError:\n            logging.error(\"Never let one_row_per_frame.csv open when Cellects runs\")\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.MotionAnalysis.check_converted_video_type","title":"<code>check_converted_video_type()</code>","text":"<p>Check if the converted video type is uint8 and normalize it if necessary.</p> Source code in <code>src/cellects/core/motion_analysis.py</code> <pre><code>def check_converted_video_type(self):\n    \"\"\"\n    Check if the converted video type is uint8 and normalize it if necessary.\n    \"\"\"\n    if self.converted_video.dtype != \"uint8\":\n        self.converted_video = bracket_to_uint8_image_contrast(self.converted_video)\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.MotionAnalysis.detect_growth_transitions","title":"<code>detect_growth_transitions()</code>","text":"<p>Detect growth transitions in a biological image processing context.</p> <p>Analyzes the growth transitions of a shape within an arena, determining whether growth is isotropic and identifying any breaking points.</p> <p>Notes:     This method modifies the <code>one_descriptor_per_arena</code> dictionary in place     to include growth transition information.</p> Source code in <code>src/cellects/core/motion_analysis.py</code> <pre><code>def detect_growth_transitions(self):\n    \"\"\"\n    Detect growth transitions in a biological image processing context.\n\n    Analyzes the growth transitions of a shape within an arena, determining\n    whether growth is isotropic and identifying any breaking points.\n\n    Notes:\n        This method modifies the `one_descriptor_per_arena` dictionary in place\n        to include growth transition information.\n\n    \"\"\"\n    if self.vars['iso_digi_analysis'] and not self.vars['several_blob_per_arena']:\n        self.one_descriptor_per_arena['iso_digi_transi'] = pd.NA\n        if not pd.isna(self.one_descriptor_per_arena['first_move']):\n            logging.info(f\"Arena n\u00b0{self.one_descriptor_per_arena['arena']}. Starting growth transition analysis.\")\n\n            # II) Once a pseudopod is deployed, look for a disk/ around the original shape\n            growth_begining = self.surfarea &lt; ((self.surfarea[0] * 1.2) + ((self.dims[1] / 4) * (self.dims[2] / 4)))\n            dilated_origin = cv2.dilate(self.binary[self.one_descriptor_per_arena['first_move'], :, :], kernel=cross_33, iterations=10, borderType=cv2.BORDER_CONSTANT, borderValue=0)\n            isisotropic = np.sum(self.binary[:, :, :] * dilated_origin, (1, 2))\n            isisotropic *= growth_begining\n            # Ask if the dilated origin area is 90% covered during the growth beginning\n            isisotropic = isisotropic &gt; 0.9 * dilated_origin.sum()\n            if np.any(isisotropic):\n                self.one_descriptor_per_arena['is_growth_isotropic'] = 1\n                # Determine a solidity reference to look for a potential breaking of the isotropic growth\n                if self.compute_solidity_separately:\n                    solidity_reference = np.mean(self.solidity[:self.one_descriptor_per_arena['first_move']])\n                    different_solidity = self.solidity &lt; (0.9 * solidity_reference)\n                    del self.solidity\n                else:\n                    solidity_reference = np.mean(\n                        self.one_row_per_frame.iloc[:(self.one_descriptor_per_arena['first_move']), :][\"solidity\"])\n                    different_solidity = self.one_row_per_frame[\"solidity\"].values &lt; (0.9 * solidity_reference)\n                # Make sure that isotropic breaking not occur before isotropic growth\n                if np.any(different_solidity):\n                    self.one_descriptor_per_arena[\"iso_digi_transi\"] = np.nonzero(different_solidity)[0][0] * self.time_interval\n            else:\n                self.one_descriptor_per_arena['is_growth_isotropic'] = 0\n        else:\n            self.one_descriptor_per_arena['is_growth_isotropic'] = pd.NA\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.MotionAnalysis.detection","title":"<code>detection(compute_all_possibilities=False)</code>","text":"<p>Perform frame-by-frame or luminosity-based segmentation on video data to detect cell motion and growth.</p> <p>This function processes video frames using either frame-by-frame segmentation or luminosity-based segmentation algorithms to detect cell motion and growth. It handles drift correction, adjusts parameters based on configuration settings, and applies logical operations to combine results from different segmentation methods.</p> <p>Parameters:</p> Name Type Description Default <code>compute_all_possibilities</code> <code>bool</code> <p>Flag to determine if all segmentation possibilities should be computed, by default False</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> Notes <p>This function modifies the instance variables <code>self.segmented</code>, <code>self.converted_video</code>, and potentially <code>self.luminosity_segmentation</code> and <code>self.gradient_segmentation</code>. Depending on the configuration settings, it performs various segmentation algorithms and updates the instance variables accordingly.</p> Source code in <code>src/cellects/core/motion_analysis.py</code> <pre><code>def detection(self, compute_all_possibilities: bool=False):\n    \"\"\"\n\n        Perform frame-by-frame or luminosity-based segmentation on video data to detect cell motion and growth.\n\n        This function processes video frames using either frame-by-frame segmentation or luminosity-based\n        segmentation algorithms to detect cell motion and growth. It handles drift correction, adjusts parameters\n        based on configuration settings, and applies logical operations to combine results from different segmentation\n        methods.\n\n        Parameters\n        ----------\n        compute_all_possibilities : bool, optional\n            Flag to determine if all segmentation possibilities should be computed, by default False\n\n        Returns\n        -------\n        None\n\n        Notes\n        -----\n        This function modifies the instance variables `self.segmented`, `self.converted_video`,\n        and potentially `self.luminosity_segmentation` and `self.gradient_segmentation`.\n        Depending on the configuration settings, it performs various segmentation algorithms and updates\n        the instance variables accordingly.\n\n    \"\"\"\n    if self.start is None:\n        self.start = 1\n    else:\n        self.start = np.max((self.start, 1))\n    self.lost_frames = np.min((self.step, self.dims[0] // 4))\n    # I/ Image by image segmentation algorithms\n    # If images contain a drift correction (zeros at borders of the image,\n    # Replace these 0 by normal background values before segmenting\n    if self.vars['frame_by_frame_segmentation'] or compute_all_possibilities:\n        logging.info(f\"Arena n\u00b0{self.one_descriptor_per_arena['arena']}. Detect cell motion and growth using the frame by frame segmentation algorithm\")\n        self.segmented = np.zeros(self.dims, dtype=np.uint8)\n        for t in np.arange(self.dims[0]):#20):#\n            analysisi = self.frame_by_frame_segmentation(t, self.segmented[t - 1, ...])\n            self.segmented[t, ...] = analysisi.binary_image\n\n            if self.vars['lose_accuracy_to_save_memory']:\n                self.converted_video[t, ...] = bracket_to_uint8_image_contrast(analysisi.image)\n            else:\n                self.converted_video[t, ...] = analysisi.image\n            if self.vars['convert_for_motion']['logical'] != 'None':\n                if self.vars['lose_accuracy_to_save_memory']:\n                    self.converted_video2[t, ...] = bracket_to_uint8_image_contrast(analysisi.image2)\n                else:\n                    self.converted_video2[t, ...] = analysisi.image2\n\n    if self.vars['color_number'] == 2:\n        luminosity_segmentation, l_threshold_over_time = self.lum_value_segmentation(self.converted_video, do_threshold_segmentation=self.vars['do_threshold_segmentation'] or compute_all_possibilities)\n        self.converted_video = self.smooth_pixel_slopes(self.converted_video)\n        gradient_segmentation = None\n        if self.vars['do_slope_segmentation'] or compute_all_possibilities:\n            gradient_segmentation = self.lum_slope_segmentation(self.converted_video)\n            if gradient_segmentation is not None:\n                gradient_segmentation[-self.lost_frames:, ...] = np.repeat(gradient_segmentation[-self.lost_frames, :, :][np.newaxis, :, :], self.lost_frames, axis=0)\n        if self.vars['convert_for_motion']['logical'] != 'None':\n            if self.vars['do_threshold_segmentation'] or compute_all_possibilities:\n                luminosity_segmentation2, l_threshold_over_time2 = self.lum_value_segmentation(self.converted_video2, do_threshold_segmentation=True)\n                if luminosity_segmentation is None:\n                    luminosity_segmentation = luminosity_segmentation2\n                if luminosity_segmentation is not None:\n                    if self.vars['convert_for_motion']['logical'] == 'Or':\n                        luminosity_segmentation = np.logical_or(luminosity_segmentation, luminosity_segmentation2)\n                    elif self.vars['convert_for_motion']['logical'] == 'And':\n                        luminosity_segmentation = np.logical_and(luminosity_segmentation, luminosity_segmentation2)\n                    elif self.vars['convert_for_motion']['logical'] == 'Xor':\n                        luminosity_segmentation = np.logical_xor(luminosity_segmentation, luminosity_segmentation2)\n            self.converted_video2 = self.smooth_pixel_slopes(self.converted_video2)\n            if self.vars['do_slope_segmentation'] or compute_all_possibilities:\n                gradient_segmentation2 = self.lum_slope_segmentation(self.converted_video2)\n                if gradient_segmentation2 is not None:\n                    gradient_segmentation2[-self.lost_frames:, ...] = np.repeat(gradient_segmentation2[-self.lost_frames, :, :][np.newaxis, :, :], self.lost_frames, axis=0)\n                if gradient_segmentation is None:\n                    gradient_segmentation = gradient_segmentation2\n                if gradient_segmentation is not None:\n                    if self.vars['convert_for_motion']['logical'] == 'Or':\n                        gradient_segmentation = np.logical_or(gradient_segmentation, gradient_segmentation2)\n                    elif self.vars['convert_for_motion']['logical'] == 'And':\n                        gradient_segmentation = np.logical_and(gradient_segmentation, gradient_segmentation2)\n                    elif self.vars['convert_for_motion']['logical'] == 'Xor':\n                        gradient_segmentation = np.logical_xor(gradient_segmentation, gradient_segmentation2)\n\n        if compute_all_possibilities:\n            logging.info(f\"Arena n\u00b0{self.one_descriptor_per_arena['arena']}. Compute all options to detect cell motion and growth. Maximal growth per frame: {self.vars['maximal_growth_factor']}\")\n            if luminosity_segmentation is not None:\n                self.luminosity_segmentation = np.nonzero(luminosity_segmentation)\n            if gradient_segmentation is not None:\n                self.gradient_segmentation = np.nonzero(gradient_segmentation)\n            if luminosity_segmentation is not None and gradient_segmentation is not None:\n                self.logical_and = np.nonzero(np.logical_and(luminosity_segmentation, gradient_segmentation))\n                self.logical_or = np.nonzero(np.logical_or(luminosity_segmentation, gradient_segmentation))\n        elif not self.vars['frame_by_frame_segmentation']:\n            if self.vars['do_threshold_segmentation'] and not self.vars['do_slope_segmentation']:\n                logging.info(f\"Arena n\u00b0{self.one_descriptor_per_arena['arena']}. Detect with luminosity threshold segmentation algorithm\")\n                self.segmented = luminosity_segmentation\n            if self.vars['do_slope_segmentation']:\n                logging.info(f\"Arena n\u00b0{self.one_descriptor_per_arena['arena']}. Detect with luminosity slope segmentation algorithm\")\n                self.segmented = gradient_segmentation\n            if np.logical_and(self.vars['do_threshold_segmentation'], self.vars['do_slope_segmentation']):\n                if self.vars['true_if_use_light_AND_slope_else_OR']:\n                    logging.info(f\"Arena n\u00b0{self.one_descriptor_per_arena['arena']}. Detection resuts from threshold AND slope segmentation algorithms\")\n                    if luminosity_segmentation is not None and gradient_segmentation is not None:\n                        self.segmented = np.logical_and(luminosity_segmentation, gradient_segmentation)\n                else:\n                    logging.info(f\"Arena n\u00b0{self.one_descriptor_per_arena['arena']}. Detection resuts from threshold OR slope segmentation algorithms\")\n                    if luminosity_segmentation is not None and gradient_segmentation is not None:\n                        self.segmented = np.logical_or(luminosity_segmentation, gradient_segmentation)\n            self.segmented = self.segmented.astype(np.uint8)\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.MotionAnalysis.fractal_descriptions","title":"<code>fractal_descriptions()</code>","text":"<p>Method for analyzing fractal patterns in binary data.</p> <p>Fractal analysis is performed on the binary representation of the data, optionally considering network dynamics if specified. The results include fractal dimensions, R-values, and box counts for the data.</p> <p>If network analysis is enabled, additional fractal dimensions, R-values, and box counts are calculated for the inner network. If 'output_in_mm' is True, then values in mm can be obtained.</p> Source code in <code>src/cellects/core/motion_analysis.py</code> <pre><code>def fractal_descriptions(self):\n    \"\"\"\n\n    Method for analyzing fractal patterns in binary data.\n\n    Fractal analysis is performed on the binary representation of the data,\n    optionally considering network dynamics if specified. The results\n    include fractal dimensions, R-values, and box counts for the data.\n\n    If network analysis is enabled, additional fractal dimensions,\n    R-values, and box counts are calculated for the inner network.\n    If 'output_in_mm' is True, then values in mm can be obtained.\n\n    \"\"\"\n    if self.vars['fractal_analysis']:\n        logging.info(f\"Arena n\u00b0{self.one_descriptor_per_arena['arena']}. Starting fractal analysis.\")\n\n        if self.vars['save_coord_network']:\n            box_counting_dimensions = np.zeros((self.dims[0], 7), dtype=np.float64)\n        else:\n            box_counting_dimensions = np.zeros((self.dims[0], 3), dtype=np.float64)\n\n        for t in np.arange(self.dims[0]):\n            if self.vars['save_coord_network']:\n                current_network = np.zeros(self.dims[1:], dtype=np.uint8)\n                net_t = self.coord_network[1:, self.coord_network[0, :] == t]\n                current_network[net_t[0], net_t[1]] = 1\n                box_counting_dimensions[t, 0] = current_network.sum()\n                zoomed_binary, side_lengths = prepare_box_counting(self.binary[t, ...], min_mesh_side=self.vars[\n                    'fractal_box_side_threshold'], zoom_step=self.vars['fractal_zoom_step'], contours=True)\n                box_counting_dimensions[t, 1], box_counting_dimensions[t, 2], box_counting_dimensions[\n                    t, 3] = box_counting_dimension(zoomed_binary, side_lengths)\n                zoomed_binary, side_lengths = prepare_box_counting(current_network,\n                                                                   min_mesh_side=self.vars[\n                                                                       'fractal_box_side_threshold'],\n                                                                   zoom_step=self.vars['fractal_zoom_step'],\n                                                                   contours=False)\n                box_counting_dimensions[t, 4], box_counting_dimensions[t, 5], box_counting_dimensions[\n                    t, 6] = box_counting_dimension(zoomed_binary, side_lengths)\n            else:\n                zoomed_binary, side_lengths = prepare_box_counting(self.binary[t, ...],\n                                                                   min_mesh_side=self.vars['fractal_box_side_threshold'],\n                                                                   zoom_step=self.vars['fractal_zoom_step'], contours=True)\n                box_counting_dimensions[t, :] = box_counting_dimension(zoomed_binary, side_lengths)\n\n        if self.vars['save_coord_network']:\n            self.one_row_per_frame[\"inner_network_size\"] = box_counting_dimensions[:, 0]\n            self.one_row_per_frame[\"fractal_dimension\"] = box_counting_dimensions[:, 1]\n            self.one_row_per_frame[\"fractal_r_value\"] = box_counting_dimensions[:, 2]\n            self.one_row_per_frame[\"fractal_box_nb\"] = box_counting_dimensions[:, 3]\n            self.one_row_per_frame[\"inner_network_fractal_dimension\"] = box_counting_dimensions[:, 4]\n            self.one_row_per_frame[\"inner_network_fractal_r_value\"] = box_counting_dimensions[:, 5]\n            self.one_row_per_frame[\"inner_network_fractal_box_nb\"] = box_counting_dimensions[:, 6]\n            if self.vars['output_in_mm']:\n                self.one_row_per_frame[\"inner_network_size\"] *= self.vars['average_pixel_size']\n        else:\n            self.one_row_per_frame[\"fractal_dimension\"] = box_counting_dimensions[:, 0]\n            self.one_row_per_frame[\"fractal_box_nb\"] = box_counting_dimensions[:, 1]\n            self.one_row_per_frame[\"fractal_r_value\"] = box_counting_dimensions[:, 2]\n\n        if self.vars['save_coord_network']:\n            del self.coord_network\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.MotionAnalysis.frame_by_frame_segmentation","title":"<code>frame_by_frame_segmentation(t, previous_binary_image=None)</code>","text":"<p>Frame-by-frame segmentation of a video.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>int</code> <p>The time index of the frame to process.</p> required <code>previous_binary_image</code> <code>NDArray</code> <p>The binary image from the previous frame. Default is <code>None</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>OneImageAnalysis</code> <p>An object containing the analysis of the current frame.</p> Source code in <code>src/cellects/core/motion_analysis.py</code> <pre><code>def frame_by_frame_segmentation(self, t: int, previous_binary_image: NDArray=None):\n    \"\"\"\n\n    Frame-by-frame segmentation of a video.\n\n    Parameters\n    ----------\n    t : int\n        The time index of the frame to process.\n    previous_binary_image : NDArray, optional\n        The binary image from the previous frame. Default is `None`.\n\n    Returns\n    -------\n    OneImageAnalysis\n        An object containing the analysis of the current frame.\n    \"\"\"\n    contrasted_im = bracket_to_uint8_image_contrast(self.converted_video[t, :, :])\n    # 1. Get the mask valid for a number of images around it (step).\n    allowed_window = None\n    if self.vars['drift_already_corrected']:\n        half_step = np.ceil(self.step / 2).astype(int)\n        t_start = t - half_step\n        t_end = t + half_step\n        t_start = np.max((t_start, 0))\n        t_end = np.min((t_end, self.dims[0]))\n        min_y, max_y = np.max(self.drift_mask_coord[t_start:t_end, 0]), np.min(self.drift_mask_coord[t_start:t_end, 1])\n        min_x, max_x = np.max(self.drift_mask_coord[t_start:t_end, 2]), np.min(self.drift_mask_coord[t_start:t_end, 3])\n        allowed_window = min_y, max_y, min_x, max_x\n\n    analysisi = OneImageAnalysis(contrasted_im)\n    if self.vars['convert_for_motion']['logical'] != 'None':\n        contrasted_im2 = bracket_to_uint8_image_contrast(self.converted_video2[t, :, :])\n        analysisi.image2 = contrasted_im2\n\n    if previous_binary_image is None or t == 0:\n        analysisi.previous_binary_image = self.origin\n    else:\n        analysisi.previous_binary_image = previous_binary_image\n\n    analysisi.segmentation(self.vars['convert_for_motion']['logical'], self.vars['color_number'],\n                           bio_label=self.vars[\"bio_label\"], bio_label2=self.vars[\"bio_label2\"],\n                           rolling_window_segmentation=self.vars['rolling_window_segmentation'],\n                           lighter_background=self.vars['lighter_background'],\n                           allowed_window=allowed_window, filter_spec=self.vars['filter_spec']) # filtering already done when creating converted_video\n\n    return analysisi\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.MotionAnalysis.get_covering_duration","title":"<code>get_covering_duration(step)</code>","text":"<p>Determine the number of frames necessary for a pixel to get covered.</p> <p>This function identifies the time when significant growth or motion occurs in a video and calculates the number of frames needed for a pixel to be completely covered. It also handles noise and ensures that the calculated step value is reasonable.</p> <p>Parameters:</p> Name Type Description Default <code>step</code> <code>int</code> <p>The initial step size for frame analysis.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If an error occurs during the calculation process.</p> Notes <p>This function may modify several instance attributes including <code>substantial_time</code>, <code>step</code>, and <code>start</code>.</p> Source code in <code>src/cellects/core/motion_analysis.py</code> <pre><code>def get_covering_duration(self, step: int):\n    \"\"\"\n    Determine the number of frames necessary for a pixel to get covered.\n\n    This function identifies the time when significant growth or motion occurs\n    in a video and calculates the number of frames needed for a pixel to be\n    completely covered. It also handles noise and ensures that the calculated\n    step value is reasonable.\n\n    Parameters\n    ----------\n    step : int\n        The initial step size for frame analysis.\n\n    Raises\n    ------\n    Exception\n        If an error occurs during the calculation process.\n\n    Notes\n    -----\n    This function may modify several instance attributes including\n    `substantial_time`, `step`, and `start`.\n    \"\"\"\n    logging.info(f\"Arena n\u00b0{self.one_descriptor_per_arena['arena']}. Find a frame with a significant growth/motion and determine the number of frames necessary for a pixel to get covered\")\n    ## Find the time at which growth reached a substantial growth.\n    self.substantial_time = self.start\n    # To avoid noisy images to have deleterious effects, make sure that area area reaches the threshold thrice.\n    occurrence = 0\n    allowed_window = None\n    if self.vars['drift_already_corrected']:\n        allowed_window = self.drift_mask_coord[:, 0].max(), self.drift_mask_coord[:, 1].min(), self.drift_mask_coord[:, 2].max(), self.drift_mask_coord[:, 3].min()\n    prev_bin_im = self.origin\n    while np.logical_and(occurrence &lt; 3, self.substantial_time &lt; (self.dims[0] - step - 1)):\n        self.substantial_time += step\n        growth_vision = self.frame_by_frame_segmentation(self.substantial_time, prev_bin_im)\n        prev_bin_im = growth_vision.binary_image * self.borders\n        surfarea = np.sum(prev_bin_im)\n        prev_bin_im = np.logical_or(prev_bin_im, self.origin).astype(np.uint8)\n        if surfarea &gt; self.substantial_growth:\n            occurrence += 1\n    # get a rough idea of the area covered during this time\n    if (self.substantial_time - self.start) &gt; 20:\n        if self.vars['lighter_background']:\n            growth = (np.sum(self.converted_video[self.start:(self.start + 10), :, :], 0) / 10) - (np.sum(self.converted_video[(self.substantial_time - 10):self.substantial_time, :, :], 0) / 10)\n        else:\n            growth = (np.sum(self.converted_video[(self.substantial_time - 10):self.substantial_time, :, :], 0) / 10) - (\n                        np.sum(self.converted_video[self.start:(self.start + 10), :, :], 0) / 10)\n    else:\n        if self.vars['lighter_background']:\n            growth = self.converted_video[self.start, ...] - self.converted_video[self.substantial_time, ...]\n        else:\n            growth = self.converted_video[self.substantial_time, ...] - self.converted_video[self.start, ...]\n    intensity_extent = np.ptp(self.converted_video[self.start:self.substantial_time, :, :], axis=0)\n    growth[np.logical_or(growth &lt; 0, intensity_extent &lt; np.median(intensity_extent))] = 0\n    growth = bracket_to_uint8_image_contrast(growth)\n    growth *= self.borders\n    growth_vision = OneImageAnalysis(growth)\n    growth_vision.segmentation(allowed_window=allowed_window)\n    if self.vars['several_blob_per_arena']:\n        _, _, stats, _ = cv2.connectedComponentsWithStats(self.origin)\n        do_erode = np.any(stats[1:, 4] &gt; 50)\n    else:\n        do_erode = self.origin.sum() &gt; 50\n    if do_erode:\n        self.substantial_image = cv2.erode(growth_vision.binary_image, cross_33, iterations=2)\n    else:\n        self.substantial_image = growth_vision.binary_image\n\n    if np.any(self.substantial_image):\n        natural_noise = np.nonzero(intensity_extent == np.min(intensity_extent))\n        natural_noise = self.converted_video[self.start:self.substantial_time, natural_noise[0][0], natural_noise[1][0]]\n        natural_noise = moving_average(natural_noise, 5)\n        natural_noise = np.ptp(natural_noise)\n        subst_idx = np.nonzero(self.substantial_image)\n        cover_lengths = np.zeros(len(subst_idx[0]), dtype=np.uint32)\n        for index in np.arange(len(subst_idx[0])):\n            vector = self.converted_video[self.start:self.substantial_time, subst_idx[0][index], subst_idx[1][index]]\n            left, right = find_major_incline(vector, natural_noise)\n            # If find_major_incline did find a major incline: (otherwise it put 0 to left and 1 to right)\n            if not np.logical_and(left == 0, right == 1):\n                cover_lengths[index] = len(vector[left:-right])\n        # If this analysis fails put a deterministic step\n        if len(cover_lengths[cover_lengths &gt; 0]) &gt; 0:\n            self.step = (np.round(np.mean(cover_lengths[cover_lengths &gt; 0])).astype(int) // 2) + 1\n            logging.info(f\"Arena n\u00b0{self.one_descriptor_per_arena['arena']}. Pre-processing detection: the time for a pixel to get covered is set to {self.step}\")\n        else:\n            logging.info(f\"Arena n\u00b0{self.one_descriptor_per_arena['arena']}. Pre-processing detection: could not automatically find the time for a pixel to get covered. Default value is 1 for video length &lt; 40 and 10 otherwise\")\n\n        # Make sure to avoid a step overestimation\n        if self.step &gt; self.dims[0] // 20:\n            self.step: int = self.dims[0] // 20\n\n        if self.step == 0:\n            self.step: int = 1\n    # When the first_move_threshold is not stringent enough the program may detect a movement due to noise\n    # In that case, the substantial_image is empty and there is no reason to proceed further\n    else:\n        self.start = None\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.MotionAnalysis.get_descriptors_from_binary","title":"<code>get_descriptors_from_binary(release_memory=True)</code>","text":"<p>Methods: get_descriptors_from_binary</p> Summary <p>Generates shape descriptors for binary images, computes these descriptors for each frame and handles colony tracking. This method can optionally release memory to reduce usage, apply scaling factors to descriptors in millimeters and computes solidity separately if requested.</p> <p>Parameters:</p> Name Type Description Default <code>release_memory</code> <code>bool</code> <p>Flag to determine whether memory should be released after computation. Default is True.</p> <code>True</code> <p>Other Parameters:</p> Name Type Description <code>**self</code> <p>DataFrame to store one row of descriptors per frame. - 'arena': Arena identifier, repeated for each frame. - 'time': Array of time values corresponding to frames.</p> <code>**self</code> <p>3D array representing binary images over time. - t,x,y: Time index, x-coordinate, and y-coordinate.</p> <code>**self</code> <p>Tuple containing image dimensions. - 0: Number of time frames. - 1,2: Image width and height respectively.</p> <code>**self</code> <p>Array containing surface areas for each frame.</p> <code>**self</code> <p>Time interval between frames, calculated only if provided timings are non-zero.</p> Notes <p>This method uses various helper methods and classes like <code>ShapeDescriptors</code> for computing shape descriptors, <code>PercentAndTimeTracker</code> for progress tracking, and other image processing techniques such as connected components analysis.</p> Source code in <code>src/cellects/core/motion_analysis.py</code> <pre><code>def get_descriptors_from_binary(self, release_memory: bool=True):\n    \"\"\"\n\n    Methods: get_descriptors_from_binary\n\n    Summary\n    -------\n    Generates shape descriptors for binary images, computes these descriptors for each frame and handles colony\n    tracking. This method can optionally release memory to reduce usage, apply scaling factors to descriptors\n    in millimeters and computes solidity separately if requested.\n\n    Parameters\n    ----------\n    release_memory : bool, optional\n        Flag to determine whether memory should be released after computation. Default is True.\n\n    Other Parameters\n    ----------------\n    **self.one_row_per_frame**\n        DataFrame to store one row of descriptors per frame.\n        - **'arena'**: Arena identifier, repeated for each frame.\n        - **'time'**: Array of time values corresponding to frames.\n\n    **self.binary**\n        3D array representing binary images over time.\n        - **t,x,y**: Time index, x-coordinate, and y-coordinate.\n\n    **self.dims**\n        Tuple containing image dimensions.\n        - **0**: Number of time frames.\n        - **1,2**: Image width and height respectively.\n\n    **self.surfarea**\n        Array containing surface areas for each frame.\n\n    **self.time_interval**\n        Time interval between frames, calculated only if provided timings are non-zero.\n\n    Notes\n    -----\n    This method uses various helper methods and classes like `ShapeDescriptors` for computing shape descriptors,\n    `PercentAndTimeTracker` for progress tracking, and other image processing techniques such as connected components analysis.\n\n    \"\"\"\n    logging.info(f\"Arena n\u00b0{self.one_descriptor_per_arena['arena']}. Computing and saving specimen(s) coordinates and required descriptors\")\n    if release_memory:\n        self.substantial_image = None\n        self.covering_intensity = None\n        self.segmented = None\n        self.gravity_field = None\n        self.sun = None\n        self.rays = None\n        self.holes = None\n        collect()\n    self.surfarea = self.binary.sum((1, 2))\n    timings = self.vars['exif']\n    if len(timings) &lt; self.dims[0]:\n        timings = np.arange(self.dims[0])\n    if np.any(timings &gt; 0):\n        self.time_interval = np.mean(np.diff(timings))\n    else:\n        self.time_interval = 1.\n    timings = timings[:self.dims[0]]\n\n    # Detect first motion\n    self.one_descriptor_per_arena['first_move'] = detect_first_move(self.surfarea, self.vars['first_move_threshold'])\n\n    self.compute_solidity_separately: bool = self.vars['iso_digi_analysis'] and not self.vars['several_blob_per_arena'] and not self.vars['descriptors']['solidity']\n    if self.compute_solidity_separately:\n        self.solidity = np.zeros(self.dims[0], dtype=np.float64)\n    if not self.vars['several_blob_per_arena']:\n        # solidity must be added if detect growth transition is computed\n        if self.compute_solidity_separately:\n            for t in np.arange(self.dims[0]):\n                solidity = ShapeDescriptors(self.binary[t, :, :], [\"solidity\"])\n                self.solidity[t] = solidity.descriptors[\"solidity\"]\n        self.one_row_per_frame = compute_one_descriptor_per_frame(self.binary,\n                                                                  self.one_descriptor_per_arena['arena'], timings,\n                                                                  self.vars['descriptors'],\n                                                                  self.vars['output_in_mm'],\n                                                                  self.vars['average_pixel_size'],\n                                                                  self.vars['do_fading'],\n                                                                   self.vars['save_coord_specimen'])\n    else:\n        self.one_row_per_frame = compute_one_descriptor_per_colony(self.binary,\n                                                                   self.one_descriptor_per_arena['arena'], timings,\n                                                                   self.vars['descriptors'],\n                                                                   self.vars['output_in_mm'],\n                                                                   self.vars['average_pixel_size'],\n                                                                   self.vars['do_fading'],\n                                                                   self.vars['first_move_threshold'],\n                                                                   self.vars['save_coord_specimen'])\n    self.one_descriptor_per_arena[\"final_area\"] = self.binary[-1, :, :].sum()\n    if self.vars['output_in_mm']:\n        self.one_descriptor_per_arena = scale_descriptors(self.one_descriptor_per_arena, self.vars['average_pixel_size'])\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.MotionAnalysis.get_origin_shape","title":"<code>get_origin_shape()</code>","text":"<p>Determine the origin shape and initialize variables based on the state of the current analysis.</p> <p>This method analyzes the initial frame or frames to determine the origin shape of an object in a video, initializing necessary variables and matrices for further processing.</p> <p>Attributes Modified:     start: (int) Indicates the starting frame index.     origin_idx: (np.ndarray) The indices of non-zero values in the origin matrix.     covering_intensity: (np.ndarray) Matrix used for pixel fading intensity.     substantial_growth: (int) Represents a significant growth measure based on the origin.</p> <p>Notes:     - The method behavior varies if 'origin_state' is set to \"constant\" or not.     - If the background is lighter, 'covering_intensity' matrix is initialized.     - Uses connected components to determine which shape is closest to the center       or largest, based on 'appearance_detection_method'.</p> Source code in <code>src/cellects/core/motion_analysis.py</code> <pre><code>def get_origin_shape(self):\n    \"\"\"\n    Determine the origin shape and initialize variables based on the state of the current analysis.\n\n    This method analyzes the initial frame or frames to determine the origin shape\n    of an object in a video, initializing necessary variables and matrices for\n    further processing.\n\n\n    Attributes Modified:\n        start: (int) Indicates the starting frame index.\n        origin_idx: (np.ndarray) The indices of non-zero values in the origin matrix.\n        covering_intensity: (np.ndarray) Matrix used for pixel fading intensity.\n        substantial_growth: (int) Represents a significant growth measure based on the origin.\n\n    Notes:\n        - The method behavior varies if 'origin_state' is set to \"constant\" or not.\n        - If the background is lighter, 'covering_intensity' matrix is initialized.\n        - Uses connected components to determine which shape is closest to the center\n          or largest, based on 'appearance_detection_method'.\n    \"\"\"\n    logging.info(f\"Arena n\u00b0{self.one_descriptor_per_arena['arena']}. Make sure of origin shape\")\n    if self.vars['drift_already_corrected']:\n        self.drift_mask_coord = np.zeros((self.dims[0], 4), dtype=np.uint32)\n        for frame_i in np.arange(self.dims[0]):  # 100):#\n            true_pixels = np.nonzero(self.converted_video[frame_i, ...])\n            self.drift_mask_coord[frame_i, :] = np.min(true_pixels[0]), np.max(true_pixels[0]) + 1, np.min(true_pixels[1]), np.max(true_pixels[1]) + 1\n        if np.all(self.drift_mask_coord[:, 0] == 0) and np.all(self.drift_mask_coord[:, 1] == self.dims[1] - 1) and np.all(\n                self.drift_mask_coord[:, 2] == 0) and np.all(self.drift_mask_coord[:, 3] == self.dims[2] - 1):\n            logging.error(f\"Drift correction has been wrongly detected. Images do not contain zero-valued pixels\")\n            self.vars['drift_already_corrected'] = False\n    self.start = 1\n    if self.vars['origin_state'] == \"invisible\":\n        self.start += self.vars['first_detection_frame']\n        analysisi = self.frame_by_frame_segmentation(self.start, self.origin)\n        # Use connected components to find which shape is the nearest from the image center.\n        if self.vars['several_blob_per_arena']:\n            self.origin = analysisi.binary_image\n        else:\n            if self.vars['appearance_detection_method'] == 'largest':\n                self.origin = keep_one_connected_component(analysisi.binary_image)\n            elif self.vars['appearance_detection_method'] == 'most_central':\n                nb_components, output, stats, centroids = cv2.connectedComponentsWithStats(analysisi.binary_image,\n                                                                                           connectivity=8)\n                center = np.array((self.dims[2] // 2, self.dims[1] // 2))\n                stats = np.zeros(nb_components - 1)\n                for shape_i in np.arange(1, nb_components):\n                    stats[shape_i - 1] = eudist(center, centroids[shape_i, :])\n                # The shape having the minimal euclidean distance from the center will be the original shape\n                self.origin = np.zeros((self.dims[1], self.dims[2]), dtype=np.uint8)\n                self.origin[output == (np.argmin(stats) + 1)] = 1\n    self.origin_idx = np.nonzero(self.origin)\n    if self.vars['origin_state'] == \"constant\":\n        if self.vars['lighter_background']:\n            # Initialize the covering_intensity matrix as a reference for pixel fading\n            self.covering_intensity[self.origin_idx[0], self.origin_idx[1]] = 200\n    self.substantial_growth = np.min((1.2 * self.origin.sum(), self.origin.sum() + 250))\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.MotionAnalysis.initialize_post_processing","title":"<code>initialize_post_processing()</code>","text":"<p>Initialize post-processing for video analysis.</p> <p>This function initializes various parameters and prepares the binary representation used in post-processing of video data. It logs information about the settings, handles initial origin states, sets up segmentation data, calculates surface areas, and optionally corrects errors around the initial shape or prevents fast growth near the periphery.</p> Notes <p>This function performs several initialization steps and logs relevant information, including handling different origin states, updating segmentation data, and calculating the gravity field based on binary representation.</p> Source code in <code>src/cellects/core/motion_analysis.py</code> <pre><code>def initialize_post_processing(self):\n    \"\"\"\n\n    Initialize post-processing for video analysis.\n\n    This function initializes various parameters and prepares the binary\n    representation used in post-processing of video data. It logs information about\n    the settings, handles initial origin states, sets up segmentation data,\n    calculates surface areas, and optionally corrects errors around the initial\n    shape or prevents fast growth near the periphery.\n\n    Notes\n    -----\n    This function performs several initialization steps and logs relevant information,\n    including handling different origin states, updating segmentation data, and\n    calculating the gravity field based on binary representation.\n\n    \"\"\"\n    ## Initialization\n    logging.info(f\"Arena n\u00b0{self.one_descriptor_per_arena['arena']}. Starting Post_processing. Fading detection: {self.vars['do_fading']}: {self.vars['fading']}, Subtract background: {self.vars['subtract_background']}, Correct errors around initial shape: {self.vars['correct_errors_around_initial']}, Connect distant shapes: {self.vars['detection_range_factor'] &gt; 0}, How to select appearing cell(s): {self.vars['appearance_detection_method']}\")\n    self.binary = np.zeros(self.dims[:3], dtype=np.uint8)\n    if self.origin.shape[0] != self.binary[self.start - 1, :, :].shape[0] or self.origin.shape[1] != self.binary[self.start - 1, :, :].shape[1]:\n        logging.error(\"Unaltered videos deprecated, they have been created with different settings.\\nDelete .npy videos and Data to run Cellects quickly.pkl and re-run\")\n\n    if self.vars['origin_state'] == \"invisible\":\n        self.binary[self.start - 1, :, :] = deepcopy(self.origin)\n        self.covering_intensity[self.origin_idx[0], self.origin_idx[1]] = self.converted_video[self.start, self.origin_idx[0], self.origin_idx[1]]\n    else:\n        if self.vars['origin_state'] == \"fluctuating\":\n            self.covering_intensity[self.origin_idx[0], self.origin_idx[1]] = np.median(self.converted_video[:self.start, self.origin_idx[0], self.origin_idx[1]], axis=0)\n\n        self.binary[:self.start, :, :] = np.repeat(np.expand_dims(self.origin, 0), self.start, axis=0)\n        if self.start &lt; self.step:\n            frames_to_assess = self.step\n            self.segmented[self.start - 1, ...] = self.binary[self.start - 1, :, :]\n            for t in np.arange(self.start, self.lost_frames):\n                # Only keep pixels that are always detected\n                always_found = np.sum(self.segmented[t:(t + frames_to_assess), ...], 0)\n                always_found = always_found == frames_to_assess\n                # Remove too small shapes\n                without_small, stats, centro = cc(always_found.astype(np.uint8))\n                large_enough = np.nonzero(stats[1:, 4] &gt; ((self.vars['first_move_threshold'] + 1) // 2))[0]\n                if len(large_enough) &gt; 0:\n                    always_found *= np.isin(always_found, large_enough + 1)\n                    always_found = np.logical_or(always_found, self.segmented[t - 1, ...])\n                    self.segmented[t, ...] *= always_found\n                else:\n                    self.segmented[t, ...] = 0\n                self.segmented[t, ...] = np.logical_or(self.segmented[t - 1, ...], self.segmented[t, ...])\n    self.mean_distance_per_frame = None\n    self.surfarea = np.zeros(self.dims[0], dtype =np.uint64)\n    self.surfarea[:self.start] = np.sum(self.binary[:self.start, :, :], (1, 2))\n    self.gravity_field = inverted_distance_transform(self.binary[(self.start - 1), :, :],\n                                       np.sqrt(np.sum(self.binary[(self.start - 1), :, :])))\n    if self.vars['correct_errors_around_initial']:\n        self.rays, self.sun = draw_me_a_sun(self.binary[(self.start - 1), :, :], ray_length_coef=1)  # plt.imshow(sun)\n        self.holes = np.zeros(self.dims[1:], dtype=np.uint8)\n        self.pixel_ring_depth += 2\n        self.update_ring_width()\n\n    if self.vars['prevent_fast_growth_near_periphery']:\n        self.near_periphery = np.zeros(self.dims[1:])\n        if self.vars['arena_shape'] == 'circle':\n            periphery_width = self.vars['periphery_width'] * 2\n            elliperiphery = create_ellipse(self.dims[1] - periphery_width, self.dims[2] - periphery_width, min_size=3)\n            half_width = periphery_width // 2\n            if periphery_width % 2 == 0:\n                self.near_periphery[half_width:-half_width, half_width:-half_width] = elliperiphery\n            else:\n                self.near_periphery[half_width:-half_width - 1, half_width:-half_width - 1] = elliperiphery\n            self.near_periphery = 1 - self.near_periphery\n        else:\n            self.near_periphery[:self.vars['periphery_width'], :] = 1\n            self.near_periphery[-self.vars['periphery_width']:, :] = 1\n            self.near_periphery[:, :self.vars['periphery_width']] = 1\n            self.near_periphery[:, -self.vars['periphery_width']:] = 1\n        self.near_periphery = np.nonzero(self.near_periphery)\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.MotionAnalysis.load_images_and_videos","title":"<code>load_images_and_videos(videos_already_in_ram, i)</code>","text":"<p>Load images and videos from disk or RAM.</p> <p>Parameters:</p> Name Type Description Default <code>videos_already_in_ram</code> <code>ndarray or None</code> <p>Video data that is already loaded into RAM. If <code>None</code>, videos will be loaded from disk.</p> required <code>i</code> <code>int</code> <p>Index used to select the origin and background data.</p> required Notes <p>This method logs information about the arena number and loads necessary data from disk or RAM based on whether videos are already in memory. It sets various attributes like <code>self.origin</code>, <code>self.background</code>, and <code>self.converted_video</code>.</p> Source code in <code>src/cellects/core/motion_analysis.py</code> <pre><code>def load_images_and_videos(self, videos_already_in_ram, i: int):\n    \"\"\"\n\n    Load images and videos from disk or RAM.\n\n    Parameters\n    ----------\n    videos_already_in_ram : numpy.ndarray or None\n        Video data that is already loaded into RAM. If `None`, videos will be\n        loaded from disk.\n    i : int\n        Index used to select the origin and background data.\n\n    Notes\n    -----\n    This method logs information about the arena number and loads necessary data\n    from disk or RAM based on whether videos are already in memory. It sets various\n    attributes like `self.origin`, `self.background`, and `self.converted_video`.\n\n    \"\"\"\n    logging.info(f\"Arena n\u00b0{self.one_descriptor_per_arena['arena']}. Load images and videos\")\n    if 'bb_coord' in self.vars:\n        crop_top, crop_bot, crop_left, crop_right, top, bot, left, right = self.vars['bb_coord']\n    elif videos_already_in_ram is not None:\n        if isinstance(videos_already_in_ram, list):\n            crop_bot, crop_right = videos_already_in_ram[0].shape[1], videos_already_in_ram[0].shape[2]\n        else:\n            crop_bot, crop_right = videos_already_in_ram.shape[1], videos_already_in_ram.shape[2]\n        crop_top, crop_left, top, bot, left, right = 0, 0, [0], [crop_bot], [0], [crop_right]\n    if isinstance(self.vars['origin_list'][i], Tuple):\n        self.origin_idx = self.vars['origin_list'][i]\n        frame_height = bot[i] - top[i]\n        true_frame_width = right[i] - left[i]\n        self.origin = np.zeros((frame_height, true_frame_width), dtype=np.uint8)\n        self.origin[self.origin_idx[0], self.origin_idx[1]] = 1\n    else:\n        self.origin = self.vars['origin_list'][i]\n        frame_height = self.origin.shape[0]\n        true_frame_width = self.origin.shape[1]\n\n    vid_name = None\n    if self.vars['video_list'] is not None:\n        vid_name = self.vars['video_list'][i]\n    self.background = None\n    if len(self.vars['background_list']) &gt; 0:\n        self.background = self.vars['background_list'][i]\n    self.background2 = None\n    if 'background_list2' in self.vars and len(self.vars['background_list2']) &gt; 0:\n        self.background2 = self.vars['background_list2'][i]\n    vids = read_one_arena(self.one_descriptor_per_arena['arena'], self.vars['already_greyscale'],\n                          self.vars['convert_for_motion'], videos_already_in_ram, true_frame_width, vid_name,\n                          self.background, self.background2)\n    self.visu, self.converted_video, self.converted_video2 = vids\n    # When the video(s) already exists (not just written as .pny), they need to be sliced:\n    if self.visu is not None:\n        if self.visu.shape[1] != frame_height or self.visu.shape[2] != true_frame_width:\n            self.visu = self.visu[:, crop_top:crop_bot, crop_left:crop_right, ...]\n            self.visu = self.visu[:, top[i]:bot[i], left[i]:right[i], ...]\n            if self.converted_video is not None:\n                self.converted_video = self.converted_video[:, crop_top:crop_bot, crop_left:crop_right]\n                self.converted_video = self.converted_video[:, top[i]:bot[i], left[i]:right[i]]\n                if self.converted_video2 is not None:\n                    self.converted_video2 = self.converted_video2[:, crop_top:crop_bot, crop_left:crop_right]\n                    self.converted_video2 = self.converted_video2[:, top[i]:bot[i], left[i]:right[i]]\n\n    if self.converted_video is None:\n        logging.info(\n            f\"Arena n\u00b0{self.one_descriptor_per_arena['arena']}. Convert the RGB visu video into a greyscale image using the color space combination: {self.vars['convert_for_motion']}\")\n        vids = convert_subtract_and_filter_video(self.visu, self.vars['convert_for_motion'],\n                                                 self.background, self.background2,\n                                                 self.vars['lose_accuracy_to_save_memory'],\n                                                 self.vars['filter_spec'])\n        self.converted_video, self.converted_video2 = vids\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.MotionAnalysis.lum_slope_segmentation","title":"<code>lum_slope_segmentation(converted_video)</code>","text":"<p>Perform lum slope segmentation on the given video.</p> <p>Parameters:</p> Name Type Description Default <code>converted_video</code> <code>NDArray</code> <p>The input video array for segmentation processing.</p> required <p>Returns:</p> Type Description <code>NDArray</code> <p>Segmented gradient array of the video. If segmentation fails, returns <code>None</code> for the corresponding frames.</p> Notes <p>This function may consume significant memory and adjusts data types (float32 or float64) based on available RAM.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; result = lum_slope_segmentation(converted_video)\n</code></pre> Source code in <code>src/cellects/core/motion_analysis.py</code> <pre><code>def lum_slope_segmentation(self, converted_video: NDArray) -&gt; NDArray:\n    \"\"\"\n    Perform lum slope segmentation on the given video.\n\n    Parameters\n    ----------\n    converted_video : NDArray\n        The input video array for segmentation processing.\n\n    Returns\n    -------\n    NDArray\n        Segmented gradient array of the video. If segmentation fails,\n        returns `None` for the corresponding frames.\n\n    Notes\n    -----\n    This function may consume significant memory and adjusts\n    data types (float32 or float64) based on available RAM.\n\n    Examples\n    --------\n    &gt;&gt;&gt; result = lum_slope_segmentation(converted_video)\n    \"\"\"\n    shape_motion_failed : bool = False\n    # 2) Contrast increase\n    oridx = np.nonzero(self.origin)\n    notoridx = np.nonzero(1 - self.origin)\n    do_increase_contrast = np.mean(converted_video[0, oridx[0], oridx[1]]) * 10 &gt; np.mean(\n            converted_video[0, notoridx[0], notoridx[1]])\n    necessary_memory = self.dims[0] * self.dims[1] * self.dims[2] * 64 * 2 * 1.16415e-10\n    available_memory = (virtual_memory().available &gt;&gt; 30) - self.vars['min_ram_free']\n    if self.vars['lose_accuracy_to_save_memory']:\n        derive = converted_video.astype(np.float32)\n    else:\n        derive = converted_video.astype(np.float64)\n    if necessary_memory &gt; available_memory:\n        converted_video = None\n\n    if do_increase_contrast:\n        derive = np.square(derive)\n\n    # 3) Get the gradient\n    necessary_memory = derive.size * 64 * 4 * 1.16415e-10\n    available_memory = (virtual_memory().available &gt;&gt; 30) - self.vars['min_ram_free']\n    if necessary_memory &gt; available_memory:\n        for cy in np.arange(self.dims[1]):\n            for cx in np.arange(self.dims[2]):\n                if self.vars['lose_accuracy_to_save_memory']:\n                    derive[:, cy, cx] = np.gradient(derive[:, cy, cx], self.step).astype(np.float32)\n                else:\n                    derive[:, cy, cx] = np.gradient(derive[:, cy, cx], self.step)\n    else:\n        if self.vars['lose_accuracy_to_save_memory']:\n            derive = np.gradient(derive, self.step, axis=0).astype(np.float32)\n        else:\n            derive = np.gradient(derive, self.step, axis=0)\n\n    # 4) Segment\n    if self.vars['lighter_background']:\n        covering_slopes = np.min(derive[:self.substantial_time, :, :], 0) * self.substantial_image\n    else:\n        covering_slopes = np.max(derive[:self.substantial_time, :, :], 0) * self.substantial_image\n    covering_slopes = covering_slopes[covering_slopes != 0]\n    if len(covering_slopes) == 0:\n        shape_motion_failed = True\n\n    gradient_segmentation = None\n    if not shape_motion_failed:\n        gradient_segmentation = np.zeros(self.dims, np.uint8)\n        ####\n        # ease_slope_segmentation = 0.8\n        value_segmentation_thresholds = np.arange(0.8, -0.7, -0.1)\n        validated_thresholds = np.zeros(value_segmentation_thresholds.shape, dtype=bool)\n        counter = 0\n        while_condition = True\n        max_motion_per_frame = (self.dims[1] * self.dims[2]) * self.vars['maximal_growth_factor']\n        # Try different values of do_slope_segmentation and keep the one that does not\n        # segment more than x percent of the image\n        while counter &lt; value_segmentation_thresholds.shape[0]:\n            ease_slope_segmentation = value_segmentation_thresholds[counter]\n            if self.vars['lighter_background']:\n                gradient_threshold = (1 + ease_slope_segmentation) * np.max(covering_slopes)\n                sample = np.less(derive[:self.substantial_time], gradient_threshold)\n            else:\n                gradient_threshold = (1 - ease_slope_segmentation) * np.min(covering_slopes)\n                sample = np.greater(derive[:self.substantial_time], gradient_threshold)\n            changing_pixel_number = np.sum(np.absolute(np.diff(sample.astype(np.int8), 1, 0)), (1, 2))\n            validation = np.max(np.sum(sample, (1, 2))) &lt; max_motion_per_frame and (\n                    np.max(changing_pixel_number) &lt; max_motion_per_frame)\n            validated_thresholds[counter] = validation\n            if np.any(validated_thresholds):\n                if not validation:\n                    break\n            counter += 1\n            # If any threshold is accepted, use their average to proceed the final thresholding\n        valid_number = validated_thresholds.sum()\n        if valid_number &gt; 0:\n            if valid_number &gt; 2:\n                index_to_keep = 2\n            else:\n                index_to_keep = valid_number - 1\n            ease_slope_segmentation = value_segmentation_thresholds[\n                np.uint8(np.floor(np.mean(np.nonzero(validated_thresholds)[0][index_to_keep])))]\n\n            if self.vars['lighter_background']:\n                gradient_threshold = (1 - ease_slope_segmentation) * np.max(covering_slopes)\n                gradient_segmentation[:-self.lost_frames, :, :] = np.less(derive, gradient_threshold)[\n                    self.lost_frames:, :, :]\n            else:\n                gradient_threshold = (1 - ease_slope_segmentation) * np.min(covering_slopes)\n                gradient_segmentation[:-self.lost_frames, :, :] = np.greater(derive, gradient_threshold)[\n                    self.lost_frames:, :, :]\n        else:\n            if self.vars['lighter_background']:\n                gradient_segmentation[:-self.lost_frames, :, :] = (derive &lt; (np.min(derive, (1, 2)) * 1.1)[:, None, None])[self.lost_frames:, :, :]\n            else:\n                gradient_segmentation[:-self.lost_frames, :, :] = (derive &gt; (np.max(derive, (1, 2)) * 0.1)[:, None, None])[self.lost_frames:, :, :]\n    return gradient_segmentation\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.MotionAnalysis.lum_value_segmentation","title":"<code>lum_value_segmentation(converted_video, do_threshold_segmentation)</code>","text":"<p>Perform segmentation based on luminosity values from a video.</p> <p>Parameters:</p> Name Type Description Default <code>converted_video</code> <code>NDArray</code> <p>The input video data in a NumPy array format.</p> required <code>do_threshold_segmentation</code> <code>bool</code> <p>Flag to determine whether threshold segmentation should be applied.</p> required <p>Returns:</p> Type Description <code>Tuple[NDArray, NDArray]</code> <p>A tuple containing two NumPy arrays: - The first array is the luminosity segmentation of the video. - The second array represents the luminosity threshold over time.</p> Notes <p>This function operates under the assumption that there is sufficient motion in the video data. If no valid thresholds are found for segmentation, the function returns None for <code>luminosity_segmentation</code>.</p> Source code in <code>src/cellects/core/motion_analysis.py</code> <pre><code>def lum_value_segmentation(self, converted_video: NDArray, do_threshold_segmentation: bool) -&gt; Tuple[NDArray, NDArray]:\n    \"\"\"\n    Perform segmentation based on luminosity values from a video.\n\n    Parameters\n    ----------\n    converted_video : NDArray\n        The input video data in a NumPy array format.\n    do_threshold_segmentation : bool\n        Flag to determine whether threshold segmentation should be applied.\n\n    Returns\n    -------\n    Tuple[NDArray, NDArray]\n        A tuple containing two NumPy arrays:\n        - The first array is the luminosity segmentation of the video.\n        - The second array represents the luminosity threshold over time.\n\n    Notes\n    -----\n    This function operates under the assumption that there is sufficient motion in the video data.\n    If no valid thresholds are found for segmentation, the function returns None for\n    `luminosity_segmentation`.\n    \"\"\"\n    shape_motion_failed: bool = False\n    if self.vars['lighter_background']:\n        covering_l_values = np.min(converted_video[:self.substantial_time, :, :],\n                                         0) * self.substantial_image\n    else:\n        covering_l_values = np.max(converted_video[:self.substantial_time, :, :],\n                                         0) * self.substantial_image\n    # Avoid errors by checking whether the covering values are nonzero\n    covering_l_values = covering_l_values[covering_l_values != 0]\n    if len(covering_l_values) == 0:\n        shape_motion_failed = True\n\n    luminosity_segmentation = None\n    l_threshold_over_time = None\n    if not shape_motion_failed:\n        value_segmentation_thresholds = np.arange(0.8, -0.7, -0.1)\n        validated_thresholds = np.zeros(value_segmentation_thresholds.shape, dtype=bool)\n        counter = 0\n        while_condition = True\n        max_motion_per_frame = (self.dims[1] * self.dims[2]) * self.vars['maximal_growth_factor'] * 2\n        if self.vars['lighter_background']:\n            basic_bckgrnd_values = np.quantile(converted_video[:(self.lost_frames + 1), ...], 0.9, axis=(1, 2))\n        else:\n            basic_bckgrnd_values = np.quantile(converted_video[:(self.lost_frames + 1), ...], 0.1, axis=(1, 2))\n        # Try different values of do_threshold_segmentation and keep the one that does not\n        # segment more than x percent of the image\n        while counter &lt;= 14:\n            value_threshold = value_segmentation_thresholds[counter]\n            if self.vars['lighter_background']:\n                l_threshold = (1 + value_threshold) * np.max(covering_l_values)\n            else:\n                l_threshold = (1 - value_threshold) * np.min(covering_l_values)\n            starting_segmentation, l_threshold_over_time = segment_with_lum_value(converted_video[:(self.lost_frames + 1), ...],\n                                                           basic_bckgrnd_values, l_threshold,\n                                                           self.vars['lighter_background'])\n\n            changing_pixel_number = np.sum(np.absolute(np.diff(starting_segmentation.astype(np.int8), 1, 0)), (1, 2))\n            validation = np.max(np.sum(starting_segmentation, (1, 2))) &lt; max_motion_per_frame and (\n                    np.max(changing_pixel_number) &lt; max_motion_per_frame)\n            validated_thresholds[counter] = validation\n            if np.any(validated_thresholds):\n                if not validation:\n                    break\n            counter += 1\n        # If any threshold is accepted, use their average to proceed the final thresholding\n        valid_number = validated_thresholds.sum()\n        if valid_number &gt; 0:\n            if valid_number &gt; 2:\n                index_to_keep = 2\n            else:\n                index_to_keep = valid_number - 1\n            value_threshold = value_segmentation_thresholds[\n                np.uint8(np.floor(np.mean(np.nonzero(validated_thresholds)[0][index_to_keep])))]\n        else:\n            value_threshold = 0\n\n        if self.vars['lighter_background']:\n            l_threshold = (1 + value_threshold) * np.max(covering_l_values)\n        else:\n            l_threshold = (1 - value_threshold) * np.min(covering_l_values)\n        if do_threshold_segmentation:\n            if self.vars['lighter_background']:\n                basic_bckgrnd_values = np.quantile(converted_video, 0.9, axis=(1, 2))\n            else:\n                basic_bckgrnd_values = np.quantile(converted_video, 0.1, axis=(1, 2))\n            luminosity_segmentation, l_threshold_over_time = segment_with_lum_value(converted_video, basic_bckgrnd_values,\n                                                             l_threshold, self.vars['lighter_background'])\n        else:\n            luminosity_segmentation, l_threshold_over_time = segment_with_lum_value(converted_video[:(self.lost_frames + 1), ...],\n                                                           basic_bckgrnd_values, l_threshold,\n                                                           self.vars['lighter_background'])\n    return luminosity_segmentation, l_threshold_over_time\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.MotionAnalysis.networks_analysis","title":"<code>networks_analysis(show_seg=False)</code>","text":"<pre><code>    Perform network detection within a given arena.\n\n    This function carries out the task of detecting networks in an arena\n    based on several parameters and variables. It involves checking video\n    type, performing network detection over time, potentially detecting\n    pseudopods, and smoothing segmentation. The results can be visualized or saved.\n</code></pre> <p>Extract and analyze graphs from a binary representation of network dynamics, producing vertex         and edge tables that represent the graph structure over time.</p> <pre><code>    Args:\n        None\n\n    Attributes:\n        vars (dict): Dictionary of variables that control the graph extraction process.\n            - 'save_graph': Boolean indicating if graph extraction should be performed.\n            - 'save_coord_network': Boolean indicating if the coordinate network should be saved.\n\n        one_descriptor_per_arena (dict): Dictionary containing descriptors for each arena.\n\n        dims (tuple): Tuple containing dimension information.\n            - [0]: Integer representing the number of time steps.\n            - [1]: Integer representing the y-dimension size.\n            - [2]: Integer representing the x-dimension size.\n\n        origin (np.ndarray): Binary image representing the origin of the network.\n\n        binary (np.ndarray): Binary representation of network dynamics over time.\n            Shape: (time_steps, y_dimension, x_dimension).\n\n        converted_video (np.ndarray): Converted video data.\n            Shape: (y_dimension, x_dimension, time_steps).\n\n        network_dynamics (np.ndarray): Network dynamics representation.\n            Shape: (time_steps, y_dimension, x_dimension).\n\n    Notes:\n        - This method performs graph extraction and saves the vertex and edge tables to CSV files.\n        - The CSV files are named according to the arena, time steps, and dimensions.\n\n    Args:\n        show_seg: bool = False\n            A flag that determines whether to display the segmentation visually.\n</code></pre> Source code in <code>src/cellects/core/motion_analysis.py</code> <pre><code>    def networks_analysis(self, show_seg: bool=False):\n        \"\"\"\n        Perform network detection within a given arena.\n\n        This function carries out the task of detecting networks in an arena\n        based on several parameters and variables. It involves checking video\n        type, performing network detection over time, potentially detecting\n        pseudopods, and smoothing segmentation. The results can be visualized or saved.\nExtract and analyze graphs from a binary representation of network dynamics, producing vertex\n        and edge tables that represent the graph structure over time.\n\n        Args:\n            None\n\n        Attributes:\n            vars (dict): Dictionary of variables that control the graph extraction process.\n                - 'save_graph': Boolean indicating if graph extraction should be performed.\n                - 'save_coord_network': Boolean indicating if the coordinate network should be saved.\n\n            one_descriptor_per_arena (dict): Dictionary containing descriptors for each arena.\n\n            dims (tuple): Tuple containing dimension information.\n                - [0]: Integer representing the number of time steps.\n                - [1]: Integer representing the y-dimension size.\n                - [2]: Integer representing the x-dimension size.\n\n            origin (np.ndarray): Binary image representing the origin of the network.\n\n            binary (np.ndarray): Binary representation of network dynamics over time.\n                Shape: (time_steps, y_dimension, x_dimension).\n\n            converted_video (np.ndarray): Converted video data.\n                Shape: (y_dimension, x_dimension, time_steps).\n\n            network_dynamics (np.ndarray): Network dynamics representation.\n                Shape: (time_steps, y_dimension, x_dimension).\n\n        Notes:\n            - This method performs graph extraction and saves the vertex and edge tables to CSV files.\n            - The CSV files are named according to the arena, time steps, and dimensions.\n\n        Args:\n            show_seg: bool = False\n                A flag that determines whether to display the segmentation visually.\n        \"\"\"\n        coord_pseudopods = None\n        if not self.vars['several_blob_per_arena'] and self.vars['save_coord_network']:\n            self.check_converted_video_type()\n\n            if self.vars['origin_state'] == \"constant\":\n                self.coord_network, coord_pseudopods = detect_network_dynamics(self.converted_video, self.binary,\n                                                           self.one_descriptor_per_arena['arena'], 0,\n                                                           self.visu, self.origin, True, True,\n                                                           self.vars['save_coord_network'], show_seg)\n            else:\n                self.coord_network, coord_pseudopods = detect_network_dynamics(self.converted_video, self.binary,\n                                                           self.one_descriptor_per_arena['arena'], 0,\n                                                           self.visu, None, True, True,\n                                                           self.vars['save_coord_network'], show_seg)\n\n        if not self.vars['several_blob_per_arena'] and self.vars['save_graph']:\n            if self.coord_network is None:\n                self.coord_network = np.array(np.nonzero(self.binary))\n            if self.vars['origin_state'] == \"constant\":\n                extract_graph_dynamics(self.converted_video, self.coord_network, self.one_descriptor_per_arena['arena'],\n                                       0, self.origin, coord_pseudopods)\n            else:\n                extract_graph_dynamics(self.converted_video, self.coord_network, self.one_descriptor_per_arena['arena'],\n                                       0, None, coord_pseudopods)\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.MotionAnalysis.save_efficiency_tests","title":"<code>save_efficiency_tests()</code>","text":"<p>Provide images allowing to assess the analysis efficiency</p> <p>This method generates two test images used for assessing the efficiency of the analysis. It performs various operations on video frames to create these images, including copying and manipulating frames from the video, detecting contours on binary images, and drawing the arena label on the left of the frames.</p> Source code in <code>src/cellects/core/motion_analysis.py</code> <pre><code>def save_efficiency_tests(self):\n    \"\"\"\n    Provide images allowing to assess the analysis efficiency\n\n    This method generates two test images used for assessing\n    the efficiency of the analysis. It performs various operations on\n    video frames to create these images, including copying and manipulating\n    frames from the video, detecting contours on binary images,\n    and drawing the arena label on the left of the frames.\n    \"\"\"\n    # Provide images allowing to assess the analysis efficiency\n    if self.dims[0] &gt; 1:\n        after_one_tenth_of_time = np.ceil(self.dims[0] / 10).astype(np.uint64)\n    else:\n        after_one_tenth_of_time = 0\n\n    last_good_detection = self.dims[0] - 1\n    if self.dims[0] &gt; self.lost_frames:\n        if self.vars['do_threshold_segmentation']:\n            last_good_detection -= self.lost_frames\n    else:\n        last_good_detection = 0\n    if self.visu is None:\n        if len(self.converted_video.shape) == 3:\n            self.converted_video = np.stack((self.converted_video, self.converted_video, self.converted_video),\n                                         axis=3)\n        self.efficiency_test_1 = deepcopy(self.converted_video[after_one_tenth_of_time, ...])\n        self.efficiency_test_2 = deepcopy(self.converted_video[last_good_detection, ...])\n    else:\n        self.efficiency_test_1 = deepcopy(self.visu[after_one_tenth_of_time, :, :, :])\n        self.efficiency_test_2 = deepcopy(self.visu[last_good_detection, :, :, :])\n\n    position = (25, self.dims[1] // 2)\n    text = str(self.one_descriptor_per_arena['arena'])\n    contours = np.nonzero(get_contours(self.binary[after_one_tenth_of_time, :, :]))\n    self.efficiency_test_1[contours[0], contours[1], :] = self.vars['contour_color']\n    self.efficiency_test_1 = cv2.putText(self.efficiency_test_1, text, position, cv2.FONT_HERSHEY_SIMPLEX, 1,\n                                    (self.vars[\"contour_color\"], self.vars[\"contour_color\"],\n                                     self.vars[\"contour_color\"], 255), 3)\n\n    eroded_binary = cv2.erode(self.binary[last_good_detection, :, :], cross_33)\n    contours = np.nonzero(self.binary[last_good_detection, :, :] - eroded_binary)\n    self.efficiency_test_2[contours[0], contours[1], :] = self.vars['contour_color']\n    self.efficiency_test_2 = cv2.putText(self.efficiency_test_2, text, position, cv2.FONT_HERSHEY_SIMPLEX, 1,\n                                     (self.vars[\"contour_color\"], self.vars[\"contour_color\"],\n                                      self.vars[\"contour_color\"], 255), 3)\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.MotionAnalysis.save_results","title":"<code>save_results()</code>","text":"<p>Save the results of testing and video processing.</p> <p>This method handles the saving of efficiency tests, video files, and CSV data related to test results. It checks for existing files before writing new data. Additionally, it cleans up temporary files if configured to do so.</p> Source code in <code>src/cellects/core/motion_analysis.py</code> <pre><code>def save_results(self):\n    \"\"\"\n    Save the results of testing and video processing.\n\n    This method handles the saving of efficiency tests, video files,\n    and CSV data related to test results. It checks for existing files before writing new data.\n    Additionally, it cleans up temporary files if configured to do so.\n    \"\"\"\n    self.save_efficiency_tests()\n    self.save_video()\n    if self.vars['several_blob_per_arena']:\n        try:\n            with open(f\"one_row_per_frame_arena{self.one_descriptor_per_arena['arena']}.csv\", 'w') as file:\n                self.one_row_per_frame.to_csv(file, sep=';', index=False, lineterminator='\\n')\n        except PermissionError:\n            logging.error(f\"Never let one_row_per_frame_arena{self.one_descriptor_per_arena['arena']}.csv open when Cellects runs\")\n\n        create_new_csv: bool = False\n        if os.path.isfile(\"one_row_per_arena.csv\"):\n            try:\n                with open(f\"one_row_per_arena.csv\", 'r') as file:\n                    stats = pd.read_csv(file, header=0, sep=\";\")\n            except PermissionError:\n                logging.error(\"Never let one_row_per_arena.csv open when Cellects runs\")\n\n            if len(self.one_descriptor_per_arena) == len(stats.columns) - 1:\n                try:\n                    with open(f\"one_row_per_arena.csv\", 'w') as file:\n                        stats.iloc[(self.one_descriptor_per_arena['arena'] - 1), 1:] = self.one_descriptor_per_arena.values()\n                        stats.to_csv(file, sep=';', index=False, lineterminator='\\n')\n                except PermissionError:\n                    logging.error(\"Never let one_row_per_arena.csv open when Cellects runs\")\n            else:\n                create_new_csv = True\n        else:\n            create_new_csv = True\n        if create_new_csv:\n            with open(f\"one_row_per_arena.csv\", 'w') as file:\n                stats = pd.DataFrame(np.zeros((len(self.vars['analyzed_individuals']), len(self.one_descriptor_per_arena))),\n                           columns=list(self.one_descriptor_per_arena.keys()))\n                stats.iloc[(self.one_descriptor_per_arena['arena'] - 1), :] = self.one_descriptor_per_arena.values()\n                stats.to_csv(file, sep=';', index=False, lineterminator='\\n')\n    if not self.vars['keep_unaltered_videos'] and os.path.isfile(f\"ind_{self.one_descriptor_per_arena['arena']}.npy\"):\n        os.remove(f\"ind_{self.one_descriptor_per_arena['arena']}.npy\")\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.MotionAnalysis.save_video","title":"<code>save_video()</code>","text":"<p>Save processed video with contours and other annotations.</p> <p>This method processes the binary image to extract contours, overlay them on a video, and save the resulting video file.</p> <p>Notes:     - This method uses OpenCV for image processing and contour extraction.     - The processed video includes contours colored according to the       <code>contour_color</code> specified in the variables.     - Additional annotations such as time in minutes are added to each       frame if applicable.</p> Source code in <code>src/cellects/core/motion_analysis.py</code> <pre><code>def save_video(self):\n    \"\"\"\n    Save processed video with contours and other annotations.\n\n    This method processes the binary image to extract contours, overlay them\n    on a video, and save the resulting video file.\n\n    Notes:\n        - This method uses OpenCV for image processing and contour extraction.\n        - The processed video includes contours colored according to the\n          `contour_color` specified in the variables.\n        - Additional annotations such as time in minutes are added to each\n          frame if applicable.\n\n    \"\"\"\n    if self.vars['save_processed_videos']:\n        self.check_converted_video_type()\n        if len(self.converted_video.shape) == 3:\n            self.converted_video = np.stack((self.converted_video, self.converted_video, self.converted_video),\n                                            axis=3)\n        for t in np.arange(self.dims[0]):\n\n            eroded_binary = cv2.erode(self.binary[t, :, :], cross_33)\n            contours = np.nonzero(self.binary[t, :, :] - eroded_binary)\n            self.converted_video[t, contours[0], contours[1], :] = self.vars['contour_color']\n            if \"iso_digi_transi\" in self.one_descriptor_per_arena.keys():\n                if self.vars['iso_digi_analysis']  and not self.vars['several_blob_per_arena'] and not pd.isna(self.one_descriptor_per_arena[\"iso_digi_transi\"]):\n                    if self.one_descriptor_per_arena['is_growth_isotropic'] == 1:\n                        if t &lt; self.one_descriptor_per_arena[\"iso_digi_transi\"]:\n                            self.converted_video[t, contours[0], contours[1], :] = 0, 0, 255\n        del self.binary\n        del self.surfarea\n        del self.borders\n        del self.origin\n        del self.origin_idx\n        del self.mean_intensity_per_frame\n        del self.erodila_disk\n        collect()\n        if self.visu is None:\n            true_frame_width = self.dims[2]\n            if len(self.vars['background_list']) == 0:\n                self.background = None\n            else:\n                self.background = self.vars['background_list'][self.one_descriptor_per_arena['arena'] - 1]\n            if os.path.isfile(f\"ind_{self.one_descriptor_per_arena['arena']}.npy\"):\n                self.visu = video2numpy(f\"ind_{self.one_descriptor_per_arena['arena']}.npy\",\n                          None, true_frame_width=true_frame_width)\n            else:\n                self.visu = self.converted_video\n            if len(self.visu.shape) == 3:\n                self.visu = np.stack((self.visu, self.visu, self.visu), axis=3)\n        self.converted_video = np.concatenate((self.visu, self.converted_video), axis=2)\n\n        if np.any(self.one_row_per_frame['time'] &gt; 0):\n            position = (5, self.dims[1] - 5)\n            if self.vars['time_step_is_arbitrary']:\n                time_unit = \"\"\n            else:\n                time_unit = \" min\"\n            for t in np.arange(self.dims[0]):\n                image = self.converted_video[t, ...]\n                text = str(self.one_row_per_frame['time'][t]) + time_unit\n                image = cv2.putText(image,  # numpy array on which text is written\n                                text,  # text\n                                position,  # position at which writing has to start\n                                cv2.FONT_HERSHEY_SIMPLEX,  # font family\n                                1,  # font size\n                                (self.vars[\"contour_color\"], self.vars[\"contour_color\"], self.vars[\"contour_color\"], 255),  #(209, 80, 0, 255),  \n                                2)  # font stroke\n                self.converted_video[t, ...] = image\n        vid_name = f\"ind_{self.one_descriptor_per_arena['arena']}{self.vars['videos_extension']}\"\n        write_video(self.converted_video, vid_name, is_color=True, fps=self.vars['video_fps'])\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.MotionAnalysis.smooth_pixel_slopes","title":"<code>smooth_pixel_slopes(converted_video)</code>","text":"<p>Apply smoothing to pixel slopes in a video by convolving with a moving average kernel.</p> <p>Parameters:</p> Name Type Description Default <code>converted_video</code> <code>NDArray</code> <p>The input video array to be smoothed.</p> required <p>Returns:</p> Type Description <code>NDArray</code> <p>Smoothed video array with pixel slopes averaged using a moving average kernel.</p> <p>Raises:</p> Type Description <code>MemoryError</code> <p>If there is not enough RAM available to perform the smoothing operation.</p> Notes <p>This function applies a moving average kernel to each pixel across the frames of the input video. The smoothing operation can be repeated based on user-defined settings. The precision of the output array is controlled by a flag that determines whether to save memory at the cost of accuracy.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; smoothed = smooth_pixel_slopes(converted_video)\n&gt;&gt;&gt; print(smoothed.shape)  # Expected output will vary depending on the input video shape\n</code></pre> Source code in <code>src/cellects/core/motion_analysis.py</code> <pre><code>def smooth_pixel_slopes(self, converted_video: NDArray) -&gt; NDArray:\n    \"\"\"\n    Apply smoothing to pixel slopes in a video by convolving with a moving average kernel.\n\n    Parameters\n    ----------\n    converted_video : NDArray\n        The input video array to be smoothed.\n\n    Returns\n    -------\n    NDArray\n        Smoothed video array with pixel slopes averaged using a moving average kernel.\n\n    Raises\n    ------\n    MemoryError\n        If there is not enough RAM available to perform the smoothing operation.\n\n    Notes\n    -----\n    This function applies a moving average kernel to each pixel across the frames of\n    the input video. The smoothing operation can be repeated based on user-defined settings.\n    The precision of the output array is controlled by a flag that determines whether to\n    save memory at the cost of accuracy.\n\n    Examples\n    --------\n    &gt;&gt;&gt; smoothed = smooth_pixel_slopes(converted_video)\n    &gt;&gt;&gt; print(smoothed.shape)  # Expected output will vary depending on the input video shape\n    \"\"\"\n    try:\n        if self.vars['lose_accuracy_to_save_memory']:\n            smoothed_video = np.zeros(self.dims, dtype=np.float32)\n            smooth_kernel = np.ones(self.step, dtype=np.float64) / self.step\n            for i in np.arange(converted_video.shape[1]):\n                for j in np.arange(converted_video.shape[2]):\n                    padded = np.pad(converted_video[:, i, j] / self.mean_intensity_per_frame,\n                                 (self.step // 2, self.step - 1 - self.step // 2), mode='edge')\n                    moving_average = np.convolve(padded, smooth_kernel, mode='valid')\n                    if self.vars['repeat_video_smoothing'] &gt; 1:\n                        for it in np.arange(1, self.vars['repeat_video_smoothing']):\n                            padded = np.pad(moving_average,\n                                         (self.step // 2, self.step - 1 - self.step // 2), mode='edge')\n                            moving_average = np.convolve(padded, smooth_kernel, mode='valid')\n                    smoothed_video[:, i, j] = moving_average.astype(np.float32)\n        else:\n            smoothed_video = np.zeros(self.dims, dtype=np.float64)\n            smooth_kernel = np.ones(self.step) / self.step\n            for i in np.arange(converted_video.shape[1]):\n                for j in np.arange(converted_video.shape[2]):\n                    padded = np.pad(converted_video[:, i, j] / self.mean_intensity_per_frame,\n                                 (self.step // 2, self.step - 1 - self.step // 2), mode='edge')\n                    moving_average = np.convolve(padded, smooth_kernel, mode='valid')\n                    if self.vars['repeat_video_smoothing'] &gt; 1:\n                        for it in np.arange(1, self.vars['repeat_video_smoothing']):\n                            padded = np.pad(moving_average,\n                                         (self.step // 2, self.step - 1 - self.step // 2), mode='edge')\n                            moving_average = np.convolve(padded, smooth_kernel, mode='valid')\n                    smoothed_video[:, i, j] = moving_average\n        return smoothed_video\n\n    except MemoryError:\n        logging.error(\"Not enough RAM available to smooth pixel curves. Detection may fail.\")\n        smoothed_video = converted_video\n        return smoothed_video\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.MotionAnalysis.study_cytoscillations","title":"<code>study_cytoscillations(show_seg=False)</code>","text":"<p>Study the cytoskeletal oscillations within a video frame by frame.</p> <p>This method performs an analysis of cytoskeletal oscillations in the video, identifying regions of influx and efflux based on pixel connectivity. It also handles memory allocation for the oscillations video, computes connected components, and optionally displays the segmented regions.</p> <p>Args:     show_seg (bool): If True, display the segmentation results.</p> Source code in <code>src/cellects/core/motion_analysis.py</code> <pre><code>def study_cytoscillations(self, show_seg: bool=False):\n    \"\"\"\n\n        Study the cytoskeletal oscillations within a video frame by frame.\n\n        This method performs an analysis of cytoskeletal oscillations in the video,\n        identifying regions of influx and efflux based on pixel connectivity.\n        It also handles memory allocation for the oscillations video, computes\n        connected components, and optionally displays the segmented regions.\n\n        Args:\n            show_seg (bool): If True, display the segmentation results.\n    \"\"\"\n    if self.vars['save_coord_thickening_slimming'] or self.vars['oscilacyto_analysis']:\n        oscillations_video = detect_oscillations_dynamics(self.converted_video, self.binary,\n                                                          self.one_descriptor_per_arena['arena'], self.start,\n                                                          self.vars['expected_oscillation_period'],\n                                                          self.time_interval,\n                                                          self.vars['minimal_oscillating_cluster_size'],\n                                                          self.vars['min_ram_free'],\n                                                          self.vars['lose_accuracy_to_save_memory'],\n                                                          self.vars['save_coord_thickening_slimming'])\n        del oscillations_video\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.MotionAnalysis.update_ring_width","title":"<code>update_ring_width()</code>","text":"<p>Update the <code>pixel_ring_depth</code> and create an erodila disk.</p> <p>This method ensures that the pixel ring depth is odd and at least 3, then creates an erodila disk of that size.</p> Source code in <code>src/cellects/core/motion_analysis.py</code> <pre><code>def update_ring_width(self):\n    \"\"\"\n\n    Update the `pixel_ring_depth` and create an erodila disk.\n\n    This method ensures that the pixel ring depth is odd and at least 3,\n    then creates an erodila disk of that size.\n    \"\"\"\n    # Make sure that self.pixels_depths are odd and greater than 3\n    if self.pixel_ring_depth &lt;= 3:\n        self.pixel_ring_depth = 3\n    if self.pixel_ring_depth % 2 == 0:\n        self.pixel_ring_depth = self.pixel_ring_depth + 1\n    self.erodila_disk = create_ellipse(self.pixel_ring_depth, self.pixel_ring_depth, min_size=3).astype(np.uint8)\n    self.max_distance = self.pixel_ring_depth * self.vars['detection_range_factor']\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.MotionAnalysis.update_shape","title":"<code>update_shape(show_seg)</code>","text":"<p>Update the shape of detected objects in the current frame by analyzing segmentation potentials and applying morphological operations.</p> <p>Parameters:</p> Name Type Description Default <code>show_seg</code> <code>bool</code> <p>Flag indicating whether to display segmentation results.</p> required Notes <p>This function performs several operations to update the shape of detected objects: - Analyzes segmentation potentials from previous frames. - Applies morphological operations to refine the shape. - Updates internal state variables such as <code>binary</code> and <code>covering_intensity</code>.</p> Source code in <code>src/cellects/core/motion_analysis.py</code> <pre><code>def update_shape(self, show_seg: bool):\n    \"\"\"\n    Update the shape of detected objects in the current frame by analyzing\n    segmentation potentials and applying morphological operations.\n\n    Parameters\n    ----------\n    show_seg : bool\n        Flag indicating whether to display segmentation results.\n\n    Notes\n    -----\n    This function performs several operations to update the shape of detected objects:\n    - Analyzes segmentation potentials from previous frames.\n    - Applies morphological operations to refine the shape.\n    - Updates internal state variables such as `binary` and `covering_intensity`.\n\n    \"\"\"\n    # Get from gradients, a 2D matrix of potentially covered pixels\n    # I/ dilate the shape made with covered pixels to assess for covering\n\n    # I/ 1) Only keep pixels that have been detected at least two times in the three previous frames\n    if self.dims[0] &lt; 100:\n        new_potentials = self.segmented[self.t, :, :]\n    else:\n        if self.t &gt; 1:\n            new_potentials = np.sum(self.segmented[(self.t - 2): (self.t + 1), :, :], 0, dtype=np.uint8)\n        else:\n            new_potentials = np.sum(self.segmented[: (self.t + 1), :, :], 0, dtype=np.uint8)\n        new_potentials[new_potentials == 1] = 0\n        new_potentials[new_potentials &gt; 1] = 1\n\n    # I/ 2) If an image displays more new potential pixels than 50% of image pixels,\n    # one of these images is considered noisy and we try taking only one.\n    frame_counter = -1\n    maximal_size = 0.5 * new_potentials.size\n    if (self.vars[\"do_threshold_segmentation\"] or self.vars[\"frame_by_frame_segmentation\"]) and self.t &gt; np.max((self.start + self.step, 6)):\n       maximal_size = np.min((np.max(self.binary[:self.t].sum((1, 2))) * (1 + self.vars['maximal_growth_factor']), self.borders.sum()))\n    while np.logical_and(np.sum(new_potentials) &gt; maximal_size,\n                         frame_counter &lt;= 5):  # np.logical_and(np.sum(new_potentials &gt; 0) &gt; 5 * np.sum(dila_ring), frame_counter &lt;= 5):\n        frame_counter += 1\n        if frame_counter &gt; self.t:\n            break\n        else:\n            if frame_counter &lt; 5:\n                new_potentials = self.segmented[self.t - frame_counter, :, :]\n            else:\n            # If taking only one image is not enough, use the inverse of the fadinged matrix as new_potentials\n            # Given it haven't been processed by any slope calculation, it should be less noisy\n                new_potentials = np.sum(self.segmented[(self.t - 5): (self.t + 1), :, :], 0, dtype=np.uint8)\n                new_potentials[new_potentials &lt; 6] = 0\n                new_potentials[new_potentials == 6] = 1\n\n\n    new_shape = deepcopy(self.binary[self.t - 1, :, :])\n    new_potentials = cv2.morphologyEx(new_potentials, cv2.MORPH_CLOSE, cross_33)\n    new_potentials = cv2.morphologyEx(new_potentials, cv2.MORPH_OPEN, cross_33) * self.borders\n    new_shape = np.logical_or(new_shape, new_potentials).astype(np.uint8)\n    # Add distant shapes within a radius, score every added pixels according to their distance\n    if not self.vars['several_blob_per_arena']:\n        if new_shape.sum() == 0:\n            new_shape = deepcopy(new_potentials)\n        else:\n            pads = ProgressivelyAddDistantShapes(new_potentials, new_shape, self.max_distance)\n            r = weakref.ref(pads)\n            # If max_distance is non nul look for distant shapes\n            pads.consider_shapes_sizes(self.vars['min_size_for_connection'],\n                                                 self.vars['max_size_for_connection'])\n            pads.connect_shapes(only_keep_connected_shapes=True, rank_connecting_pixels=True)\n\n            new_shape = deepcopy(pads.expanded_shape)\n            new_shape[new_shape &gt; 1] = 1\n            if np.logical_and(self.t &gt; self.step, self.t &lt; self.dims[0]):\n                if np.any(pads.expanded_shape &gt; 5):\n                    # Add distant shapes back in time at the covering speed of neighbors\n                    self.binary[self.t][np.nonzero(new_shape)] = 1\n                    self.binary[(self.step):(self.t + 1), :, :] = \\\n                        pads.modify_past_analysis(self.binary[(self.step):(self.t + 1), :, :],\n                                                  self.segmented[(self.step):(self.t + 1), :, :])\n                    new_shape = deepcopy(self.binary[self.t, :, :])\n            pads = None\n\n        # Fill holes\n        new_shape = cv2.morphologyEx(new_shape, cv2.MORPH_CLOSE, cross_33)\n\n    if self.vars['do_fading'] and (self.t &gt; self.step + self.lost_frames):\n        # Shape Erosion\n        # I/ After a substantial growth, erode the shape made with covered pixels to assess for fading\n        # Use the newly covered pixels to calculate their mean covering intensity\n        new_idx = np.nonzero(np.logical_xor(new_shape, self.binary[self.t - 1, :, :]))\n        start_intensity_monitoring = self.t - self.lost_frames - self.step\n        end_intensity_monitoring = self.t - self.lost_frames\n        self.covering_intensity[new_idx[0], new_idx[1]] = np.median(self.converted_video[start_intensity_monitoring:end_intensity_monitoring, new_idx[0], new_idx[1]], axis=0)\n        previous_binary = self.binary[self.t - 1, :, :]\n        greyscale_image = self.converted_video[self.t - self.lost_frames, :, :]\n        protect_from_fading = None\n        if self.vars['origin_state'] == 'constant':\n            protect_from_fading = self.origin\n        new_shape, self.covering_intensity = cell_leaving_detection(new_shape, self.covering_intensity, previous_binary, greyscale_image, self.vars['fading'], self.vars['lighter_background'], self.vars['several_blob_per_arena'], self.erodila_disk, protect_from_fading)\n\n    self.covering_intensity *= new_shape\n    self.binary[self.t, :, :] = new_shape * self.borders\n    self.surfarea[self.t] = np.sum(self.binary[self.t, :, :])\n\n    # Calculate the mean distance covered per frame and correct for a ring of not really fading pixels\n    if self.mean_distance_per_frame is None:\n        if self.vars['correct_errors_around_initial'] and not self.vars['several_blob_per_arena']:\n            if np.logical_and((self.t % 20) == 0,\n                              np.logical_and(self.surfarea[self.t] &gt; self.substantial_growth,\n                                             self.surfarea[self.t] &lt; self.substantial_growth * 2)):\n                shape = self.binary[self.t, :, :] * self.sun\n                back = (1 - self.binary[self.t, :, :]) * self.sun\n                for ray in self.rays:\n                    # For each sun's ray, see how they cross the shape/back and\n                    # store the gravity_field value of these pixels (distance to the original shape).\n                    ray_through_shape = (shape == ray) * self.gravity_field\n                    ray_through_back = (back == ray) * self.gravity_field\n                    if np.any(ray_through_shape):\n                        if np.any(ray_through_back):\n                            # If at least one back pixel is nearer to the original shape than a shape pixel,\n                            # there is a hole to fill.\n                            if np.any(ray_through_back &gt; np.min(ray_through_shape[ray_through_shape &gt; 0])):\n                                # Check if the nearest pixels are shape, if so, supress them until the nearest pixel\n                                # becomes back\n                                while np.max(ray_through_back) &lt;= np.max(ray_through_shape):\n                                    ray_through_shape[ray_through_shape == np.max(ray_through_shape)] = 0\n                                # Now, all back pixels that are nearer than the closest shape pixel should get filled\n                                # To do so, replace back pixels further than the nearest shape pixel by 0\n                                ray_through_back[ray_through_back &lt; np.max(ray_through_shape)] = 0\n                                self.holes[np.nonzero(ray_through_back)] = 1\n                        else:\n                            self.rays = np.concatenate((self.rays[:(ray - 2)], self.rays[(ray - 1):]))\n                    ray_through_shape = None\n                    ray_through_back = None\n        if np.any(self.surfarea[:self.t] &gt; self.substantial_growth * 2):\n\n            if self.vars['correct_errors_around_initial'] and not self.vars['several_blob_per_arena']:\n                # Apply the hole correction\n                self.holes = cv2.morphologyEx(self.holes, cv2.MORPH_CLOSE, cross_33, iterations=10)\n                # If some holes are not covered by now\n                if np.any(self.holes * (1 - self.binary[self.t, :, :])):\n                    self.binary[:(self.t + 1), :, :], holes_time_end, distance_against_time = \\\n                        dynamically_expand_to_fill_holes(self.binary[:(self.t + 1), :, :], self.holes)\n                    if holes_time_end is not None:\n                        self.binary[holes_time_end:(self.t + 1), :, :] += self.binary[holes_time_end, :, :]\n                        self.binary[holes_time_end:(self.t + 1), :, :][\n                            self.binary[holes_time_end:(self.t + 1), :, :] &gt; 1] = 1\n                        self.surfarea[:(self.t + 1)] = np.sum(self.binary[:(self.t + 1), :, :], (1, 2))\n\n                else:\n                    distance_against_time = [1, 2]\n            else:\n                distance_against_time = [1, 2]\n            distance_against_time = np.diff(distance_against_time)\n            if len(distance_against_time) &gt; 0:\n                self.mean_distance_per_frame = np.mean(- distance_against_time)\n            else:\n                self.mean_distance_per_frame = 1\n\n    if self.vars['prevent_fast_growth_near_periphery']:\n        # growth_near_periphery = np.diff(self.binary[self.t-1:self.t+1, :, :] * self.near_periphery, axis=0)\n        growth_near_periphery = np.diff(self.binary[self.t-1:self.t+1, self.near_periphery[0], self.near_periphery[1]], axis=0)\n        if (growth_near_periphery == 1).sum() &gt; self.vars['max_periphery_growth']:\n            # self.binary[self.t, self.near_periphery[0], self.near_periphery[1]] = self.binary[self.t - 1, self.near_periphery[0], self.near_periphery[1]]\n            periphery_to_remove = np.zeros(self.dims[1:], dtype=np.uint8)\n            periphery_to_remove[self.near_periphery[0], self.near_periphery[1]] = self.binary[self.t, self.near_periphery[0], self.near_periphery[1]]\n            shapes, stats, centers = cc(periphery_to_remove)\n            periphery_to_remove = np.nonzero(np.isin(shapes, np.nonzero(stats[:, 4] &gt; self.vars['max_periphery_growth'])[0][1:]))\n            self.binary[self.t, periphery_to_remove[0], periphery_to_remove[1]] = self.binary[self.t - 1, periphery_to_remove[0], periphery_to_remove[1]]\n            if not self.vars['several_blob_per_arena']:\n                shapes, stats, centers = cc(self.binary[self.t, ...])\n                shapes[shapes != 1] = 0\n                self.binary[self.t, ...] = shapes\n\n    # Display\n    if show_seg:\n        if self.visu is not None:\n            im_to_display = deepcopy(self.visu[self.t, ...])\n            contours = np.nonzero(cv2.morphologyEx(self.binary[self.t, :, :], cv2.MORPH_GRADIENT, cross_33))\n            if self.vars['lighter_background']:\n                im_to_display[contours[0], contours[1]] = 0\n            else:\n                im_to_display[contours[0], contours[1]] = 255\n        else:\n            im_to_display = self.binary[self.t, :, :] * 255\n        imtoshow = cv2.resize(im_to_display, (540, 540))\n        cv2.imshow(\"shape_motion\", imtoshow)\n        cv2.waitKey(1)\n    self.t += 1\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.NetworkDetection","title":"<code>NetworkDetection</code>","text":"<p>NetworkDetection</p> <p>Class for detecting vessels in images using Frangi and Sato filters with various parameter sets. It applies different thresholding methods, calculates quality metrics, and selects the best detection method.</p> Source code in <code>src/cellects/image_analysis/network_functions.py</code> <pre><code>class  NetworkDetection:\n    \"\"\"\n    NetworkDetection\n\n    Class for detecting vessels in images using Frangi and Sato filters with various parameter sets.\n    It applies different thresholding methods, calculates quality metrics, and selects the best detection method.\n    \"\"\"\n    def __init__(self, greyscale_image: NDArray[np.uint8], possibly_filled_pixels: NDArray[np.uint8]=None, add_rolling_window: bool=False, origin_to_add: NDArray[np.uint8]=None, best_result: dict=None):\n        \"\"\"\n        Initialize the object with given parameters.\n\n        Parameters\n        ----------\n        greyscale_image : NDArray[np.uint8]\n            The input greyscale image.\n        possibly_filled_pixels : NDArray[np.uint8], optional\n            Image containing possibly filled pixels. Defaults to None.\n        add_rolling_window : bool, optional\n            Flag to add rolling window. Defaults to False.\n        origin_to_add : NDArray[np.uint8], optional\n            Origin to add. Defaults to None.\n        best_result : dict, optional\n            Best result dictionary. Defaults to None.\n        \"\"\"\n        self.greyscale_image = greyscale_image\n        if possibly_filled_pixels is None:\n            self.possibly_filled_pixels = np.ones(self.greyscale_image.shape, dtype=np.uint8)\n        else:\n            self.possibly_filled_pixels = possibly_filled_pixels\n        self.best_result = best_result\n        self.add_rolling_window = add_rolling_window\n        self.origin_to_add = origin_to_add\n        self.frangi_beta = 1.\n        self.frangi_gamma = 1.\n        self.black_ridges = True\n\n    def apply_frangi_variations(self) -&gt; list:\n        \"\"\"\n        Applies various Frangi filter variations with different sigma values and thresholding methods.\n\n        This method applies the Frangi vesselness filter with multiple sets of sigma values\n        to detect vessels at different scales. It applies both Otsu thresholding and rolling window\n        segmentation to the filtered results and calculates binary quality indices.\n\n        Returns\n        -------\n        results : list of dict\n            A list containing dictionaries with the method name, binary result, quality index,\n            filtered image, filter type, rolling window flag, and sigma values used.\n        \"\"\"\n        results = []\n\n        # Parameter variations for Frangi filter\n        frangi_sigmas = {\n            's_fine_vessels': [0.75],\n            'fine_vessels': [0.5, 1.0],  # Very fine capillaries, thin fibers\n            'small_vessels': [1.0, 2.0],  # Small vessels, fine structures\n            'multi_scale_medium': [1.0, 2.0, 3.0],  # Standard multi-scale\n            'ultra_fine': [0.3, 0.5, 0.8],  # Ultra-fine structures\n            'comprehensive': [0.5, 1.0, 2.0, 4.0],  # Multi-scale\n            'retinal_vessels': [1.0, 2.0, 4.0, 8.0],  # Optimized for retinal imaging\n            'microscopy': [0.5, 1.0, 1.5, 2.5],  # Microscopy applications\n            'broad_spectrum': [0.5, 1.5, 3.0, 6.0, 10.0]\n        }\n\n        for i, (key, sigmas) in enumerate(frangi_sigmas.items()):\n            # Apply Frangi filter\n            frangi_result = frangi(self.greyscale_image, sigmas=sigmas, beta=self.frangi_beta, gamma=self.frangi_gamma, black_ridges=self.black_ridges)\n\n            # Apply both thresholding methods\n            # Method 1: Otsu thresholding\n            thresh_otsu = threshold_otsu(frangi_result)\n            binary_otsu = frangi_result &gt; thresh_otsu\n            quality_otsu = binary_quality_index(self.possibly_filled_pixels * binary_otsu)\n\n            # Method 2: Rolling window thresholding\n\n            # Store results\n            results.append({\n                'method': f'f_{sigmas}_thresh',\n                'binary': binary_otsu,\n                'quality': quality_otsu,\n                'filtered': frangi_result,\n                'filter': f'Frangi',\n                'rolling_window': False,\n                'sigmas': sigmas\n            })\n            # Method 2: Rolling window thresholding\n            if self.add_rolling_window:\n                binary_rolling = rolling_window_segmentation(frangi_result, self.possibly_filled_pixels, patch_size=(10, 10))\n                quality_rolling = binary_quality_index(binary_rolling)\n                results.append({\n                    'method': f'f_{sigmas}_roll',\n                    'binary': binary_rolling,\n                    'quality': quality_rolling,\n                    'filtered': frangi_result,\n                    'filter': f'Frangi',\n                    'rolling_window': True,\n                    'sigmas': sigmas\n                })\n\n        return results\n\n\n    def apply_sato_variations(self) -&gt; list:\n        \"\"\"\n        Apply various Sato filter variations to an image and store the results.\n\n        This function applies different parameter sets for the Sato vesselness\n        filter to an image, applies two thresholding methods (Otsu and rolling window),\n        and stores the results. The function supports optional rolling window\n        segmentation based on a configuration flag.\n\n        Returns\n        -------\n        list of dict\n            A list containing dictionaries with the results for each filter variation.\n            Each dictionary includes method name, binary image, quality index,\n            filtered result, filter type, rolling window flag, and sigma values.\n        \"\"\"\n        results = []\n\n        # Parameter variations for Frangi filter\n        sato_sigmas = {\n            'super_small_tubes': [0.01, 0.05, 0.1, 0.15],  #\n            'small_tubes': [0.1, 0.2, 0.4, 0.8],  #\n            's_thick_ridges': [0.25, 0.75],  # Thick ridges/tubes\n            'small_multi_scale': [0.1, 0.2, 0.4, 0.8, 1.6],  #\n            'fine_ridges': [0.8, 1.5],  # Fine ridge detection\n            'medium_ridges': [1.5, 3.0],  # Medium ridge structures\n            'multi_scale_fine': [0.8, 1.5, 2.5],  # Multi-scale fine detection\n            'multi_scale_standard': [1.0, 2.5, 5.0],  # Standard multi-scale\n            'edge_enhanced': [0.5, 1.0, 2.0],  # Edge-enhanced detection\n            'noise_robust': [1.5, 2.5, 4.0],  # Robust to noise\n            'fingerprint': [1.0, 1.5, 2.0, 3.0],  # Fingerprint ridge detection\n            'geological': [2.0, 5.0, 10.0, 15.0]  # Geological structures\n        }\n\n        for i, (key, sigmas) in enumerate(sato_sigmas.items()):\n            # Apply sato filter\n            sato_result = sato(self.greyscale_image, sigmas=sigmas, black_ridges=self.black_ridges, mode='reflect')\n\n            # Apply both thresholding methods\n            # Method 1: Otsu thresholding\n            thresh_otsu = threshold_otsu(sato_result)\n            binary_otsu = sato_result &gt; thresh_otsu\n            quality_otsu = binary_quality_index(self.possibly_filled_pixels * binary_otsu)\n\n\n            # Store results\n            results.append({\n                'method': f's_{sigmas}_thresh',\n                'binary': binary_otsu,\n                'quality': quality_otsu,\n                'filtered': sato_result,\n                'filter': f'Sato',\n                'rolling_window': False,\n                'sigmas': sigmas\n            })\n\n            # Method 2: Rolling window thresholding\n            if self.add_rolling_window:\n                binary_rolling = rolling_window_segmentation(sato_result, self.possibly_filled_pixels, patch_size=(10, 10))\n                quality_rolling = binary_quality_index(binary_rolling)\n\n                results.append({\n                    'method': f's_{sigmas}_roll',\n                    'binary': binary_rolling,\n                    'quality': quality_rolling,\n                    'filtered': sato_result,\n                    'filter': f'Sato',\n                    'rolling_window': True,\n                    'sigmas': sigmas\n                })\n\n        return results\n\n    def get_best_network_detection_method(self):\n        \"\"\"\n        Get the best network detection method based on quality metrics.\n\n        This function applies Frangi and Sato variations, combines their results,\n        calculates quality metrics for each result, and selects the best method.\n\n        Attributes\n        ----------\n        all_results : list of dicts\n            Combined results from Frangi and Sato variations.\n        quality_metrics : ndarray of float64\n            Quality metrics for each detection result.\n        best_idx : int\n            Index of the best detection method based on quality metrics.\n        best_result : dict\n            The best detection result from all possible methods.\n        incomplete_network : ndarray of bool\n            Binary representation of the best detection result.\n\n        Examples\n        ----------\n        &gt;&gt;&gt; possibly_filled_pixels = np.zeros((9, 9), dtype=np.uint8)\n        &gt;&gt;&gt; possibly_filled_pixels[3:6, 3:6] = 1\n        &gt;&gt;&gt; possibly_filled_pixels[1:6, 3] = 1\n        &gt;&gt;&gt; possibly_filled_pixels[6:-1, 5] = 1\n        &gt;&gt;&gt; possibly_filled_pixels[4, 1:-1] = 1\n        &gt;&gt;&gt; greyscale_image = possibly_filled_pixels.copy()\n        &gt;&gt;&gt; greyscale_image[greyscale_image &gt; 0] = np.random.randint(170, 255, possibly_filled_pixels.sum())\n        &gt;&gt;&gt; greyscale_image[greyscale_image == 0] = np.random.randint(0, 120, possibly_filled_pixels.size - possibly_filled_pixels.sum())\n        &gt;&gt;&gt; add_rolling_window=False\n        &gt;&gt;&gt; origin_to_add = np.zeros((9, 9), dtype=np.uint8)\n        &gt;&gt;&gt; origin_to_add[3:6, 3:6] = 1\n        &gt;&gt;&gt; NetDet = NetworkDetection(greyscale_image, possibly_filled_pixels, add_rolling_window, origin_to_add)\n        &gt;&gt;&gt; NetDet.get_best_network_detection_method()\n        &gt;&gt;&gt; print(NetDet.best_result['method'])\n        &gt;&gt;&gt; print(NetDet.best_result['binary'])\n        &gt;&gt;&gt; print(NetDet.best_result['quality'])\n        &gt;&gt;&gt; print(NetDet.best_result['filtered'])\n        &gt;&gt;&gt; print(NetDet.best_result['filter'])\n        &gt;&gt;&gt; print(NetDet.best_result['rolling_window'])\n        &gt;&gt;&gt; print(NetDet.best_result['sigmas'])\n        bgr_image = np.random.randint(0, 256, (100, 100, 3), dtype=np.uint8)\n        \"\"\"\n        frangi_res = self.apply_frangi_variations()\n        sato_res = self.apply_sato_variations()\n        self.all_results = frangi_res + sato_res\n        self.quality_metrics = np.array([result['quality'] for result in self.all_results])\n        self.best_idx = np.argmax(self.quality_metrics)\n        self.best_result = self.all_results[self.best_idx]\n        self.incomplete_network = self.best_result['binary'] * self.possibly_filled_pixels\n\n\n    def detect_network(self):\n        \"\"\"\n        Process and detect network features in the greyscale image.\n\n        This method applies a frangi or sato filter based on the best result and\n        performs segmentation using either rolling window or Otsu's thresholding.\n        The final network detection result is stored in `self.incomplete_network`.\n        \"\"\"\n        if self.best_result['filter'] == 'Frangi':\n            filtered_result = frangi(self.greyscale_image, sigmas=self.best_result['sigmas'], beta=self.frangi_beta, gamma=self.frangi_gamma, black_ridges=self.black_ridges)\n        else:\n            filtered_result = sato(self.greyscale_image, sigmas=self.best_result['sigmas'], black_ridges=self.black_ridges, mode='reflect')\n\n        if self.best_result['rolling_window']:\n            binary_image = rolling_window_segmentation(filtered_result, self.possibly_filled_pixels, patch_size=(10, 10))\n        else:\n            thresh_otsu = threshold_otsu(filtered_result)\n            binary_image = filtered_result &gt; thresh_otsu\n        self.incomplete_network = binary_image * self.possibly_filled_pixels\n\n    def change_greyscale(self, img: NDArray[np.uint8], first_dict: dict):\n        \"\"\"\n        Change the image to greyscale using color space combinations.\n\n        This function converts an input image to greyscale by generating\n        and applying a combination of color spaces specified in the dictionary.\n        The resulting greyscale image is stored as an attribute of the instance.\n\n        Parameters\n        ----------\n        img : ndarray of uint8\n            The input image to be converted to greyscale.\n        \"\"\"\n        self.greyscale_image, g2, all_c_spaces, first_pc_vector  = generate_color_space_combination(img, list(first_dict.keys()), first_dict)\n\n    def detect_pseudopods(self, lighter_background: bool, pseudopod_min_width: int=5, pseudopod_min_size: int=50, only_one_connected_component: bool=True):\n        \"\"\"\n        Detect pseudopods in a binary image.\n\n        Identify and process regions that resemble pseudopods based on width, size,\n        and connectivity criteria. This function is used to detect and label areas\n        that are indicative of pseudopod-like structures within a binary image.\n\n        Parameters\n        ----------\n        lighter_background : bool\n            Boolean flag to indicate if the background should be considered lighter.\n        pseudopod_min_width : int, optional\n            Minimum width for pseudopods to be considered valid. Default is 5.\n        pseudopod_min_size : int, optional\n            Minimum size for pseudopods to be considered valid. Default is 50.\n        only_one_connected_component : bool, optional\n            Flag to ensure only one connected component is kept. Default is True.\n\n        Returns\n        -------\n        None\n\n        Notes\n        -----\n        This function modifies internal attributes of the object, specifically setting `self.pseudopods` to an array indicating pseudopod regions.\n\n        Examples\n        --------\n        &gt;&gt;&gt; result = detect_pseudopods(True, 5, 50)\n        &gt;&gt;&gt; print(self.pseudopods)\n        array([[0, 1, ..., 0],\n               [0, 0, ..., 0],\n               ...,\n               [0, 1, ..., 0]], dtype=uint8)\n\n        \"\"\"\n\n        closed_im = close_holes(self.possibly_filled_pixels)\n        dist_trans = distance_transform_edt(closed_im)\n        dist_trans = dist_trans.max() - dist_trans\n        # Add dilatation of bracket of distances from medial_axis to the multiplication\n        if lighter_background:\n            grey = self.greyscale_image.max() - self.greyscale_image\n        else:\n            grey = self.greyscale_image\n        if self.origin_to_add is not None:\n            dist_trans_ori = distance_transform_edt(1 - self.origin_to_add)\n            scored_im = dist_trans * dist_trans_ori * grey\n        else:\n            scored_im = (dist_trans**2) * grey\n        scored_im = bracket_to_uint8_image_contrast(scored_im)\n        thresh = threshold_otsu(scored_im)\n        thresh = find_threshold_given_mask(scored_im, self.possibly_filled_pixels, min_threshold=thresh)\n        high_int_in_periphery = (scored_im &gt; thresh).astype(np.uint8) * self.possibly_filled_pixels\n\n        _, pseudopod_widths = morphology.medial_axis(high_int_in_periphery, return_distance=True, rng=0)\n        bin_im = pseudopod_widths &gt;= pseudopod_min_width\n        dil_bin_im = cv2.dilate(bin_im.astype(np.uint8), kernel=create_ellipse(7, 7).astype(np.uint8), iterations=1)\n        bin_im = high_int_in_periphery * dil_bin_im\n        nb, shapes, stats, centro = cv2.connectedComponentsWithStats(bin_im)\n        true_pseudopods = np.nonzero(stats[:, 4] &gt; pseudopod_min_size)[0][1:]\n        true_pseudopods = np.isin(shapes, true_pseudopods)\n\n        # Make sure that the tubes connecting two pseudopods belong to pseudopods if removing pseudopods cuts the network\n        complete_network = np.logical_or(true_pseudopods, self.incomplete_network).astype(np.uint8)\n        if only_one_connected_component:\n            complete_network = keep_one_connected_component(complete_network)\n            without_pseudopods = complete_network.copy()\n            without_pseudopods[true_pseudopods] = 0\n            only_connected_network = keep_one_connected_component(without_pseudopods)\n            self.pseudopods = (1 - only_connected_network) * complete_network  * self.possibly_filled_pixels\n        else:\n            self.pseudopods = true_pseudopods.astype(np.uint8)\n\n    def merge_network_with_pseudopods(self):\n        \"\"\"\n        Merge the incomplete network with pseudopods.\n\n        This method combines the incomplete network and pseudopods to form\n        the complete network. The incomplete network is updated by subtracting\n        areas where pseudopods are present.\n        \"\"\"\n        self.complete_network = np.logical_or(self.incomplete_network, self.pseudopods).astype(np.uint8)\n        self.incomplete_network *= (1 - self.pseudopods)\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.NetworkDetection.__init__","title":"<code>__init__(greyscale_image, possibly_filled_pixels=None, add_rolling_window=False, origin_to_add=None, best_result=None)</code>","text":"<p>Initialize the object with given parameters.</p> <p>Parameters:</p> Name Type Description Default <code>greyscale_image</code> <code>NDArray[uint8]</code> <p>The input greyscale image.</p> required <code>possibly_filled_pixels</code> <code>NDArray[uint8]</code> <p>Image containing possibly filled pixels. Defaults to None.</p> <code>None</code> <code>add_rolling_window</code> <code>bool</code> <p>Flag to add rolling window. Defaults to False.</p> <code>False</code> <code>origin_to_add</code> <code>NDArray[uint8]</code> <p>Origin to add. Defaults to None.</p> <code>None</code> <code>best_result</code> <code>dict</code> <p>Best result dictionary. Defaults to None.</p> <code>None</code> Source code in <code>src/cellects/image_analysis/network_functions.py</code> <pre><code>def __init__(self, greyscale_image: NDArray[np.uint8], possibly_filled_pixels: NDArray[np.uint8]=None, add_rolling_window: bool=False, origin_to_add: NDArray[np.uint8]=None, best_result: dict=None):\n    \"\"\"\n    Initialize the object with given parameters.\n\n    Parameters\n    ----------\n    greyscale_image : NDArray[np.uint8]\n        The input greyscale image.\n    possibly_filled_pixels : NDArray[np.uint8], optional\n        Image containing possibly filled pixels. Defaults to None.\n    add_rolling_window : bool, optional\n        Flag to add rolling window. Defaults to False.\n    origin_to_add : NDArray[np.uint8], optional\n        Origin to add. Defaults to None.\n    best_result : dict, optional\n        Best result dictionary. Defaults to None.\n    \"\"\"\n    self.greyscale_image = greyscale_image\n    if possibly_filled_pixels is None:\n        self.possibly_filled_pixels = np.ones(self.greyscale_image.shape, dtype=np.uint8)\n    else:\n        self.possibly_filled_pixels = possibly_filled_pixels\n    self.best_result = best_result\n    self.add_rolling_window = add_rolling_window\n    self.origin_to_add = origin_to_add\n    self.frangi_beta = 1.\n    self.frangi_gamma = 1.\n    self.black_ridges = True\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.NetworkDetection.apply_frangi_variations","title":"<code>apply_frangi_variations()</code>","text":"<p>Applies various Frangi filter variations with different sigma values and thresholding methods.</p> <p>This method applies the Frangi vesselness filter with multiple sets of sigma values to detect vessels at different scales. It applies both Otsu thresholding and rolling window segmentation to the filtered results and calculates binary quality indices.</p> <p>Returns:</p> Name Type Description <code>results</code> <code>list of dict</code> <p>A list containing dictionaries with the method name, binary result, quality index, filtered image, filter type, rolling window flag, and sigma values used.</p> Source code in <code>src/cellects/image_analysis/network_functions.py</code> <pre><code>def apply_frangi_variations(self) -&gt; list:\n    \"\"\"\n    Applies various Frangi filter variations with different sigma values and thresholding methods.\n\n    This method applies the Frangi vesselness filter with multiple sets of sigma values\n    to detect vessels at different scales. It applies both Otsu thresholding and rolling window\n    segmentation to the filtered results and calculates binary quality indices.\n\n    Returns\n    -------\n    results : list of dict\n        A list containing dictionaries with the method name, binary result, quality index,\n        filtered image, filter type, rolling window flag, and sigma values used.\n    \"\"\"\n    results = []\n\n    # Parameter variations for Frangi filter\n    frangi_sigmas = {\n        's_fine_vessels': [0.75],\n        'fine_vessels': [0.5, 1.0],  # Very fine capillaries, thin fibers\n        'small_vessels': [1.0, 2.0],  # Small vessels, fine structures\n        'multi_scale_medium': [1.0, 2.0, 3.0],  # Standard multi-scale\n        'ultra_fine': [0.3, 0.5, 0.8],  # Ultra-fine structures\n        'comprehensive': [0.5, 1.0, 2.0, 4.0],  # Multi-scale\n        'retinal_vessels': [1.0, 2.0, 4.0, 8.0],  # Optimized for retinal imaging\n        'microscopy': [0.5, 1.0, 1.5, 2.5],  # Microscopy applications\n        'broad_spectrum': [0.5, 1.5, 3.0, 6.0, 10.0]\n    }\n\n    for i, (key, sigmas) in enumerate(frangi_sigmas.items()):\n        # Apply Frangi filter\n        frangi_result = frangi(self.greyscale_image, sigmas=sigmas, beta=self.frangi_beta, gamma=self.frangi_gamma, black_ridges=self.black_ridges)\n\n        # Apply both thresholding methods\n        # Method 1: Otsu thresholding\n        thresh_otsu = threshold_otsu(frangi_result)\n        binary_otsu = frangi_result &gt; thresh_otsu\n        quality_otsu = binary_quality_index(self.possibly_filled_pixels * binary_otsu)\n\n        # Method 2: Rolling window thresholding\n\n        # Store results\n        results.append({\n            'method': f'f_{sigmas}_thresh',\n            'binary': binary_otsu,\n            'quality': quality_otsu,\n            'filtered': frangi_result,\n            'filter': f'Frangi',\n            'rolling_window': False,\n            'sigmas': sigmas\n        })\n        # Method 2: Rolling window thresholding\n        if self.add_rolling_window:\n            binary_rolling = rolling_window_segmentation(frangi_result, self.possibly_filled_pixels, patch_size=(10, 10))\n            quality_rolling = binary_quality_index(binary_rolling)\n            results.append({\n                'method': f'f_{sigmas}_roll',\n                'binary': binary_rolling,\n                'quality': quality_rolling,\n                'filtered': frangi_result,\n                'filter': f'Frangi',\n                'rolling_window': True,\n                'sigmas': sigmas\n            })\n\n    return results\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.NetworkDetection.apply_sato_variations","title":"<code>apply_sato_variations()</code>","text":"<p>Apply various Sato filter variations to an image and store the results.</p> <p>This function applies different parameter sets for the Sato vesselness filter to an image, applies two thresholding methods (Otsu and rolling window), and stores the results. The function supports optional rolling window segmentation based on a configuration flag.</p> <p>Returns:</p> Type Description <code>list of dict</code> <p>A list containing dictionaries with the results for each filter variation. Each dictionary includes method name, binary image, quality index, filtered result, filter type, rolling window flag, and sigma values.</p> Source code in <code>src/cellects/image_analysis/network_functions.py</code> <pre><code>def apply_sato_variations(self) -&gt; list:\n    \"\"\"\n    Apply various Sato filter variations to an image and store the results.\n\n    This function applies different parameter sets for the Sato vesselness\n    filter to an image, applies two thresholding methods (Otsu and rolling window),\n    and stores the results. The function supports optional rolling window\n    segmentation based on a configuration flag.\n\n    Returns\n    -------\n    list of dict\n        A list containing dictionaries with the results for each filter variation.\n        Each dictionary includes method name, binary image, quality index,\n        filtered result, filter type, rolling window flag, and sigma values.\n    \"\"\"\n    results = []\n\n    # Parameter variations for Frangi filter\n    sato_sigmas = {\n        'super_small_tubes': [0.01, 0.05, 0.1, 0.15],  #\n        'small_tubes': [0.1, 0.2, 0.4, 0.8],  #\n        's_thick_ridges': [0.25, 0.75],  # Thick ridges/tubes\n        'small_multi_scale': [0.1, 0.2, 0.4, 0.8, 1.6],  #\n        'fine_ridges': [0.8, 1.5],  # Fine ridge detection\n        'medium_ridges': [1.5, 3.0],  # Medium ridge structures\n        'multi_scale_fine': [0.8, 1.5, 2.5],  # Multi-scale fine detection\n        'multi_scale_standard': [1.0, 2.5, 5.0],  # Standard multi-scale\n        'edge_enhanced': [0.5, 1.0, 2.0],  # Edge-enhanced detection\n        'noise_robust': [1.5, 2.5, 4.0],  # Robust to noise\n        'fingerprint': [1.0, 1.5, 2.0, 3.0],  # Fingerprint ridge detection\n        'geological': [2.0, 5.0, 10.0, 15.0]  # Geological structures\n    }\n\n    for i, (key, sigmas) in enumerate(sato_sigmas.items()):\n        # Apply sato filter\n        sato_result = sato(self.greyscale_image, sigmas=sigmas, black_ridges=self.black_ridges, mode='reflect')\n\n        # Apply both thresholding methods\n        # Method 1: Otsu thresholding\n        thresh_otsu = threshold_otsu(sato_result)\n        binary_otsu = sato_result &gt; thresh_otsu\n        quality_otsu = binary_quality_index(self.possibly_filled_pixels * binary_otsu)\n\n\n        # Store results\n        results.append({\n            'method': f's_{sigmas}_thresh',\n            'binary': binary_otsu,\n            'quality': quality_otsu,\n            'filtered': sato_result,\n            'filter': f'Sato',\n            'rolling_window': False,\n            'sigmas': sigmas\n        })\n\n        # Method 2: Rolling window thresholding\n        if self.add_rolling_window:\n            binary_rolling = rolling_window_segmentation(sato_result, self.possibly_filled_pixels, patch_size=(10, 10))\n            quality_rolling = binary_quality_index(binary_rolling)\n\n            results.append({\n                'method': f's_{sigmas}_roll',\n                'binary': binary_rolling,\n                'quality': quality_rolling,\n                'filtered': sato_result,\n                'filter': f'Sato',\n                'rolling_window': True,\n                'sigmas': sigmas\n            })\n\n    return results\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.NetworkDetection.change_greyscale","title":"<code>change_greyscale(img, first_dict)</code>","text":"<p>Change the image to greyscale using color space combinations.</p> <p>This function converts an input image to greyscale by generating and applying a combination of color spaces specified in the dictionary. The resulting greyscale image is stored as an attribute of the instance.</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>ndarray of uint8</code> <p>The input image to be converted to greyscale.</p> required Source code in <code>src/cellects/image_analysis/network_functions.py</code> <pre><code>def change_greyscale(self, img: NDArray[np.uint8], first_dict: dict):\n    \"\"\"\n    Change the image to greyscale using color space combinations.\n\n    This function converts an input image to greyscale by generating\n    and applying a combination of color spaces specified in the dictionary.\n    The resulting greyscale image is stored as an attribute of the instance.\n\n    Parameters\n    ----------\n    img : ndarray of uint8\n        The input image to be converted to greyscale.\n    \"\"\"\n    self.greyscale_image, g2, all_c_spaces, first_pc_vector  = generate_color_space_combination(img, list(first_dict.keys()), first_dict)\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.NetworkDetection.detect_network","title":"<code>detect_network()</code>","text":"<p>Process and detect network features in the greyscale image.</p> <p>This method applies a frangi or sato filter based on the best result and performs segmentation using either rolling window or Otsu's thresholding. The final network detection result is stored in <code>self.incomplete_network</code>.</p> Source code in <code>src/cellects/image_analysis/network_functions.py</code> <pre><code>def detect_network(self):\n    \"\"\"\n    Process and detect network features in the greyscale image.\n\n    This method applies a frangi or sato filter based on the best result and\n    performs segmentation using either rolling window or Otsu's thresholding.\n    The final network detection result is stored in `self.incomplete_network`.\n    \"\"\"\n    if self.best_result['filter'] == 'Frangi':\n        filtered_result = frangi(self.greyscale_image, sigmas=self.best_result['sigmas'], beta=self.frangi_beta, gamma=self.frangi_gamma, black_ridges=self.black_ridges)\n    else:\n        filtered_result = sato(self.greyscale_image, sigmas=self.best_result['sigmas'], black_ridges=self.black_ridges, mode='reflect')\n\n    if self.best_result['rolling_window']:\n        binary_image = rolling_window_segmentation(filtered_result, self.possibly_filled_pixels, patch_size=(10, 10))\n    else:\n        thresh_otsu = threshold_otsu(filtered_result)\n        binary_image = filtered_result &gt; thresh_otsu\n    self.incomplete_network = binary_image * self.possibly_filled_pixels\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.NetworkDetection.detect_pseudopods","title":"<code>detect_pseudopods(lighter_background, pseudopod_min_width=5, pseudopod_min_size=50, only_one_connected_component=True)</code>","text":"<p>Detect pseudopods in a binary image.</p> <p>Identify and process regions that resemble pseudopods based on width, size, and connectivity criteria. This function is used to detect and label areas that are indicative of pseudopod-like structures within a binary image.</p> <p>Parameters:</p> Name Type Description Default <code>lighter_background</code> <code>bool</code> <p>Boolean flag to indicate if the background should be considered lighter.</p> required <code>pseudopod_min_width</code> <code>int</code> <p>Minimum width for pseudopods to be considered valid. Default is 5.</p> <code>5</code> <code>pseudopod_min_size</code> <code>int</code> <p>Minimum size for pseudopods to be considered valid. Default is 50.</p> <code>50</code> <code>only_one_connected_component</code> <code>bool</code> <p>Flag to ensure only one connected component is kept. Default is True.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> Notes <p>This function modifies internal attributes of the object, specifically setting <code>self.pseudopods</code> to an array indicating pseudopod regions.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; result = detect_pseudopods(True, 5, 50)\n&gt;&gt;&gt; print(self.pseudopods)\narray([[0, 1, ..., 0],\n       [0, 0, ..., 0],\n       ...,\n       [0, 1, ..., 0]], dtype=uint8)\n</code></pre> Source code in <code>src/cellects/image_analysis/network_functions.py</code> <pre><code>def detect_pseudopods(self, lighter_background: bool, pseudopod_min_width: int=5, pseudopod_min_size: int=50, only_one_connected_component: bool=True):\n    \"\"\"\n    Detect pseudopods in a binary image.\n\n    Identify and process regions that resemble pseudopods based on width, size,\n    and connectivity criteria. This function is used to detect and label areas\n    that are indicative of pseudopod-like structures within a binary image.\n\n    Parameters\n    ----------\n    lighter_background : bool\n        Boolean flag to indicate if the background should be considered lighter.\n    pseudopod_min_width : int, optional\n        Minimum width for pseudopods to be considered valid. Default is 5.\n    pseudopod_min_size : int, optional\n        Minimum size for pseudopods to be considered valid. Default is 50.\n    only_one_connected_component : bool, optional\n        Flag to ensure only one connected component is kept. Default is True.\n\n    Returns\n    -------\n    None\n\n    Notes\n    -----\n    This function modifies internal attributes of the object, specifically setting `self.pseudopods` to an array indicating pseudopod regions.\n\n    Examples\n    --------\n    &gt;&gt;&gt; result = detect_pseudopods(True, 5, 50)\n    &gt;&gt;&gt; print(self.pseudopods)\n    array([[0, 1, ..., 0],\n           [0, 0, ..., 0],\n           ...,\n           [0, 1, ..., 0]], dtype=uint8)\n\n    \"\"\"\n\n    closed_im = close_holes(self.possibly_filled_pixels)\n    dist_trans = distance_transform_edt(closed_im)\n    dist_trans = dist_trans.max() - dist_trans\n    # Add dilatation of bracket of distances from medial_axis to the multiplication\n    if lighter_background:\n        grey = self.greyscale_image.max() - self.greyscale_image\n    else:\n        grey = self.greyscale_image\n    if self.origin_to_add is not None:\n        dist_trans_ori = distance_transform_edt(1 - self.origin_to_add)\n        scored_im = dist_trans * dist_trans_ori * grey\n    else:\n        scored_im = (dist_trans**2) * grey\n    scored_im = bracket_to_uint8_image_contrast(scored_im)\n    thresh = threshold_otsu(scored_im)\n    thresh = find_threshold_given_mask(scored_im, self.possibly_filled_pixels, min_threshold=thresh)\n    high_int_in_periphery = (scored_im &gt; thresh).astype(np.uint8) * self.possibly_filled_pixels\n\n    _, pseudopod_widths = morphology.medial_axis(high_int_in_periphery, return_distance=True, rng=0)\n    bin_im = pseudopod_widths &gt;= pseudopod_min_width\n    dil_bin_im = cv2.dilate(bin_im.astype(np.uint8), kernel=create_ellipse(7, 7).astype(np.uint8), iterations=1)\n    bin_im = high_int_in_periphery * dil_bin_im\n    nb, shapes, stats, centro = cv2.connectedComponentsWithStats(bin_im)\n    true_pseudopods = np.nonzero(stats[:, 4] &gt; pseudopod_min_size)[0][1:]\n    true_pseudopods = np.isin(shapes, true_pseudopods)\n\n    # Make sure that the tubes connecting two pseudopods belong to pseudopods if removing pseudopods cuts the network\n    complete_network = np.logical_or(true_pseudopods, self.incomplete_network).astype(np.uint8)\n    if only_one_connected_component:\n        complete_network = keep_one_connected_component(complete_network)\n        without_pseudopods = complete_network.copy()\n        without_pseudopods[true_pseudopods] = 0\n        only_connected_network = keep_one_connected_component(without_pseudopods)\n        self.pseudopods = (1 - only_connected_network) * complete_network  * self.possibly_filled_pixels\n    else:\n        self.pseudopods = true_pseudopods.astype(np.uint8)\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.NetworkDetection.get_best_network_detection_method","title":"<code>get_best_network_detection_method()</code>","text":"<p>Get the best network detection method based on quality metrics.</p> <p>This function applies Frangi and Sato variations, combines their results, calculates quality metrics for each result, and selects the best method.</p> <p>Attributes:</p> Name Type Description <code>all_results</code> <code>list of dicts</code> <p>Combined results from Frangi and Sato variations.</p> <code>quality_metrics</code> <code>ndarray of float64</code> <p>Quality metrics for each detection result.</p> <code>best_idx</code> <code>int</code> <p>Index of the best detection method based on quality metrics.</p> <code>best_result</code> <code>dict</code> <p>The best detection result from all possible methods.</p> <code>incomplete_network</code> <code>ndarray of bool</code> <p>Binary representation of the best detection result.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; possibly_filled_pixels = np.zeros((9, 9), dtype=np.uint8)\n&gt;&gt;&gt; possibly_filled_pixels[3:6, 3:6] = 1\n&gt;&gt;&gt; possibly_filled_pixels[1:6, 3] = 1\n&gt;&gt;&gt; possibly_filled_pixels[6:-1, 5] = 1\n&gt;&gt;&gt; possibly_filled_pixels[4, 1:-1] = 1\n&gt;&gt;&gt; greyscale_image = possibly_filled_pixels.copy()\n&gt;&gt;&gt; greyscale_image[greyscale_image &gt; 0] = np.random.randint(170, 255, possibly_filled_pixels.sum())\n&gt;&gt;&gt; greyscale_image[greyscale_image == 0] = np.random.randint(0, 120, possibly_filled_pixels.size - possibly_filled_pixels.sum())\n&gt;&gt;&gt; add_rolling_window=False\n&gt;&gt;&gt; origin_to_add = np.zeros((9, 9), dtype=np.uint8)\n&gt;&gt;&gt; origin_to_add[3:6, 3:6] = 1\n&gt;&gt;&gt; NetDet = NetworkDetection(greyscale_image, possibly_filled_pixels, add_rolling_window, origin_to_add)\n&gt;&gt;&gt; NetDet.get_best_network_detection_method()\n&gt;&gt;&gt; print(NetDet.best_result['method'])\n&gt;&gt;&gt; print(NetDet.best_result['binary'])\n&gt;&gt;&gt; print(NetDet.best_result['quality'])\n&gt;&gt;&gt; print(NetDet.best_result['filtered'])\n&gt;&gt;&gt; print(NetDet.best_result['filter'])\n&gt;&gt;&gt; print(NetDet.best_result['rolling_window'])\n&gt;&gt;&gt; print(NetDet.best_result['sigmas'])\nbgr_image = np.random.randint(0, 256, (100, 100, 3), dtype=np.uint8)\n</code></pre> Source code in <code>src/cellects/image_analysis/network_functions.py</code> <pre><code>def get_best_network_detection_method(self):\n    \"\"\"\n    Get the best network detection method based on quality metrics.\n\n    This function applies Frangi and Sato variations, combines their results,\n    calculates quality metrics for each result, and selects the best method.\n\n    Attributes\n    ----------\n    all_results : list of dicts\n        Combined results from Frangi and Sato variations.\n    quality_metrics : ndarray of float64\n        Quality metrics for each detection result.\n    best_idx : int\n        Index of the best detection method based on quality metrics.\n    best_result : dict\n        The best detection result from all possible methods.\n    incomplete_network : ndarray of bool\n        Binary representation of the best detection result.\n\n    Examples\n    ----------\n    &gt;&gt;&gt; possibly_filled_pixels = np.zeros((9, 9), dtype=np.uint8)\n    &gt;&gt;&gt; possibly_filled_pixels[3:6, 3:6] = 1\n    &gt;&gt;&gt; possibly_filled_pixels[1:6, 3] = 1\n    &gt;&gt;&gt; possibly_filled_pixels[6:-1, 5] = 1\n    &gt;&gt;&gt; possibly_filled_pixels[4, 1:-1] = 1\n    &gt;&gt;&gt; greyscale_image = possibly_filled_pixels.copy()\n    &gt;&gt;&gt; greyscale_image[greyscale_image &gt; 0] = np.random.randint(170, 255, possibly_filled_pixels.sum())\n    &gt;&gt;&gt; greyscale_image[greyscale_image == 0] = np.random.randint(0, 120, possibly_filled_pixels.size - possibly_filled_pixels.sum())\n    &gt;&gt;&gt; add_rolling_window=False\n    &gt;&gt;&gt; origin_to_add = np.zeros((9, 9), dtype=np.uint8)\n    &gt;&gt;&gt; origin_to_add[3:6, 3:6] = 1\n    &gt;&gt;&gt; NetDet = NetworkDetection(greyscale_image, possibly_filled_pixels, add_rolling_window, origin_to_add)\n    &gt;&gt;&gt; NetDet.get_best_network_detection_method()\n    &gt;&gt;&gt; print(NetDet.best_result['method'])\n    &gt;&gt;&gt; print(NetDet.best_result['binary'])\n    &gt;&gt;&gt; print(NetDet.best_result['quality'])\n    &gt;&gt;&gt; print(NetDet.best_result['filtered'])\n    &gt;&gt;&gt; print(NetDet.best_result['filter'])\n    &gt;&gt;&gt; print(NetDet.best_result['rolling_window'])\n    &gt;&gt;&gt; print(NetDet.best_result['sigmas'])\n    bgr_image = np.random.randint(0, 256, (100, 100, 3), dtype=np.uint8)\n    \"\"\"\n    frangi_res = self.apply_frangi_variations()\n    sato_res = self.apply_sato_variations()\n    self.all_results = frangi_res + sato_res\n    self.quality_metrics = np.array([result['quality'] for result in self.all_results])\n    self.best_idx = np.argmax(self.quality_metrics)\n    self.best_result = self.all_results[self.best_idx]\n    self.incomplete_network = self.best_result['binary'] * self.possibly_filled_pixels\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.NetworkDetection.merge_network_with_pseudopods","title":"<code>merge_network_with_pseudopods()</code>","text":"<p>Merge the incomplete network with pseudopods.</p> <p>This method combines the incomplete network and pseudopods to form the complete network. The incomplete network is updated by subtracting areas where pseudopods are present.</p> Source code in <code>src/cellects/image_analysis/network_functions.py</code> <pre><code>def merge_network_with_pseudopods(self):\n    \"\"\"\n    Merge the incomplete network with pseudopods.\n\n    This method combines the incomplete network and pseudopods to form\n    the complete network. The incomplete network is updated by subtracting\n    areas where pseudopods are present.\n    \"\"\"\n    self.complete_network = np.logical_or(self.incomplete_network, self.pseudopods).astype(np.uint8)\n    self.incomplete_network *= (1 - self.pseudopods)\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.PickleRick","title":"<code>PickleRick</code>","text":"<p>A class to handle safe file reading and writing operations using pickle.</p> <p>This class ensures that files are not being accessed concurrently by creating a lock file (PickleRickX.pkl) to signal that the file is open. It includes methods to check for the lock file, write data safely, and read data safely.</p> Source code in <code>src/cellects/utils/load_display_save.py</code> <pre><code>class PickleRick:\n    \"\"\"\n    A class to handle safe file reading and writing operations using pickle.\n\n    This class ensures that files are not being accessed concurrently by\n    creating a lock file (PickleRickX.pkl) to signal that the file is open.\n    It includes methods to check for the lock file, write data safely,\n    and read data safely.\n    \"\"\"\n    def __init__(self, pickle_rick_number=\"\"):\n        \"\"\"\n        Initialize a new instance of the class.\n\n        This constructor sets up initial attributes for tracking Rick's state, including\n        a boolean flag for waiting for Pickle Rick, a counter, the provided pickle Rick number,\n        and the time when the first check was performed.\n\n        Parameters\n        ----------\n        pickle_rick_number : str, optional\n            The number associated with Pickle Rick. Defaults to an empty string.\n        \"\"\"\n        self.wait_for_pickle_rick: bool = False\n        self.counter = 0\n        self.pickle_rick_number = pickle_rick_number\n        self.first_check_time = default_timer()\n\n    def _check_that_file_is_not_open(self):\n        \"\"\"\n        Check if a specific pickle file exists and handle it accordingly.\n\n        This function checks whether a file named `PickleRick{self.pickle_rick_number}.pkl`\n        exists. If the file has not been modified for more than 2 seconds, it is removed.\n        The function then updates an attribute to indicate whether the file exists.\n\n        Parameters\n        ----------\n        self : PickleRickObject\n            The instance of the class containing this method.\n\n        Returns\n        -------\n        None\n            This function does not return any value.\n            It updates the `self.wait_for_pickle_rick` attribute.\n\n        Notes\n        -----\n        This function removes the pickle file if it has not been modified for more than 2 seconds.\n        The `self.wait_for_pickle_rick` attribute is updated based on the existence of the file.\n        \"\"\"\n        if os.path.isfile(f\"PickleRick{self.pickle_rick_number}.pkl\"):\n            if default_timer() - self.first_check_time &gt; 2:\n                os.remove(f\"PickleRick{self.pickle_rick_number}.pkl\")\n            # logging.error((f\"Cannot read/write, Trying again... tip: unlock by deleting the file named PickleRick{self.pickle_rick_number}.pkl\"))\n        self.wait_for_pickle_rick = os.path.isfile(f\"PickleRick{self.pickle_rick_number}.pkl\")\n\n    def _write_pickle_rick(self):\n        \"\"\"\n        Write pickle data to a file for Pickle Rick.\n\n        Parameters\n        ----------\n        self : object\n            The instance of the class that this method belongs to.\n            This typically contains attributes and methods relevant to managing\n            pickle operations for Pickle Rick.\n\n        Raises\n        ------\n        Exception\n            General exception raised if there is any issue with writing the file.\n            The error details are logged.\n\n        Notes\n        -----\n        This function creates a file named `PickleRick{self.pickle_rick_number}.pkl`\n        with a dictionary indicating readiness for Pickle Rick.\n\n        Examples\n        --------\n        &gt;&gt;&gt; obj = PickleRick()  # Assuming `YourClassInstance` is the class containing this method\n        &gt;&gt;&gt; obj.pickle_rick_number = 1  # Set an example value for the attribute\n        &gt;&gt;&gt; obj._write_pickle_rick()     # Call the method to create and write to file\n        \"\"\"\n        try:\n            with open(f\"PickleRick{self.pickle_rick_number}.pkl\", 'wb') as file_to_write:\n                pickle.dump({'wait_for_pickle_rick': True}, file_to_write)\n        except Exception as exc:\n            logging.error(f\"Don't know how but Pickle Rick failed... Error is: {exc}\")\n\n    def _delete_pickle_rick(self):\n        \"\"\"\n\n        Delete a specific Pickle Rick file.\n\n        Deletes the pickle file associated with the current instance's\n        `pickle_rick_number`.\n\n        Raises\n        ------\n        FileNotFoundError\n            If the file with name `PickleRick{self.pickle_rick_number}.pkl` does not exist.\n        \"\"\"\n        if os.path.isfile(f\"PickleRick{self.pickle_rick_number}.pkl\"):\n            os.remove(f\"PickleRick{self.pickle_rick_number}.pkl\")\n\n    def write_file(self, file_content, file_name):\n        \"\"\"\n        Write content to a file with error handling and retry logic.\n\n        This function attempts to write the provided content into a file.\n        If it fails, it retries up to 100 times with some additional checks\n        and delays. Note that the content is serialized using pickle.\n\n        Parameters\n        ----------\n        file_content : Any\n            The data to be written into the file. This will be pickled.\n        file_name : str\n            The name of the file where data should be written.\n\n        Returns\n        -------\n        None\n\n        Raises\n        ------\n        Exception\n            If the file cannot be written after 100 attempts, an error is logged.\n\n        Notes\n        -----\n        This function uses pickle to serialize the data, which can introduce security risks\n        if untrusted content is being written. It performs some internal state checks,\n        such as verifying that the target file isn't open and whether it should delete\n        some internal state, represented by `_delete_pickle_rick`.\n\n        The function implements a retry mechanism with a backoff strategy that can include\n        random delays, though the example code does not specify these details explicitly.\n\n        Examples\n        --------\n        &gt;&gt;&gt; result = PickleRick().write_file({'key': 'value'}, 'test.pkl')\n        Success to write file\n        \"\"\"\n        self.counter += 1\n        if self.counter &lt; 100:\n            if self.counter &gt; 95:\n                self._delete_pickle_rick()\n            # time.sleep(np.random.choice(np.arange(1, os.cpu_count(), 0.5)))\n            self._check_that_file_is_not_open()\n            if self.wait_for_pickle_rick:\n                time.sleep(2)\n                self.write_file(file_content, file_name)\n            else:\n                self._write_pickle_rick()\n                try:\n                    with open(file_name, 'wb') as file_to_write:\n                        pickle.dump(file_content, file_to_write, protocol=0)\n                    self._delete_pickle_rick()\n                    logging.info(f\"Success to write file\")\n                except Exception as exc:\n                    logging.error(f\"The Pickle error on the file {file_name} is: {exc}\")\n                    self._delete_pickle_rick()\n                    self.write_file(file_content, file_name)\n        else:\n            logging.error(f\"Failed to write {file_name}\")\n\n    def read_file(self, file_name):\n        \"\"\"\n        Reads the contents of a file using pickle and returns it.\n\n        Parameters\n        ----------\n        file_name : str\n            The name of the file to be read.\n\n        Returns\n        -------\n        Union[Any, None]\n            The content of the file if successfully read; otherwise, `None`.\n\n        Raises\n        ------\n        Exception\n            If there is an error reading the file.\n\n        Notes\n        -----\n        This function attempts to read a file multiple times if it fails.\n        If the number of attempts exceeds 1000, it logs an error and returns `None`.\n\n        Examples\n        --------\n        &gt;&gt;&gt; PickleRick().read_file(\"example.pkl\")\n        \"\"\"\n        self.counter += 1\n        if self.counter &lt; 1000:\n            if self.counter &gt; 950:\n                self._delete_pickle_rick()\n            self._check_that_file_is_not_open()\n            if self.wait_for_pickle_rick:\n                time.sleep(2)\n                self.read_file(file_name)\n            else:\n                self._write_pickle_rick()\n                try:\n                    with open(file_name, 'rb') as fileopen:\n                        file_content = pickle.load(fileopen)\n                except Exception as exc:\n                    logging.error(f\"The Pickle error on the file {file_name} is: {exc}\")\n                    file_content = None\n                self._delete_pickle_rick()\n                if file_content is None:\n                    self.read_file(file_name)\n                else:\n                    logging.info(f\"Success to read file\")\n                return file_content\n        else:\n            logging.error(f\"Failed to read {file_name}\")\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.PickleRick.__init__","title":"<code>__init__(pickle_rick_number='')</code>","text":"<p>Initialize a new instance of the class.</p> <p>This constructor sets up initial attributes for tracking Rick's state, including a boolean flag for waiting for Pickle Rick, a counter, the provided pickle Rick number, and the time when the first check was performed.</p> <p>Parameters:</p> Name Type Description Default <code>pickle_rick_number</code> <code>str</code> <p>The number associated with Pickle Rick. Defaults to an empty string.</p> <code>''</code> Source code in <code>src/cellects/utils/load_display_save.py</code> <pre><code>def __init__(self, pickle_rick_number=\"\"):\n    \"\"\"\n    Initialize a new instance of the class.\n\n    This constructor sets up initial attributes for tracking Rick's state, including\n    a boolean flag for waiting for Pickle Rick, a counter, the provided pickle Rick number,\n    and the time when the first check was performed.\n\n    Parameters\n    ----------\n    pickle_rick_number : str, optional\n        The number associated with Pickle Rick. Defaults to an empty string.\n    \"\"\"\n    self.wait_for_pickle_rick: bool = False\n    self.counter = 0\n    self.pickle_rick_number = pickle_rick_number\n    self.first_check_time = default_timer()\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.PickleRick.read_file","title":"<code>read_file(file_name)</code>","text":"<p>Reads the contents of a file using pickle and returns it.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>The name of the file to be read.</p> required <p>Returns:</p> Type Description <code>Union[Any, None]</code> <p>The content of the file if successfully read; otherwise, <code>None</code>.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If there is an error reading the file.</p> Notes <p>This function attempts to read a file multiple times if it fails. If the number of attempts exceeds 1000, it logs an error and returns <code>None</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; PickleRick().read_file(\"example.pkl\")\n</code></pre> Source code in <code>src/cellects/utils/load_display_save.py</code> <pre><code>def read_file(self, file_name):\n    \"\"\"\n    Reads the contents of a file using pickle and returns it.\n\n    Parameters\n    ----------\n    file_name : str\n        The name of the file to be read.\n\n    Returns\n    -------\n    Union[Any, None]\n        The content of the file if successfully read; otherwise, `None`.\n\n    Raises\n    ------\n    Exception\n        If there is an error reading the file.\n\n    Notes\n    -----\n    This function attempts to read a file multiple times if it fails.\n    If the number of attempts exceeds 1000, it logs an error and returns `None`.\n\n    Examples\n    --------\n    &gt;&gt;&gt; PickleRick().read_file(\"example.pkl\")\n    \"\"\"\n    self.counter += 1\n    if self.counter &lt; 1000:\n        if self.counter &gt; 950:\n            self._delete_pickle_rick()\n        self._check_that_file_is_not_open()\n        if self.wait_for_pickle_rick:\n            time.sleep(2)\n            self.read_file(file_name)\n        else:\n            self._write_pickle_rick()\n            try:\n                with open(file_name, 'rb') as fileopen:\n                    file_content = pickle.load(fileopen)\n            except Exception as exc:\n                logging.error(f\"The Pickle error on the file {file_name} is: {exc}\")\n                file_content = None\n            self._delete_pickle_rick()\n            if file_content is None:\n                self.read_file(file_name)\n            else:\n                logging.info(f\"Success to read file\")\n            return file_content\n    else:\n        logging.error(f\"Failed to read {file_name}\")\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.PickleRick.write_file","title":"<code>write_file(file_content, file_name)</code>","text":"<p>Write content to a file with error handling and retry logic.</p> <p>This function attempts to write the provided content into a file. If it fails, it retries up to 100 times with some additional checks and delays. Note that the content is serialized using pickle.</p> <p>Parameters:</p> Name Type Description Default <code>file_content</code> <code>Any</code> <p>The data to be written into the file. This will be pickled.</p> required <code>file_name</code> <code>str</code> <p>The name of the file where data should be written.</p> required <p>Returns:</p> Type Description <code>None</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If the file cannot be written after 100 attempts, an error is logged.</p> Notes <p>This function uses pickle to serialize the data, which can introduce security risks if untrusted content is being written. It performs some internal state checks, such as verifying that the target file isn't open and whether it should delete some internal state, represented by <code>_delete_pickle_rick</code>.</p> <p>The function implements a retry mechanism with a backoff strategy that can include random delays, though the example code does not specify these details explicitly.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; result = PickleRick().write_file({'key': 'value'}, 'test.pkl')\nSuccess to write file\n</code></pre> Source code in <code>src/cellects/utils/load_display_save.py</code> <pre><code>def write_file(self, file_content, file_name):\n    \"\"\"\n    Write content to a file with error handling and retry logic.\n\n    This function attempts to write the provided content into a file.\n    If it fails, it retries up to 100 times with some additional checks\n    and delays. Note that the content is serialized using pickle.\n\n    Parameters\n    ----------\n    file_content : Any\n        The data to be written into the file. This will be pickled.\n    file_name : str\n        The name of the file where data should be written.\n\n    Returns\n    -------\n    None\n\n    Raises\n    ------\n    Exception\n        If the file cannot be written after 100 attempts, an error is logged.\n\n    Notes\n    -----\n    This function uses pickle to serialize the data, which can introduce security risks\n    if untrusted content is being written. It performs some internal state checks,\n    such as verifying that the target file isn't open and whether it should delete\n    some internal state, represented by `_delete_pickle_rick`.\n\n    The function implements a retry mechanism with a backoff strategy that can include\n    random delays, though the example code does not specify these details explicitly.\n\n    Examples\n    --------\n    &gt;&gt;&gt; result = PickleRick().write_file({'key': 'value'}, 'test.pkl')\n    Success to write file\n    \"\"\"\n    self.counter += 1\n    if self.counter &lt; 100:\n        if self.counter &gt; 95:\n            self._delete_pickle_rick()\n        # time.sleep(np.random.choice(np.arange(1, os.cpu_count(), 0.5)))\n        self._check_that_file_is_not_open()\n        if self.wait_for_pickle_rick:\n            time.sleep(2)\n            self.write_file(file_content, file_name)\n        else:\n            self._write_pickle_rick()\n            try:\n                with open(file_name, 'wb') as file_to_write:\n                    pickle.dump(file_content, file_to_write, protocol=0)\n                self._delete_pickle_rick()\n                logging.info(f\"Success to write file\")\n            except Exception as exc:\n                logging.error(f\"The Pickle error on the file {file_name} is: {exc}\")\n                self._delete_pickle_rick()\n                self.write_file(file_content, file_name)\n    else:\n        logging.error(f\"Failed to write {file_name}\")\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.ad_pad","title":"<code>ad_pad(arr)</code>","text":"<p>Pad the input array with a single layer of zeros around its edges.</p> <p>Parameters:</p> Name Type Description Default <code>arr</code> <code>ndarray</code> <p>The input array to pad. Must be at least 2-dimensional.</p> required <p>Returns:</p> Name Type Description <code>padded_arr</code> <code>ndarray</code> <p>The output array with a single 0-padded layer around its edges.</p> Notes <p>This function uses NumPy's <code>pad</code> with mode='constant' to add a single layer of zeros around the edges of the input array.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; arr = np.array([[1, 2], [3, 4]])\n&gt;&gt;&gt; ad_pad(arr)\narray([[0, 0, 0, 0],\n   [0, 1, 2, 0],\n   [0, 3, 4, 0],\n   [0, 0, 0, 0]])\n</code></pre> Source code in <code>src/cellects/image_analysis/network_functions.py</code> <pre><code>def ad_pad(arr: NDArray) -&gt; NDArray:\n    \"\"\"\n    Pad the input array with a single layer of zeros around its edges.\n\n    Parameters\n    ----------\n    arr : ndarray\n        The input array to pad. Must be at least 2-dimensional.\n\n    Returns\n    -------\n    padded_arr : ndarray\n        The output array with a single 0-padded layer around its edges.\n\n    Notes\n    -----\n    This function uses NumPy's `pad` with mode='constant' to add a single layer\n    of zeros around the edges of the input array.\n\n    Examples\n    --------\n    &gt;&gt;&gt; arr = np.array([[1, 2], [3, 4]])\n    &gt;&gt;&gt; ad_pad(arr)\n    array([[0, 0, 0, 0],\n       [0, 1, 2, 0],\n       [0, 3, 4, 0],\n       [0, 0, 0, 0]])\n    \"\"\"\n    return np.pad(arr, [(1, ), (1, )], mode='constant')\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.add_padding","title":"<code>add_padding(array_list)</code>","text":"<p>Add padding to each 2D array in a list.</p> <p>Parameters:</p> Name Type Description Default <code>array_list</code> <code>list of ndarrays</code> <p>List of 2D NumPy arrays to be processed.</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>list of ndarrays</code> <p>List of 2D NumPy arrays with the padding removed.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; array_list = [np.array([[1, 2], [3, 4]])]\n&gt;&gt;&gt; padded_list = add_padding(array_list)\n&gt;&gt;&gt; print(padded_list[0])\n[[0 0 0]\n [0 1 2 0]\n [0 3 4 0]\n [0 0 0]]\n</code></pre> Source code in <code>src/cellects/image_analysis/network_functions.py</code> <pre><code>def add_padding(array_list: list) -&gt; list:\n    \"\"\"\n    Add padding to each 2D array in a list.\n\n    Parameters\n    ----------\n    array_list : list of ndarrays\n        List of 2D NumPy arrays to be processed.\n\n    Returns\n    -------\n    out : list of ndarrays\n        List of 2D NumPy arrays with the padding removed.\n\n    Examples\n    --------\n    &gt;&gt;&gt; array_list = [np.array([[1, 2], [3, 4]])]\n    &gt;&gt;&gt; padded_list = add_padding(array_list)\n    &gt;&gt;&gt; print(padded_list[0])\n    [[0 0 0]\n     [0 1 2 0]\n     [0 3 4 0]\n     [0 0 0]]\n    \"\"\"\n    new_array_list = []\n    for arr in array_list:\n        new_array_list.append(ad_pad(arr))\n    return new_array_list\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.binary_quality_index","title":"<code>binary_quality_index(binary_img)</code>","text":"<p>Calculate the binary quality index for a binary image.</p> <p>The binary quality index is computed based on the perimeter of the largest connected component in the binary image, normalized by the total number of pixels.</p> <p>Parameters:</p> Name Type Description Default <code>binary_img</code> <code>ndarray of uint8</code> <p>Input binary image array.</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>float</code> <p>The binary quality index value.</p> Source code in <code>src/cellects/image_analysis/image_segmentation.py</code> <pre><code>def binary_quality_index(binary_img: NDArray[np.uint8]) -&gt; float:\n    \"\"\"\n    Calculate the binary quality index for a binary image.\n\n    The binary quality index is computed based on the perimeter of the largest\n    connected component in the binary image, normalized by the total number of\n    pixels.\n\n    Parameters\n    ----------\n    binary_img : ndarray of uint8\n        Input binary image array.\n\n    Returns\n    -------\n    out : float\n        The binary quality index value.\n    \"\"\"\n    if np.any(binary_img):\n        # SD = ShapeDescriptors(binary_img, [\"euler_number\"])\n        # index = - SD.descriptors['euler_number']\n        size, largest_cc = get_largest_connected_component(binary_img)\n        index = np.square(perimeter(largest_cc)) / binary_img.sum()\n        # index = (largest_cc.sum() * perimeter(largest_cc)) / binary_img.sum()\n    else:\n        index = 0.\n    return index\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.bracket_to_uint8_image_contrast","title":"<code>bracket_to_uint8_image_contrast(image)</code>","text":"<p>Convert an image with bracket contrast values to uint8 type.</p> <p>This function normalizes an input image by scaling the minimum and maximum values of the image to the range [0, 255] and then converts it to uint8 data type.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>Input image as a numpy array with floating-point values.</p> required <p>Returns:</p> Type Description <code>ndarray of uint8</code> <p>Output image converted to uint8 type after normalization.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; image = np.random.randint(0, 255, (10, 10), dtype=np.uint8)\n&gt;&gt;&gt; res = bracket_to_uint8_image_contrast(image)\n&gt;&gt;&gt; print(res)\n</code></pre> <pre><code>&gt;&gt;&gt; image = np.zeros((10, 10), dtype=np.uint8)\n&gt;&gt;&gt; res = bracket_to_uint8_image_contrast(image)\n&gt;&gt;&gt; print(res)\n</code></pre> Source code in <code>src/cellects/utils/formulas.py</code> <pre><code>@njit()\ndef bracket_to_uint8_image_contrast(image: NDArray):\n    \"\"\"\n    Convert an image with bracket contrast values to uint8 type.\n\n    This function normalizes an input image by scaling the minimum and maximum\n    values of the image to the range [0, 255] and then converts it to uint8\n    data type.\n\n    Parameters\n    ----------\n    image : ndarray\n        Input image as a numpy array with floating-point values.\n\n    Returns\n    -------\n    ndarray of uint8\n        Output image converted to uint8 type after normalization.\n\n    Examples\n    --------\n    &gt;&gt;&gt; image = np.random.randint(0, 255, (10, 10), dtype=np.uint8)\n    &gt;&gt;&gt; res = bracket_to_uint8_image_contrast(image)\n    &gt;&gt;&gt; print(res)\n\n    &gt;&gt;&gt; image = np.zeros((10, 10), dtype=np.uint8)\n    &gt;&gt;&gt; res = bracket_to_uint8_image_contrast(image)\n    &gt;&gt;&gt; print(res)\n    \"\"\"\n    image -= image.min()\n    if image.max() == 0:\n        return np.zeros_like(image, dtype=np.uint8)\n    else:\n        return to_uint8(255 * (image / np.max(image)))\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.close_holes","title":"<code>close_holes(binary_img)</code>","text":"<p>Close holes in a binary image using connected components analysis.</p> <p>This function identifies and closes small holes within the foreground objects of a binary image. It uses connected component analysis to find and fill holes that are smaller than the main object.</p> <p>Parameters:</p> Name Type Description Default <code>binary_img</code> <code>ndarray of uint8</code> <p>Binary input image where holes need to be closed.</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>ndarray of uint8</code> <p>Binary image with closed holes.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; binary_img = np.zeros((10, 10), dtype=np.uint8)\n&gt;&gt;&gt; binary_img[2:8, 2:8] = 1\n&gt;&gt;&gt; binary_img[4:6, 4:6] = 0  # Creating a hole\n&gt;&gt;&gt; result = close_holes(binary_img)\n&gt;&gt;&gt; print(result)\n[[0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0]\n [0 0 1 1 1 1 1 1 0 0]\n [0 0 1 1 1 1 1 1 0 0]\n [0 0 1 1 1 1 1 1 0 0]\n [0 0 1 1 1 1 1 1 0 0]\n [0 0 1 1 1 1 1 1 0 0]\n [0 0 1 1 1 1 1 1 0 0]\n [0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0]]\n</code></pre> Source code in <code>src/cellects/image_analysis/morphological_operations.py</code> <pre><code>def close_holes(binary_img: NDArray[np.uint8]) -&gt; NDArray[np.uint8]:\n    \"\"\"\n    Close holes in a binary image using connected components analysis.\n\n    This function identifies and closes small holes within the foreground objects of a binary image. It uses connected component analysis to find and fill holes that are smaller than the main object.\n\n    Parameters\n    ----------\n    binary_img : ndarray of uint8\n        Binary input image where holes need to be closed.\n\n    Returns\n    -------\n    out : ndarray of uint8\n        Binary image with closed holes.\n\n    Examples\n    --------\n    &gt;&gt;&gt; binary_img = np.zeros((10, 10), dtype=np.uint8)\n    &gt;&gt;&gt; binary_img[2:8, 2:8] = 1\n    &gt;&gt;&gt; binary_img[4:6, 4:6] = 0  # Creating a hole\n    &gt;&gt;&gt; result = close_holes(binary_img)\n    &gt;&gt;&gt; print(result)\n    [[0 0 0 0 0 0 0 0 0 0]\n     [0 0 0 0 0 0 0 0 0 0]\n     [0 0 1 1 1 1 1 1 0 0]\n     [0 0 1 1 1 1 1 1 0 0]\n     [0 0 1 1 1 1 1 1 0 0]\n     [0 0 1 1 1 1 1 1 0 0]\n     [0 0 1 1 1 1 1 1 0 0]\n     [0 0 1 1 1 1 1 1 0 0]\n     [0 0 0 0 0 0 0 0 0 0]\n     [0 0 0 0 0 0 0 0 0 0]]\n    \"\"\"\n    #### Third version ####\n    nb, new_order = cv2.connectedComponents(1 - binary_img)\n    if nb &gt; 2:\n        binary_img[new_order &gt; 1] = 1\n    return binary_img\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.combine_color_spaces","title":"<code>combine_color_spaces(c_space_dict, all_c_spaces, subtract_background=None)</code>","text":"<p>Combine color spaces from a dictionary and generate an analyzable image.</p> <p>This function processes multiple color spaces defined in <code>c_space_dict</code>, combines them according to given coefficients, and produces a normalized image that can be converted to uint8. Optionally subtracts background from the resultant image.</p> <p>Parameters:</p> Name Type Description Default <code>c_space_dict</code> <code>dict</code> <p>Dictionary containing color spaces and their respective coefficients.</p> required <code>all_c_spaces</code> <code>Dict</code> <p>Dictionary of all available color spaces in the image.</p> required <code>subtract_background</code> <code>NDArray</code> <p>Background image to subtract from the resultant image. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>out</code> <code>NDArray</code> <p>Processed and normalized image in float64 format, ready for uint8 conversion.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; c_space_dict = Dict()\n&gt;&gt;&gt; c_space_dict['hsv'] = np.array((0, 1, 1))\n&gt;&gt;&gt; all_c_spaces = Dict()\n&gt;&gt;&gt; all_c_spaces['bgr'] = np.random.rand(5, 5, 3)\n&gt;&gt;&gt; all_c_spaces['hsv'] = np.random.rand(5, 5, 3)\n&gt;&gt;&gt; background = np.zeros((5, 5))\n&gt;&gt;&gt; result = combine_color_spaces(c_space_dict, all_c_spaces)\n&gt;&gt;&gt; print(result.shape)\n(5, 5)\n</code></pre> Source code in <code>src/cellects/image_analysis/image_segmentation.py</code> <pre><code>@njit()\ndef combine_color_spaces(c_space_dict: Dict, all_c_spaces: Dict, subtract_background: NDArray=None) -&gt; NDArray:\n    \"\"\"\n    Combine color spaces from a dictionary and generate an analyzable image.\n\n    This function processes multiple color spaces defined in `c_space_dict`, combines\n    them according to given coefficients, and produces a normalized image that can be\n    converted to uint8. Optionally subtracts background from the resultant image.\n\n    Parameters\n    ----------\n    c_space_dict : dict\n        Dictionary containing color spaces and their respective coefficients.\n    all_c_spaces : Dict\n        Dictionary of all available color spaces in the image.\n    subtract_background : NDArray, optional\n        Background image to subtract from the resultant image. Defaults to None.\n\n    Returns\n    -------\n    out : NDArray\n        Processed and normalized image in float64 format, ready for uint8 conversion.\n\n    Examples\n    --------\n    &gt;&gt;&gt; c_space_dict = Dict()\n    &gt;&gt;&gt; c_space_dict['hsv'] = np.array((0, 1, 1))\n    &gt;&gt;&gt; all_c_spaces = Dict()\n    &gt;&gt;&gt; all_c_spaces['bgr'] = np.random.rand(5, 5, 3)\n    &gt;&gt;&gt; all_c_spaces['hsv'] = np.random.rand(5, 5, 3)\n    &gt;&gt;&gt; background = np.zeros((5, 5))\n    &gt;&gt;&gt; result = combine_color_spaces(c_space_dict, all_c_spaces)\n    &gt;&gt;&gt; print(result.shape)\n    (5, 5)\n    \"\"\"\n    image = np.zeros((all_c_spaces['bgr'].shape[0], all_c_spaces['bgr'].shape[1]), dtype=np.float64)\n    for space, channels in c_space_dict.items():\n        image += c_space_dict[space][0] * all_c_spaces[space][:, :, 0] + c_space_dict[space][1] * \\\n                 all_c_spaces[space][:, :, 1] + c_space_dict[space][2] * all_c_spaces[space][:, :, 2]\n    if subtract_background is not None:\n        # add (resp. subtract) the most negative (resp. smallest) value to the whole matrix to get a min = 0\n        image -= np.min(image)\n        # Make analysable this image by bracketing its values between 0 and 255 and converting it to uint8\n        max_im = np.max(image)\n        if max_im != 0:\n            image = 255 * (image / np.max(image))\n        if image.sum() &gt; subtract_background.sum():\n            image -= subtract_background\n        else:\n            image = subtract_background - image\n    # add (resp. subtract) the most negative (resp. smallest) value to the whole matrix to get a min = 0\n    image -= np.min(image)\n    # Make analysable this image by bracketing its values between 0 and 255\n    max_im = np.max(image)\n    if max_im != 0:\n        image = 255 * (image / max_im)\n    return image\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.create_empty_videos","title":"<code>create_empty_videos(image_list, cr, lose_accuracy_to_save_memory, already_greyscale, csc_dict)</code>","text":"<p>Create empty video arrays based on input parameters.</p> <p>Parameters:</p> Name Type Description Default <code>image_list</code> <code>list</code> <p>List of images.</p> required <code>cr</code> <code>list</code> <p>Crop region defined by [x_start, y_start, x_end, y_end].</p> required <code>lose_accuracy_to_save_memory</code> <code>bool</code> <p>Boolean flag to determine if memory should be saved by using uint8 data type.</p> required <code>already_greyscale</code> <code>bool</code> <p>Boolean flag indicating if the images are already in greyscale format.</p> required <code>csc_dict</code> <code>dict</code> <p>Dictionary containing color space conversion settings, including 'logical' key.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing three elements:     - <code>visu</code>: NumPy array with shape (len(image_list), cr[1] - cr[0] + 1, cr[3] - cr[2] + 1, 3) and dtype uint8 for RGB images.     - <code>converted_video</code>: NumPy array with shape (len(image_list), cr[1] - cr[0] + 1, cr[3] - cr[2] + 1) and dtype uint8 or float according to <code>lose_accuracy_to_save_memory</code>.     - <code>converted_video2</code>: NumPy array with shape same as <code>converted_video</code> and dtype uint8 or float according to <code>lose_accuracy_to_save_memory</code>.</p> Notes <p>Performance considerations:     - If <code>lose_accuracy_to_save_memory</code> is True, the function uses np.uint8 for memory efficiency.     - If <code>already_greyscale</code> is False, additional arrays are created to store RGB data.</p> Source code in <code>src/cellects/utils/load_display_save.py</code> <pre><code>def create_empty_videos(image_list: list, cr: list, lose_accuracy_to_save_memory: bool,\n                        already_greyscale: bool, csc_dict: dict):\n    \"\"\"\n\n    Create empty video arrays based on input parameters.\n\n    Parameters\n    ----------\n    image_list : list\n        List of images.\n    cr : list\n        Crop region defined by [x_start, y_start, x_end, y_end].\n    lose_accuracy_to_save_memory : bool\n        Boolean flag to determine if memory should be saved by using uint8 data type.\n    already_greyscale : bool\n        Boolean flag indicating if the images are already in greyscale format.\n    csc_dict : dict\n        Dictionary containing color space conversion settings, including 'logical' key.\n\n    Returns\n    -------\n    tuple\n        A tuple containing three elements:\n            - `visu`: NumPy array with shape (len(image_list), cr[1] - cr[0] + 1, cr[3] - cr[2] + 1, 3) and dtype uint8 for RGB images.\n            - `converted_video`: NumPy array with shape (len(image_list), cr[1] - cr[0] + 1, cr[3] - cr[2] + 1) and dtype uint8 or float according to `lose_accuracy_to_save_memory`.\n            - `converted_video2`: NumPy array with shape same as `converted_video` and dtype uint8 or float according to `lose_accuracy_to_save_memory`.\n\n    Notes\n    -----\n    Performance considerations:\n        - If `lose_accuracy_to_save_memory` is True, the function uses np.uint8 for memory efficiency.\n        - If `already_greyscale` is False, additional arrays are created to store RGB data.\n    \"\"\"\n    visu, converted_video, converted_video2 = None, None, None\n    dims = len(image_list), cr[1] - cr[0], cr[3] - cr[2]\n    if lose_accuracy_to_save_memory:\n        converted_video = np.zeros(dims, dtype=np.uint8)\n    else:\n        converted_video = np.zeros(dims, dtype=float)\n    if not already_greyscale:\n        visu = np.zeros((dims[0], dims[1], dims[2], 3), dtype=np.uint8)\n        if csc_dict['logical'] != 'None':\n            if lose_accuracy_to_save_memory:\n                converted_video2 = np.zeros(dims, dtype=np.uint8)\n            else:\n                converted_video2 = np.zeros(dims, dtype=float)\n    return visu, converted_video, converted_video2\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.detect_network_dynamics","title":"<code>detect_network_dynamics(converted_video, binary, arena_label=1, starting_time=0, visu=None, origin=None, smooth_segmentation_over_time=True, detect_pseudopods=True, save_coord_network=True, show_seg=False)</code>","text":"<p>Detects and tracks dynamic features (e.g., pseudopods) in a biological network over time from video data.</p> <p>Analyzes spatiotemporal dynamics of a network structure using binary masks and grayscale video data. Processes each frame to detect network components, optionally identifies pseudopods, applies temporal smoothing, and generates visualization overlays. Saves coordinate data for detected networks if enabled.</p> <p>Parameters:</p> Name Type Description Default <code>converted_video</code> <code>NDArray</code> <p>Input video data array with shape (time x y x z) representing grayscale intensities.</p> required <code>binary</code> <code>NDArray[uint8]</code> <p>Binary mask array with shape (time x y x z) indicating filled regions in each frame.</p> required <code>arena_label</code> <code>int</code> <p>Unique identifier for the current processing arena/session to name saved output files.</p> <code>1</code> <code>starting_time</code> <code>int</code> <p>Zero-based index of the first frame to begin network detection and analysis from.</p> <code>0</code> <code>visu</code> <code>NDArray</code> <p>Visualization video array (time x y x z) with RGB channels for overlay rendering.</p> <code>None</code> <code>origin</code> <code>NDArray[uint8]</code> <p>Binary mask defining a central region of interest to exclude from network detection.</p> <code>None</code> <code>smooth_segmentation_over_time</code> <code>(bool, optional(default=True))</code> <p>Flag indicating whether to apply temporal smoothing using adjacent frame data.</p> <code>True</code> <code>detect_pseudopods</code> <code>(bool, optional(default=True))</code> <p>Determines if pseudopod regions should be detected and merged with the network.</p> <code>True</code> <code>save_coord_network</code> <code>(bool, optional(default=True))</code> <p>Controls saving of detected network/pseudopod coordinates as NumPy arrays.</p> <code>True</code> <code>show_seg</code> <code>(bool, optional(default=False))</code> <p>Enables real-time visualization display during processing.</p> <code>False</code> <p>Returns:</p> Type Description <code>NDArray[uint8]</code> <p>3D array containing detected network structures with shape (time x y x z). Uses: - <code>0</code> for background, - <code>1</code> for regular network components, - <code>2</code> for pseudopod regions when detect_pseudopods is True.</p> Notes <ul> <li>Memory-intensive operations on large arrays may require system resources.</li> <li>Temporal smoothing effectiveness depends on network dynamics consistency between frames.</li> <li>Pseudopod detection requires sufficient contrast with the background in grayscale images.</li> </ul> Source code in <code>src/cellects/image_analysis/network_functions.py</code> <pre><code>def detect_network_dynamics(converted_video: NDArray, binary: NDArray[np.uint8], arena_label: int=1,\n                            starting_time: int=0, visu: NDArray=None, origin: NDArray[np.uint8]=None,\n                            smooth_segmentation_over_time: bool = True, detect_pseudopods: bool = True,\n                            save_coord_network: bool = True, show_seg: bool = False):\n    \"\"\"\n    Detects and tracks dynamic features (e.g., pseudopods) in a biological network over time from video data.\n\n    Analyzes spatiotemporal dynamics of a network structure using binary masks and grayscale video data. Processes each frame to detect network components, optionally identifies pseudopods, applies temporal smoothing, and generates visualization overlays. Saves coordinate data for detected networks if enabled.\n\n    Parameters\n    ----------\n    converted_video : NDArray\n        Input video data array with shape (time x y x z) representing grayscale intensities.\n    binary : NDArray[np.uint8]\n        Binary mask array with shape (time x y x z) indicating filled regions in each frame.\n    arena_label : int\n        Unique identifier for the current processing arena/session to name saved output files.\n    starting_time : int\n        Zero-based index of the first frame to begin network detection and analysis from.\n    visu : NDArray\n        Visualization video array (time x y x z) with RGB channels for overlay rendering.\n    origin : NDArray[np.uint8]\n        Binary mask defining a central region of interest to exclude from network detection.\n    smooth_segmentation_over_time : bool, optional (default=True)\n        Flag indicating whether to apply temporal smoothing using adjacent frame data.\n    detect_pseudopods : bool, optional (default=True)\n        Determines if pseudopod regions should be detected and merged with the network.\n    save_coord_network : bool, optional (default=True)\n        Controls saving of detected network/pseudopod coordinates as NumPy arrays.\n    show_seg : bool, optional (default=False)\n        Enables real-time visualization display during processing.\n\n    Returns\n    -------\n    NDArray[np.uint8]\n        3D array containing detected network structures with shape (time x y x z). Uses:\n        - `0` for background,\n        - `1` for regular network components,\n        - `2` for pseudopod regions when detect_pseudopods is True.\n\n    Notes\n    -----\n    - Memory-intensive operations on large arrays may require system resources.\n    - Temporal smoothing effectiveness depends on network dynamics consistency between frames.\n    - Pseudopod detection requires sufficient contrast with the background in grayscale images.\n    \"\"\"\n    logging.info(f\"Arena n\u00b0{arena_label}. Starting network detection.\")\n    # converted_video = self.converted_video; binary=self.binary; arena_label=1; starting_time=0; visu=self.visu; origin=None; smooth_segmentation_over_time=True; detect_pseudopods=True;save_coord_network=True; show_seg=False\n    dims = binary.shape\n    pseudopod_min_size = 50\n    if detect_pseudopods:\n        pseudopod_vid = np.zeros_like(binary, dtype=bool)\n    potential_network = np.zeros_like(binary, dtype=bool)\n    network_dynamics = np.zeros_like(binary, dtype=np.uint8)\n    do_convert = True\n    if visu is None:\n        do_convert = False\n        visu = np.stack((converted_video, converted_video, converted_video), axis=3)\n        greyscale = converted_video[-1, ...]\n    else:\n        greyscale = visu[-1, ...].mean(axis=-1)\n    NetDet = NetworkDetection(greyscale, possibly_filled_pixels=binary[-1, ...],\n                              origin_to_add=origin)\n    NetDet.get_best_network_detection_method()\n    if do_convert:\n        NetDet.greyscale_image = converted_video[-1, ...]\n    lighter_background = NetDet.greyscale_image[binary[-1, ...] &gt; 0].mean() &lt; NetDet.greyscale_image[\n        binary[-1, ...] == 0].mean()\n\n    for t in np.arange(starting_time, dims[0]):  # 20):#\n        if do_convert:\n            greyscale = visu[t, ...].mean(axis=-1)\n        else:\n            greyscale = converted_video[t, ...]\n        NetDet_fast = NetworkDetection(greyscale, possibly_filled_pixels=binary[t, ...],\n                                       origin_to_add=origin, best_result=NetDet.best_result)\n        NetDet_fast.detect_network()\n        NetDet_fast.greyscale_image = converted_video[t, ...]\n        if detect_pseudopods:\n            NetDet_fast.detect_pseudopods(lighter_background, pseudopod_min_size=pseudopod_min_size)\n            NetDet_fast.merge_network_with_pseudopods()\n            pseudopod_vid[t, ...] = NetDet_fast.pseudopods\n        potential_network[t, ...] = NetDet_fast.complete_network\n    if dims[0] == 1:\n        network_dynamics = potential_network\n    else:\n        for t in np.arange(starting_time, dims[0]):  # 20):#\n            if smooth_segmentation_over_time:\n                if 2 &lt;= t &lt;= (dims[0] - 2):\n                    computed_network = potential_network[(t - 2):(t + 3), :, :].sum(axis=0)\n                    computed_network[computed_network == 1] = 0\n                    computed_network[computed_network &gt; 1] = 1\n                else:\n                    if t &lt; 2:\n                        computed_network = potential_network[:2, :, :].sum(axis=0)\n                    else:\n                        computed_network = potential_network[-2:, :, :].sum(axis=0)\n                    computed_network[computed_network &gt; 0] = 1\n            else:\n                computed_network = computed_network[t, :, :].copy()\n\n            if origin is not None:\n                computed_network = computed_network * (1 - origin)\n                origin_contours = get_contours(origin)\n                complete_network = np.logical_or(origin_contours, computed_network).astype(np.uint8)\n            else:\n                complete_network = computed_network\n            complete_network = keep_one_connected_component(complete_network)\n\n            if detect_pseudopods:\n                # Make sure that removing pseudopods do not cut the network:\n                without_pseudopods = complete_network * (1 - pseudopod_vid[t])\n                only_connected_network = keep_one_connected_component(without_pseudopods)\n                # # Option A: To add these cutting regions to the pseudopods do:\n                pseudopods = (1 - only_connected_network) * complete_network\n                pseudopod_vid[t] = pseudopods\n            network_dynamics[t] = complete_network\n\n            imtoshow = visu[t, ...]\n            eroded_binary = cv2.erode(network_dynamics[t, ...], cross_33)\n            net_coord = np.nonzero(network_dynamics[t, ...] - eroded_binary)\n            imtoshow[net_coord[0], net_coord[1], :] = (34, 34, 158)\n            if show_seg:\n                cv2.imshow(\"\", cv2.resize(imtoshow, (1000, 1000)))\n                cv2.waitKey(1)\n            else:\n                visu[t, ...] = imtoshow\n            if show_seg:\n                cv2.destroyAllWindows()\n\n    network_coord = smallest_memory_array(np.nonzero(network_dynamics), \"uint\")\n    pseudopod_coord = None\n    if detect_pseudopods:\n        network_dynamics[pseudopod_vid &gt; 0] = 2\n        pseudopod_coord = smallest_memory_array(np.nonzero(pseudopod_vid), \"uint\")\n        if save_coord_network:\n            np.save(f\"coord_pseudopods{arena_label}_t{dims[0]}_y{dims[1]}_x{dims[2]}.npy\", pseudopod_coord)\n    if save_coord_network:\n        np.save(f\"coord_network{arena_label}_t{dims[0]}_y{dims[1]}_x{dims[2]}.npy\", network_coord)\n    return network_coord, pseudopod_coord\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.display_boxes","title":"<code>display_boxes(binary_image, box_diameter, show=True)</code>","text":"<p>Display grid lines on a binary image at specified box diameter intervals.</p> <p>This function displays the given binary image with vertical and horizontal grid lines drawn at regular intervals defined by <code>box_diameter</code>. The function returns the total number of grid lines drawn.</p> <p>Parameters:</p> Name Type Description Default <code>binary_image</code> <code>ndarray</code> <p>Binary image on which to draw the grid lines.</p> required <code>box_diameter</code> <code>int</code> <p>Diameter of each box in pixels.</p> required <p>Returns:</p> Name Type Description <code>line_nb</code> <code>int</code> <p>Number of grid lines drawn, both vertical and horizontal.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; binary_image = np.random.randint(0, 2, (100, 100), dtype=np.uint8)\n&gt;&gt;&gt; display_boxes(binary_image, box_diameter=25)\n</code></pre> Source code in <code>src/cellects/utils/load_display_save.py</code> <pre><code>def display_boxes(binary_image: NDArray, box_diameter: int, show: bool = True):\n    \"\"\"\n    Display grid lines on a binary image at specified box diameter intervals.\n\n    This function displays the given binary image with vertical and horizontal\n    grid lines drawn at regular intervals defined by `box_diameter`. The function\n    returns the total number of grid lines drawn.\n\n    Parameters\n    ----------\n    binary_image : ndarray\n        Binary image on which to draw the grid lines.\n    box_diameter : int\n        Diameter of each box in pixels.\n\n    Returns\n    -------\n    line_nb : int\n        Number of grid lines drawn, both vertical and horizontal.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; binary_image = np.random.randint(0, 2, (100, 100), dtype=np.uint8)\n    &gt;&gt;&gt; display_boxes(binary_image, box_diameter=25)\n    \"\"\"\n    plt.imshow(binary_image, cmap='gray', extent=(0, binary_image.shape[1], 0, binary_image.shape[0]))\n    height, width = binary_image.shape\n    line_nb = 0\n    for x in range(0, width + 1, box_diameter):\n        line_nb += 1\n        plt.axvline(x=x, color='white', linewidth=1)\n    for y in range(0, height + 1, box_diameter):\n        line_nb += 1\n        plt.axhline(y=y, color='white', linewidth=1)\n\n    if show:\n        plt.show()\n\n    return line_nb\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.display_network_methods","title":"<code>display_network_methods(network_detection, save_path=None)</code>","text":"<p>Display segmentation results from a network detection object.</p> Extended Description <p>Plots the binary segmentation results for various methods stored in <code>network_detection.all_results</code>. Highlights the best result based on quality metrics and allows for saving the figure to a file.</p> <p>Parameters:</p> Name Type Description Default <code>network_detection</code> <code>object</code> <p>An object containing segmentation results and quality metrics.</p> required <code>save_path</code> <code>str</code> <p>Path to save the figure. If <code>None</code>, the plot is displayed.</p> <code>None</code> Source code in <code>src/cellects/utils/load_display_save.py</code> <pre><code>def display_network_methods(network_detection: object, save_path: str=None):\n    \"\"\"\n\n    Display segmentation results from a network detection object.\n\n    Extended Description\n    --------------------\n\n    Plots the binary segmentation results for various methods stored in ``network_detection.all_results``.\n    Highlights the best result based on quality metrics and allows for saving the figure to a file.\n\n    Parameters\n    ----------\n    network_detection : object\n        An object containing segmentation results and quality metrics.\n    save_path : str, optional\n        Path to save the figure. If ``None``, the plot is displayed.\n\n    \"\"\"\n    row_nb = 6\n    fig, axes = plt.subplots(int(np.ceil(len(network_detection.all_results) / row_nb)), row_nb, figsize=(100, 100))\n    fig.suptitle(f'Segmentation Comparison: Frangi + Sato Variations', fontsize=16)\n\n    # Plot all results\n    for idx, result in enumerate(network_detection.all_results):\n        row = idx // row_nb\n        col = idx % row_nb\n\n        ax = axes[row, col]\n\n        # Display binary segmentation result\n        ax.imshow(result['binary'], cmap='gray')\n\n        # Create title with filter info and quality score\n        title = f\"{result['method']}: {str(np.round(network_detection.quality_metrics[idx], 0))}\"\n\n        # Highlight the best result\n        if idx == network_detection.best_idx:\n            ax.set_title(title, fontsize=8, color='red', fontweight='bold')\n            ax.add_patch(plt.Rectangle((0, 0), result['binary'].shape[1] - 1,\n                                       result['binary'].shape[0] - 1,\n                                       fill=False, edgecolor='red', linewidth=3))\n        else:\n            ax.set_title(title, fontsize=8)\n\n        ax.axis('off')\n    plt.tight_layout()\n\n    if save_path is not None:\n        plt.savefig(save_path, bbox_inches='tight', pad_inches=0., transparent=True, dpi=500)\n        plt.close()\n    else:\n        plt.show()\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.eudist","title":"<code>eudist(v1, v2)</code>","text":"<p>Calculate the Euclidean distance between two points in n-dimensional space.</p> <p>Parameters:</p> Name Type Description Default <code>v1</code> <code>iterable of float</code> <p>The coordinates of the first point.</p> required <code>v2</code> <code>iterable of float</code> <p>The coordinates of the second point.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The Euclidean distance between <code>v1</code> and <code>v2</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>v1</code> and <code>v2</code> do not have the same length.</p> Notes <p>The Euclidean distance is calculated using the standard formula: \u221a((x2 \u2212 x1)^2 + (y2 \u2212 y1)^2 + ...).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; v1 = [1.0, 2.0]\n&gt;&gt;&gt; v2 = [4.0, 6.0]\n&gt;&gt;&gt; eudist(v1, v2)\n5.0\n</code></pre> <pre><code>&gt;&gt;&gt; v1 = [1.0, 2.0, 3.0]\n&gt;&gt;&gt; v2 = [4.0, 6.0, 8.0]\n&gt;&gt;&gt; eudist(v1, v2)\n7.0710678118654755\n</code></pre> Source code in <code>src/cellects/utils/formulas.py</code> <pre><code>def eudist(v1, v2) -&gt; float:\n    \"\"\"\n    Calculate the Euclidean distance between two points in n-dimensional space.\n\n    Parameters\n    ----------\n    v1 : iterable of float\n        The coordinates of the first point.\n    v2 : iterable of float\n        The coordinates of the second point.\n\n    Returns\n    -------\n    float\n        The Euclidean distance between `v1` and `v2`.\n\n    Raises\n    ------\n    ValueError\n        If `v1` and `v2` do not have the same length.\n\n    Notes\n    -----\n    The Euclidean distance is calculated using the standard formula:\n    \u221a((x2 \u2212 x1)^2 + (y2 \u2212 y1)^2 + ...).\n\n    Examples\n    --------\n    &gt;&gt;&gt; v1 = [1.0, 2.0]\n    &gt;&gt;&gt; v2 = [4.0, 6.0]\n    &gt;&gt;&gt; eudist(v1, v2)\n    5.0\n\n    &gt;&gt;&gt; v1 = [1.0, 2.0, 3.0]\n    &gt;&gt;&gt; v2 = [4.0, 6.0, 8.0]\n    &gt;&gt;&gt; eudist(v1, v2)\n    7.0710678118654755\n    \"\"\"\n    dist = [(a - b)**2 for a, b in zip(v1, v2)]\n    dist = np.sqrt(np.sum(dist))\n    return dist\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.extract_graph_dynamics","title":"<code>extract_graph_dynamics(converted_video, coord_network, arena_label, starting_time=0, origin=None, coord_pseudopods=None)</code>","text":"<p>Extracts dynamic graph data from video frames based on network dynamics.</p> <p>This function processes time-series binary network structures to extract evolving vertices and edges over time. It computes spatial relationships between networks and an origin point through image processing steps including contour detection, padding for alignment, skeleton extraction, and morphological analysis. Vertex and edge attributes like position, connectivity, width, intensity, and betweenness are compiled into tables saved as CSV files.</p> <p>Parameters:</p> Name Type Description Default <code>converted_video</code> <code>NDArray</code> <p>3D video data array (t x y x) containing pixel intensities used for calculating edge intensity attributes during table generation.</p> required <code>coord_network</code> <code>NDArray[uint8]</code> <p>3D binary network mask array (t x y x) representing connectivity structures across time points.</p> required <code>arena_label</code> <code>int</code> <p>Unique identifier to prefix output filenames corresponding to specific experimental arenas.</p> required <code>starting_time</code> <code>(int, optional(default=0))</code> <p>Time index within <code>coord_network</code> to begin processing from (exclusive of origin initialization).</p> <code>0</code> <code>origin</code> <code>(NDArray[uint8], optional(default=None))</code> <p>Binary mask identifying the region of interest's central origin for spatial reference during network comparison.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <code>Saves two CSV files in working directory:</code> <code>1. `vertex_table{arena_label}_t{T}_y{Y}_x{X}.csv` - Vertex table with time, coordinates, and connectivity information</code> <code>2. `edge_table{arena_label}_t{T}_y{Y}_x{X}.csv` - Edge table containing attributes like length, width, intensity, and betweenness</code> Notes <p>Output CSVs use NumPy arrays converted to pandas DataFrames with columns: - Vertex table includes timestamps (t), coordinates (y,x), and connectivity flags. - Edge table contains betweenness centrality calculated during skeleton processing. Origin contours are spatially aligned through padding operations to maintain coordinate consistency across time points.</p> Source code in <code>src/cellects/image_analysis/network_functions.py</code> <pre><code>def extract_graph_dynamics(converted_video: NDArray, coord_network: NDArray, arena_label: int,\n                            starting_time: int=0, origin: NDArray[np.uint8]=None, coord_pseudopods: NDArray=None):\n    \"\"\"\n    Extracts dynamic graph data from video frames based on network dynamics.\n\n    This function processes time-series binary network structures to extract evolving vertices and edges over time. It computes spatial relationships between networks and an origin point through image processing steps including contour detection, padding for alignment, skeleton extraction, and morphological analysis. Vertex and edge attributes like position, connectivity, width, intensity, and betweenness are compiled into tables saved as CSV files.\n\n    Parameters\n    ----------\n    converted_video : NDArray\n        3D video data array (t x y x) containing pixel intensities used for calculating edge intensity attributes during table generation.\n    coord_network : NDArray[np.uint8]\n        3D binary network mask array (t x y x) representing connectivity structures across time points.\n    arena_label : int\n        Unique identifier to prefix output filenames corresponding to specific experimental arenas.\n    starting_time : int, optional (default=0)\n        Time index within `coord_network` to begin processing from (exclusive of origin initialization).\n    origin : NDArray[np.uint8], optional (default=None)\n        Binary mask identifying the region of interest's central origin for spatial reference during network comparison.\n\n    Returns\n    -------\n    None\n    Saves two CSV files in working directory:\n    1. `vertex_table{arena_label}_t{T}_y{Y}_x{X}.csv` - Vertex table with time, coordinates, and connectivity information\n    2. `edge_table{arena_label}_t{T}_y{Y}_x{X}.csv` - Edge table containing attributes like length, width, intensity, and betweenness\n\n    Notes\n    ---\n    Output CSVs use NumPy arrays converted to pandas DataFrames with columns:\n    - Vertex table includes timestamps (t), coordinates (y,x), and connectivity flags.\n    - Edge table contains betweenness centrality calculated during skeleton processing.\n    Origin contours are spatially aligned through padding operations to maintain coordinate consistency across time points.\n    \"\"\"\n    logging.info(f\"Arena n\u00b0{arena_label}. Starting graph extraction.\")\n    # converted_video = self.converted_video; coord_network=self.coord_network; arena_label=1; starting_time=0; origin=self.origin\n    dims = converted_video.shape[:3]\n    if origin is not None:\n        _, _, _, origin_centroid = cv2.connectedComponentsWithStats(origin)\n        origin_centroid = np.round((origin_centroid[1, 1], origin_centroid[1, 0])).astype(np.int64)\n        pad_origin_centroid = origin_centroid + 1\n        origin_contours = get_contours(origin)\n        pad_origin = add_padding([origin])[0]\n    else:\n        pad_origin_centroid = None\n        pad_origin = None\n        origin_contours = None\n    vertex_table = None\n    for t in np.arange(starting_time, dims[0]): # t=320   Y, X = 729, 554\n        computed_network = np.zeros((dims[1], dims[2]), dtype=np.uint8)\n        net_t = coord_network[1:, coord_network[0, :] == t]\n        computed_network[net_t[0], net_t[1]] = 1\n        if origin is not None:\n            computed_network = computed_network * (1 - origin)\n            computed_network = np.logical_or(origin_contours, computed_network).astype(np.uint8)\n        else:\n            computed_network = computed_network.astype(np.uint8)\n        if computed_network.any():\n            computed_network = keep_one_connected_component(computed_network)\n            pad_network = add_padding([computed_network])[0]\n            pad_skeleton, pad_distances, pad_origin_contours = get_skeleton_and_widths(pad_network, pad_origin,\n                                                                                           pad_origin_centroid)\n            edge_id = EdgeIdentification(pad_skeleton, pad_distances, t)\n            edge_id.run_edge_identification()\n            if pad_origin_contours is not None:\n                origin_contours = remove_padding([pad_origin_contours])[0]\n            growing_areas = None\n            if coord_pseudopods is not None:\n                growing_areas = coord_pseudopods[1:, coord_pseudopods[0, :] == t]\n            edge_id.make_vertex_table(origin_contours, growing_areas)\n            edge_id.make_edge_table(converted_video[t, ...])\n            edge_id.vertex_table = np.hstack((np.repeat(t, edge_id.vertex_table.shape[0])[:, None], edge_id.vertex_table))\n            edge_id.edge_table = np.hstack((np.repeat(t, edge_id.edge_table.shape[0])[:, None], edge_id.edge_table))\n            if vertex_table is None:\n                vertex_table = edge_id.vertex_table.copy()\n                edge_table = edge_id.edge_table.copy()\n            else:\n                vertex_table = np.vstack((vertex_table, edge_id.vertex_table))\n                edge_table = np.vstack((edge_table, edge_id.edge_table))\n\n    vertex_table = pd.DataFrame(vertex_table, columns=[\"t\", \"y\", \"x\", \"vertex_id\", \"is_tip\", \"origin\",\n                                                       \"vertex_connected\"])\n    edge_table = pd.DataFrame(edge_table,\n                              columns=[\"t\", \"edge_id\", \"vertex1\", \"vertex2\", \"length\", \"average_width\", \"intensity\",\n                                       \"betweenness_centrality\"])\n    vertex_table.to_csv(\n        f\"vertex_table{arena_label}_t{dims[0]}_y{dims[1]}_x{dims[2]}.csv\")\n    edge_table.to_csv(\n        f\"edge_table{arena_label}_t{dims[0]}_y{dims[1]}_x{dims[2]}.csv\")\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.extract_time","title":"<code>extract_time(image_list, pathway='', raw_images=False)</code>","text":"<p>Extract timestamps from a list of images.</p> <p>This function extracts the DateTimeOriginal or datetime values from the EXIF data of a list of image files, and computes the total time in seconds.</p> <p>Parameters:</p> Name Type Description Default <code>image_list</code> <code>list of str</code> <p>List of image file names.</p> required <code>pathway</code> <code>str</code> <p>Path to the directory containing the images. Default is an empty string.</p> <code>''</code> <code>raw_images</code> <code>bool</code> <p>If True, use the exifread library. Otherwise, use the exif library. Default is False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>time</code> <code>ndarray of int64</code> <p>Array containing the total time in seconds for each image.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pathway = Path(__name__).resolve().parents[0] / \"data\" / \"single_experiment\"\n&gt;&gt;&gt; image_list = ['image1.tif', 'image2.tif']\n&gt;&gt;&gt; time = extract_time(image_list, pathway)\n&gt;&gt;&gt; print(time)\narray([0, 0])\n</code></pre> Source code in <code>src/cellects/utils/load_display_save.py</code> <pre><code>def extract_time(image_list: list, pathway=\"\", raw_images:bool=False):\n    \"\"\"\n    Extract timestamps from a list of images.\n\n    This function extracts the DateTimeOriginal or datetime values from\n    the EXIF data of a list of image files, and computes the total time in seconds.\n\n    Parameters\n    ----------\n    image_list : list of str\n        List of image file names.\n    pathway : str, optional\n        Path to the directory containing the images. Default is an empty string.\n    raw_images : bool, optional\n        If True, use the exifread library. Otherwise, use the exif library.\n        Default is False.\n\n    Returns\n    -------\n    time : ndarray of int64\n        Array containing the total time in seconds for each image.\n\n    Examples\n    --------\n    &gt;&gt;&gt; pathway = Path(__name__).resolve().parents[0] / \"data\" / \"single_experiment\"\n    &gt;&gt;&gt; image_list = ['image1.tif', 'image2.tif']\n    &gt;&gt;&gt; time = extract_time(image_list, pathway)\n    &gt;&gt;&gt; print(time)\n    array([0, 0])\n\n    \"\"\"\n    if isinstance(pathway, str):\n        pathway = Path(pathway)\n    nb = len(image_list)\n    timings = np.zeros((nb, 6), dtype=np.int64)\n    if raw_images:\n        for i in np.arange(nb):\n            with open(pathway / image_list, 'rb') as image_file:\n                my_image = exifread.process_file(image_file, details=False, stop_tag='DateTimeOriginal')\n                datetime = my_image[\"EXIF DateTimeOriginal\"]\n            datetime = datetime.values[:10] + ':' + datetime.values[11:]\n            timings[i, :] = datetime.split(':')\n    else:\n        for i in np.arange(nb):\n            with open(pathway / image_list[i], 'rb') as image_file:\n                my_image = Image(image_file)\n                if my_image.has_exif:\n                    datetime = my_image.datetime\n                    datetime = datetime[:10] + ':' + datetime[11:]\n                    timings[i, :] = datetime.split(':')\n\n    if np.all(timings[:, 0] == timings[0, 0]):\n        if np.all(timings[:, 1] == timings[0, 1]):\n            if np.all(timings[:, 2] == timings[0, 2]):\n                time = timings[:, 3] * 3600 + timings[:, 4] * 60 + timings[:, 5]\n            else:\n                time = timings[:, 2] * 86400 + timings[:, 3] * 3600 + timings[:, 4] * 60 + timings[:, 5]\n        else:\n            days_per_month = (31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31)\n            for j in np.arange(nb):\n                month_number = timings[j, 1]#int(timings[j, 1])\n                timings[j, 1] = days_per_month[month_number] * month_number\n            time = (timings[:, 1] + timings[:, 2]) * 86400 + timings[:, 3] * 3600 + timings[:, 4] * 60 + timings[:, 5]\n        #time = int(time)\n    else:\n        time = np.repeat(0, nb)#arange(1, nb * 60, 60)#\"Do not experiment the 31th of december!!!\"\n    if time.sum() == 0:\n        time = np.repeat(0, nb)#arange(1, nb * 60, 60)\n    return time\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.find_common_coord","title":"<code>find_common_coord(array1, array2)</code>","text":"<p>Find common coordinates between two arrays.</p> <p>This function compares the given 2D <code>array1</code> and <code>array2</code> to determine if there are any common coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>array1</code> <code>ndarray of int</code> <p>A 2D numpy ndarray.</p> required <code>array2</code> <code>ndarray of int</code> <p>Another 2D numpy ndarray.</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>ndarray of bool</code> <p>A boolean numpy ndarray where True indicates common coordinates.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; array1 = np.array([[1, 2], [3, 4]])\n&gt;&gt;&gt; array2 = np.array([[5, 6], [1, 2]])\n&gt;&gt;&gt; result = find_common_coord(array1, array2)\n&gt;&gt;&gt; print(result)\narray([ True, False])\n</code></pre> Source code in <code>src/cellects/utils/formulas.py</code> <pre><code>def find_common_coord(array1: NDArray[int], array2: NDArray[int]) -&gt; NDArray[bool]:\n    \"\"\"Find common coordinates between two arrays.\n\n    This function compares the given 2D `array1` and `array2`\n    to determine if there are any common coordinates.\n\n    Parameters\n    ----------\n    array1 : ndarray of int\n        A 2D numpy ndarray.\n    array2 : ndarray of int\n        Another 2D numpy ndarray.\n\n    Returns\n    -------\n    out : ndarray of bool\n        A boolean numpy ndarray where True indicates common\n        coordinates.\n\n    Examples\n    --------\n    &gt;&gt;&gt; array1 = np.array([[1, 2], [3, 4]])\n    &gt;&gt;&gt; array2 = np.array([[5, 6], [1, 2]])\n    &gt;&gt;&gt; result = find_common_coord(array1, array2)\n    &gt;&gt;&gt; print(result)\n    array([ True, False])\"\"\"\n    return (array1[:, None, :] == array2[None, :, :]).all(-1).any(-1)\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.find_duplicates_coord","title":"<code>find_duplicates_coord(array1)</code>","text":"<p>Find duplicate rows in a 2D array and return their coordinate indices.</p> <p>Given a 2D NumPy array, this function identifies rows that are duplicated (i.e., appear more than once) and returns a boolean array indicating their positions.</p> <p>Parameters:</p> Name Type Description Default <code>array1</code> <code>ndarray of int</code> <p>Input 2D array of shape (n_rows, n_columns) from which to find duplicate rows.</p> required <p>Returns:</p> Name Type Description <code>duplicates</code> <code>ndarray of bool</code> <p>Boolean array of shape (n_rows,), where <code>True</code> indicates that the corresponding row in <code>array1</code> is a duplicate.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; array1 = np.array([[1, 2], [3, 4], [1, 2], [5, 6]])\n&gt;&gt;&gt; find_duplicates_coord(array1)\narray([ True, False,  True, False])\n</code></pre> Source code in <code>src/cellects/utils/formulas.py</code> <pre><code>def find_duplicates_coord(array1: NDArray[int]) -&gt; NDArray[bool]:\n    \"\"\"\n    Find duplicate rows in a 2D array and return their coordinate indices.\n\n    Given a 2D NumPy array, this function identifies rows that are duplicated (i.e., appear more than once) and returns a boolean array indicating their positions.\n\n    Parameters\n    ----------\n    array1 : ndarray of int\n        Input 2D array of shape (n_rows, n_columns) from which to find duplicate rows.\n\n    Returns\n    -------\n    duplicates : ndarray of bool\n        Boolean array of shape (n_rows,), where `True` indicates that the corresponding row in `array1` is a duplicate.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; array1 = np.array([[1, 2], [3, 4], [1, 2], [5, 6]])\n    &gt;&gt;&gt; find_duplicates_coord(array1)\n    array([ True, False,  True, False])\"\"\"\n    unique_rows, inverse_indices = np.unique(array1, axis=0, return_inverse=True)\n    counts = np.bincount(inverse_indices)\n    # A row is duplicate if its count &gt; 1\n    return counts[inverse_indices] &gt; 1\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.find_threshold_given_mask","title":"<code>find_threshold_given_mask(greyscale, mask, min_threshold=0)</code>","text":"<p>Find the optimal threshold value for a greyscale image given a mask.</p> <p>This function performs a binary search to find the optimal threshold that maximizes the separation between two regions defined by the mask. The search is bounded by a minimum threshold value.</p> <p>Parameters:</p> Name Type Description Default <code>greyscale</code> <code>ndarray of uint8</code> <p>The greyscale image array.</p> required <code>mask</code> <code>ndarray of uint8</code> <p>The binary mask array where positive values define region A and zero values define region B.</p> required <code>min_threshold</code> <code>uint8</code> <p>The minimum threshold value for the search. Defaults to 0.</p> <code>0</code> <p>Returns:</p> Name Type Description <code>out</code> <code>uint8</code> <p>The optimal threshold value found.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; greyscale = np.array([[255, 128, 54], [0, 64, 20]], dtype=np.uint8)\n&gt;&gt;&gt; mask = np.array([[1, 1, 0], [0, 0, 0]], dtype=np.uint8)\n&gt;&gt;&gt; find_threshold_given_mask(greyscale, mask)\n54\n</code></pre> Source code in <code>src/cellects/image_analysis/image_segmentation.py</code> <pre><code>def find_threshold_given_mask(greyscale: NDArray[np.uint8], mask: np.uint8, min_threshold: np.uint8=0) -&gt; np.uint8:\n    \"\"\"\n    Find the optimal threshold value for a greyscale image given a mask.\n\n    This function performs a binary search to find the optimal threshold\n    that maximizes the separation between two regions defined by the mask.\n    The search is bounded by a minimum threshold value.\n\n    Parameters\n    ----------\n    greyscale : ndarray of uint8\n        The greyscale image array.\n    mask : ndarray of uint8\n        The binary mask array where positive values define region A and zero values define region B.\n    min_threshold : uint8, optional\n        The minimum threshold value for the search. Defaults to 0.\n\n    Returns\n    -------\n    out : uint8\n        The optimal threshold value found.\n\n    Examples\n    --------\n    &gt;&gt;&gt; greyscale = np.array([[255, 128, 54], [0, 64, 20]], dtype=np.uint8)\n    &gt;&gt;&gt; mask = np.array([[1, 1, 0], [0, 0, 0]], dtype=np.uint8)\n    &gt;&gt;&gt; find_threshold_given_mask(greyscale, mask)\n    54\n    \"\"\"\n    region_a = greyscale[mask &gt; 0]\n    if len(region_a) == 0:\n        return np.uint8(255)\n    region_b = greyscale[mask == 0]\n    if len(region_b) == 0:\n        return min_threshold\n    else:\n        low = min_threshold\n        high = 255\n        best_thresh = low\n\n        while 0 &lt;= low &lt;= high:\n            mid = (low + high) // 2\n            count_a, count_b = _get_counts_jit(mid, region_a, region_b)\n\n            if count_a &gt; count_b:\n                # Try to find a lower threshold that still satisfies the condition\n                best_thresh = mid\n                high = mid - 1\n            else:\n                if count_a == 0 and count_b == 0:\n                    best_thresh = greyscale.mean()\n                    break\n                # Need higher threshold\n                low = mid + 1\n    return best_thresh\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.generate_color_space_combination","title":"<code>generate_color_space_combination(bgr_image, c_spaces, first_dict, second_dict={}, background=None, background2=None, convert_to_uint8=False, all_c_spaces={})</code>","text":"<p>Generate color space combinations for an input image.</p> <p>This function generates a grayscale image by combining multiple color spaces from an input BGR image and provided dictionaries. Optionally, it can also generate a second grayscale image using another dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>bgr_image</code> <code>ndarray of uint8</code> <p>The input image in BGR color space.</p> required <code>c_spaces</code> <code>list</code> <p>List of color spaces to consider for combination.</p> required <code>first_dict</code> <code>Dict</code> <p>Dictionary containing color space and transformation details for the first grayscale image.</p> required <code>second_dict</code> <code>Dict</code> <p>Dictionary containing color space and transformation details for the second grayscale image.</p> <code>{}</code> <code>background</code> <code>ndarray</code> <p>Background image to be used. Default is None.</p> <code>None</code> <code>background2</code> <code>ndarray</code> <p>Second background image to be used for the second grayscale image. Default is None.</p> <code>None</code> <code>convert_to_uint8</code> <code>bool</code> <p>Flag indicating whether to convert the output images to uint8. Default is False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>out</code> <code>tuple of ndarray of uint8</code> <p>A tuple containing the first and second grayscale images.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; bgr_image = np.random.randint(0, 256, (100, 100, 3), dtype=np.uint8)\n&gt;&gt;&gt; c_spaces = ['bgr', 'hsv']\n&gt;&gt;&gt; first_dict = Dict()\n&gt;&gt;&gt; first_dict['bgr'] = np.array((0, 1, 1))\n&gt;&gt;&gt; second_dict = Dict()\n&gt;&gt;&gt; second_dict['hsv'] = np.array((0, 0, 1))\n&gt;&gt;&gt; greyscale_image1, greyscale_image2 = generate_color_space_combination(bgr_image, c_spaces, first_dict, second_dict)\n&gt;&gt;&gt; print(greyscale_image1.shape)\n(100, 100)\n</code></pre> Source code in <code>src/cellects/image_analysis/image_segmentation.py</code> <pre><code>def generate_color_space_combination(bgr_image: NDArray[np.uint8], c_spaces: list, first_dict: Dict, second_dict: Dict={}, background: NDArray=None, background2: NDArray=None, convert_to_uint8: bool=False, all_c_spaces: dict={}) -&gt; NDArray[np.uint8]:\n    \"\"\"\n    Generate color space combinations for an input image.\n\n    This function generates a grayscale image by combining multiple color spaces\n    from an input BGR image and provided dictionaries. Optionally, it can also generate\n    a second grayscale image using another dictionary.\n\n    Parameters\n    ----------\n    bgr_image : ndarray of uint8\n        The input image in BGR color space.\n    c_spaces : list\n        List of color spaces to consider for combination.\n    first_dict : Dict\n        Dictionary containing color space and transformation details for the first grayscale image.\n    second_dict : Dict, optional\n        Dictionary containing color space and transformation details for the second grayscale image.\n    background : ndarray, optional\n        Background image to be used. Default is None.\n    background2 : ndarray, optional\n        Second background image to be used for the second grayscale image. Default is None.\n    convert_to_uint8 : bool, optional\n        Flag indicating whether to convert the output images to uint8. Default is False.\n\n    Returns\n    -------\n    out : tuple of ndarray of uint8\n        A tuple containing the first and second grayscale images.\n\n    Examples\n    --------\n    &gt;&gt;&gt; bgr_image = np.random.randint(0, 256, (100, 100, 3), dtype=np.uint8)\n    &gt;&gt;&gt; c_spaces = ['bgr', 'hsv']\n    &gt;&gt;&gt; first_dict = Dict()\n    &gt;&gt;&gt; first_dict['bgr'] = np.array((0, 1, 1))\n    &gt;&gt;&gt; second_dict = Dict()\n    &gt;&gt;&gt; second_dict['hsv'] = np.array((0, 0, 1))\n    &gt;&gt;&gt; greyscale_image1, greyscale_image2 = generate_color_space_combination(bgr_image, c_spaces, first_dict, second_dict)\n    &gt;&gt;&gt; print(greyscale_image1.shape)\n    (100, 100)\n    \"\"\"\n    greyscale_image2 = None\n    first_pc_vector = None\n    if \"PCA\" in c_spaces:\n        greyscale_image, var_ratio, first_pc_vector = extract_first_pc(bgr_image)\n    else:\n        if len(all_c_spaces) == 0:\n            all_c_spaces = get_color_spaces(bgr_image, c_spaces)\n        try:\n            greyscale_image = combine_color_spaces(first_dict, all_c_spaces, background)\n        except:\n            first_dict = translate_dict(first_dict)\n            greyscale_image = combine_color_spaces(first_dict, all_c_spaces, background)\n        if len(second_dict) &gt; 0:\n            greyscale_image2 = combine_color_spaces(second_dict, all_c_spaces, background2)\n\n    if convert_to_uint8:\n        greyscale_image = bracket_to_uint8_image_contrast(greyscale_image)\n        if greyscale_image2 is not None and len(second_dict) &gt; 0:\n            greyscale_image2 = bracket_to_uint8_image_contrast(greyscale_image2)\n    return greyscale_image, greyscale_image2, all_c_spaces, first_pc_vector\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.get_all_line_coordinates","title":"<code>get_all_line_coordinates(start_point, end_points)</code>","text":"<p>Get all line coordinates between start point and end points.</p> <p>This function computes the coordinates of lines connecting a start point to multiple end points, converting input arrays to float if necessary before processing.</p> <p>Parameters:</p> Name Type Description Default <code>start_point</code> <code>NDArray[float]</code> <p>Starting coordinate point for the lines. Can be of any numeric type, will be converted to float if needed.</p> required <code>end_points</code> <code>NDArray[float]</code> <p>Array of end coordinate points for the lines. Can be of any numeric type, will be converted to float if needed.</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>List[NDArray[int]]</code> <p>A list of numpy arrays containing the coordinates of each line as integer values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; start_point = np.array([0, 0])\n&gt;&gt;&gt; end_points = np.array([[1, 2], [3, 4]])\n&gt;&gt;&gt; get_all_line_coordinates(start_point, end_points)\n[array([[0, 0],\n   [0, 1],\n   [1, 2]], dtype=uint64), array([[0, 0],\n   [1, 1],\n   [1, 2],\n   [2, 3],\n   [3, 4]], dtype=uint64)]\n</code></pre> Source code in <code>src/cellects/image_analysis/morphological_operations.py</code> <pre><code>def get_all_line_coordinates(start_point: NDArray[int], end_points: NDArray[int]) -&gt; NDArray[int]:\n    \"\"\"\n    Get all line coordinates between start point and end points.\n\n    This function computes the coordinates of lines connecting a\n    start point to multiple end points, converting input arrays to float\n    if necessary before processing.\n\n    Parameters\n    ----------\n    start_point : NDArray[float]\n        Starting coordinate point for the lines. Can be of any numeric type,\n        will be converted to float if needed.\n    end_points : NDArray[float]\n        Array of end coordinate points for the lines. Can be of any\n        numeric type, will be converted to float if needed.\n\n    Returns\n    -------\n    out : List[NDArray[int]]\n        A list of numpy arrays containing the coordinates of each line\n        as integer values.\n\n    Examples\n    --------\n    &gt;&gt;&gt; start_point = np.array([0, 0])\n    &gt;&gt;&gt; end_points = np.array([[1, 2], [3, 4]])\n    &gt;&gt;&gt; get_all_line_coordinates(start_point, end_points)\n    [array([[0, 0],\n       [0, 1],\n       [1, 2]], dtype=uint64), array([[0, 0],\n       [1, 1],\n       [1, 2],\n       [2, 3],\n       [3, 4]], dtype=uint64)]\n    \"\"\"\n    lines = []\n    for end_point in end_points:\n        line_coords = get_line_points(start_point, end_point)\n        lines.append(np.array(line_coords, dtype=np.uint64))\n    return lines\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.get_branches_and_tips_coord","title":"<code>get_branches_and_tips_coord(pad_vertices, pad_tips)</code>","text":"<p>Extracts the coordinates of branches and tips from vertices and tips binary images.</p> <p>This function calculates branch coordinates by subtracting tips from vertices. Then it finds and outputs the non-zero indices of branches and tips separatly.</p> <p>Parameters:</p> Name Type Description Default <code>pad_vertices</code> <code>ndarray</code> <p>Array containing the vertices to be padded.</p> required <code>pad_tips</code> <code>ndarray</code> <p>Array containing the tips of the padding.</p> required <p>Returns:</p> Name Type Description <code>branch_v_coord</code> <code>ndarray</code> <p>Coordinates of branches derived from subtracting tips from vertices.</p> <code>tips_coord</code> <code>ndarray</code> <p>Coordinates of the tips.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; branch_v, tip_c = get_branches_and_tips_coord(pad_vertices, pad_tips)\n&gt;&gt;&gt; branch_v\n&gt;&gt;&gt; tip_c\n</code></pre> Source code in <code>src/cellects/image_analysis/network_functions.py</code> <pre><code>def get_branches_and_tips_coord(pad_vertices: NDArray[np.uint8], pad_tips: NDArray[np.uint8]) -&gt; Tuple[NDArray, NDArray]:\n    \"\"\"\n    Extracts the coordinates of branches and tips from vertices and tips binary images.\n\n    This function calculates branch coordinates by subtracting\n    tips from vertices. Then it finds and outputs the non-zero indices of branches and tips separatly.\n\n    Parameters\n    ----------\n    pad_vertices : ndarray\n        Array containing the vertices to be padded.\n    pad_tips : ndarray\n        Array containing the tips of the padding.\n\n    Returns\n    -------\n    branch_v_coord : ndarray\n        Coordinates of branches derived from subtracting tips from vertices.\n    tips_coord : ndarray\n        Coordinates of the tips.\n\n    Examples\n    --------\n    &gt;&gt;&gt; branch_v, tip_c = get_branches_and_tips_coord(pad_vertices, pad_tips)\n    &gt;&gt;&gt; branch_v\n    &gt;&gt;&gt; tip_c\n    \"\"\"\n    pad_branches = pad_vertices - pad_tips\n    branch_v_coord = np.transpose(np.array(np.nonzero(pad_branches)))\n    tips_coord = np.transpose(np.array(np.nonzero(pad_tips)))\n    return branch_v_coord, tips_coord\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.get_color_spaces","title":"<code>get_color_spaces(bgr_image, space_names='')</code>","text":"<p>Convert a BGR image into various color spaces.</p> <p>Converts the input BGR image to specified color spaces and returns them as a dictionary. If no space names are provided, converts to all default color spaces (LAB, HSV, LUV, HLS, YUV). If 'logical' is in the space names, it will be removed before conversion.</p> <p>Parameters:</p> Name Type Description Default <code>bgr_image</code> <code>ndarray of uint8</code> <p>Input image in BGR color space.</p> required <code>space_names</code> <code>list of str</code> <p>List of color spaces to convert the image to. Defaults to none.</p> <code>''</code> <p>Returns:</p> Name Type Description <code>out</code> <code>dict</code> <p>Dictionary with keys as color space names and values as the converted images.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; bgr_image = np.zeros((5, 5, 3), dtype=np.uint8)\n&gt;&gt;&gt; c_spaces = get_color_spaces(bgr_image, ['lab', 'hsv'])\n&gt;&gt;&gt; print(list(c_spaces.keys()))\n['bgr', 'lab', 'hsv']\n</code></pre> Source code in <code>src/cellects/image_analysis/image_segmentation.py</code> <pre><code>def get_color_spaces(bgr_image: NDArray[np.uint8], space_names: list=\"\") -&gt; Dict:\n    \"\"\"\n    Convert a BGR image into various color spaces.\n\n    Converts the input BGR image to specified color spaces and returns them\n    as a dictionary. If no space names are provided, converts to all default\n    color spaces (LAB, HSV, LUV, HLS, YUV). If 'logical' is in the space names,\n    it will be removed before conversion.\n\n    Parameters\n    ----------\n    bgr_image : ndarray of uint8\n        Input image in BGR color space.\n    space_names : list of str, optional\n        List of color spaces to convert the image to. Defaults to none.\n\n    Returns\n    -------\n    out : dict\n        Dictionary with keys as color space names and values as the converted images.\n\n    Examples\n    --------\n    &gt;&gt;&gt; bgr_image = np.zeros((5, 5, 3), dtype=np.uint8)\n    &gt;&gt;&gt; c_spaces = get_color_spaces(bgr_image, ['lab', 'hsv'])\n    &gt;&gt;&gt; print(list(c_spaces.keys()))\n    ['bgr', 'lab', 'hsv']\n    \"\"\"\n    if 'logical' in space_names:\n        space_names.pop(np.nonzero(np.array(space_names, dtype=str) == 'logical')[0][0])\n    c_spaces = Dict()\n    c_spaces['bgr'] = bgr_image.astype(np.float64)\n    if len(space_names) == 0:\n        c_spaces['lab'] = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2LAB).astype(np.float64)\n        c_spaces['hsv'] = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2HSV).astype(np.float64)\n        c_spaces['luv'] = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2LUV).astype(np.float64)\n        c_spaces['hls'] = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2HLS).astype(np.float64)\n        c_spaces['yuv'] = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2YUV).astype(np.float64)\n    else:\n        if np.isin('lab', space_names):\n            c_spaces['lab'] = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2LAB).astype(np.float64)\n        if np.isin('hsv', space_names):\n            c_spaces['hsv'] = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2HSV).astype(np.float64)\n        if np.isin('luv', space_names):\n            c_spaces['luv'] = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2LUV).astype(np.float64)\n        if np.isin('hls', space_names):\n            c_spaces['hls'] = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2HLS).astype(np.float64)\n        if np.isin('yuv', space_names):\n            c_spaces['yuv'] = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2YUV).astype(np.float64)\n    return c_spaces\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.get_contour_width_from_im_shape","title":"<code>get_contour_width_from_im_shape(im_shape)</code>","text":"<p>Calculate the contour width based on image shape.</p> <p>Parameters:</p> Name Type Description Default <code>im_shape</code> <code>tuple of int, two items</code> <p>The dimensions of the image.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The calculated contour width.</p> Source code in <code>src/cellects/utils/formulas.py</code> <pre><code>def get_contour_width_from_im_shape(im_shape: Tuple) -&gt; int:\n    \"\"\"\n    Calculate the contour width based on image shape.\n\n    Parameters\n    ----------\n    im_shape : tuple of int, two items\n        The dimensions of the image.\n\n    Returns\n    -------\n    int\n        The calculated contour width.\n    \"\"\"\n    return np.max((np.round(np.log10(np.max(im_shape)) - 2).astype(int), 2))\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.get_contours","title":"<code>get_contours(binary_image)</code>","text":"<p>Find and return the contours of a binary image.</p> <p>This function erodes the input binary image using a 3x3 cross-shaped structuring element and then subtracts the eroded image from the original to obtain the contours.</p> <p>Parameters:</p> Name Type Description Default <code>binary_image</code> <code>ndarray of uint8</code> <p>Input binary image from which to extract contours.</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>ndarray of uint8</code> <p>Image containing only the contours extracted from <code>binary_image</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; binary_image = np.zeros((10, 10), dtype=np.uint8)\n&gt;&gt;&gt; binary_image[2:8, 2:8] = 1\n&gt;&gt;&gt; result = get_contours(binary_image)\n&gt;&gt;&gt; print(result)\n[[0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0]\n [0 0 1 1 1 1 1 1 0 0]\n [0 0 1 0 0 0 0 1 0 0]\n [0 0 1 0 0 0 0 1 0 0]\n [0 0 1 0 0 0 0 1 0 0]\n [0 0 1 0 0 0 0 1 0 0]\n [0 0 1 1 1 1 1 1 0 0]\n [0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0]]\n</code></pre> Source code in <code>src/cellects/image_analysis/morphological_operations.py</code> <pre><code>def get_contours(binary_image: NDArray[np.uint8]) -&gt; NDArray[np.uint8]:\n    \"\"\"\n    Find and return the contours of a binary image.\n\n    This function erodes the input binary image using a 3x3 cross-shaped\n    structuring element and then subtracts the eroded image from the original to obtain the contours.\n\n    Parameters\n    ----------\n    binary_image : ndarray of uint8\n        Input binary image from which to extract contours.\n\n    Returns\n    -------\n    out : ndarray of uint8\n        Image containing only the contours extracted from `binary_image`.\n\n    Examples\n    --------\n    &gt;&gt;&gt; binary_image = np.zeros((10, 10), dtype=np.uint8)\n    &gt;&gt;&gt; binary_image[2:8, 2:8] = 1\n    &gt;&gt;&gt; result = get_contours(binary_image)\n    &gt;&gt;&gt; print(result)\n    [[0 0 0 0 0 0 0 0 0 0]\n     [0 0 0 0 0 0 0 0 0 0]\n     [0 0 1 1 1 1 1 1 0 0]\n     [0 0 1 0 0 0 0 1 0 0]\n     [0 0 1 0 0 0 0 1 0 0]\n     [0 0 1 0 0 0 0 1 0 0]\n     [0 0 1 0 0 0 0 1 0 0]\n     [0 0 1 1 1 1 1 1 0 0]\n     [0 0 0 0 0 0 0 0 0 0]\n     [0 0 0 0 0 0 0 0 0 0]]\n    \"\"\"\n    if not isinstance(binary_image.dtype, np.uint8):\n        binary_image = binary_image.astype(np.uint8)\n    if np.all(binary_image):\n        contours = 1 - image_borders(binary_image.shape)\n    elif np.any(binary_image):\n        eroded_binary = cv2.erode(binary_image, cross_33, borderType=cv2.BORDER_CONSTANT, borderValue=0)\n        contours = binary_image - eroded_binary\n    else:\n        contours = binary_image\n    return contours\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.get_h5_keys","title":"<code>get_h5_keys(file_name)</code>","text":"<p>Retrieve all keys from a given HDF5 file.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>The path to the HDF5 file from which keys are to be retrieved.</p> required <p>Returns:</p> Type Description <code>list of str</code> <p>A list containing all the keys present in the specified HDF5 file.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the specified HDF5 file does not exist.</p> Source code in <code>src/cellects/utils/load_display_save.py</code> <pre><code>def get_h5_keys(file_name):\n    \"\"\"\n    Retrieve all keys from a given HDF5 file.\n\n    Parameters\n    ----------\n    file_name : str\n        The path to the HDF5 file from which keys are to be retrieved.\n\n    Returns\n    -------\n    list of str\n        A list containing all the keys present in the specified HDF5 file.\n\n    Raises\n    ------\n    FileNotFoundError\n        If the specified HDF5 file does not exist.\n    \"\"\"\n    try:\n        with h5py.File(file_name, 'r') as h5f:\n            all_keys = list(h5f.keys())\n            return all_keys\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"The file '{file_name}' does not exist.\")\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.get_inertia_axes","title":"<code>get_inertia_axes(mo)</code>","text":"<p>Calculate the inertia axes of a moment object.</p> <p>This function computes the barycenters, central moments, and the lengths of the major and minor axes, as well as their orientation.</p> <p>Parameters:</p> Name Type Description Default <code>mo</code> <code>dict</code> <p>Dictionary containing moments, which should include keys: 'm00', 'm10', 'm01', 'm20', and 'm11'.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing:     - cx : float         The x-coordinate of the barycenter.     - cy : float         The y-coordinate of the barycenter.     - major_axis_len : float         The length of the major axis.     - minor_axis_len : float         The length of the minor axis.     - axes_orientation : float         The orientation of the axes in radians.</p> Notes <p>This function uses Numba's @njit decorator for performance. The moments in the input dictionary should be computed from the same image region.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; mo = {'m00': 1.0, 'm10': 2.0, 'm01': 3.0, 'm20': 4.0, 'm11': 5.0}\n&gt;&gt;&gt; get_inertia_axes(mo)\n(2.0, 3.0, 9.165151389911677, 0.8421875803239, 0.7853981633974483)\n</code></pre> Source code in <code>src/cellects/utils/formulas.py</code> <pre><code>@njit()\ndef get_inertia_axes(mo: dict) -&gt; Tuple[float, float, float, float, float]:\n    \"\"\"\n    Calculate the inertia axes of a moment object.\n\n    This function computes the barycenters, central moments,\n    and the lengths of the major and minor axes, as well as\n    their orientation.\n\n    Parameters\n    ----------\n    mo : dict\n        Dictionary containing moments, which should include keys:\n        'm00', 'm10', 'm01', 'm20', and 'm11'.\n\n    Returns\n    -------\n    tuple\n        A tuple containing:\n            - cx : float\n                The x-coordinate of the barycenter.\n            - cy : float\n                The y-coordinate of the barycenter.\n            - major_axis_len : float\n                The length of the major axis.\n            - minor_axis_len : float\n                The length of the minor axis.\n            - axes_orientation : float\n                The orientation of the axes in radians.\n\n    Notes\n    -----\n    This function uses Numba's @njit decorator for performance.\n    The moments in the input dictionary should be computed from\n    the same image region.\n\n    Examples\n    --------\n    &gt;&gt;&gt; mo = {'m00': 1.0, 'm10': 2.0, 'm01': 3.0, 'm20': 4.0, 'm11': 5.0}\n    &gt;&gt;&gt; get_inertia_axes(mo)\n    (2.0, 3.0, 9.165151389911677, 0.8421875803239, 0.7853981633974483)\n\n    \"\"\"\n    #L. Rocha, L. Velho and P.C.P. Calvalho (2002)\n    #http://sibgrapi.sid.inpe.br/col/sid.inpe.br/banon/2002/10.23.11.34/doc/35.pdf\n    # http://raphael.candelier.fr/?blog=Image%20Moments\n\n    # Calculate barycenters\n    cx = mo[\"m10\"] / mo[\"m00\"]\n    cy = mo[\"m01\"] / mo[\"m00\"]\n    # Calculate central moments\n    c20 = (mo[\"m20\"] / mo[\"m00\"]) - np.square(cx)\n    c02 = (mo[\"m02\"] / mo[\"m00\"]) - np.square(cy)\n    c11 = (mo[\"m11\"] / mo[\"m00\"]) - (cx * cy)\n    # Calculate major and minor axi lengths OK\n    major_axis_len = np.sqrt(6 * (c20 + c02 + np.sqrt(np.square(2 * c11) + np.square(c20 - c02))))\n    minor_axis_len = np.sqrt(6 * (c20 + c02 - np.sqrt(np.square(2 * c11) + np.square(c20 - c02))))\n    if (c20 - c02) != 0:\n        axes_orientation = (0.5 * np.arctan((2 * c11) / (c20 - c02))) + ((c20 &lt; c02) * (np.pi /2))\n    else:\n        axes_orientation = 0.\n    return cx, cy, major_axis_len, minor_axis_len, axes_orientation\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.get_inner_vertices","title":"<code>get_inner_vertices(pad_skeleton, potential_tips, cnv4, cnv8)</code>","text":"<p>Get inner vertices from skeleton image.</p> <p>This function identifies and returns the inner vertices of a skeletonized image. It processes potential tips to determine which pixels should be considered as vertices based on their neighbor count and connectivity.</p> <p>Parameters:</p> Name Type Description Default <code>pad_skeleton</code> <code>ndarray of uint8</code> <p>The padded skeleton image.</p> required <code>potential_tips</code> <code>ndarray of uint8</code> <p>Potential tip points in the skeleton. Defaults to pad_tips.</p> required <code>cnv4</code> <code>object</code> <p>Object for handling 4-connections.</p> required <code>cnv8</code> <code>object</code> <p>Object for handling 8-connections.</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>tuple of ndarray of uint8, ndarray of uint8</code> <p>A tuple containing the final vertices matrix and the updated potential tips.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pad_vertices, potential_tips = get_inner_vertices(pad_skeleton, potential_tips)\n&gt;&gt;&gt; print(pad_vertices)\n</code></pre> Source code in <code>src/cellects/image_analysis/network_functions.py</code> <pre><code>def get_inner_vertices(pad_skeleton: NDArray[np.uint8], potential_tips: NDArray[np.uint8], cnv4: object, cnv8: object) -&gt; Tuple[NDArray[np.uint8], NDArray[np.uint8]]: # potential_tips=pad_tips\n    \"\"\"\n    Get inner vertices from skeleton image.\n\n    This function identifies and returns the inner vertices of a skeletonized image.\n    It processes potential tips to determine which pixels should be considered as\n    vertices based on their neighbor count and connectivity.\n\n    Parameters\n    ----------\n    pad_skeleton : ndarray of uint8\n        The padded skeleton image.\n    potential_tips : ndarray of uint8, optional\n        Potential tip points in the skeleton. Defaults to pad_tips.\n    cnv4 : object\n        Object for handling 4-connections.\n    cnv8 : object\n        Object for handling 8-connections.\n\n    Returns\n    -------\n    out : tuple of ndarray of uint8, ndarray of uint8\n        A tuple containing the final vertices matrix and the updated potential tips.\n\n    Examples\n    --------\n    &gt;&gt;&gt; pad_vertices, potential_tips = get_inner_vertices(pad_skeleton, potential_tips)\n    &gt;&gt;&gt; print(pad_vertices)\n    \"\"\"\n\n    # Initiate the vertices final matrix as a copy of the potential_tips\n    pad_vertices = deepcopy(potential_tips)\n    for neighbor_nb in [8, 7, 6, 5, 4]:\n        # All pixels having neighbor_nb neighbor are potential vertices\n        potential_vertices = np.zeros(potential_tips.shape, dtype=np.uint8)\n\n        potential_vertices[cnv8.equal_neighbor_nb == neighbor_nb] = 1\n        # remove the false intersections that are a neighbor of a previously detected intersection\n        # Dilate vertices to make sure that no neighbors of the current potential vertices are already vertices.\n        dilated_previous_intersections = cv2.dilate(pad_vertices, cross_33, iterations=1)\n        potential_vertices *= (1 - dilated_previous_intersections)\n        pad_vertices[np.nonzero(potential_vertices)] = 1\n\n    # Having 3 neighbors is ambiguous\n    with_3_neighbors = cnv8.equal_neighbor_nb == 3\n    if np.any(with_3_neighbors):\n        # We compare 8-connections with 4-connections\n        # We loop over all 3 connected\n        coord_3 = np.nonzero(with_3_neighbors)\n        for y3, x3 in zip(coord_3[0], coord_3[1]): # y3, x3 = 3,7\n            # If, in the neighborhood of the 3, there is at least a 2 (in 8) that is 0 (in 4), and not a termination: the 3 is a node\n            has_2_8neigh = cnv8.equal_neighbor_nb[(y3 - 1):(y3 + 2), (x3 - 1):(x3 + 2)] &gt; 0  # 1\n            has_2_8neigh_without_focal = has_2_8neigh.copy()\n            has_2_8neigh_without_focal[1, 1] = 0\n            node_but_not_term = pad_vertices[(y3 - 1):(y3 + 2), (x3 - 1):(x3 + 2)] * (1 - potential_tips[(y3 - 1):(y3 + 2), (x3 - 1):(x3 + 2)])\n            all_are_node_but_not_term = np.array_equal(has_2_8neigh_without_focal, node_but_not_term)\n            if np.any(has_2_8neigh * (1 - all_are_node_but_not_term)):\n                # At least 3 of the 8neigh are not connected:\n                has_2_8neigh_without_focal = np.pad(has_2_8neigh_without_focal, [(1,), (1,)], mode='constant')\n                cnv_8con = CompareNeighborsWithValue(has_2_8neigh_without_focal, 4)\n                cnv_8con.is_equal(1, and_itself=True)\n                disconnected_nb = has_2_8neigh_without_focal.sum() - (cnv_8con.equal_neighbor_nb &gt; 0).sum()\n                if disconnected_nb &gt; 2:\n                    pad_vertices[y3, x3] = 1\n    # Now there may be too many vertices:\n    # - Those that are 4-connected:\n    nb, sh, st, ce = cv2.connectedComponentsWithStats(pad_vertices, connectivity=4)\n    problematic_vertices = np.nonzero(st[:, 4] &gt; 1)[0][1:]\n    for prob_v in problematic_vertices:\n        vertices_group = sh == prob_v\n        # If there is a tip in the group, do\n        if np.any(potential_tips[vertices_group]):\n            # Change the most connected one from tip to vertex\n            curr_neighbor_nb = cnv8.equal_neighbor_nb * vertices_group\n            wrong_tip = np.nonzero(curr_neighbor_nb == curr_neighbor_nb.max())\n            potential_tips[wrong_tip] = 0\n        else:\n            #  otherwise do:\n            # Find the most 8-connected one, if its 4-connected neighbors have no more 8-connexions than 4-connexions + 1, they can be removed\n            # Otherwise,\n            # Find the most 4-connected one, and remove its 4 connected neighbors having only 1 or other 8-connexion\n\n            c = zoom_on_nonzero(vertices_group)\n            # 1. Find the most 8-connected one:\n            sub_v_grp = vertices_group[c[0]:c[1], c[2]:c[3]]\n            c8 = cnv8.equal_neighbor_nb[c[0]:c[1], c[2]:c[3]]\n            vertices_group_8 = c8 * sub_v_grp\n            max_8_con = vertices_group_8.max()\n            most_8_con = np.nonzero(vertices_group_8 == max_8_con)\n            # c4[(most_8_con[0][0] - 1):(most_8_con[0][0] + 2), (most_8_con[1][0] - 1):(most_8_con[1][0] + 2)]\n            if len(most_8_con[0]) == 1:\n                skel_copy = pad_skeleton[c[0]:c[1], c[2]:c[3]].copy()\n                skel_copy[most_8_con] = 0\n                sub_cnv8 = CompareNeighborsWithValue(skel_copy, 8)\n                sub_cnv8.is_equal(1, and_itself=False)\n                sub_cnv4 = CompareNeighborsWithValue(skel_copy, 4)\n                sub_cnv4.is_equal(1, and_itself=False)\n                v_to_remove = sub_v_grp * (sub_cnv8.equal_neighbor_nb &lt;= sub_cnv4.equal_neighbor_nb + 1)\n            else:\n                c4 = cnv4.equal_neighbor_nb[c[0]:c[1], c[2]:c[3]]\n                # 1. # Find the most 4-connected one:\n                vertices_group_4 = c4 * sub_v_grp\n                max_con = vertices_group_4.max()\n                most_con = np.nonzero(vertices_group_4 == max_con)\n                if len(most_con[0]) &lt; sub_v_grp.sum():\n                    # 2. Check its 4-connected neighbors and remove those having only 1 other 8-connexion\n                    skel_copy = pad_skeleton[c[0]:c[1], c[2]:c[3]].copy()\n                    skel_copy[most_con] = 0\n                    skel_copy[most_con[0] - 1, most_con[1]] = 0\n                    skel_copy[most_con[0] + 1, most_con[1]] = 0\n                    skel_copy[most_con[0], most_con[1] - 1] = 0\n                    skel_copy[most_con[0], most_con[1] + 1] = 0\n                    sub_cnv8 = CompareNeighborsWithValue(skel_copy, 8)\n                    sub_cnv8.is_equal(1, and_itself=False)\n                    # There are:\n                    v_to_remove = ((vertices_group_4 &gt; 0) * sub_cnv8.equal_neighbor_nb) == 1\n                else:\n                    v_to_remove = np.zeros(sub_v_grp.shape, dtype=bool)\n            pad_vertices[c[0]:c[1], c[2]:c[3]][v_to_remove] = 0\n\n    # Other vertices to remove:\n    # - Those that are forming a cross with 0 at the center while the skeleton contains 1\n    cnv4_false = CompareNeighborsWithValue(pad_vertices, 4)\n    cnv4_false.is_equal(1, and_itself=False)\n    cross_vertices = cnv4_false.equal_neighbor_nb == 4\n    wrong_cross_vertices = cross_vertices * pad_skeleton\n    if wrong_cross_vertices.any():\n        pad_vertices[np.nonzero(wrong_cross_vertices)] = 1\n        cross_fix = cv2.dilate(wrong_cross_vertices, kernel=cross_33, iterations=1)\n        # Remove the 4-connected vertices that have no more than 4 8-connected neighbors\n        # i.e. the three on the side of the surrounded 0 and only one on edge on the other side\n        cross_fix = ((cnv8.equal_neighbor_nb * cross_fix) == 4) * (1 - wrong_cross_vertices)\n        pad_vertices *= (1 - cross_fix)\n    return pad_vertices, potential_tips\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.get_kurtosis","title":"<code>get_kurtosis(mo, binary_image, cx, cy, sx, sy)</code>","text":"<p>Calculate the kurtosis of a binary image.</p> <p>The function calculates the fourth moment (kurtosis) of the given binary image around the specified center coordinates with an option to specify the size of the square window.</p> <p>Parameters:</p> Name Type Description Default <code>mo</code> <code>dict</code> <p>Dictionary containing moments of binary image.</p> required <code>binary_image</code> <code>ndarray</code> <p>A 2D numpy ndarray representing a binary image.</p> required <code>cx</code> <code>int or float</code> <p>The x-coordinate of the center point of the square window.</p> required <code>cy</code> <code>int or float</code> <p>The y-coordinate of the center point of the square window.</p> required <code>sx</code> <code>int or float</code> <p>The x-length of the square window (width).</p> required <code>sy</code> <code>int or float</code> <p>The y-length of the square window (height).</p> required <p>Returns:</p> Type Description <code>float</code> <p>The kurtosis value calculated from the moments.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; mo = np.array([[0, 1], [2, 3]])\n&gt;&gt;&gt; binary_image = np.array([[1, 0], [0, 1]])\n&gt;&gt;&gt; cx = 2\n&gt;&gt;&gt; cy = 3\n&gt;&gt;&gt; sx = 5\n&gt;&gt;&gt; sy = 6\n&gt;&gt;&gt; result = get_kurtosis(mo, binary_image, cx, cy, sx, sy)\n&gt;&gt;&gt; print(result)\nexpected output\n</code></pre> Source code in <code>src/cellects/utils/formulas.py</code> <pre><code>def get_kurtosis(mo: dict, binary_image: NDArray, cx: float, cy: float, sx: float, sy: float) -&gt; Tuple[float, float]:\n    \"\"\"\n    Calculate the kurtosis of a binary image.\n\n    The function calculates the fourth moment (kurtosis) of the given\n    binary image around the specified center coordinates with an option\n    to specify the size of the square window.\n\n    Parameters\n    ----------\n    mo : dict\n        Dictionary containing moments of binary image.\n    binary_image : np.ndarray\n        A 2D numpy ndarray representing a binary image.\n    cx : int or float\n        The x-coordinate of the center point of the square window.\n    cy : int or float\n        The y-coordinate of the center point of the square window.\n    sx : int or float\n        The x-length of the square window (width).\n    sy : int or float\n        The y-length of the square window (height).\n\n    Returns\n    -------\n    float\n        The kurtosis value calculated from the moments.\n\n    Examples\n    --------\n    &gt;&gt;&gt; mo = np.array([[0, 1], [2, 3]])\n    &gt;&gt;&gt; binary_image = np.array([[1, 0], [0, 1]])\n    &gt;&gt;&gt; cx = 2\n    &gt;&gt;&gt; cy = 3\n    &gt;&gt;&gt; sx = 5\n    &gt;&gt;&gt; sy = 6\n    &gt;&gt;&gt; result = get_kurtosis(mo, binary_image, cx, cy, sx, sy)\n    &gt;&gt;&gt; print(result)\n    expected output\n    \"\"\"\n    x4, y4 = get_power_dists(binary_image, cx, cy, 4)\n    X4, Y4 = np.meshgrid(x4, y4)\n    m4x, m4y = get_var(mo, binary_image, X4, Y4)\n    return get_skewness_kurtosis(m4x, m4y, sx, sy, 4)\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.get_min_or_max_euclidean_pair","title":"<code>get_min_or_max_euclidean_pair(coords, min_or_max='max')</code>","text":"<p>Find the pair of points in a given set with the minimum or maximum Euclidean distance.</p> <p>Parameters:</p> Name Type Description Default <code>coords</code> <code>Union[ndarray, Tuple]</code> <p>An Nx2 numpy array or a tuple of two arrays, each containing the x and y coordinates of points.</p> required <code>min_or_max</code> <code>str</code> <p>Whether to find the 'min' or 'max' distance pair. Default is 'max'.</p> <code>'max'</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>A tuple containing the coordinates of the two points that form the minimum or maximum distance pair.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>min_or_max</code> is not 'min' or 'max'.</p> Notes <ul> <li>The function first computes all pairwise distances in condensed form using <code>pdist</code>.</li> <li>Then, it finds the index of the minimum or maximum distance.</li> <li>Finally, it maps this index to the actual point indices using a binary search method.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; coords = np.array([[0, 1], [2, 3], [4, 5]])\n&gt;&gt;&gt; point1, point2 = get_min_or_max_euclidean_pair(coords, min_or_max=\"max\")\n&gt;&gt;&gt; print(point1)\n[0 1]\n&gt;&gt;&gt; print(point2)\n[4 5]\n&gt;&gt;&gt; coords = (np.array([0, 2, 4, 8, 1, 5]), np.array([0, 2, 4, 8, 0, 5]))\n&gt;&gt;&gt; point1, point2 = get_min_or_max_euclidean_pair(coords, min_or_max=\"min\")\n&gt;&gt;&gt; print(point1)\n[0 0]\n&gt;&gt;&gt; print(point2)\n[1 0]\n</code></pre> Source code in <code>src/cellects/image_analysis/morphological_operations.py</code> <pre><code>def get_min_or_max_euclidean_pair(coords, min_or_max: str=\"max\") -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Find the pair of points in a given set with the minimum or maximum Euclidean distance.\n\n    Parameters\n    ----------\n    coords : Union[np.ndarray, Tuple]\n        An Nx2 numpy array or a tuple of two arrays, each containing the x and y coordinates of points.\n    min_or_max : str, optional\n        Whether to find the 'min' or 'max' distance pair. Default is 'max'.\n\n    Returns\n    -------\n    Tuple[np.ndarray, np.ndarray]\n        A tuple containing the coordinates of the two points that form the minimum or maximum distance pair.\n\n    Raises\n    ------\n    ValueError\n        If `min_or_max` is not 'min' or 'max'.\n\n    Notes\n    -----\n    - The function first computes all pairwise distances in condensed form using `pdist`.\n    - Then, it finds the index of the minimum or maximum distance.\n    - Finally, it maps this index to the actual point indices using a binary search method.\n\n    Examples\n    --------\n    &gt;&gt;&gt; coords = np.array([[0, 1], [2, 3], [4, 5]])\n    &gt;&gt;&gt; point1, point2 = get_min_or_max_euclidean_pair(coords, min_or_max=\"max\")\n    &gt;&gt;&gt; print(point1)\n    [0 1]\n    &gt;&gt;&gt; print(point2)\n    [4 5]\n    &gt;&gt;&gt; coords = (np.array([0, 2, 4, 8, 1, 5]), np.array([0, 2, 4, 8, 0, 5]))\n    &gt;&gt;&gt; point1, point2 = get_min_or_max_euclidean_pair(coords, min_or_max=\"min\")\n    &gt;&gt;&gt; print(point1)\n    [0 0]\n    &gt;&gt;&gt; print(point2)\n    [1 0]\n\n    \"\"\"\n    if isinstance(coords, Tuple):\n        coords = np.column_stack(coords)\n    N = coords.shape[0]\n    if N &lt;= 1:\n        return (coords[0], coords[0]) if N == 1 else None\n\n    # Step 1: Compute all pairwise distances in condensed form\n    distances = pdist(coords)\n\n    # Step 2: Find the index of the maximum distance\n    if min_or_max == \"max\":\n        idx = np.argmax(distances)\n    elif min_or_max == \"min\":\n        idx = np.argmin(distances)\n    else:\n        raise ValueError\n\n    # Step 3: Map this index to (i, j) using a binary search method\n\n    def get_pair_index(k):\n        low, high = 0, N\n        while low &lt; high:\n            mid = (low + high) // 2\n            total = mid * (2 * N - mid - 1) // 2\n            if total &lt;= k:\n                low = mid + 1\n            else:\n                high = mid\n\n        i = low - 1\n        prev_sum = i * (2 * N - i - 1) // 2\n        j_index_in_row = k - prev_sum\n        return i, i + j_index_in_row + 1  # Ensure j &gt; i\n\n    i, j = get_pair_index(idx)\n    return coords[i], coords[j]\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.get_mpl_colormap","title":"<code>get_mpl_colormap(cmap_name)</code>","text":"<p>Returns a linear color range array for the given matplotlib colormap.</p> <p>Parameters:</p> Name Type Description Default <code>cmap_name</code> <code>str</code> <p>The name of the colormap to get.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>A 256x1x3 array of bytes representing the linear color range.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; result = get_mpl_colormap('viridis')\n&gt;&gt;&gt; print(result.shape)\n(256, 1, 3)\n</code></pre> Source code in <code>src/cellects/utils/load_display_save.py</code> <pre><code>def get_mpl_colormap(cmap_name: str):\n    \"\"\"\n    Returns a linear color range array for the given matplotlib colormap.\n\n    Parameters\n    ----------\n    cmap_name : str\n        The name of the colormap to get.\n\n    Returns\n    -------\n    numpy.ndarray\n        A 256x1x3 array of bytes representing the linear color range.\n\n    Examples\n    --------\n    &gt;&gt;&gt; result = get_mpl_colormap('viridis')\n    &gt;&gt;&gt; print(result.shape)\n    (256, 1, 3)\n\n    \"\"\"\n    cmap = plt.get_cmap(cmap_name)\n\n    # Initialize the matplotlib color map\n    sm = plt.cm.ScalarMappable(cmap=cmap)\n\n    # Obtain linear color range\n    color_range = sm.to_rgba(np.linspace(0, 1, 256), bytes=True)[:, 2::-1]\n\n    return color_range.reshape(256, 1, 3)\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.get_neighbor_comparisons","title":"<code>get_neighbor_comparisons(pad_skeleton)</code>","text":"<p>Get neighbor comparisons for a padded skeleton.</p> <p>This function creates two <code>CompareNeighborsWithValue</code> objects with different neighborhood sizes (4 and 8) and checks if the neighbors are equal to 1. It returns both comparison objects.</p> <p>Parameters:</p> Name Type Description Default <code>pad_skeleton</code> <code>ndarray of uint8</code> <p>The input padded skeleton array.</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>tuple of CompareNeighborsWithValue, CompareNeighborsWithValue</code> <p>Two comparison objects for 4 and 8 neighbors.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; cnv4, cnv8 = get_neighbor_comparisons(pad_skeleton)\n</code></pre> Source code in <code>src/cellects/image_analysis/network_functions.py</code> <pre><code>def get_neighbor_comparisons(pad_skeleton: NDArray[np.uint8]) -&gt; Tuple[object, object]:\n    \"\"\"\n    Get neighbor comparisons for a padded skeleton.\n\n    This function creates two `CompareNeighborsWithValue` objects with different\n    neighborhood sizes (4 and 8) and checks if the neighbors are equal to 1. It\n    returns both comparison objects.\n\n    Parameters\n    ----------\n    pad_skeleton : ndarray of uint8\n        The input padded skeleton array.\n\n    Returns\n    -------\n    out : tuple of CompareNeighborsWithValue, CompareNeighborsWithValue\n        Two comparison objects for 4 and 8 neighbors.\n\n    Examples\n    --------\n    &gt;&gt;&gt; cnv4, cnv8 = get_neighbor_comparisons(pad_skeleton)\n    \"\"\"\n    cnv4 = CompareNeighborsWithValue(pad_skeleton, 4)\n    cnv4.is_equal(1, and_itself=True)\n    cnv8 = CompareNeighborsWithValue(pad_skeleton, 8)\n    cnv8.is_equal(1, and_itself=True)\n    return cnv4, cnv8\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.get_newly_explored_area","title":"<code>get_newly_explored_area(binary_vid)</code>","text":"<p>Get newly explored area in a binary video.</p> <p>Calculate the number of new pixels that have become active (==1) from the previous frame in a binary video representation.</p> <p>Parameters:</p> Name Type Description Default <code>binary_vid</code> <code>ndarray</code> <p>The current frame of the binary video.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>An array containing the number of new active pixels for each row.</p> Notes <p>This function uses Numba's @njit decorator for performance.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; binary_vid=np.zeros((4, 5, 5), dtype=np.uint8)\n&gt;&gt;&gt; binary_vid[:2, 3, 3] = 1\n&gt;&gt;&gt; binary_vid[1, 4, 3] = 1\n&gt;&gt;&gt; binary_vid[2, 3, 4] = 1\n&gt;&gt;&gt; binary_vid[3, 2, 3] = 1\n&gt;&gt;&gt; get_newly_explored_area(binary_vid)\narray([0, 1, 1, 1])\n</code></pre> <pre><code>&gt;&gt;&gt; binary_vid=np.zeros((5, 5), dtype=np.uint8)[None, :, :]\n&gt;&gt;&gt; get_newly_explored_area(binary_vid)\narray([0])\n</code></pre> Source code in <code>src/cellects/utils/formulas.py</code> <pre><code>@njit()\ndef get_newly_explored_area(binary_vid: NDArray[np.uint8]) -&gt; NDArray:\n    \"\"\"\n    Get newly explored area in a binary video.\n\n    Calculate the number of new pixels that have become active (==1) from\n    the previous frame in a binary video representation.\n\n    Parameters\n    ----------\n    binary_vid : np.ndarray\n        The current frame of the binary video.\n\n    Returns\n    -------\n    np.ndarray\n        An array containing the number of new active pixels for each row.\n\n    Notes\n    -----\n    This function uses Numba's @njit decorator for performance.\n\n    Examples\n    --------\n    &gt;&gt;&gt; binary_vid=np.zeros((4, 5, 5), dtype=np.uint8)\n    &gt;&gt;&gt; binary_vid[:2, 3, 3] = 1\n    &gt;&gt;&gt; binary_vid[1, 4, 3] = 1\n    &gt;&gt;&gt; binary_vid[2, 3, 4] = 1\n    &gt;&gt;&gt; binary_vid[3, 2, 3] = 1\n    &gt;&gt;&gt; get_newly_explored_area(binary_vid)\n    array([0, 1, 1, 1])\n\n    &gt;&gt;&gt; binary_vid=np.zeros((5, 5), dtype=np.uint8)[None, :, :]\n    &gt;&gt;&gt; get_newly_explored_area(binary_vid)\n    array([0])\n    \"\"\"\n    return ((binary_vid - binary_vid[0, ...]) == 1).reshape(binary_vid.shape[0], - 1).sum(1)\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.get_power_dists","title":"<code>get_power_dists(binary_image, cx, cy, n)</code>","text":"<p>Calculate the power distributions based on the given center coordinates and exponent.</p> <p>This function computes the <code>n</code>th powers of x and y distances from a given center point <code>(cx, cy)</code> for each pixel in the binary image.</p> <p>Parameters:</p> Name Type Description Default <code>binary_image</code> <code>ndarray</code> <p>A 2D array (binary image) where the power distributions are calculated.</p> required <code>cx</code> <code>float</code> <p>The x-coordinate of the center point.</p> required <code>cy</code> <code>float</code> <p>The y-coordinate of the center point.</p> required <code>n</code> <code>int</code> <p>The exponent for power distribution calculation.</p> required <p>Returns:</p> Type Description <code>tuple[ndarray, ndarray]</code> <p>A tuple containing two arrays: - The first array contains the <code>n</code>th power of x distances from the center. - The second array contains the <code>n</code>th power of y distances from the center.</p> Notes <p>This function uses Numba's <code>@njit</code> decorator for performance optimization. Ensure that <code>binary_image</code> is a NumPy ndarray to avoid type issues.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; binary_image = np.zeros((10, 10))\n&gt;&gt;&gt; xn, yn = get_power_dists(binary_image, 5.0, 5.0, 2)\n&gt;&gt;&gt; print(xn.shape), print(yn.shape)\n(10,) (10,)\n</code></pre> Source code in <code>src/cellects/utils/formulas.py</code> <pre><code>@njit()\ndef get_power_dists(binary_image: np.ndarray, cx: float, cy: float, n: int):\n    \"\"\"\n    Calculate the power distributions based on the given center coordinates and exponent.\n\n    This function computes the `n`th powers of x and y distances from\n    a given center point `(cx, cy)` for each pixel in the binary image.\n\n    Parameters\n    ----------\n    binary_image : np.ndarray\n        A 2D array (binary image) where the power distributions are calculated.\n    cx : float\n        The x-coordinate of the center point.\n    cy : float\n        The y-coordinate of the center point.\n    n : int\n        The exponent for power distribution calculation.\n\n    Returns\n    -------\n    tuple[np.ndarray, np.ndarray]\n        A tuple containing two arrays:\n        - The first array contains the `n`th power of x distances from the center.\n        - The second array contains the `n`th power of y distances from the center.\n\n    Notes\n    -----\n    This function uses Numba's `@njit` decorator for performance optimization.\n    Ensure that `binary_image` is a NumPy ndarray to avoid type issues.\n\n    Examples\n    --------\n    &gt;&gt;&gt; binary_image = np.zeros((10, 10))\n    &gt;&gt;&gt; xn, yn = get_power_dists(binary_image, 5.0, 5.0, 2)\n    &gt;&gt;&gt; print(xn.shape), print(yn.shape)\n    (10,) (10,)\n    \"\"\"\n    xn = (np.arange(binary_image.shape[1]) - cx) ** n\n    yn = (np.arange(binary_image.shape[0]) - cy) ** n\n    return xn, yn\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.get_skeleton_and_widths","title":"<code>get_skeleton_and_widths(pad_network, pad_origin=None, pad_origin_centroid=None)</code>","text":"<p>Get skeleton and widths from a network.</p> <p>This function computes the morphological skeleton of a network and calculates the distances to the closest zero pixel for each non-zero pixel using medial_axis. If pad_origin is provided, it adds a central contour. Finally, the function removes small loops and keeps only one connected component.</p> <p>Parameters:</p> Name Type Description Default <code>pad_network</code> <code>ndarray of uint8</code> <p>The binary pad network image.</p> required <code>pad_origin</code> <code>ndarray of uint8</code> <p>An array indicating the origin for adding central contour.</p> <code>None</code> <code>pad_origin_centroid</code> <code>ndarray</code> <p>The centroid of the pad origin. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>out</code> <code>tuple(ndarray of uint8, ndarray of uint8, ndarray of uint8)</code> <p>A tuple containing: - pad_skeleton: The skeletonized image. - pad_distances: The distances to the closest zero pixel. - pad_origin_contours: The contours of the central origin, or None if not   used.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pad_network = np.array([[0, 1], [1, 0]])\n&gt;&gt;&gt; skeleton, distances, contours = get_skeleton_and_widths(pad_network)\n&gt;&gt;&gt; print(skeleton)\n</code></pre> Source code in <code>src/cellects/image_analysis/network_functions.py</code> <pre><code>def get_skeleton_and_widths(pad_network: NDArray[np.uint8], pad_origin: NDArray[np.uint8]=None, pad_origin_centroid: NDArray[np.int64]=None) -&gt; Tuple[NDArray[np.uint8], NDArray[np.float64], NDArray[np.uint8]]:\n    \"\"\"\n    Get skeleton and widths from a network.\n\n    This function computes the morphological skeleton of a network and calculates\n    the distances to the closest zero pixel for each non-zero pixel using medial_axis.\n    If pad_origin is provided, it adds a central contour. Finally, the function\n    removes small loops and keeps only one connected component.\n\n    Parameters\n    ----------\n    pad_network : ndarray of uint8\n        The binary pad network image.\n    pad_origin : ndarray of uint8, optional\n        An array indicating the origin for adding central contour.\n    pad_origin_centroid : ndarray, optional\n        The centroid of the pad origin. Defaults to None.\n\n    Returns\n    -------\n    out : tuple(ndarray of uint8, ndarray of uint8, ndarray of uint8)\n        A tuple containing:\n        - pad_skeleton: The skeletonized image.\n        - pad_distances: The distances to the closest zero pixel.\n        - pad_origin_contours: The contours of the central origin, or None if not\n          used.\n\n    Examples\n    --------\n    &gt;&gt;&gt; pad_network = np.array([[0, 1], [1, 0]])\n    &gt;&gt;&gt; skeleton, distances, contours = get_skeleton_and_widths(pad_network)\n    &gt;&gt;&gt; print(skeleton)\n    \"\"\"\n    pad_skeleton, pad_distances = morphology.medial_axis(pad_network, return_distance=True, rng=0)\n    pad_skeleton = pad_skeleton.astype(np.uint8)\n    if pad_origin is not None:\n        pad_skeleton, pad_distances, pad_origin_contours = _add_central_contour(pad_skeleton, pad_distances, pad_origin, pad_network, pad_origin_centroid)\n    else:\n        pad_origin_contours = None\n    pad_skeleton, pad_distances = remove_small_loops(pad_skeleton, pad_distances)\n    pad_skeleton = keep_one_connected_component(pad_skeleton)\n    pad_distances *= pad_skeleton\n    return pad_skeleton, pad_distances, pad_origin_contours\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.get_skewness","title":"<code>get_skewness(mo, binary_image, cx, cy, sx, sy)</code>","text":"<p>Calculate skewness of the given moment.</p> <p>This function computes the skewness based on the third moments and the central moments of a binary image.</p> <p>Parameters:</p> Name Type Description Default <code>mo</code> <code>dict</code> <p>Dictionary containing moments of binary image.</p> required <code>binary_image</code> <code>ndarray</code> <p>Binary image as a 2D numpy array.</p> required <code>cx</code> <code>float</code> <p>Description of parameter <code>cx</code>.</p> required <code>cy</code> <code>float</code> <p>Description of parameter <code>cy</code>.</p> required <code>sx</code> <code>float</code> <p>Description of parameter <code>sx</code>.</p> required <code>sy</code> <code>float</code> <p>Description of parameter <code>sy</code>.</p> required <p>Returns:</p> Type Description <code>Tuple[float, float]</code> <p>Tuple containing skewness values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; result = get_skewness(mo=example_mo, binary_image=binary_img,\n... cx=0.5, cy=0.5, sx=1.0, sy=1.0)\n&gt;&gt;&gt; print(result)\n(skewness_x, skewness_y)  # Example output\n</code></pre> Source code in <code>src/cellects/utils/formulas.py</code> <pre><code>def get_skewness(mo: dict, binary_image: NDArray, cx: float, cy: float, sx: float, sy: float) -&gt; Tuple[float, float]:\n    \"\"\"Calculate skewness of the given moment.\n\n    This function computes the skewness based on the third moments\n    and the central moments of a binary image.\n\n    Parameters\n    ----------\n    mo : dict\n        Dictionary containing moments of binary image.\n    binary_image : ndarray\n        Binary image as a 2D numpy array.\n    cx : float\n        Description of parameter `cx`.\n    cy : float\n        Description of parameter `cy`.\n    sx : float\n        Description of parameter `sx`.\n    sy : float\n        Description of parameter `sy`.\n\n    Returns\n    -------\n    Tuple[float, float]\n        Tuple containing skewness values.\n\n    Examples\n    --------\n    &gt;&gt;&gt; result = get_skewness(mo=example_mo, binary_image=binary_img,\n    ... cx=0.5, cy=0.5, sx=1.0, sy=1.0)\n    &gt;&gt;&gt; print(result)\n    (skewness_x, skewness_y)  # Example output\n    \"\"\"\n    x3, y3 = get_power_dists(binary_image, cx, cy, 3)\n    X3, Y3 = np.meshgrid(x3, y3)\n    m3x, m3y = get_var(mo, binary_image, X3, Y3)\n    return get_skewness_kurtosis(m3x, m3y, sx, sy, 3)\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.get_skewness_kurtosis","title":"<code>get_skewness_kurtosis(mnx, mny, sx, sy, n)</code>","text":"<p>Calculates skewness and kurtosis of a distribution.</p> <p>This function computes the skewness and kurtosis from given statistical moments, standard deviations, and order of moments.</p> <p>Parameters:</p> Name Type Description Default <code>mnx</code> <code>float</code> <p>The third moment about the mean for x.</p> required <code>mny</code> <code>float</code> <p>The fourth moment about the mean for y.</p> required <code>sx</code> <code>float</code> <p>The standard deviation of x.</p> required <code>sy</code> <code>float</code> <p>The standard deviation of y.</p> required <code>n</code> <code>int</code> <p>Order of the moment (3 for skewness, 4 for kurtosis).</p> required <p>Returns:</p> Name Type Description <code>skewness</code> <code>float</code> <p>The computed skewness.</p> <code>kurtosis</code> <code>float</code> <p>The computed kurtosis.</p> Notes <p>This function uses Numba's <code>@njit</code> decorator for performance. Ensure that the values of <code>mnx</code>, <code>mny</code>, <code>sx</code>, and <code>sy</code> are non-zero to avoid division by zero. If <code>n = 3</code>, the function calculates skewness. If <code>n = 4</code>, it calculates kurtosis.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; skewness, kurtosis = get_skewness_kurtosis(1.5, 2.0, 0.5, 0.75, 3)\n&gt;&gt;&gt; print(\"Skewness:\", skewness)\nSkewness: 8.0\n&gt;&gt;&gt; print(\"Kurtosis:\", kurtosis)\nKurtosis: nan\n</code></pre> Source code in <code>src/cellects/utils/formulas.py</code> <pre><code>@njit()\ndef get_skewness_kurtosis(mnx: float, mny: float, sx: float, sy: float, n: int) -&gt; Tuple[float, float]:\n    \"\"\"\n    Calculates skewness and kurtosis of a distribution.\n\n    This function computes the skewness and kurtosis from given statistical\n    moments, standard deviations, and order of moments.\n\n    Parameters\n    ----------\n    mnx : float\n        The third moment about the mean for x.\n    mny : float\n        The fourth moment about the mean for y.\n    sx : float\n        The standard deviation of x.\n    sy : float\n        The standard deviation of y.\n    n : int\n        Order of the moment (3 for skewness, 4 for kurtosis).\n\n    Returns\n    -------\n    skewness : float\n        The computed skewness.\n    kurtosis : float\n        The computed kurtosis.\n\n    Notes\n    -----\n    This function uses Numba's `@njit` decorator for performance.\n    Ensure that the values of `mnx`, `mny`, `sx`, and `sy` are non-zero to avoid division by zero.\n    If `n = 3`, the function calculates skewness. If `n = 4`, it calculates kurtosis.\n\n    Examples\n    --------\n    &gt;&gt;&gt; skewness, kurtosis = get_skewness_kurtosis(1.5, 2.0, 0.5, 0.75, 3)\n    &gt;&gt;&gt; print(\"Skewness:\", skewness)\n    Skewness: 8.0\n    &gt;&gt;&gt; print(\"Kurtosis:\", kurtosis)\n    Kurtosis: nan\n\n    \"\"\"\n    if sx == 0:\n        fx = 0\n    else:\n        fx = mnx / sx ** n\n\n    if sy == 0:\n        fy = 0\n    else:\n        fy = mny / sy ** n\n\n    return fx, fy\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.get_standard_deviations","title":"<code>get_standard_deviations(mo, binary_image, cx, cy)</code>","text":"<p>Return spatial standard deviations for a given moment and binary image.</p> <p>This function computes the square root of variances along <code>x</code> (horizontal) and <code>y</code> (vertical) axes for the given binary image and moment.</p> <p>Parameters:</p> Name Type Description Default <code>mo</code> <code>dict</code> <p>Dictionary containing moments of binary image.</p> required <code>binary_image</code> <code>ndarray of bool or int8</code> <p>The binary input image where the moments are computed.</p> required <code>cx</code> <code>float64</code> <p>X-coordinate of center of mass (horizontal position).</p> required <code>cy</code> <code>float64</code> <p>Y-coordinate of center of mass (vertical position).</p> required <p>Returns:</p> Type Description <code>tuple[ndarray of float64, ndarray of float64]</code> <p>Tuple containing the standard deviations along the x and y axes.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>binary_image</code> is not a binary image or has an invalid datatype.</p> Notes <p>This function uses the <code>get_power_dists</code> and <code>get_var</code> functions to compute the distributed variances, which are then transformed into standard deviations.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; binary_image = np.array([[0, 1], [1, 0]], dtype=np.int8)\n&gt;&gt;&gt; mo = np.array([[2.0], [3.0]])\n&gt;&gt;&gt; cx, cy = 1.5, 1.5\n&gt;&gt;&gt; stdx, stdy = get_standard_deviations(mo, binary_image, cx, cy)\n&gt;&gt;&gt; print(stdx)\n[1.1]\n&gt;&gt;&gt; print(stdy)\n[0.8366600265...]\n</code></pre> Source code in <code>src/cellects/utils/formulas.py</code> <pre><code>def get_standard_deviations(mo: dict, binary_image: NDArray, cx: float, cy: float) -&gt; Tuple[float, float]:\n    \"\"\"\n    Return spatial standard deviations for a given moment and binary image.\n\n    This function computes the square root of variances along `x` (horizontal)\n    and `y` (vertical) axes for the given binary image and moment.\n\n    Parameters\n    ----------\n    mo : dict\n        Dictionary containing moments of binary image.\n    binary_image : ndarray of bool or int8\n        The binary input image where the moments are computed.\n    cx : float64\n        X-coordinate of center of mass (horizontal position).\n    cy : float64\n        Y-coordinate of center of mass (vertical position).\n\n    Returns\n    -------\n    tuple[ndarray of float64, ndarray of float64]\n        Tuple containing the standard deviations along the x and y axes.\n\n    Raises\n    ------\n    ValueError\n        If `binary_image` is not a binary image or has an invalid datatype.\n\n    Notes\n    -----\n    This function uses the `get_power_dists` and `get_var` functions to compute\n    the distributed variances, which are then transformed into standard deviations.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; binary_image = np.array([[0, 1], [1, 0]], dtype=np.int8)\n    &gt;&gt;&gt; mo = np.array([[2.0], [3.0]])\n    &gt;&gt;&gt; cx, cy = 1.5, 1.5\n    &gt;&gt;&gt; stdx, stdy = get_standard_deviations(mo, binary_image, cx, cy)\n    &gt;&gt;&gt; print(stdx)\n    [1.1]\n    &gt;&gt;&gt; print(stdy)\n    [0.8366600265...]\n    \"\"\"\n    x2, y2 = get_power_dists(binary_image, cx, cy, 2)\n    X2, Y2 = np.meshgrid(x2, y2)\n    vx, vy = get_var(mo, binary_image, X2, Y2)\n    return np.sqrt(vx), np.sqrt(vy)\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.get_terminations_and_their_connected_nodes","title":"<code>get_terminations_and_their_connected_nodes(pad_skeleton, cnv4, cnv8)</code>","text":"<p>Get terminations in a skeleton and their connected nodes.</p> <p>This function identifies termination points in a padded skeleton array based on pixel connectivity, marking them and their connected nodes.</p> <p>Parameters:</p> Name Type Description Default <code>pad_skeleton</code> <code>ndarray of uint8</code> <p>The padded skeleton array where terminations are to be identified.</p> required <code>cnv4</code> <code>object</code> <p>Convolution object with 4-connectivity for neighbor comparison.</p> required <code>cnv8</code> <code>object</code> <p>Convolution object with 8-connectivity for neighbor comparison.</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>ndarray of uint8</code> <p>Array containing marked terminations and their connected nodes.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; result = get_terminations_and_their_connected_nodes(pad_skeleton, cnv4, cnv8)\n&gt;&gt;&gt; print(result)\n</code></pre> Source code in <code>src/cellects/image_analysis/network_functions.py</code> <pre><code>def get_terminations_and_their_connected_nodes(pad_skeleton: NDArray[np.uint8], cnv4: object, cnv8: object) -&gt; NDArray[np.uint8]:\n    \"\"\"\n    Get terminations in a skeleton and their connected nodes.\n\n    This function identifies termination points in a padded skeleton array\n    based on pixel connectivity, marking them and their connected nodes.\n\n    Parameters\n    ----------\n    pad_skeleton : ndarray of uint8\n        The padded skeleton array where terminations are to be identified.\n    cnv4 : object\n        Convolution object with 4-connectivity for neighbor comparison.\n    cnv8 : object\n        Convolution object with 8-connectivity for neighbor comparison.\n\n    Returns\n    -------\n    out : ndarray of uint8\n        Array containing marked terminations and their connected nodes.\n\n    Examples\n    --------\n    &gt;&gt;&gt; result = get_terminations_and_their_connected_nodes(pad_skeleton, cnv4, cnv8)\n    &gt;&gt;&gt; print(result)\n    \"\"\"\n    # All pixels having only one neighbor, and containing the value 1, are terminations for sure\n    potential_tips = np.zeros(pad_skeleton.shape, dtype=np.uint8)\n    potential_tips[cnv8.equal_neighbor_nb == 1] = 1\n    # Add more terminations using 4-connectivity\n    # If a pixel is 1 (in 4) and all its neighbors are neighbors (in 4), it is a termination\n\n    coord1_4 = cnv4.equal_neighbor_nb == 1\n    if np.any(coord1_4):\n        coord1_4 = np.nonzero(coord1_4)\n        for y1, x1 in zip(coord1_4[0], coord1_4[1]): # y1, x1 = 3,5\n            # If, in the neighborhood of the 1 (in 4), all (in 8) its neighbors are 4-connected together, and none of them are terminations, the 1 is a termination\n            is_4neigh = cnv4.equal_neighbor_nb[(y1 - 1):(y1 + 2), (x1 - 1):(x1 + 2)] != 0\n            all_4_connected = pad_skeleton[(y1 - 1):(y1 + 2), (x1 - 1):(x1 + 2)] == is_4neigh\n            is_not_term = 1 - potential_tips[y1, x1]\n            if np.all(all_4_connected * is_not_term):\n                is_4neigh[1, 1] = 0\n                is_4neigh = np.pad(is_4neigh, [(1,), (1,)], mode='constant')\n                cnv_4con = CompareNeighborsWithValue(is_4neigh, 4)\n                cnv_4con.is_equal(1, and_itself=True)\n                all_connected = (is_4neigh.sum() - (cnv_4con.equal_neighbor_nb &gt; 0).sum()) == 0\n                # If they are connected, it can be a termination\n                if all_connected:\n                    # If its closest neighbor is above 3 (in 8), this one is also a node\n                    is_closest_above_3 = cnv8.equal_neighbor_nb[(y1 - 1):(y1 + 2), (x1 - 1):(x1 + 2)] * cross_33 &gt; 3\n                    if np.any(is_closest_above_3):\n                        Y, X = np.nonzero(is_closest_above_3)\n                        Y += y1 - 1\n                        X += x1 - 1\n                        potential_tips[Y, X] = 1\n                    potential_tips[y1, x1] = 1\n    return potential_tips\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.get_var","title":"<code>get_var(mo, binary_image, Xn, Yn)</code>","text":"<p>Compute the center of mass in 2D space.</p> <p>This function calculates the weighted average position (centroid) of a binary image using given pixel coordinates and moments.</p> <p>Parameters:</p> Name Type Description Default <code>mo</code> <code>dict</code> <p>Dictionary containing moments of binary image.</p> required <code>binary_image</code> <code>ndarray</code> <p>2D binary image where non-zero pixels are considered.</p> required <code>Xn</code> <code>ndarray</code> <p>Array of x-coordinates for each pixel in <code>binary_image</code>.</p> required <code>Yn</code> <code>ndarray</code> <p>Array of y-coordinates for each pixel in <code>binary_image</code>.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple of two floats <code>(vx, vy)</code> representing the centroid coordinates.</p> <p>Raises:</p> Type Description <code>ZeroDivisionError</code> <p>If <code>mo['m00']</code> is zero, indicating no valid pixels in the image. The function raises a <code>ZeroDivisionError</code>.</p> Notes <p>Performance considerations: This function uses Numba's <code>@njit</code> decorator for performance.</p> Source code in <code>src/cellects/utils/formulas.py</code> <pre><code>@njit()\ndef get_var(mo: dict, binary_image: NDArray, Xn: NDArray, Yn: NDArray) -&gt; Tuple[float, float]:\n    \"\"\"\n    Compute the center of mass in 2D space.\n\n    This function calculates the weighted average position (centroid) of\n    a binary image using given pixel coordinates and moments.\n\n    Parameters\n    ----------\n    mo : dict\n        Dictionary containing moments of binary image.\n    binary_image : ndarray\n        2D binary image where non-zero pixels are considered.\n    Xn : ndarray\n        Array of x-coordinates for each pixel in `binary_image`.\n    Yn : ndarray\n        Array of y-coordinates for each pixel in `binary_image`.\n\n    Returns\n    -------\n    tuple\n        A tuple of two floats `(vx, vy)` representing the centroid coordinates.\n\n    Raises\n    ------\n    ZeroDivisionError\n        If `mo['m00']` is zero, indicating no valid pixels in the image.\n        The function raises a `ZeroDivisionError`.\n\n    Notes\n    -----\n    Performance considerations: This function uses Numba's `@njit` decorator for performance.\n    \"\"\"\n    if mo['m00'] == 0:\n        vx, vy = 0., 0.\n    else:\n        vx = np.sum(binary_image * Xn) / mo[\"m00\"]\n        vy = np.sum(binary_image * Yn) / mo[\"m00\"]\n    return vx, vy\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.get_vertices_and_tips_from_skeleton","title":"<code>get_vertices_and_tips_from_skeleton(pad_skeleton)</code>","text":"<p>Get vertices and tips from a padded skeleton.</p> <p>This function identifies the vertices and tips of a skeletonized image. Tips are endpoints of the skeleton while vertices include tips and points where three or more edges meet.</p> <p>Parameters:</p> Name Type Description Default <code>pad_skeleton</code> <code>ndarray of uint8</code> <p>Input skeleton image that has been padded.</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>tuple (ndarray of uint8, ndarray of uint8)</code> <p>Tuple containing arrays of vertex points and tip points.</p> Source code in <code>src/cellects/image_analysis/network_functions.py</code> <pre><code>def get_vertices_and_tips_from_skeleton(pad_skeleton: NDArray[np.uint8]) -&gt; Tuple[NDArray[np.uint8], NDArray[np.uint8]]:\n    \"\"\"\n    Get vertices and tips from a padded skeleton.\n\n    This function identifies the vertices and tips of a skeletonized image.\n    Tips are endpoints of the skeleton while vertices include tips and points where three or more edges meet.\n\n    Parameters\n    ----------\n    pad_skeleton : ndarray of uint8\n        Input skeleton image that has been padded.\n\n    Returns\n    -------\n    out : tuple (ndarray of uint8, ndarray of uint8)\n        Tuple containing arrays of vertex points and tip points.\n    \"\"\"\n    cnv4, cnv8 = get_neighbor_comparisons(pad_skeleton)\n    potential_tips = get_terminations_and_their_connected_nodes(pad_skeleton, cnv4, cnv8)\n    pad_vertices, pad_tips = get_inner_vertices(pad_skeleton, potential_tips, cnv4, cnv8)\n    return pad_vertices, pad_tips\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.image_borders","title":"<code>image_borders(dimensions, shape='rectangular')</code>","text":"<p>Create an image with borders, either rectangular or circular.</p> <p>Parameters:</p> Name Type Description Default <code>dimensions</code> <code>tuple</code> <p>The dimensions of the image (height, width).</p> required <code>shape</code> <code>str</code> <p>The shape of the borders. Options are \"rectangular\" or \"circular\". Defaults to \"rectangular\".</p> <code>'rectangular'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>ndarray of uint8</code> <p>The image with borders. If the shape is \"circular\", an ellipse border; if \"rectangular\", a rectangular border.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; borders = image_borders((3, 3), \"rectangular\")\n&gt;&gt;&gt; print(borders)\n[[0 0 0]\n [0 1 0]\n [0 0 0]]\n</code></pre> Source code in <code>src/cellects/image_analysis/morphological_operations.py</code> <pre><code>def image_borders(dimensions: tuple, shape: str=\"rectangular\") -&gt; NDArray[np.uint8]:\n    \"\"\"\n    Create an image with borders, either rectangular or circular.\n\n    Parameters\n    ----------\n    dimensions : tuple\n        The dimensions of the image (height, width).\n    shape : str, optional\n        The shape of the borders. Options are \"rectangular\" or \"circular\".\n        Defaults to \"rectangular\".\n\n    Returns\n    -------\n    out : ndarray of uint8\n        The image with borders. If the shape is \"circular\", an ellipse border;\n        if \"rectangular\", a rectangular border.\n\n    Examples\n    --------\n    &gt;&gt;&gt; borders = image_borders((3, 3), \"rectangular\")\n    &gt;&gt;&gt; print(borders)\n    [[0 0 0]\n     [0 1 0]\n     [0 0 0]]\n    \"\"\"\n    if shape == \"circular\":\n        borders = create_ellipse(dimensions[0], dimensions[0])\n        img_contours = image_borders(dimensions)\n        borders = borders * img_contours\n    else:\n        borders = np.ones(dimensions, dtype=np.uint8)\n        borders[0, :] = 0\n        borders[:, 0] = 0\n        borders[- 1, :] = 0\n        borders[:, - 1] = 0\n    return borders\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.is_raw_image","title":"<code>is_raw_image(image_path)</code>","text":"<p>Determine if the image path corresponds to a raw image.</p> <p>Parameters:</p> Name Type Description Default <code>image_path</code> <code>str</code> <p>The file path of the image.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the image is considered raw, False otherwise.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; result = is_raw_image(\"image.jpg\")\n&gt;&gt;&gt; print(result)\nFalse\n</code></pre> Source code in <code>src/cellects/utils/load_display_save.py</code> <pre><code>def is_raw_image(image_path) -&gt; bool:\n    \"\"\"\n    Determine if the image path corresponds to a raw image.\n\n    Parameters\n    ----------\n    image_path : str\n        The file path of the image.\n\n    Returns\n    -------\n    bool\n        True if the image is considered raw, False otherwise.\n\n    Examples\n    --------\n    &gt;&gt;&gt; result = is_raw_image(\"image.jpg\")\n    &gt;&gt;&gt; print(result)\n    False\n    \"\"\"\n    ext = image_path.split(\".\")[-1]\n    if np.isin(ext, opencv_accepted_formats):\n        raw_image = False\n    else:\n        raw_image = True\n    return raw_image\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.keep_one_connected_component","title":"<code>keep_one_connected_component(binary_image)</code>","text":"<p>Keep only one connected component in a binary image.</p> <p>This function filters out all but the largest connected component in a binary image, effectively isolating it from other noise or objects. The function ensures the input is in uint8 format before processing.</p> <p>Parameters:</p> Name Type Description Default <code>binary_image</code> <code>ndarray of uint8</code> <p>Binary image containing one or more connected components.</p> required <p>Returns:</p> Type Description <code>ndarray of uint8</code> <p>Image with only the largest connected component retained.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; all_shapes = np.zeros((5, 5), dtype=np.uint8)\n&gt;&gt;&gt; all_shapes[0:2, 0:2] = 1\n&gt;&gt;&gt; all_shapes[3:4, 3:4] = 1\n&gt;&gt;&gt; res = keep_one_connected_component(all_shapes)\n&gt;&gt;&gt; print(res)\n[[1 1 0 0 0]\n [1 1 0 0 0]\n [0 0 0 0 0]\n [0 0 0 0 0]\n [0 0 0 0 0]]\n</code></pre> Source code in <code>src/cellects/image_analysis/morphological_operations.py</code> <pre><code>def keep_one_connected_component(binary_image: NDArray[np.uint8])-&gt; NDArray[np.uint8]:\n    \"\"\"\n    Keep only one connected component in a binary image.\n\n    This function filters out all but the largest connected component in\n    a binary image, effectively isolating it from other noise or objects.\n    The function ensures the input is in uint8 format before processing.\n\n    Parameters\n    ----------\n    binary_image : ndarray of uint8\n        Binary image containing one or more connected components.\n\n    Returns\n    -------\n    ndarray of uint8\n        Image with only the largest connected component retained.\n\n    Examples\n    -------\n    &gt;&gt;&gt; all_shapes = np.zeros((5, 5), dtype=np.uint8)\n    &gt;&gt;&gt; all_shapes[0:2, 0:2] = 1\n    &gt;&gt;&gt; all_shapes[3:4, 3:4] = 1\n    &gt;&gt;&gt; res = keep_one_connected_component(all_shapes)\n    &gt;&gt;&gt; print(res)\n    [[1 1 0 0 0]\n     [1 1 0 0 0]\n     [0 0 0 0 0]\n     [0 0 0 0 0]\n     [0 0 0 0 0]]\n    \"\"\"\n    if binary_image.dtype != np.uint8:\n        binary_image = binary_image.astype(np.uint8)\n    num_labels, sh = cv2.connectedComponents(binary_image)\n    if num_labels &lt;= 1:\n        return binary_image.astype(np.uint8)\n    else:\n        return keep_largest_shape(sh)\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.linear_model","title":"<code>linear_model(x, a, b)</code>","text":"<p>Perform a linear transformation on input data using slope and intercept.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array_like</code> <p>Input data.</p> required <code>a</code> <code>float</code> <p>Slope coefficient.</p> required <code>b</code> <code>float</code> <p>Intercept.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Resulting value from linear transformation: <code>a</code> * <code>x</code> + <code>b</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; result = linear_model(5, 2.0, 1.5)\n&gt;&gt;&gt; print(result)\n11.5\n</code></pre> Notes <p>This function uses Numba's @njit decorator for performance.</p> Source code in <code>src/cellects/utils/formulas.py</code> <pre><code>@njit()\ndef linear_model(x: NDArray, a: float, b: float) -&gt; float:\n    \"\"\"\n    Perform a linear transformation on input data using slope and intercept.\n\n    Parameters\n    ----------\n    x : array_like\n        Input data.\n    a : float\n        Slope coefficient.\n    b : float\n        Intercept.\n\n    Returns\n    -------\n    float\n        Resulting value from linear transformation: `a` * `x` + `b`.\n\n    Examples\n    --------\n    &gt;&gt;&gt; result = linear_model(5, 2.0, 1.5)\n    &gt;&gt;&gt; print(result)  # doctest: +SKIP\n    11.5\n\n    Notes\n    -----\n    This function uses Numba's @njit decorator for performance.\n    \"\"\"\n    return a * x + b\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.movie","title":"<code>movie(video, increase_contrast=True)</code>","text":"Summary <p>Processes a video to display each frame with optional contrast increase and resizing.</p> <p>Parameters:</p> Name Type Description Default <code>video</code> <code>ndarray</code> <p>The input video represented as a 3D NumPy array.</p> required <code>increase_contrast</code> <code>bool</code> <p>Flag to increase the contrast of each frame (default is True).</p> <code>True</code> <p>Other Parameters:</p> Name Type Description <code>keyboard</code> <code>int</code> <p>Key to wait for during the display of each frame.</p> <code>increase_contrast</code> <code>bool</code> <p>Whether to increase contrast for the displayed frames.</p> <p>Returns:</p> Type Description <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>video</code> is not a 3D NumPy array.</p> Notes <p>This function uses OpenCV's <code>imshow</code> to display each frame. Ensure that the required OpenCV dependencies are met.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; movie(video)\nProcesses and displays a video with default settings.\n&gt;&gt;&gt; movie(video, keyboard=0)\nProcesses and displays a video waiting for the SPACE key between frames.\n&gt;&gt;&gt; movie(video, increase_contrast=False)\nProcesses and displays a video without increasing contrast.\n</code></pre> Source code in <code>src/cellects/utils/load_display_save.py</code> <pre><code>def movie(video, increase_contrast: bool=True):\n    \"\"\"\n    Summary\n    -------\n    Processes a video to display each frame with optional contrast increase and resizing.\n\n    Parameters\n    ----------\n    video : numpy.ndarray\n        The input video represented as a 3D NumPy array.\n    increase_contrast : bool, optional\n        Flag to increase the contrast of each frame (default is True).\n\n    Other Parameters\n    ----------------\n    keyboard : int, optional\n        Key to wait for during the display of each frame.\n    increase_contrast : bool, optional\n        Whether to increase contrast for the displayed frames.\n\n    Returns\n    -------\n    None\n\n    Raises\n    ------\n    ValueError\n        If `video` is not a 3D NumPy array.\n\n    Notes\n    -----\n    This function uses OpenCV's `imshow` to display each frame. Ensure that the required\n    OpenCV dependencies are met.\n\n    Examples\n    --------\n    &gt;&gt;&gt; movie(video)\n    Processes and displays a video with default settings.\n    &gt;&gt;&gt; movie(video, keyboard=0)\n    Processes and displays a video waiting for the SPACE key between frames.\n    &gt;&gt;&gt; movie(video, increase_contrast=False)\n    Processes and displays a video without increasing contrast.\n\n    \"\"\"\n    for i in np.arange(video.shape[0]):\n        image = video[i, :, :]\n        if np.any(image):\n            if increase_contrast:\n                image = bracket_to_uint8_image_contrast(image)\n            final_img = cv2.resize(image, (500, 500))\n            cv2.imshow('Motion analysis', final_img)\n            if cv2.waitKey(25) &amp; 0xFF == ord('q'):\n                break\n    cv2.destroyAllWindows()\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.moving_average","title":"<code>moving_average(vector, step)</code>","text":"<p>Calculate the moving average of a given vector with specified step size.</p> <p>Computes the moving average of input <code>vector</code> using specified <code>step</code> size. NaN values are treated as zeros in the calculation to allow for continuous averaging.</p> <p>Parameters:</p> Name Type Description Default <code>vector</code> <code>ndarray</code> <p>Input vector for which to calculate the moving average.</p> required <code>step</code> <code>int</code> <p>Size of the window for computing the moving average.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Vector containing the moving averages of the input vector.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>step</code> is less than 1.</p> <code>ValueError</code> <p>If the input vector has no valid (non-NaN) elements.</p> Notes <ul> <li>The function considers NaN values as zeros during the averaging process.</li> <li>If <code>step</code> is greater than or equal to the length of the vector, a warning will be raised.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; vector = np.array([1.0, 2.0, np.nan, 4.0, 5.0])\n&gt;&gt;&gt; step = 3\n&gt;&gt;&gt; result = moving_average(vector, step)\n&gt;&gt;&gt; print(result)\n[1.5 2.33333333 3.66666667 4.         nan]\n</code></pre> Source code in <code>src/cellects/utils/formulas.py</code> <pre><code>def moving_average(vector: NDArray, step: int) -&gt; NDArray[float]:\n    \"\"\"\n    Calculate the moving average of a given vector with specified step size.\n\n    Computes the moving average of input `vector` using specified `step`\n    size. NaN values are treated as zeros in the calculation to allow\n    for continuous averaging.\n\n    Parameters\n    ----------\n    vector : ndarray\n        Input vector for which to calculate the moving average.\n    step : int\n        Size of the window for computing the moving average.\n\n    Returns\n    -------\n    numpy.ndarray\n        Vector containing the moving averages of the input vector.\n\n    Raises\n    ------\n    ValueError\n        If `step` is less than 1.\n    ValueError\n        If the input vector has no valid (non-NaN) elements.\n\n    Notes\n    -----\n    - The function considers NaN values as zeros during the averaging process.\n    - If `step` is greater than or equal to the length of the vector, a warning will be raised.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; vector = np.array([1.0, 2.0, np.nan, 4.0, 5.0])\n    &gt;&gt;&gt; step = 3\n    &gt;&gt;&gt; result = moving_average(vector, step)\n    &gt;&gt;&gt; print(result)\n    [1.5 2.33333333 3.66666667 4.         nan]\n    \"\"\"\n    substep = np.array((- int(np.floor((step - 1) / 2)), int(np.ceil((step - 1) / 2))))\n    sums = np.zeros(vector.shape)\n    n_okays = deepcopy(sums)\n    true_numbers = np.logical_not(np.isnan(vector))\n    vector[np.logical_not(true_numbers)] = 0\n    for step_i in np.arange(substep[1] + 1):\n        sums[step_i: (sums.size - step_i)] = sums[step_i: (sums.size - step_i)] + vector[(2 * step_i):]\n        n_okays[step_i: (sums.size - step_i)] = n_okays[step_i: (sums.size - step_i)] + true_numbers[(2 * step_i):]\n        if np.logical_and(step_i &gt; 0, step_i &lt; np.absolute(substep[0])):\n            sums[step_i: (sums.size - step_i)] = sums[step_i: (sums.size - step_i)] + vector[:(sums.size - (2 * step_i)):]\n            n_okays[step_i: (sums.size - step_i)] = n_okays[step_i: (sums.size - step_i)] + true_numbers[:(\n                        true_numbers.size - (2 * step_i))]\n    vector = sums / n_okays\n    return vector\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.njit","title":"<code>njit(*args, **kwargs)</code>","text":"<p>numba.njit decorator that can be disabled. Useful for testing.</p> Source code in <code>src/cellects/utils/decorators.py</code> <pre><code>def njit(*args, **kwargs):\n    \"\"\" numba.njit decorator that can be disabled. Useful for testing.\n    \"\"\"\n    if USE_NUMBA:\n        return _real_njit(*args, **kwargs)\n    # test mode: return identity decorator\n    def deco(func):\n        return func\n    return deco\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.read_and_rotate","title":"<code>read_and_rotate(image_name, prev_img=None, raw_images=False, is_landscape=True, crop_coord=None)</code>","text":"<p>Read and rotate an image based on specified parameters.</p> <p>This function reads an image from the given file name, optionally rotates it by 90 degrees clockwise or counterclockwise based on its dimensions and the <code>is_landscape</code> flag, and applies cropping if specified. It also compares rotated images against a previous image to choose the best rotation.</p> <p>Parameters:</p> Name Type Description Default <code>image_name</code> <code>str</code> <p>Name of the image file to read.</p> required <code>prev_img</code> <code>ndarray</code> <p>Previous image for comparison. Default is <code>None</code>.</p> <code>None</code> <code>raw_images</code> <code>bool</code> <p>Flag to read raw images. Default is <code>False</code>.</p> <code>False</code> <code>is_landscape</code> <code>bool</code> <p>Flag to determine if the image should be considered in landscape mode. Default is <code>True</code>.</p> <code>True</code> <code>crop_coord</code> <code>ndarray</code> <p>Coordinates for cropping the image. Default is <code>None</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Rotated and optionally cropped image.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the specified image file does not exist.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pathway = Path(__name__).resolve().parents[0] / \"data\" / \"single_experiment\"\n&gt;&gt;&gt; image_name = 'image1.tif'\n&gt;&gt;&gt; image = read_and_rotate(pathway /image_name)\n&gt;&gt;&gt; print(image.shape)\n(245, 300, 3)\n</code></pre> Source code in <code>src/cellects/utils/load_display_save.py</code> <pre><code>def read_and_rotate(image_name, prev_img: NDArray=None, raw_images: bool=False, is_landscape: bool=True, crop_coord: NDArray=None) -&gt; NDArray:\n    \"\"\"\n    Read and rotate an image based on specified parameters.\n\n    This function reads an image from the given file name, optionally rotates\n    it by 90 degrees clockwise or counterclockwise based on its dimensions and\n    the `is_landscape` flag, and applies cropping if specified. It also compares\n    rotated images against a previous image to choose the best rotation.\n\n    Parameters\n    ----------\n    image_name : str\n        Name of the image file to read.\n    prev_img : ndarray, optional\n        Previous image for comparison. Default is `None`.\n    raw_images : bool, optional\n        Flag to read raw images. Default is `False`.\n    is_landscape : bool, optional\n        Flag to determine if the image should be considered in landscape mode.\n        Default is `True`.\n    crop_coord : ndarray, optional\n        Coordinates for cropping the image. Default is `None`.\n\n    Returns\n    -------\n    ndarray\n        Rotated and optionally cropped image.\n\n    Raises\n    ------\n    FileNotFoundError\n        If the specified image file does not exist.\n\n    Examples\n    ------\n    &gt;&gt;&gt; pathway = Path(__name__).resolve().parents[0] / \"data\" / \"single_experiment\"\n    &gt;&gt;&gt; image_name = 'image1.tif'\n    &gt;&gt;&gt; image = read_and_rotate(pathway /image_name)\n    &gt;&gt;&gt; print(image.shape)\n    (245, 300, 3)\n    \"\"\"\n    if not os.path.exists(image_name):\n        raise FileNotFoundError(image_name)\n    img = readim(image_name, raw_images)\n    if (img.shape[0] &gt; img.shape[1] and is_landscape) or (img.shape[0] &lt; img.shape[1] and not is_landscape):\n        clockwise = cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)\n        if crop_coord is not None:\n            clockwise = clockwise[crop_coord[0]:crop_coord[1], crop_coord[2]:crop_coord[3], ...]\n        if prev_img is not None:\n            prev_img = np.int16(prev_img)\n            clock_diff = sum_of_abs_differences(prev_img, np.int16(clockwise))\n            counter_clockwise = cv2.rotate(img, cv2.ROTATE_90_COUNTERCLOCKWISE)\n            if crop_coord is not None:\n                counter_clockwise = counter_clockwise[crop_coord[0]:crop_coord[1], crop_coord[2]:crop_coord[3], ...]\n            counter_clock_diff = sum_of_abs_differences(prev_img, np.int16(counter_clockwise))\n            if clock_diff &gt; counter_clock_diff:\n                img = counter_clockwise\n            else:\n                img = clockwise\n        else:\n            img = clockwise\n    else:\n        if crop_coord is not None:\n            img = img[crop_coord[0]:crop_coord[1], crop_coord[2]:crop_coord[3], ...]\n    return img\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.read_h5_array","title":"<code>read_h5_array(file_name, key='data')</code>","text":"<p>Read data array from an HDF5 file.</p> <p>This function reads a specific dataset from an HDF5 file using the provided key.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>The path to the HDF5 file.</p> required <code>key</code> <code>str</code> <p>The dataset name within the HDF5 file.</p> <code>'data'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The data array from the specified dataset in the HDF5 file.</p> Source code in <code>src/cellects/utils/load_display_save.py</code> <pre><code>def read_h5_array(file_name, key: str=\"data\"):\n    \"\"\"\n    Read data array from an HDF5 file.\n\n    This function reads a specific dataset from an HDF5 file using the provided key.\n\n    Parameters\n    ----------\n    file_name : str\n        The path to the HDF5 file.\n    key : str, optional, default: 'data'\n        The dataset name within the HDF5 file.\n\n    Returns\n    -------\n    ndarray\n        The data array from the specified dataset in the HDF5 file.\n    \"\"\"\n    try:\n        with h5py.File(file_name, 'r') as h5f:\n            if key in h5f:\n                data = h5f[key][:]\n                return data\n            else:\n                raise KeyError(f\"Dataset '{key}' not found in file '{file_name}'.\")\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"The file '{file_name}' does not exist.\")\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.read_one_arena","title":"<code>read_one_arena(arena_label, already_greyscale, csc_dict, videos_already_in_ram=None, true_frame_width=None, vid_name=None, background=None, background2=None)</code>","text":"<p>Read a single arena's video data, potentially converting it from color to greyscale.</p> <p>Parameters:</p> Name Type Description Default <code>arena_label</code> <code>int</code> <p>The label of the arena.</p> required <code>already_greyscale</code> <code>bool</code> <p>Whether the video is already in greyscale format.</p> required <code>csc_dict</code> <code>dict</code> <p>Dictionary containing color space conversion settings.</p> required <code>videos_already_in_ram</code> <code>ndarray</code> <p>Pre-loaded video frames in memory. Default is None.</p> <code>None</code> <code>true_frame_width</code> <code>int</code> <p>The true width of the video frames. Default is None.</p> <code>None</code> <code>vid_name</code> <code>str</code> <p>Name of the video file. Default is None.</p> <code>None</code> <code>background</code> <code>ndarray</code> <p>Background image for subtractions. Default is None.</p> <code>None</code> <code>background2</code> <code>ndarray</code> <p>Second background image for subtractions. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing:     - visu: np.ndarray or None, the visual frame.     - converted_video: np.ndarray or None, the video data converted as needed.     - converted_video2: np.ndarray or None, additional video data if necessary.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the specified video file does not exist.</p> <code>ValueError</code> <p>If the video data shape is invalid.</p> Notes <p>This function assumes that <code>video2numpy</code> is a helper function available in the scope. For optimal performance, ensure all video data fits in RAM.</p> Source code in <code>src/cellects/utils/load_display_save.py</code> <pre><code>def read_one_arena(arena_label, already_greyscale:bool, csc_dict: dict, videos_already_in_ram=None,\n                   true_frame_width=None, vid_name: str=None, background: NDArray=None, background2: NDArray=None):\n    \"\"\"\n    Read a single arena's video data, potentially converting it from color to greyscale.\n\n    Parameters\n    ----------\n    arena_label : int\n        The label of the arena.\n    already_greyscale : bool\n        Whether the video is already in greyscale format.\n    csc_dict : dict\n        Dictionary containing color space conversion settings.\n    videos_already_in_ram : np.ndarray, optional\n        Pre-loaded video frames in memory. Default is None.\n    true_frame_width : int, optional\n        The true width of the video frames. Default is None.\n    vid_name : str, optional\n        Name of the video file. Default is None.\n    background : np.ndarray, optional\n        Background image for subtractions. Default is None.\n    background2 : np.ndarray, optional\n        Second background image for subtractions. Default is None.\n\n    Returns\n    -------\n    tuple\n        A tuple containing:\n            - visu: np.ndarray or None, the visual frame.\n            - converted_video: np.ndarray or None, the video data converted as needed.\n            - converted_video2: np.ndarray or None, additional video data if necessary.\n\n    Raises\n    ------\n    FileNotFoundError\n        If the specified video file does not exist.\n    ValueError\n        If the video data shape is invalid.\n\n    Notes\n    -----\n    This function assumes that `video2numpy` is a helper function available in the scope.\n    For optimal performance, ensure all video data fits in RAM.\n    \"\"\"\n    visu, converted_video, converted_video2 = None, None, None\n    logging.info(f\"Arena n\u00b0{arena_label}. Load images and videos\")\n    if videos_already_in_ram is not None:\n        if already_greyscale:\n            converted_video = videos_already_in_ram\n        else:\n            if csc_dict['logical'] == 'None':\n                visu, converted_video = videos_already_in_ram\n            else:\n                visu, converted_video, converted_video2 = videos_already_in_ram\n    else:\n        if vid_name is not None:\n            if already_greyscale:\n                converted_video = video2numpy(vid_name, None, background, background2, true_frame_width)\n                if len(converted_video.shape) == 4:\n                    converted_video = converted_video[:, :, :, 0]\n            else:\n                visu = video2numpy(vid_name, None, background, background2, true_frame_width)\n        else:\n            vid_name = f\"ind_{arena_label}.npy\"\n            if os.path.isfile(vid_name):\n                if already_greyscale:\n                    converted_video = video2numpy(vid_name, None, background, background2, true_frame_width)\n                    if len(converted_video.shape) == 4:\n                        converted_video = converted_video[:, :, :, 0]\n                else:\n                    visu = video2numpy(vid_name, None, background, background2, true_frame_width)\n    return visu, converted_video, converted_video2\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.readim","title":"<code>readim(image_path, raw_image=False)</code>","text":"<p>Read an image from a file and optionally process it.</p> <p>Parameters:</p> Name Type Description Default <code>image_path</code> <code>str</code> <p>Path to the image file.</p> required <code>raw_image</code> <code>bool</code> <p>If True, logs an error message indicating that the raw image format cannot be processed. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The decoded image represented as a NumPy array of shape (height, width, channels).</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If <code>raw_image</code> is set to True, logs an error indicating that the raw image format cannot be processed.</p> Notes <p>Although <code>raw_image</code> is set to False by default, currently it does not perform any raw image processing.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; cv2.imread(\"example.jpg\")\narray([[[255, 0, 0],\n        [255, 0, 0]],\n</code></pre> <pre><code>   [[  0, 255, 0],\n    [  0, 255, 0]],\n\n   [[  0,   0, 255],\n    [  0,   0, 255]]], dtype=np.uint8)\n</code></pre> Source code in <code>src/cellects/utils/load_display_save.py</code> <pre><code>def readim(image_path, raw_image: bool=False):\n    \"\"\"\n    Read an image from a file and optionally process it.\n\n    Parameters\n    ----------\n    image_path : str\n        Path to the image file.\n    raw_image : bool, optional\n        If True, logs an error message indicating that the raw image format cannot be processed. Default is False.\n\n    Returns\n    -------\n    ndarray\n        The decoded image represented as a NumPy array of shape (height, width, channels).\n\n    Raises\n    ------\n    RuntimeError\n        If `raw_image` is set to True, logs an error indicating that the raw image format cannot be processed.\n\n    Notes\n    -----\n    Although `raw_image` is set to False by default, currently it does not perform any raw image processing.\n\n    Examples\n    --------\n    &gt;&gt;&gt; cv2.imread(\"example.jpg\")\n    array([[[255, 0, 0],\n            [255, 0, 0]],\n\n           [[  0, 255, 0],\n            [  0, 255, 0]],\n\n           [[  0,   0, 255],\n            [  0,   0, 255]]], dtype=np.uint8)\n    \"\"\"\n    if raw_image:\n        logging.error(\"Cannot read this image format. If the rawpy package can, ask for a version of Cellects using it.\")\n        # import rawpy\n        # raw = rawpy.imread(image_path)\n        # raw = raw.postprocess()\n        # return cv2.cvtColor(raw, COLOR_RGB2BGR)\n        return cv2.imread(image_path)\n    else:\n        return cv2.imread(image_path)\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.remove_coordinates","title":"<code>remove_coordinates(arr1, arr2)</code>","text":"<p>Remove coordinates from <code>arr1</code> that are present in <code>arr2</code>.</p> <p>Given two arrays of coordinates, remove rows from the first array that match any row in the second array.</p> <p>Parameters:</p> Name Type Description Default <code>arr1</code> <code>ndarray of shape (n, 2)</code> <p>Array containing coordinates to filter.</p> required <code>arr2</code> <code>ndarray of shape (m, 2)</code> <p>Array containing coordinates to match for removal.</p> required <p>Returns:</p> Type Description <code>ndarray of shape (k, 2)</code> <p>Array with coordinates from <code>arr1</code> that are not in <code>arr2</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; arr1 = np.array([[1, 2], [3, 4]])\n&gt;&gt;&gt; arr2 = np.array([[3, 4]])\n&gt;&gt;&gt; remove_coordinates(arr1, arr2)\narray([[1, 2],\n       [3, 4]])\n</code></pre> <pre><code>&gt;&gt;&gt; arr1 = np.array([[1, 2], [3, 4]])\n&gt;&gt;&gt; arr2 = np.array([[3, 2], [1, 4]])\n&gt;&gt;&gt; remove_coordinates(arr1, arr2)\narray([[1, 2],\n       [3, 4]])\n</code></pre> <pre><code>&gt;&gt;&gt; arr1 = np.array([[1, 2], [3, 4]])\n&gt;&gt;&gt; arr2 = np.array([[3, 2], [1, 2]])\n&gt;&gt;&gt; remove_coordinates(arr1, arr2)\narray([[3, 4]])\n</code></pre> <pre><code>&gt;&gt;&gt; arr1 = np.arange(200).reshape(100, 2)\n&gt;&gt;&gt; arr2 = np.array([[196, 197], [198, 199]])\n&gt;&gt;&gt; new_arr1 = remove_coordinates(arr1, arr2)\n&gt;&gt;&gt; new_arr1.shape\n(98, 2)\n</code></pre> Source code in <code>src/cellects/utils/utilitarian.py</code> <pre><code>def remove_coordinates(arr1: NDArray, arr2: NDArray) -&gt; NDArray:\n    \"\"\"\n    Remove coordinates from `arr1` that are present in `arr2`.\n\n    Given two arrays of coordinates, remove rows from the first array\n    that match any row in the second array.\n\n    Parameters\n    ----------\n    arr1 : ndarray of shape (n, 2)\n        Array containing coordinates to filter.\n    arr2 : ndarray of shape (m, 2)\n        Array containing coordinates to match for removal.\n\n    Returns\n    -------\n    ndarray of shape (k, 2)\n        Array with coordinates from `arr1` that are not in `arr2`.\n\n    Examples\n    --------\n    &gt;&gt;&gt; arr1 = np.array([[1, 2], [3, 4]])\n    &gt;&gt;&gt; arr2 = np.array([[3, 4]])\n    &gt;&gt;&gt; remove_coordinates(arr1, arr2)\n    array([[1, 2],\n           [3, 4]])\n\n    &gt;&gt;&gt; arr1 = np.array([[1, 2], [3, 4]])\n    &gt;&gt;&gt; arr2 = np.array([[3, 2], [1, 4]])\n    &gt;&gt;&gt; remove_coordinates(arr1, arr2)\n    array([[1, 2],\n           [3, 4]])\n\n    &gt;&gt;&gt; arr1 = np.array([[1, 2], [3, 4]])\n    &gt;&gt;&gt; arr2 = np.array([[3, 2], [1, 2]])\n    &gt;&gt;&gt; remove_coordinates(arr1, arr2)\n    array([[3, 4]])\n\n    &gt;&gt;&gt; arr1 = np.arange(200).reshape(100, 2)\n    &gt;&gt;&gt; arr2 = np.array([[196, 197], [198, 199]])\n    &gt;&gt;&gt; new_arr1 = remove_coordinates(arr1, arr2)\n    &gt;&gt;&gt; new_arr1.shape\n    (98, 2)\n    \"\"\"\n    if arr2.shape[0] == 0:\n        return arr1\n    else:\n        if arr1.shape[1] != 2 or arr2.shape[1] != 2:\n            raise ValueError(\"Both arrays must have shape (n, 2)\")\n        c_to_keep = ~np.all(arr1 == arr2[0], axis=1)\n        for row in arr2[1:]:\n            c_to_keep *= ~np.all(arr1 == row, axis=1)\n        return arr1[c_to_keep]\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.remove_h5_key","title":"<code>remove_h5_key(file_name, key='data')</code>","text":"<p>Remove a specified key from an HDF5 file.</p> <p>This function opens an HDF5 file in append mode and deletes the specified key if it exists. It handles exceptions related to file not found and other runtime errors.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>The path to the HDF5 file from which the key should be removed.</p> required <code>key</code> <code>str</code> <p>The name of the dataset or group to delete from the HDF5 file. Default is \"data\".</p> <code>'data'</code> <p>Returns:</p> Type Description <code>None</code> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the specified file does not exist.</p> <code>RuntimeError</code> <p>If any other error occurs during file operations.</p> Notes <p>This function modifies the HDF5 file in place. Ensure you have a backup if necessary.</p> Source code in <code>src/cellects/utils/load_display_save.py</code> <pre><code>def remove_h5_key(file_name, key: str=\"data\"):\n    \"\"\"\n    Remove a specified key from an HDF5 file.\n\n    This function opens an HDF5 file in append mode and deletes the specified\n    key if it exists. It handles exceptions related to file not found\n    and other runtime errors.\n\n    Parameters\n    ----------\n    file_name : str\n        The path to the HDF5 file from which the key should be removed.\n    key : str, optional\n        The name of the dataset or group to delete from the HDF5 file.\n        Default is \"data\".\n\n    Returns\n    -------\n    None\n\n    Raises\n    ------\n    FileNotFoundError\n        If the specified file does not exist.\n    RuntimeError\n        If any other error occurs during file operations.\n\n    Notes\n    -----\n    This function modifies the HDF5 file in place. Ensure you have a backup if necessary.\n    \"\"\"\n    try:\n        with h5py.File(file_name, 'a') as h5f:  # Open in append mode to modify the file\n            if key in h5f:\n                del h5f[key]\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"The file '{file_name}' does not exist.\")\n    except Exception as e:\n        raise RuntimeError(f\"An error occurred: {e}\")\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.remove_padding","title":"<code>remove_padding(array_list)</code>","text":"<p>Remove padding from a list of 2D arrays.</p> <p>Parameters:</p> Name Type Description Default <code>array_list</code> <code>list of ndarrays</code> <p>List of 2D NumPy arrays to be processed.</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>list of ndarrays</code> <p>List of 2D NumPy arrays with the padding removed.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; arr1 = np.array([[0, 0, 0], [0, 1, 0], [0, 0, 0]])\n&gt;&gt;&gt; arr2 = np.array([[1, 1, 1], [1, 0, 1], [1, 1, 1]])\n&gt;&gt;&gt; remove_padding([arr1, arr2])\n[array([[1]]), array([[0]])]\n</code></pre> Source code in <code>src/cellects/image_analysis/network_functions.py</code> <pre><code>def remove_padding(array_list: list) -&gt; list:\n    \"\"\"\n    Remove padding from a list of 2D arrays.\n\n    Parameters\n    ----------\n    array_list : list of ndarrays\n        List of 2D NumPy arrays to be processed.\n\n    Returns\n    -------\n    out : list of ndarrays\n        List of 2D NumPy arrays with the padding removed.\n\n    Examples\n    --------\n    &gt;&gt;&gt; arr1 = np.array([[0, 0, 0], [0, 1, 0], [0, 0, 0]])\n    &gt;&gt;&gt; arr2 = np.array([[1, 1, 1], [1, 0, 1], [1, 1, 1]])\n    &gt;&gt;&gt; remove_padding([arr1, arr2])\n    [array([[1]]), array([[0]])]\n    \"\"\"\n    new_array_list = []\n    for arr in array_list:\n        new_array_list.append(un_pad(arr))\n    return new_array_list\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.remove_small_loops","title":"<code>remove_small_loops(pad_skeleton, pad_distances=None)</code>","text":"<p>Remove small loops from a skeletonized image.</p> <p>This function identifies and removes small loops in a skeletonized image, returning the modified skeleton. If distance information is provided, it updates that as well.</p> <p>Parameters:</p> Name Type Description Default <code>pad_skeleton</code> <code>ndarray of uint8</code> <p>The skeletonized image with potential small loops.</p> required <code>pad_distances</code> <code>ndarray of float64</code> <p>The distance map corresponding to the skeleton image. Default is <code>None</code>.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>out</code> <code>ndarray of uint8 or tuple(ndarray of uint8, ndarray of float64)</code> <p>If <code>pad_distances</code> is None, returns the modified skeleton. Otherwise, returns a tuple of the modified skeleton and updated distances.</p> Source code in <code>src/cellects/image_analysis/network_functions.py</code> <pre><code>def remove_small_loops(pad_skeleton: NDArray[np.uint8], pad_distances: NDArray[np.float64]=None):\n    \"\"\"\n    Remove small loops from a skeletonized image.\n\n    This function identifies and removes small loops in a skeletonized image, returning the modified skeleton.\n    If distance information is provided, it updates that as well.\n\n    Parameters\n    ----------\n    pad_skeleton : ndarray of uint8\n        The skeletonized image with potential small loops.\n    pad_distances : ndarray of float64, optional\n        The distance map corresponding to the skeleton image. Default is `None`.\n\n    Returns\n    -------\n    out : ndarray of uint8 or tuple(ndarray of uint8, ndarray of float64)\n        If `pad_distances` is None, returns the modified skeleton. Otherwise,\n        returns a tuple of the modified skeleton and updated distances.\n    \"\"\"\n    cnv4, cnv8 = get_neighbor_comparisons(pad_skeleton)\n    # potential_tips = get_terminations_and_their_connected_nodes(pad_skeleton, cnv4, cnv8)\n\n    cnv_diag_0 = CompareNeighborsWithValue(pad_skeleton, 0)\n    cnv_diag_0.is_equal(0, and_itself=True)\n\n    cnv4_false = CompareNeighborsWithValue(pad_skeleton, 4)\n    cnv4_false.is_equal(1, and_itself=False)\n\n    loop_centers = np.logical_and((cnv4_false.equal_neighbor_nb == 4), cnv_diag_0.equal_neighbor_nb &gt; 2).astype(np.uint8)\n\n    surrounding = cv2.dilate(loop_centers, kernel=square_33)\n    surrounding -= loop_centers\n    surrounding = surrounding * cnv8.equal_neighbor_nb\n\n    # Every 2 can be replaced by 0 if the loop center becomes 1\n    filled_loops = pad_skeleton.copy()\n    filled_loops[surrounding == 2] = 0\n    filled_loops += loop_centers\n\n    new_pad_skeleton = morphology.skeletonize(filled_loops, method='lee')\n\n    # Put the new pixels in pad_distances\n    new_pixels = new_pad_skeleton * (1 - pad_skeleton)\n    pad_skeleton = new_pad_skeleton.astype(np.uint8)\n    if pad_distances is None:\n        return pad_skeleton\n    else:\n        pad_distances[np.nonzero(new_pixels)] = np.nan # 2. # Put nearest value instead?\n        pad_distances *= pad_skeleton\n        # for yi, xi in zip(npY, npX): # yi, xi = npY[0], npX[0]\n        #     distances[yi, xi] = 2.\n        return pad_skeleton, pad_distances\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.rolling_window_segmentation","title":"<code>rolling_window_segmentation(greyscale_image, possibly_filled_pixels, patch_size=(10, 10))</code>","text":"<p>Perform rolling window segmentation on a greyscale image, using potentially filled pixels and a specified patch size.</p> <p>The function divides the input greyscale image into overlapping patches defined by <code>patch_size</code>, and applies Otsu's thresholding method to each patch. The thresholds can be optionally refined using a minimization algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>greyscale_image</code> <code>ndarray of uint8</code> <p>The input greyscale image to segment.</p> required <code>possibly_filled_pixels</code> <code>ndarray of uint8</code> <p>An array indicating which pixels are possibly filled.</p> required <code>patch_size</code> <code>tuple</code> <p>The dimensions of the patches to segment. Default is (10, 10). Must be superior to (1, 1).</p> <code>(10, 10)</code> <p>Returns:</p> Name Type Description <code>output</code> <code>ndarray of uint8</code> <p>The segmented binary image where the network is marked as True.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; greyscale_image = np.array([[1, 2, 1, 1], [1, 3, 4, 1], [2, 4, 3, 1], [2, 1, 2, 1]])\n&gt;&gt;&gt; possibly_filled_pixels = greyscale_image &gt; 1\n&gt;&gt;&gt; patch_size = (2, 2)\n&gt;&gt;&gt; result = rolling_window_segmentation(greyscale_image, possibly_filled_pixels, patch_size)\n&gt;&gt;&gt; print(result)\n[[0 1 0 0]\n [0 1 1 0]\n [0 1 1 0]\n [0 0 1 0]]\n</code></pre> Source code in <code>src/cellects/image_analysis/image_segmentation.py</code> <pre><code>def rolling_window_segmentation(greyscale_image: NDArray, possibly_filled_pixels: NDArray, patch_size: tuple=(10, 10)) -&gt; NDArray[np.uint8]:\n    \"\"\"\n    Perform rolling window segmentation on a greyscale image, using potentially filled pixels and a specified patch size.\n\n    The function divides the input greyscale image into overlapping patches defined by `patch_size`,\n    and applies Otsu's thresholding method to each patch. The thresholds can be optionally\n    refined using a minimization algorithm.\n\n    Parameters\n    ----------\n    greyscale_image : ndarray of uint8\n        The input greyscale image to segment.\n    possibly_filled_pixels : ndarray of uint8\n        An array indicating which pixels are possibly filled.\n    patch_size : tuple, optional\n        The dimensions of the patches to segment. Default is (10, 10).\n        Must be superior to (1, 1).\n\n    Returns\n    -------\n    output : ndarray of uint8\n        The segmented binary image where the network is marked as True.\n\n    Examples\n    --------\n    &gt;&gt;&gt; greyscale_image = np.array([[1, 2, 1, 1], [1, 3, 4, 1], [2, 4, 3, 1], [2, 1, 2, 1]])\n    &gt;&gt;&gt; possibly_filled_pixels = greyscale_image &gt; 1\n    &gt;&gt;&gt; patch_size = (2, 2)\n    &gt;&gt;&gt; result = rolling_window_segmentation(greyscale_image, possibly_filled_pixels, patch_size)\n    &gt;&gt;&gt; print(result)\n    [[0 1 0 0]\n     [0 1 1 0]\n     [0 1 1 0]\n     [0 0 1 0]]\n    \"\"\"\n    patch_centers = [\n        np.floor(np.linspace(\n            p // 2, s - p // 2, int(np.ceil(s / (p // 2))) - 1\n        )).astype(int)\n        for s, p in zip(greyscale_image.shape, patch_size)\n    ]\n    patch_centers = np.transpose(np.meshgrid(*patch_centers), (1, 2, 0)).reshape((-1, 2))\n\n    patch_slices = [\n        tuple(slice(c - p // 2, c + p // 2, 1)\n              for c, p in zip(p_c, patch_size)) for p_c in patch_centers\n    ]\n    maximize_parameter = False\n\n    network_patches = []\n    patch_thresholds = []\n    # for patch in tqdm(patch_slices):\n    for patch in patch_slices:\n        v = greyscale_image[patch] * possibly_filled_pixels[patch]\n        if v.max() &gt; 0 and np.ptp(v) &gt; 0.5:\n            t = threshold_otsu(v)\n\n            if maximize_parameter:\n                res = minimize(_network_perimeter, x0=t, args=(v,), method='Nelder-Mead')\n                t = res.x[0]\n\n            network_patches.append(v &gt; t)\n            patch_thresholds.append(t)\n        else:\n            network_patches.append(np.zeros_like(v))\n            patch_thresholds.append(0)\n\n    network_img = np.zeros(greyscale_image.shape, dtype=np.float64)\n    count_img = np.zeros_like(greyscale_image)\n    for patch, network_patch, t in zip(patch_slices, network_patches, patch_thresholds):\n        network_img[patch] += network_patch\n        count_img[patch] += np.ones_like(network_patch)\n\n    # Safe in-place division: zeros remain where count_img == 0\n    np.divide(network_img, count_img, out=network_img, where=count_img != 0)\n\n    return (network_img &gt; 0.5).astype(np.uint8)\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.save_fig","title":"<code>save_fig(img, full_path, cmap=None)</code>","text":"<p>Save an image figure to a file with specified options.</p> <p>This function creates a matplotlib figure from the given image, optionally applies a colormap, displays it briefly, saves the figure to disk at high resolution, and closes the figure.</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>array_like(M, N, 3)</code> <p>Input image to be saved as a figure. Expected to be in RGB format.</p> required <code>full_path</code> <code>str</code> <p>The complete file path where the figure will be saved. Must include extension (e.g., '.png', '.jpg').</p> required <code>cmap</code> <code>str or None</code> <p>Colormap to be applied if the image should be displayed with a specific color map. If <code>None</code>, no colormap is applied.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>This function does not return any value. It saves the figure to disk at the specified location.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the directory in <code>full_path</code> does not exist.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; img = np.random.rand(100, 100, 3) * 255\n&gt;&gt;&gt; save_fig(img, 'test.png')\nCreates and saves a figure from the random image to 'test.png'.\n</code></pre> <pre><code>&gt;&gt;&gt; save_fig(img, 'colored_test.png', cmap='viridis')\nCreates and saves a figure from the random image with 'viridis' colormap\nto 'colored_test.png'.\n</code></pre> Source code in <code>src/cellects/utils/load_display_save.py</code> <pre><code>def save_fig(img: NDArray, full_path, cmap=None):\n    \"\"\"\n    Save an image figure to a file with specified options.\n\n    This function creates a matplotlib figure from the given image,\n    optionally applies a colormap, displays it briefly, saves the\n    figure to disk at high resolution, and closes the figure.\n\n    Parameters\n    ----------\n    img : array_like (M, N, 3)\n        Input image to be saved as a figure. Expected to be in RGB format.\n    full_path : str\n        The complete file path where the figure will be saved. Must include\n        extension (e.g., '.png', '.jpg').\n    cmap : str or None, optional\n        Colormap to be applied if the image should be displayed with a specific\n        color map. If `None`, no colormap is applied.\n\n    Returns\n    -------\n    None\n\n        This function does not return any value. It saves the figure to disk\n        at the specified location.\n\n    Raises\n    ------\n    FileNotFoundError\n        If the directory in `full_path` does not exist.\n\n    Examples\n    --------\n    &gt;&gt;&gt; img = np.random.rand(100, 100, 3) * 255\n    &gt;&gt;&gt; save_fig(img, 'test.png')\n    Creates and saves a figure from the random image to 'test.png'.\n\n    &gt;&gt;&gt; save_fig(img, 'colored_test.png', cmap='viridis')\n    Creates and saves a figure from the random image with 'viridis' colormap\n    to 'colored_test.png'.\n    \"\"\"\n    sizes = img.shape[0] / 100,  img.shape[1] / 100\n    fig = plt.figure(figsize=(sizes[0], sizes[1]))\n    ax = fig.gca()\n    if cmap is None:\n        ax.imshow(img, interpolation=\"none\")\n    else:\n        ax.imshow(img, cmap=cmap, interpolation=\"none\")\n    plt.axis('off')\n    if np.min(img.shape) &gt; 50:\n        fig.tight_layout()\n\n    fig.savefig(full_path, bbox_inches='tight', pad_inches=0., transparent=True, dpi=500)\n    plt.close(fig)\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.scale_coordinates","title":"<code>scale_coordinates(coord, scale, dims)</code>","text":"<p>Scale coordinates based on given scale factors and dimensions.</p> <p>Parameters:</p> Name Type Description Default <code>coord</code> <code>ndarray</code> <p>A 2x2 array of coordinates to be scaled.</p> required <code>scale</code> <code>tuple of float</code> <p>Scaling factors for the x and y coordinates, respectively.</p> required <code>dims</code> <code>tuple of int</code> <p>Maximum dimensions (height, width) for the scaled coordinates.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Scaled and rounded coordinates.</p> <code>int</code> <p>Minimum y-coordinate.</p> <code>int</code> <p>Maximum y-coordinate.</p> <code>int</code> <p>Minimum x-coordinate.</p> <code>int</code> <p>Maximum x-coordinate.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; coord = np.array(((47, 38), (59, 37)))\n&gt;&gt;&gt; scale = (0.92, 0.87)\n&gt;&gt;&gt; dims = (245, 300, 3)\n&gt;&gt;&gt; scaled_coord, min_y, max_y, min_x, max_x = scale_coordinates(coord, scale, dims)\n&gt;&gt;&gt; scaled_coord\narray([[43, 33],\n       [54, 32]])\n&gt;&gt;&gt; min_y, max_y\n(np.int64(43), np.int64(54))\n&gt;&gt;&gt; min_x, max_x\n(np.int64(32), np.int64(33))\n</code></pre> Notes <p>This function assumes that the input coordinates are in a specific format and will fail if not. The scaling factors should be positive.</p> Source code in <code>src/cellects/utils/formulas.py</code> <pre><code>def scale_coordinates(coord: NDArray, scale: Tuple, dims: Tuple) -&gt; Tuple[NDArray[np.int64], np.int64, np.int64, np.int64, np.int64]:\n    \"\"\"\n    Scale coordinates based on given scale factors and dimensions.\n\n    Parameters\n    ----------\n    coord : numpy.ndarray\n        A 2x2 array of coordinates to be scaled.\n    scale : tuple of float\n        Scaling factors for the x and y coordinates, respectively.\n    dims : tuple of int\n        Maximum dimensions (height, width) for the scaled coordinates.\n\n    Returns\n    -------\n    numpy.ndarray\n        Scaled and rounded coordinates.\n    int\n        Minimum y-coordinate.\n    int\n        Maximum y-coordinate.\n    int\n        Minimum x-coordinate.\n    int\n        Maximum x-coordinate.\n\n    Examples\n    --------\n    &gt;&gt;&gt; coord = np.array(((47, 38), (59, 37)))\n    &gt;&gt;&gt; scale = (0.92, 0.87)\n    &gt;&gt;&gt; dims = (245, 300, 3)\n    &gt;&gt;&gt; scaled_coord, min_y, max_y, min_x, max_x = scale_coordinates(coord, scale, dims)\n    &gt;&gt;&gt; scaled_coord\n    array([[43, 33],\n           [54, 32]])\n    &gt;&gt;&gt; min_y, max_y\n    (np.int64(43), np.int64(54))\n    &gt;&gt;&gt; min_x, max_x\n    (np.int64(32), np.int64(33))\n\n    Notes\n    -----\n    This function assumes that the input coordinates are in a specific format\n    and will fail if not. The scaling factors should be positive.\n    \"\"\"\n    coord = np.array(((np.round(coord[0][0] * scale[0]), np.round(coord[0][1] * scale[1])),\n                    (np.round(coord[1][0] * scale[0]), np.round(coord[1][1] * scale[1]))), dtype=np.int64)\n    min_y = np.max((0, np.min(coord[:, 0])))\n    max_y = np.min((dims[0], np.max(coord[:, 0])))\n    min_x = np.max((0, np.min(coord[:, 1])))\n    max_x = np.min((dims[1], np.max(coord[:, 1])))\n    return coord, min_y, max_y, min_x, max_x\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.show","title":"<code>show(img, interactive=True, cmap=None, show=True)</code>","text":"<p>Display an image using Matplotlib with optional interactivity and colormap.</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>ndarray</code> <p>The image data to be displayed.</p> required <code>interactive</code> <code>bool</code> <p>If <code>True</code>, turn on interactive mode. Default is <code>True</code>.</p> <code>True</code> <code>cmap</code> <code>str or Colormap</code> <p>The colormap to be used. If <code>None</code>, the default colormap will be used.</p> <code>None</code> <p>Other Parameters:</p> Name Type Description <code>interactive</code> <code>bool</code> <p>If <code>True</code>, turn on interactive mode. Default is <code>True</code>.</p> <code>cmap</code> <code>str or Colormap</code> <p>The colormap to be used. If <code>None</code>, the default colormap will be used.</p> <p>Returns:</p> Name Type Description <code>fig</code> <code>Figure</code> <p>The Matplotlib figure object containing the displayed image.</p> <code>ax</code> <code>AxesSubplot</code> <p>The axes on which the image is plotted.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>cmap</code> is not a recognized colormap name or object.</p> Notes <p>If interactive mode is enabled, the user can manipulate the figure window interactively.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; img = np.random.rand(100, 50)\n&gt;&gt;&gt; fig, ax = show(img)\n&gt;&gt;&gt; print(fig)\n&lt;Figure size ... with ... Axes&gt;\n</code></pre> <pre><code>&gt;&gt;&gt; fig, ax = show(img, interactive=False)\n&gt;&gt;&gt; print(fig)\n&lt;Figure size ... with ... Axes&gt;\n</code></pre> <pre><code>&gt;&gt;&gt; fig, ax = show(img, cmap='gray')\n&gt;&gt;&gt; print(fig)\n&lt;Figure size ... with .... Axes&gt;\n</code></pre> Source code in <code>src/cellects/utils/load_display_save.py</code> <pre><code>def show(img, interactive: bool=True, cmap=None, show: bool=True):\n    \"\"\"\n    Display an image using Matplotlib with optional interactivity and colormap.\n\n    Parameters\n    ----------\n    img : ndarray\n        The image data to be displayed.\n    interactive : bool, optional\n        If ``True``, turn on interactive mode. Default is ``True``.\n    cmap : str or Colormap, optional\n        The colormap to be used. If ``None``, the default colormap will\n        be used.\n\n    Other Parameters\n    ----------------\n    interactive : bool, optional\n        If ``True``, turn on interactive mode. Default is ``True``.\n    cmap : str or Colormap, optional\n        The colormap to be used. If ``None``, the default colormap will\n        be used.\n\n    Returns\n    -------\n    fig : Figure\n        The Matplotlib figure object containing the displayed image.\n    ax : AxesSubplot\n        The axes on which the image is plotted.\n\n    Raises\n    ------\n    ValueError\n        If `cmap` is not a recognized colormap name or object.\n\n    Notes\n    -----\n    If interactive mode is enabled, the user can manipulate the figure\n    window interactively.\n\n    Examples\n    --------\n    &gt;&gt;&gt; img = np.random.rand(100, 50)\n    &gt;&gt;&gt; fig, ax = show(img)\n    &gt;&gt;&gt; print(fig) # doctest: +SKIP\n    &lt;Figure size ... with ... Axes&gt;\n\n    &gt;&gt;&gt; fig, ax = show(img, interactive=False)\n    &gt;&gt;&gt; print(fig) # doctest: +SKIP\n    &lt;Figure size ... with ... Axes&gt;\n\n    &gt;&gt;&gt; fig, ax = show(img, cmap='gray')\n    &gt;&gt;&gt; print(fig) # doctest: +SKIP\n    &lt;Figure size ... with .... Axes&gt;\n    \"\"\"\n    if interactive:\n        plt.ion()\n    else:\n        plt.ioff()\n    sizes = img.shape[0] / 100,  img.shape[1] / 100\n    fig = plt.figure(figsize=(sizes[1], sizes[0]))\n    ax = fig.gca()\n    if cmap is None:\n        ax.imshow(img, interpolation=\"none\", extent=(0, sizes[1], 0, sizes[0]))\n    else:\n        ax.imshow(img, cmap=cmap, interpolation=\"none\", extent=(0, sizes[1], 0, sizes[0]))\n\n    if show:\n        fig.tight_layout()\n        fig.show()\n\n    return fig, ax\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.split_dict","title":"<code>split_dict(c_space_dict)</code>","text":"<p>Split a dictionary into two dictionaries based on specific criteria and return their keys.</p> <p>Split the input dictionary <code>c_space_dict</code> into two dictionaries: one for items not ending with '2' and another where the key is truncated by removing its last character if it does end with '2'. Additionally, return the keys that have been processed.</p> <p>Parameters:</p> Name Type Description Default <code>c_space_dict</code> <code>dict</code> <p>The dictionary to be split. Expected keys are strings and values can be any type.</p> required <p>Returns:</p> Name Type Description <code>first_dict</code> <code>dict</code> <p>Dictionary containing items from <code>c_space_dict</code> whose keys do not end with '2'.</p> <code>second_dict</code> <code>dict</code> <p>Dictionary containing items from <code>c_space_dict</code> whose keys end with '2', where the key is truncated by removing its last character.</p> <code>c_spaces</code> <code>list</code> <p>List of keys from <code>c_space_dict</code> that have been processed.</p> <p>Raises:</p> Type Description <code>None</code> Notes <p>No critical information to share.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; c_space_dict = {'key1': 10, 'key2': 20, 'logical': 30}\n&gt;&gt;&gt; first_dict, second_dict, c_spaces = split_dict(c_space_dict)\n&gt;&gt;&gt; print(first_dict)\n{'key1': 10}\n&gt;&gt;&gt; print(second_dict)\n{'key': 20}\n&gt;&gt;&gt; print(c_spaces)\n['key1', 'key']\n</code></pre> Source code in <code>src/cellects/utils/utilitarian.py</code> <pre><code>def split_dict(c_space_dict: dict) -&gt; Tuple[Dict, Dict, list]:\n    \"\"\"\n\n    Split a dictionary into two dictionaries based on specific criteria and return their keys.\n\n    Split the input dictionary `c_space_dict` into two dictionaries: one for items not\n    ending with '2' and another where the key is truncated by removing its last\n    character if it does end with '2'. Additionally, return the keys that have been\n    processed.\n\n    Parameters\n    ----------\n    c_space_dict : dict\n        The dictionary to be split. Expected keys are strings and values can be any type.\n\n    Returns\n    -------\n    first_dict : dict\n        Dictionary containing items from `c_space_dict` whose keys do not end with '2'.\n    second_dict : dict\n        Dictionary containing items from `c_space_dict` whose keys end with '2',\n        where the key is truncated by removing its last character.\n    c_spaces : list\n        List of keys from `c_space_dict` that have been processed.\n\n    Raises\n    ------\n    None\n\n    Notes\n    -----\n    No critical information to share.\n\n    Examples\n    --------\n    &gt;&gt;&gt; c_space_dict = {'key1': 10, 'key2': 20, 'logical': 30}\n    &gt;&gt;&gt; first_dict, second_dict, c_spaces = split_dict(c_space_dict)\n    &gt;&gt;&gt; print(first_dict)\n    {'key1': 10}\n    &gt;&gt;&gt; print(second_dict)\n    {'key': 20}\n    &gt;&gt;&gt; print(c_spaces)\n    ['key1', 'key']\n\n    \"\"\"\n    first_dict = Dict()\n    second_dict = Dict()\n    c_spaces = []\n    for k, v in c_space_dict.items():\n        if k == 'PCA' or k != 'logical' and np.absolute(v).sum() &gt; 0:\n            if k[-1] != '2':\n                first_dict[k] = v\n                c_spaces.append(k)\n            else:\n                second_dict[k[:-1]] = v\n                c_spaces.append(k[:-1])\n    return first_dict, second_dict, c_spaces\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.sum_of_abs_differences","title":"<code>sum_of_abs_differences(array1, array2)</code>","text":"<p>Compute the sum of absolute differences between two arrays.</p> <p>Parameters:</p> Name Type Description Default <code>array1</code> <code>NDArray</code> <p>The first input array.</p> required <code>array2</code> <code>NDArray</code> <p>The second input array.</p> required <p>Returns:</p> Type Description <code>int</code> <p>Sum of absolute differences between elements of <code>array1</code> and <code>array2</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; arr1 = np.array([1.2, 2.5, -3.7])\n&gt;&gt;&gt; arr2 = np.array([12, 25, -37])\n&gt;&gt;&gt; result = sum_of_abs_differences(arr1, arr2)\n&gt;&gt;&gt; print(result)\n66.6\n</code></pre> Source code in <code>src/cellects/utils/formulas.py</code> <pre><code>@njit()\ndef sum_of_abs_differences(array1: NDArray, array2: NDArray):\n    \"\"\"\n    Compute the sum of absolute differences between two arrays.\n\n    Parameters\n    ----------\n    array1 : NDArray\n        The first input array.\n    array2 : NDArray\n        The second input array.\n\n    Returns\n    -------\n    int\n        Sum of absolute differences between elements of `array1` and `array2`.\n\n    Examples\n    --------\n    &gt;&gt;&gt; arr1 = np.array([1.2, 2.5, -3.7])\n    &gt;&gt;&gt; arr2 = np.array([12, 25, -37])\n    &gt;&gt;&gt; result = sum_of_abs_differences(arr1, arr2)\n    &gt;&gt;&gt; print(result)\n    66.6\n    \"\"\"\n    return np.sum(np.absolute(array1 - array2))\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.to_uint8","title":"<code>to_uint8(an_array)</code>","text":"<p>Convert an array to unsigned 8-bit integers.</p> <p>Parameters:</p> Name Type Description Default <code>an_array</code> <code>ndarray</code> <p>Input array to be converted. It can be of any numeric dtype.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>The input array rounded to the nearest integer and then cast to unsigned 8-bit integers.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>an_array</code> is not a ndarray.</p> Notes <p>This function uses Numba's <code>@njit</code> decorator for performance optimization.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; result = to_uint8(np.array([1.2, 2.5, -3.7]))\n&gt;&gt;&gt; print(result)\n[1 3 0]\n</code></pre> Source code in <code>src/cellects/utils/formulas.py</code> <pre><code>@njit()\ndef to_uint8(an_array: NDArray):\n    \"\"\"\n    Convert an array to unsigned 8-bit integers.\n\n    Parameters\n    ----------\n    an_array : ndarray\n        Input array to be converted. It can be of any numeric dtype.\n\n    Returns\n    -------\n    ndarray\n        The input array rounded to the nearest integer and then cast to\n        unsigned 8-bit integers.\n\n    Raises\n    ------\n    TypeError\n        If `an_array` is not a ndarray.\n\n    Notes\n    -----\n    This function uses Numba's `@njit` decorator for performance optimization.\n\n    Examples\n    --------\n    &gt;&gt;&gt; result = to_uint8(np.array([1.2, 2.5, -3.7]))\n    &gt;&gt;&gt; print(result)\n    [1 3 0]\n    \"\"\"\n    out = np.empty_like(an_array)\n    return np.round(an_array, 0, out).astype(np.uint8)\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.translate_dict","title":"<code>translate_dict(old_dict)</code>","text":"<p>Translate a dictionary to a typed dictionary and filter out non-string values.</p> <p>Parameters:</p> Name Type Description Default <code>old_dict</code> <code>dict</code> <p>The input dictionary that may contain non-string values</p> required <p>Returns:</p> Name Type Description <code>numba_dict</code> <code>Dict</code> <p>A typed dictionary containing only the items from <code>old_dict</code> where the value is not a string</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; result = translate_dict({'a': 1., 'b': 'string', 'c': 2.0})\n&gt;&gt;&gt; print(result)\n{a: 1.0, c: 2.0}\n</code></pre> Source code in <code>src/cellects/utils/utilitarian.py</code> <pre><code>def translate_dict(old_dict: dict) -&gt; Dict:\n    \"\"\"\n    Translate a dictionary to a typed dictionary and filter out non-string values.\n\n    Parameters\n    ----------\n    old_dict : dict\n        The input dictionary that may contain non-string values\n\n    Returns\n    -------\n    numba_dict : Dict\n        A typed dictionary containing only the items from `old_dict` where the value is not a string\n\n    Examples\n    --------\n    &gt;&gt;&gt; result = translate_dict({'a': 1., 'b': 'string', 'c': 2.0})\n    &gt;&gt;&gt; print(result)\n    {a: 1.0, c: 2.0}\n    \"\"\"\n    numba_dict = Dict()\n    for k, v in old_dict.items():\n        if not isinstance(v, str):\n            numba_dict[k] = v\n    return numba_dict\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.un_pad","title":"<code>un_pad(arr)</code>","text":"<p>Unpads a 2D NumPy array by removing the first and last row/column.</p> Extended Description <p>Reduces the size of a 2D array by removing the outermost rows and columns. Useful for trimming boundaries added during padding operations.</p> <p>Parameters:</p> Name Type Description Default <code>arr</code> <code>ndarray</code> <p>Input 2D array to be unpadded. Shape (n,m) is expected.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Unpadded 2D array with shape (n-2, m-2).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; arr = np.array([[0, 0, 0],\n&gt;&gt;&gt;                 [0, 4, 0],\n&gt;&gt;&gt;                 [0, 0, 0]])\n&gt;&gt;&gt; un_pad(arr)\narray([[4]])\n</code></pre> Source code in <code>src/cellects/image_analysis/network_functions.py</code> <pre><code>def un_pad(arr: NDArray) -&gt; NDArray:\n    \"\"\"\n    Unpads a 2D NumPy array by removing the first and last row/column.\n\n    Extended Description\n    --------------------\n    Reduces the size of a 2D array by removing the outermost rows and columns.\n    Useful for trimming boundaries added during padding operations.\n\n    Parameters\n    ----------\n    arr : ndarray\n        Input 2D array to be unpadded. Shape (n,m) is expected.\n\n    Returns\n    -------\n    ndarray\n        Unpadded 2D array with shape (n-2, m-2).\n\n    Examples\n    --------\n    &gt;&gt;&gt; arr = np.array([[0, 0, 0],\n    &gt;&gt;&gt;                 [0, 4, 0],\n    &gt;&gt;&gt;                 [0, 0, 0]])\n    &gt;&gt;&gt; un_pad(arr)\n    array([[4]])\n    \"\"\"\n    return arr[1:-1, 1:-1]\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.video2numpy","title":"<code>video2numpy(vid_name, conversion_dict=None, background=None, background2=None, true_frame_width=None)</code>","text":"<p>Convert a video file to a NumPy array.</p> <p>Parameters:</p> Name Type Description Default <code>vid_name</code> <code>str</code> <p>The path to the video file. Can be a <code>.mp4</code> or <code>.npy</code>.</p> required <code>conversion_dict</code> <code>dict</code> <p>Dictionary containing color space conversion parameters.</p> <code>None</code> <code>background</code> <code>NDArray</code> <p>Background image for processing.</p> <code>None</code> <code>background2</code> <code>NDArray</code> <p>Second background image for processing.</p> <code>None</code> <code>true_frame_width</code> <code>int</code> <p>True width of the frame. If specified and the current width is double this value, adjusts to true_frame_width.</p> <code>None</code> <p>Returns:</p> Type Description <code>NDArray or tuple of NDArrays</code> <p>If conversion_dict is None, returns the video as a NumPy array. Otherwise, returns a tuple containing the original video and converted video.</p> Notes <p>This function uses OpenCV to read the contents of a <code>.mp4</code> video file.</p> Source code in <code>src/cellects/utils/load_display_save.py</code> <pre><code>def video2numpy(vid_name: str, conversion_dict=None, background: NDArray=None, background2: NDArray=None,\n                true_frame_width: int=None):\n    \"\"\"\n    Convert a video file to a NumPy array.\n\n    Parameters\n    ----------\n    vid_name : str\n        The path to the video file. Can be a `.mp4` or `.npy`.\n    conversion_dict : dict, optional\n        Dictionary containing color space conversion parameters.\n    background : NDArray, optional\n        Background image for processing.\n    background2 : NDArray, optional\n        Second background image for processing.\n    true_frame_width : int, optional\n        True width of the frame. If specified and the current width is double this value,\n        adjusts to true_frame_width.\n\n    Returns\n    -------\n    NDArray or tuple of NDArrays\n        If conversion_dict is None, returns the video as a NumPy array.\n        Otherwise, returns a tuple containing the original video and converted video.\n\n    Notes\n    -----\n    This function uses OpenCV to read the contents of a `.mp4` video file.\n    \"\"\"\n    np_loading = vid_name[-4:] == \".npy\"\n    if np_loading:\n        video = np.load(vid_name)\n        dims = list(video.shape)\n    else:\n        cap = cv2.VideoCapture(vid_name)\n        dims = [int(cap.get(cv2.CAP_PROP_FRAME_COUNT)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)), int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))]\n\n    if true_frame_width is not None:\n        if dims[2] == 2 * true_frame_width:\n            dims[2] = true_frame_width\n\n    if conversion_dict is not None:\n        first_dict, second_dict, c_spaces = split_dict(conversion_dict)\n        converted_video = np.empty(dims[:3], dtype=np.uint8)\n        if conversion_dict['logical'] == 'None':\n            converted_video2 = np.empty(dims[:3], dtype=np.uint8)\n        if np_loading:\n            for counter in np.arange(video.shape[0]):\n                img = video[counter, :, :dims[2], :]\n                greyscale_image, greyscale_image2, all_c_spaces, first_pc_vector = generate_color_space_combination(img, c_spaces,\n                                                                                     first_dict, second_dict, background=background,background2=background2,\n                                                                                     convert_to_uint8=True)\n                converted_video[counter, ...] = greyscale_image\n                if conversion_dict['logical'] == 'None':\n                    converted_video2[counter, ...] = greyscale_image2\n            video = video[:, :, :dims[2], ...]\n\n    if not np_loading:\n        # 2) Create empty arrays to store video analysis data\n        video = np.empty((dims[0], dims[1], dims[2], 3), dtype=np.uint8)\n        # 3) Read and convert the video frame by frame\n        counter = 0\n        while cap.isOpened() and counter &lt; dims[0]:\n            ret, frame = cap.read()\n            frame = frame[:, :dims[2], ...]\n            video[counter, ...] = frame\n            if conversion_dict is not None:\n                greyscale_image, greyscale_image2, all_c_spaces, first_pc_vector = generate_color_space_combination(frame, c_spaces,\n                                                                                     first_dict, second_dict, background=background,background2=background2,\n                                                                                     convert_to_uint8=True)\n                converted_video[counter, ...] = greyscale_image\n                if conversion_dict['logical'] == 'None':\n                    converted_video2[counter, ...] = greyscale_image2\n            counter += 1\n        cap.release()\n\n    if conversion_dict is None:\n        return video\n    else:\n        return video, converted_video\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.vstack_h5_array","title":"<code>vstack_h5_array(file_name, table, key='data')</code>","text":"<p>Stack tables vertically in an HDF5 file.</p> <p>This function either appends the input table to an existing dataset in the specified HDF5 file or creates a new dataset if the key doesn't exist.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>Path to the HDF5 file.</p> required <code>table</code> <code>NDArray[uint8]</code> <p>The table to be stacked vertically with the existing data.</p> required <code>key</code> <code>str</code> <p>Key under which the dataset will be stored. Defaults to 'data'.</p> <code>'data'</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; table = np.array([[1, 2], [3, 4]], dtype=np.uint8)\n&gt;&gt;&gt; vstack_h5_array('example.h5', table)\n</code></pre> Source code in <code>src/cellects/utils/load_display_save.py</code> <pre><code>def vstack_h5_array(file_name, table: NDArray, key: str=\"data\"):\n    \"\"\"\n    Stack tables vertically in an HDF5 file.\n\n    This function either appends the input table to an existing dataset\n    in the specified HDF5 file or creates a new dataset if the key doesn't exist.\n\n    Parameters\n    ----------\n    file_name : str\n        Path to the HDF5 file.\n    table : NDArray[np.uint8]\n        The table to be stacked vertically with the existing data.\n    key : str, optional\n        Key under which the dataset will be stored. Defaults to 'data'.\n\n    Examples\n    --------\n    &gt;&gt;&gt; table = np.array([[1, 2], [3, 4]], dtype=np.uint8)\n    &gt;&gt;&gt; vstack_h5_array('example.h5', table)\n    \"\"\"\n    if os.path.exists(file_name):\n        # Open the file in append mode\n        with h5py.File(file_name, 'a') as h5f:\n            if key in h5f:\n                # Append to the existing dataset\n                existing_data = h5f[key][:]\n                new_data = np.vstack((existing_data, table))\n                del h5f[key]\n                h5f.create_dataset(key, data=new_data)\n            else:\n                # Create a new dataset if the key doesn't exist\n                h5f.create_dataset(key, data=table)\n    else:\n        with h5py.File(file_name, 'w') as h5f:\n            h5f.create_dataset(key, data=table)\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.write_video","title":"<code>write_video(np_array, vid_name, is_color=True, fps=40)</code>","text":"<p>Write video from numpy array.</p> <p>Save a numpy array as a video file. Supports .npy format for saving raw numpy arrays and various video formats (mp4, avi, mkv) using OpenCV. For video formats, automatically selects a suitable codec and handles file extensions.</p> <p>Parameters:</p> Name Type Description Default <code>np_array</code> <code>ndarray of uint8</code> <p>Input array containing video frames.</p> required <code>vid_name</code> <code>str</code> <p>Filename for the output video. Can include extension or not (defaults to .mp4).</p> required <code>is_color</code> <code>bool</code> <p>Whether the video should be written in color. Defaults to True.</p> <code>True</code> <code>fps</code> <code>int</code> <p>Frame rate for the video in frames per second. Defaults to 40.</p> <code>40</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; video_array = np.random.randint(0, 255, size=(10, 100, 100, 3), dtype=np.uint8)\n&gt;&gt;&gt; write_video(video_array, 'output.mp4', True, 30)\nSaves `video_array` as a color video 'output.mp4' with FPS 30.\n&gt;&gt;&gt; video_array = np.random.randint(0, 255, size=(10, 100, 100), dtype=np.uint8)\n&gt;&gt;&gt; write_video(video_array, 'raw_data.npy')\nSaves `video_array` as a raw numpy array file without frame rate.\n</code></pre> Source code in <code>src/cellects/utils/load_display_save.py</code> <pre><code>def write_video(np_array: NDArray[np.uint8], vid_name: str, is_color: bool=True, fps: int=40):\n    \"\"\"\n    Write video from numpy array.\n\n    Save a numpy array as a video file. Supports .npy format for saving raw\n    numpy arrays and various video formats (mp4, avi, mkv) using OpenCV.\n    For video formats, automatically selects a suitable codec and handles\n    file extensions.\n\n    Parameters\n    ----------\n    np_array : ndarray of uint8\n        Input array containing video frames.\n    vid_name : str\n        Filename for the output video. Can include extension or not (defaults to .mp4).\n    is_color : bool, optional\n        Whether the video should be written in color. Defaults to True.\n    fps : int, optional\n        Frame rate for the video in frames per second. Defaults to 40.\n\n    Examples\n    --------\n    &gt;&gt;&gt; video_array = np.random.randint(0, 255, size=(10, 100, 100, 3), dtype=np.uint8)\n    &gt;&gt;&gt; write_video(video_array, 'output.mp4', True, 30)\n    Saves `video_array` as a color video 'output.mp4' with FPS 30.\n    &gt;&gt;&gt; video_array = np.random.randint(0, 255, size=(10, 100, 100), dtype=np.uint8)\n    &gt;&gt;&gt; write_video(video_array, 'raw_data.npy')\n    Saves `video_array` as a raw numpy array file without frame rate.\n    \"\"\"\n    #h265 ou h265 (mp4)\n    # linux: fourcc = 0x00000021 -&gt; don't forget to change it bellow as well\n    if vid_name[-4:] == '.npy':\n        with open(vid_name, 'wb') as file:\n             np.save(file, np_array)\n    else:\n        valid_extensions = ['.mp4', '.avi', '.mkv']\n        vid_ext = vid_name[-4:]\n        if vid_ext not in valid_extensions:\n            vid_name = vid_name[:-4]\n            vid_name += '.mp4'\n            vid_ext = '.mp4'\n        if vid_ext =='.mp4':\n            fourcc = 0x7634706d# VideoWriter_fourcc(*'FMP4') #(*'MP4V') (*'h265') (*'x264') (*'DIVX')\n        else:\n            fourcc = cv2.VideoWriter_fourcc('F', 'F', 'V', '1')  # lossless\n        size = np_array.shape[2], np_array.shape[1]\n        vid = cv2.VideoWriter(vid_name, fourcc, float(fps), tuple(size), is_color)\n        for image_i in np.arange(np_array.shape[0]):\n            image = np_array[image_i, ...]\n            vid.write(image)\n        vid.release()\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.write_video_sets","title":"<code>write_video_sets(img_list, sizes, vid_names, crop_coord, bounding_boxes, bunch_nb, video_nb_per_bunch, remaining, raw_images, is_landscape, use_list_of_vid, in_colors=False, reduce_image_dim=False, pathway='')</code>","text":"<p>Write video sets from a list of images, applying cropping and optional rotation.</p> <p>Parameters:</p> Name Type Description Default <code>img_list</code> <code>list</code> <p>List of image file names.</p> required <code>sizes</code> <code>NDArray</code> <p>Array containing the dimensions of each video frame.</p> required <code>vid_names</code> <code>list</code> <p>List of video file names to be saved.</p> required <code>crop_coord</code> <code>dict or tuple</code> <p>Coordinates for cropping regions of interest in images/videos.</p> required <code>bounding_boxes</code> <code>tuple</code> <p>Bounding box coordinates to extract sub-images from the original images.</p> required <code>bunch_nb</code> <code>int</code> <p>Number of bunches to divide the videos into.</p> required <code>video_nb_per_bunch</code> <code>int</code> <p>Number of videos per bunch.</p> required <code>remaining</code> <code>int</code> <p>Number of videos remaining after the last full bunch.</p> required <code>raw_images</code> <code>bool</code> <p>Whether the images are in raw format.</p> required <code>is_landscape</code> <code>bool</code> <p>If true, rotate the images to landscape orientation before processing.</p> required <code>use_list_of_vid</code> <code>bool</code> <p>Flag indicating if the output should be a list of videos.</p> required <code>in_colors</code> <code>bool</code> <p>If true, process images with color information. Default is False.</p> <code>False</code> <code>reduce_image_dim</code> <code>bool</code> <p>If true, reduce image dimensions. Default is False.</p> <code>False</code> <code>pathway</code> <code>str</code> <p>Path where the videos should be saved. Default is an empty string.</p> <code>''</code> Source code in <code>src/cellects/utils/load_display_save.py</code> <pre><code>def write_video_sets(img_list: list, sizes: NDArray, vid_names: list, crop_coord, bounding_boxes,\n                      bunch_nb: int, video_nb_per_bunch: int, remaining: int,\n                      raw_images: bool, is_landscape: bool, use_list_of_vid: bool,\n                      in_colors: bool=False, reduce_image_dim: bool=False, pathway: str=\"\"):\n    \"\"\"\n    Write video sets from a list of images, applying cropping and optional rotation.\n\n    Parameters\n    ----------\n    img_list : list\n        List of image file names.\n    sizes : NDArray\n        Array containing the dimensions of each video frame.\n    vid_names : list\n        List of video file names to be saved.\n    crop_coord : dict or tuple\n        Coordinates for cropping regions of interest in images/videos.\n    bounding_boxes : tuple\n        Bounding box coordinates to extract sub-images from the original images.\n    bunch_nb : int\n        Number of bunches to divide the videos into.\n    video_nb_per_bunch : int\n        Number of videos per bunch.\n    remaining : int\n        Number of videos remaining after the last full bunch.\n    raw_images : bool\n        Whether the images are in raw format.\n    is_landscape : bool\n        If true, rotate the images to landscape orientation before processing.\n    use_list_of_vid : bool\n        Flag indicating if the output should be a list of videos.\n    in_colors : bool, optional\n        If true, process images with color information. Default is False.\n    reduce_image_dim : bool, optional\n        If true, reduce image dimensions. Default is False.\n    pathway : str, optional\n        Path where the videos should be saved. Default is an empty string.\n    \"\"\"\n    top, bot, left, right = bounding_boxes\n    for bunch in np.arange(bunch_nb):\n        print(f'\\nSaving the bunch n: {bunch + 1} / {bunch_nb} of videos:', end=' ')\n        if bunch == (bunch_nb - 1) and remaining &gt; 0:\n            arenas = np.arange(bunch * video_nb_per_bunch, bunch * video_nb_per_bunch + remaining, dtype=np.uint32)\n        else:\n            arenas = np.arange(bunch * video_nb_per_bunch, (bunch + 1) * video_nb_per_bunch, dtype=np.uint32)\n        if use_list_of_vid:\n            video_bunch = [np.zeros(sizes[i, :], dtype=np.uint8) for i in arenas]\n        else:\n            video_bunch = np.zeros(np.append(sizes[0, :], len(arenas)), dtype=np.uint8)\n        prev_img = None\n        images_done = bunch * len(img_list)\n        for image_i, image_name in enumerate(img_list):\n            img = read_and_rotate(image_name, prev_img, raw_images, is_landscape, crop_coord)\n            prev_img = img.copy()\n            if not in_colors and reduce_image_dim:\n                img = img[:, :, 0]\n\n            for arena_i, arena_name in enumerate(arenas):\n                # arena_i = 0; arena_name = arena[arena_i]\n                sub_img = img[top[arena_name]: (bot[arena_name] + 1), left[arena_name]: (right[arena_name] + 1), ...]\n                if use_list_of_vid:\n                    video_bunch[arena_i][image_i, ...] = sub_img\n                else:\n                    if len(video_bunch.shape) == 5:\n                        video_bunch[image_i, :, :, :, arena_i] = sub_img\n                    else:\n                        video_bunch[image_i, :, :, arena_i] = sub_img\n        for arena_i, arena_name in enumerate(arenas):\n            if use_list_of_vid:\n                 np.save(pathway + vid_names[arena_name], video_bunch[arena_i])\n            else:\n                if len(video_bunch.shape) == 5:\n                     np.save(pathway + vid_names[arena_name], video_bunch[:, :, :, :, arena_i])\n                else:\n                     np.save(pathway + vid_names[arena_name], video_bunch[:, :, :, arena_i])\n</code></pre>"},{"location":"api/cellects/core/motion_analysis/#cellects.core.motion_analysis.zoom_on_nonzero","title":"<code>zoom_on_nonzero(binary_image, padding=2, return_coord=True)</code>","text":"<p>Crops a binary image around non-zero elements with optional padding and returns either coordinates or cropped region.</p> <p>Parameters:</p> Name Type Description Default <code>binary_image</code> <code>NDArray</code> <p>2D NumPy array containing binary values (0/1)</p> required <code>padding</code> <code>int</code> <p>Amount of zero-padding to add around the minimum bounding box</p> <code>2</code> <code>return_coord</code> <code>bool</code> <p>If True, return slice coordinates instead of cropped image</p> <code>True</code> <p>Returns:</p> Type Description <code>    If `return_coord` is True: [y_min, y_max, x_min, x_max] as 4-element Tuple.</code> <p>If False: 2D binary array representing the cropped region defined by non-zero elements plus padding.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; img = np.zeros((10,10))\n&gt;&gt;&gt; img[3:7,4:6] = 1\n&gt;&gt;&gt; result = zoom_on_nonzero(img)\n&gt;&gt;&gt; print(result)\n[1 8 2 7]\n&gt;&gt;&gt; cropped = zoom_on_nonzero(img, return_coord=False)\n&gt;&gt;&gt; print(cropped.shape)\n(6, 5)\n</code></pre> Notes <ul> <li>Returns empty slice coordinates if input contains no non-zero elements.</li> <li>Coordinate indices are 0-based and compatible with NumPy array slicing syntax.</li> </ul> Source code in <code>src/cellects/utils/load_display_save.py</code> <pre><code>def zoom_on_nonzero(binary_image:NDArray, padding: int = 2, return_coord: bool=True):\n    \"\"\"\n    Crops a binary image around non-zero elements with optional padding and returns either coordinates or cropped region.\n\n    Parameters\n    ----------\n    binary_image : NDArray\n        2D NumPy array containing binary values (0/1)\n    padding : int, default=2\n        Amount of zero-padding to add around the minimum bounding box\n    return_coord : bool, default=True\n        If True, return slice coordinates instead of cropped image\n\n    Returns\n    -------\n        If `return_coord` is True: [y_min, y_max, x_min, x_max] as 4-element Tuple.\n        If False: 2D binary array representing the cropped region defined by non-zero elements plus padding.\n\n    Examples\n    --------\n    &gt;&gt;&gt; img = np.zeros((10,10))\n    &gt;&gt;&gt; img[3:7,4:6] = 1\n    &gt;&gt;&gt; result = zoom_on_nonzero(img)\n    &gt;&gt;&gt; print(result)\n    [1 8 2 7]\n    &gt;&gt;&gt; cropped = zoom_on_nonzero(img, return_coord=False)\n    &gt;&gt;&gt; print(cropped.shape)\n    (6, 5)\n\n    Notes\n    -----\n    - Returns empty slice coordinates if input contains no non-zero elements.\n    - Coordinate indices are 0-based and compatible with NumPy array slicing syntax.\n    \"\"\"\n    y, x = np.nonzero(binary_image)\n    cy_min = np.max((0, y.min() - padding))\n    cy_max = np.min((binary_image.shape[0], y.max() + padding + 1))\n    cx_min = np.max((0, x.min() - padding))\n    cx_max = np.min((binary_image.shape[1], x.max() + padding + 1))\n    if return_coord:\n        return cy_min, cy_max, cx_min, cx_max\n    else:\n        return binary_image[cy_min:cy_max, cx_min:cx_max]\n</code></pre>"},{"location":"api/cellects/core/one_image_analysis/","title":"<code>cellects.core.one_image_analysis</code>","text":""},{"location":"api/cellects/core/one_image_analysis/#cellects.core.one_image_analysis","title":"<code>cellects.core.one_image_analysis</code>","text":"<p>Module providing tools for single-image color space analysis and segmentation.</p> <p>The OneImageAnalysis class offers comprehensive image processing capabilities including color space conversion (RGB, HSV, LAB, LUV, HLS, YUV), filtering (Gaussian, median, bilateral), segmentation (Otsu thresholding, k-means clustering), and shape-based validation. It supports multi-step optimization of color channel combinations to maximize contrast between organisms and background through automated selection workflows involving logical operations on segmented regions.</p> <p>Classes OneImageAnalysis : Analyze images using multiple color spaces for optimal segmentation</p> <p>Notes Uses QThread for background operations during combination processing.</p>"},{"location":"api/cellects/core/one_image_analysis/#cellects.core.one_image_analysis.OneImageAnalysis","title":"<code>OneImageAnalysis</code>","text":"<p>This class takes a 3D matrix (2 space and 1 color [BGR] dimensions), Its methods allow image - conversion to any bgr/hsv/lab channels - croping - rotating - filtering using some of the mainly used techniques:     - Gaussian, Median, Bilateral, Laplacian, Mexican hat - segmenting using thresholds or kmeans - shape selection according to horizontal size or shape ('circle' vs 'quadrilateral')</p> <p>ps: A viewing method displays the image before and after the most advanced modification made in instance</p> Source code in <code>src/cellects/core/one_image_analysis.py</code> <pre><code>class OneImageAnalysis:\n    \"\"\"\n        This class takes a 3D matrix (2 space and 1 color [BGR] dimensions),\n        Its methods allow image\n        - conversion to any bgr/hsv/lab channels\n        - croping\n        - rotating\n        - filtering using some of the mainly used techniques:\n            - Gaussian, Median, Bilateral, Laplacian, Mexican hat\n        - segmenting using thresholds or kmeans\n        - shape selection according to horizontal size or shape ('circle' vs 'quadrilateral')\n\n        ps: A viewing method displays the image before and after the most advanced modification made in instance\n    \"\"\"\n    def __init__(self, image, shape_number: int=1):\n        self.image = image\n        if len(self.image.shape) == 2:\n            self.already_greyscale = True\n        else:\n            self.already_greyscale = False\n        self.image2 = None\n        self.binary_image2 = None\n        self.drift_correction_already_adjusted: bool = False\n        # Create empty variables to fill in the following functions\n        self.binary_image = np.zeros(self.image.shape[:2], dtype=np.uint8)\n        self.previous_binary_image = None\n        self.validated_shapes = np.zeros(self.image.shape[:2], dtype=np.uint8)\n        self.centroids = 0\n        self.shape_number = shape_number\n        self.concomp_stats = 0\n        self.y_boundaries = None\n        self.x_boundaries = None\n        self.crop_coord = None\n        self.cropped: bool = False\n        self.subtract_background = None\n        self.subtract_background2 = None\n        self.im_combinations = None\n        self.bgr = image\n        self.colorspace_list = List((\"bgr\", \"lab\", \"hsv\", \"luv\", \"hls\", \"yuv\"))\n        self.spot_shapes = None\n        self.all_c_spaces = Dict()\n        self.hsv = None\n        self.hls = None\n        self.lab = None\n        self.luv = None\n        self.yuv = None\n        self.greyscale = None\n        self.greyscale2 = None\n        self.first_pc_vector = None\n        self.drift_mask_coord = None\n        self.saved_csc_nb = 0\n\n    def convert_and_segment(self, c_space_dict: dict, color_number=2, bio_mask: NDArray[np.uint8]=None,\n                            back_mask: NDArray[np.uint8]=None, subtract_background: NDArray=None,\n                            subtract_background2: NDArray=None, rolling_window_segmentation: dict=None,\n                            lighter_background: bool=None,\n                            allowed_window: NDArray=None, filter_spec: dict=None):\n        \"\"\"\n        Convert an image to grayscale and segment it based on specified parameters.\n\n        This method converts the given color space dictionary into grayscale\n        images, combines them with existing color spaces and performs segmentation.\n        It has special handling for images that are already in grayscale.\n\n        **Args:**\n\n        - `c_space_dict` (dict): Dictionary containing color spaces.\n        - `color_number` (int, optional): Number of colors to use in segmentation. Defaults to 2.\n        - `bio_mask` (NDArray[np.uint8], optional): Biomask for segmentation. Defaults to None.\n        - `back_mask` (NDArray[np.uint8], optional): Backmask for segmentation. Defaults to None.\n        - `subtract_background` (NDArray, optional): Background to subtract. Defaults to None.\n        - `subtract_background2` (NDArray, optional): Second background to subtract. Defaults to None.\n        - rolling_window_segmentation (dict, optional): Flag for grid segmentation. Defaults to None.\n        - `lighter_background` (bool, optional): Flag for lighter background. Defaults to None.\n        - `mask` (NDArray, optional): Additional mask for segmentation. Defaults to None.\n        - `filter_spec` (dict, optional): Filter specifications. Defaults to None.\n\n        **Attributes:**\n\n        - `self.already_greyscale` (bool): Indicates whether the image is already greyscale.\n        - `self.all_c_spaces` (list): List of color spaces.\n\n        \"\"\"\n        if not self.already_greyscale:\n            first_dict, second_dict, c_spaces = split_dict(c_space_dict)\n            self.image, self.image2, all_c_spaces, self.first_pc_vector = generate_color_space_combination(self.bgr, c_spaces, first_dict, second_dict, subtract_background, subtract_background2)\n            if len(all_c_spaces) &gt; len(self.all_c_spaces):\n                self.all_c_spaces = all_c_spaces\n\n        self.segmentation(logical=c_space_dict['logical'], color_number=color_number, bio_mask=bio_mask,\n                          back_mask=back_mask, rolling_window_segmentation=rolling_window_segmentation,\n                          lighter_background=lighter_background, allowed_window=allowed_window, filter_spec=filter_spec)\n\n\n    def segmentation(self, logical: str='None', color_number: int=2, bio_mask: NDArray[np.uint8]=None,\n                     back_mask: NDArray[np.uint8]=None, bio_label=None, bio_label2=None,\n                     rolling_window_segmentation: dict=None, lighter_background: bool=None, allowed_window: Tuple=None,\n                     filter_spec: dict=None):\n        \"\"\"\n        Implement segmentation on the image using various methods and parameters.\n\n        Args:\n            logical (str): Logical operation to perform between two binary images.\n                           Options are 'Or', 'And', 'Xor'. Default is 'None'.\n            color_number (int): Number of colors to use in segmentation. Must be greater than 2\n                                for kmeans clustering. Default is 2.\n            bio_mask (NDArray[np.uint8]): Binary mask for biological areas. Default is None.\n            back_mask (NDArray[np.uint8]): Binary mask for background areas. Default is None.\n            bio_label (Any): Label for biological features. Default is None.\n            bio_label2 (Any): Secondary label for biological features. Default is None.\n            rolling_window_segmentation (dict): Whether to perform grid segmentation. Default is None.\n            lighter_background (bool): Indicates if the background is lighter than objects.\n                                       Default is None.\n            allowed_window (Tuple): Mask to apply during segmentation. Default is None.\n            filter_spec (dict): Dictionary of filters to apply on the image before segmentation.\n\n        \"\"\"\n        # 1. Check valid pixels for segmentation (e.g. when there is a drift correction)\n        if allowed_window is None:\n            min_y, max_y, min_x, max_x = 0, self.image.shape[0] + 1, 0, self.image.shape[1] + 1\n        else:\n            min_y, max_y, min_x, max_x = allowed_window\n        greyscale = self.image[min_y:max_y, min_x:max_x].copy()\n        # 2. Apply filter on the greyscale images\n        if filter_spec is not None and filter_spec[\"filter1_type\"] != \"\":\n            greyscale = apply_filter(greyscale, filter_spec[\"filter1_type\"], filter_spec[\"filter1_param\"])\n\n        greyscale2 = None\n        if logical != 'None':\n            greyscale2 = self.image2[min_y:max_y, min_x:max_x].copy()\n            if filter_spec is not None and filter_spec[\"filter2_type\"] != \"\":\n                greyscale2 = apply_filter(greyscale2, filter_spec[\"filter2_type\"], filter_spec[\"filter2_param\"])\n\n        # 3. Do one of the three segmentation algorithms: kmeans, otsu, windowed\n        if color_number &gt; 2:\n            binary_image, binary_image2, self.bio_label, self.bio_label2  = kmeans(greyscale, greyscale2, color_number, bio_mask, back_mask, logical, bio_label, bio_label2)\n        elif rolling_window_segmentation is not None and rolling_window_segmentation['do']:\n            binary_image = windowed_thresholding(greyscale, lighter_background, rolling_window_segmentation['side_len'],\n            rolling_window_segmentation['step'], rolling_window_segmentation['min_int_var'])\n        else:\n            binary_image = otsu_thresholding(greyscale)\n        if logical != 'None' and color_number == 2:\n            if rolling_window_segmentation is not None and rolling_window_segmentation['do']:\n                binary_image2 = windowed_thresholding(greyscale2, lighter_background, rolling_window_segmentation['side_len'],\n                rolling_window_segmentation['step'], rolling_window_segmentation['min_int_var'])\n            else:\n                binary_image2 = otsu_thresholding(greyscale2)\n\n        # 4. Use previous_binary_image to make sure that the specimens are labelled with ones and the background zeros\n        if self.previous_binary_image is not None:\n            previous_binary_image = self.previous_binary_image[min_y:max_y, min_x:max_x]\n            if not (binary_image * previous_binary_image).any() or (binary_image[0, :].all() and binary_image[-1, :].all() and binary_image[:, 0].all() and binary_image[:, -1].all()):\n                # if (binary_image * (1 - previous_binary_image)).sum() &gt; (binary_image * previous_binary_image).sum() + perimeter(binary_image):\n                # Ones of the binary image have more in common with the background than with the specimen\n                binary_image = 1 - binary_image\n            if logical != 'None':\n                if (binary_image2 * (1 - previous_binary_image)).sum() &gt; (binary_image2 * previous_binary_image).sum():\n                    binary_image2 = 1 - binary_image2\n\n        # 5. Give back the image their original size and combine binary images (optional)\n        self.binary_image = np.zeros(self.image.shape, dtype=np.uint8)\n        self.binary_image[min_y:max_y, min_x:max_x] = binary_image\n        self.greyscale = np.zeros(self.image.shape, dtype=np.uint8)\n        self.greyscale[min_y:max_y, min_x:max_x] = greyscale\n        if logical != 'None':\n            self.binary_image2 = np.zeros(self.image.shape, dtype=np.uint8)\n            self.binary_image2[min_y:max_y, min_x:max_x] = binary_image2\n            self.greyscale2 = np.zeros(self.image.shape, dtype=np.uint8)\n            self.greyscale2[min_y:max_y, min_x:max_x] = greyscale2\n        if logical != 'None':\n            if logical == 'Or':\n                self.binary_image = np.logical_or(self.binary_image, self.binary_image2)\n            elif logical == 'And':\n                self.binary_image = np.logical_and(self.binary_image, self.binary_image2)\n            elif logical == 'Xor':\n                self.binary_image = np.logical_xor(self.binary_image, self.binary_image2)\n            self.binary_image = self.binary_image.astype(np.uint8)\n\n    def _get_all_color_spaces(self):\n        \"\"\"Generate and store all supported color spaces for the image.\"\"\"\n        if len(self.all_c_spaces) &lt; 6 and not self.already_greyscale:\n            self.all_c_spaces = get_color_spaces(self.bgr)\n\n    def generate_subtract_background(self, c_space_dict: dict, drift_corrected: bool=False):\n        \"\"\"\n        Generate a background-subtracted image using specified color space dictionary.\n\n        This method first checks if color spaces have already been generated or\n        if the image is greyscale. If not, it generates color spaces from the BGR\n        image. It then converts and segments the image using the provided color space\n        dictionary without grid segmentation. A disk-shaped structuring element is\n        created and used to perform a morphological opening operation on the image,\n        resulting in a background-subtracted version. If there is a second image\n        (see Also: image2), the same operation is performed on it.\n\n        Args:\n            c_space_dict (dict): Dictionary containing color space specifications\n                for the segmentation process.\n\n        Attributes:\n            disk_size: Radius of the disk-shaped structuring element\n                used for morphological operations, calculated based on image dimensions.\n            subtract_background: Background-subtracted version of `image` obtained\n                after morphological operations with the disk-shaped structuring element.\n            subtract_background2: Background-subtracted version of `image2` obtained\n                after morphological operations with the disk-shaped structuring element,\n                if `image2` is present.\"\"\"\n        logging.info(\"Generate background using the generate_subtract_background method of OneImageAnalysis class\")\n        self._get_all_color_spaces()\n        if drift_corrected:\n            # self.adjust_to_drift_correction(c_space_dict['logical'])\n            self.check_if_image_border_attest_drift_correction()\n        self.convert_and_segment(c_space_dict, rolling_window_segmentation=None, allowed_window=self.drift_mask_coord)\n        disk_size = int(np.floor(np.sqrt(np.min(self.bgr.shape[:2])) / 2))\n        disk = create_ellipse(disk_size, disk_size, min_size=3).astype(np.uint8)\n        self.subtract_background = cv2.morphologyEx(self.image, cv2.MORPH_OPEN, disk)\n        if self.image2 is not None:\n            self.subtract_background2 = cv2.morphologyEx(self.image2, cv2.MORPH_OPEN, disk)\n\n    def check_if_image_border_attest_drift_correction(self) -&gt; bool:\n        \"\"\"\n        Check if the given binary image requires border attenuation and drift correction.\n\n        In order to determine the need for border attenuation or drift correction, this function\n        evaluates the borders of a binary image. If any two opposite borders are fully black,\n        it assumes that there is an issue requiring correction.\n\n        Returns:\n            bool: True if border attenuation or drift correction is required, False otherwise.\n\n        \"\"\"\n        t = np.all(self.binary_image[0, :])\n        b = np.all(self.binary_image[-1, :])\n        l = np.all(self.binary_image[:, 0])\n        r = np.all(self.binary_image[:, -1])\n        self.drift_mask_coord = None\n        if (t and b) or (t and r) or (t and l) or (t and r) or (b and l) or (b and r) or (l and r):\n            cc_nb, shapes = cv2.connectedComponents(self.binary_image)\n            if cc_nb &gt; 1:\n                if cc_nb == 2:\n                    drift_mask_coord = np.nonzero(1 - self.binary_image)\n                else:\n                    back = np.unique(np.concatenate((shapes[0, :], shapes[-1, :],  shapes[:, 0], shapes[:, -1]), axis=0))\n                    drift_mask_coord = np.nonzero(np.logical_or(1 - self.binary_image, 1 - np.isin(shapes, back[back != 0])))\n                drift_mask_coord = (np.min(drift_mask_coord[0]), np.max(drift_mask_coord[0]) + 1,\n                                    np.min(drift_mask_coord[1]), np.max(drift_mask_coord[1]) + 1)\n                self.drift_mask_coord = drift_mask_coord\n                return True\n            else:\n                return False\n        else:\n            return False\n\n    def adjust_to_drift_correction(self, logical: str):\n        \"\"\"\n        Adjust the image and binary image to correct for drift.\n\n        This method applies a drift correction by dilating the binary image, calculating\n        the mean value of the drifted region and applying it back to the image. After this,\n        it applies Otsu's thresholding method to determine a new binary image and adjusts\n        the second image if present. The logical operation specified is then applied to the\n        binary images.\n\n        Args:\n            logical (str): Logical operation ('Or', 'And', 'Xor') to apply to the binary\n                images.\"\"\"\n        if not self.drift_correction_already_adjusted:\n            self.drift_correction_already_adjusted = True\n\n            mask = cv2.dilate(self.binary_image, kernel=cross_33)\n            mask -= self.binary_image\n            mask = np.nonzero(mask)\n            drift_correction = np.mean(self.image[mask[0], mask[1]])\n            self.image[np.nonzero(self.binary_image)] = drift_correction\n            threshold = get_otsu_threshold(self.image)\n            binary = (self.image &gt; threshold)\n            self.binary_image = binary.astype(np.uint8)\n\n            if self.image2 is not None:\n                drift_correction2 = np.mean(self.image2[mask[0], mask[1]])\n                self.image2[np.nonzero(self.binary_image)] = drift_correction2\n                threshold = get_otsu_threshold(self.image2)\n                binary1 = (self.image2 &gt; threshold)\n                binary2 = np.logical_not(binary1)\n                if binary1.sum() &lt; binary2.sum():\n                    binary = binary1\n                else:\n                    binary = binary2\n                while np.any(binary * self.binary_image2) and threshold &gt; 1:\n                    threshold -= 1\n                    binary1 = (self.image2 &gt; threshold)\n                    binary2 = np.logical_not(binary1)\n                    if binary1.sum() &lt; binary2.sum():\n                        binary = binary1\n                    else:\n                        binary = binary2\n                self.binary_image2 = binary.astype(np.uint8)\n                if logical == 'Or':\n                    self.binary_image = np.logical_or(self.binary_image, self.binary_image2)\n                elif logical == 'And':\n                    self.binary_image = np.logical_and(self.binary_image, self.binary_image2)\n                elif logical == 'Xor':\n                    self.binary_image = np.logical_xor(self.binary_image, self.binary_image2)\n                self.binary_image = self.binary_image.astype(np.uint8)\n\n    def init_combinations_lists(self):\n        self.im_combinations = []\n        self.saved_images_list = List()\n        self.converted_images_list = List()\n        self.saved_color_space_list = list()\n        self.saved_csc_nb = 0\n\n    def find_color_space_combinations(self, params: dict=None, only_bgr: bool = False):\n        logging.info(f\"Start automatic finding of color space combinations...\")\n        self.init_combinations_lists()\n        if self.image.any():\n            # 1. Set all params\n            if params is None:\n                params = init_params()\n            if params['arenas_mask'] is not None:\n                params['out_of_arenas_mask'] = 1 - params['arenas_mask']\n            if params['ref_image'] is not None:\n                params['ref_image'] = cv2.dilate(params['ref_image'], cross_33)\n            if params['several_blob_per_arena']:\n                params['con_comp_extent'] = [1, self.binary_image.size // 50]\n            else:\n                params['con_comp_extent'] = [params['blob_nb'], np.max((params['blob_nb'], self.binary_image.size // 100))]\n            im_size = self.image.shape[0] * self.image.shape[1]\n\n            if not params['several_blob_per_arena'] and params['blob_nb'] is not None and params['blob_nb'] &gt; 1 and params['are_zigzag'] is not None:\n                if params['are_zigzag'] == \"columns\":\n                    inter_dist = np.mean(np.diff(np.nonzero(self.y_boundaries)))\n                elif params['are_zigzag'] == \"rows\":\n                    inter_dist = np.mean(np.diff(np.nonzero(self.x_boundaries)))\n                else:\n                    dist1 = np.mean(np.diff(np.nonzero(self.y_boundaries)))\n                    dist2 = np.mean(np.diff(np.nonzero(self.x_boundaries)))\n                    inter_dist = np.max(dist1, dist2)\n                if params['blob_shape'] == \"rectangle\":\n                    params['max_blob_size'] = np.square(2 * inter_dist)\n                else:\n                    params['max_blob_size'] = np.pi * np.square(inter_dist)\n                params['total_surface_area'] = params['max_blob_size'] * self.sample_number\n            else:\n                params['max_blob_size'] = .9 * im_size\n                params['total_surface_area'] = .99 * im_size\n\n            # 2. Get color_space_dictionaries\n            if only_bgr:\n                if not 'bgr' in self.all_c_spaces:\n                    self.all_c_spaces['bgr'] = self.bgr\n            else:\n                self._get_all_color_spaces()\n\n            # 3. Init combination_features table\n            unaltered_blob_nb_idx, blob_number_idx, blob_shape_idx, blob_size_idx, total_area_idx, width_std_idx, height_std_idx, area_std_idx, out_of_arenas_idx, in_arena_idx, common_with_ref_idx, bio_sum_idx, back_sum_idx, score_idx = np.arange(3, 17)\n            self.factors = ['unaltered_blob_nb', 'blob_nb', 'total_area', 'width_std', 'height_std', 'area_std', 'out_of_arenas', 'in_arenas', 'common_with_ref', 'bio_sum', 'back_sum', 'score']\n            self.combination_features = pd.DataFrame(np.zeros((100, len(self.factors)), dtype=np.float64), columns=self.factors)\n\n            # 4. Test every channel separately\n            process = 'one'\n            for csc_dict in one_dict_per_channel:\n                ProcessImage([self, params, process, csc_dict])\n            # If the blob number is known, try applying filters to improve detection\n            if params['blob_nb'] is not None and (params['filter_spec'] is None or params['filter_spec']['filter1_type'] == ''):\n                if not (self.combination_features['blob_nb'].iloc[:self.saved_csc_nb] == params['blob_nb']).any():\n                    tested_filters = ['Gaussian', 'Median', 'Mexican hat', 'Laplace', '']\n                    for tested_filter in tested_filters:\n                        self.init_combinations_lists()\n                        params['filter_spec'] = {'filter1_type': tested_filter, 'filter1_param': [.5, 1.], 'filter2_type': \"\", 'filter2_param': [.5, 1.]}\n                        if 'Param1' in filter_dict[tested_filter]:\n                            params['filter_spec']['filter1_param'] = [filter_dict[tested_filter]['Param1']['Default']]\n                            if 'Param2' in filter_dict[tested_filter]:\n                                params['filter_spec']['filter1_param'].append(filter_dict[tested_filter]['Param2']['Default'])\n                        for csc_dict in one_dict_per_channel:\n                            ProcessImage([self, params, process, csc_dict])\n                        if (self.combination_features['blob_nb'].iloc[:self.saved_csc_nb] == params['blob_nb']).any():\n                            break\n\n            self.score_combination_features()\n            # 5. Try adding each valid channel with one another\n            # 5.1. Generate an index vector containing, for each color space, the channel maximizing the score\n            possibilities = []\n            self.all_combined = Dict()\n            different_color_spaces = np.unique(self.saved_color_space_list)\n            for color_space in different_color_spaces:\n                indices = np.nonzero(np.isin(self.saved_color_space_list, color_space))[0]\n                csc_idx = indices[0] + np.argmax(self.combination_features.loc[indices, 'score'])\n                possibilities.append(csc_idx)\n                for k, v in self.saved_color_space_list[csc_idx].items():\n                    self.all_combined[k] = v\n\n            # 5.2. Try combining each selected channel with every other in all possible order\n            params['possibilities'] = possibilities\n            pool = mp.ThreadPool(processes=os.cpu_count() - 1)\n            process = 'add'\n            list_args = [[self, params, process, i] for i in possibilities]\n            for process_i in pool.imap_unordered(ProcessImage, list_args):\n                pass\n\n            # 6. Take a combination of all selected channels and try to remove each color space one by one\n            ProcessImage([self, params, 'subtract', 0])\n\n            # 7. Add PCA:\n            ProcessImage([self, params, 'PCA', None])\n\n            # 8. Make logical operations between pairs of segmentation result\n            coverage = np.argsort(self.combination_features['total_area'].iloc[:self.saved_csc_nb])\n\n            # 8.1 Try a logical And between the most covered images\n            most1, most2 = coverage.values[-1], coverage.values[-2]\n            operation = {0: most1, 1: most2, 'logical': 'And'}\n            ProcessImage([self, params, 'logical', operation])\n\n            # 8.2 Try a logical Or between the least covered images\n            least1, least2 = coverage.values[0], coverage.values[1]\n            operation = {0: least1, 1: least2, 'logical': 'Or'}\n            ProcessImage([self, params, 'logical', operation])\n\n\n            # 8.3 Try a logical And between the best bio_mask images\n            if params['bio_mask'] is not None:\n                bio_sort = np.argsort(self.combination_features['bio_sum'].iloc[:self.saved_csc_nb])\n                bio1, bio2 = bio_sort.values[-1], bio_sort.values[-2]\n                operation = {0: bio1, 1: bio2, 'logical': 'And'}\n                ProcessImage([self, params, 'logical', operation])\n\n            # 8.4 Try a logical And between the best back_mask images\n            if params['back_mask'] is not None:\n                back_sort = np.argsort(self.combination_features['back_sum'].iloc[:self.saved_csc_nb])\n                back1, back2 = back_sort.values[-1], back_sort.values[-2]\n                operation = {0: back1, 1: back2, 'logical': 'And'}\n                ProcessImage([self, params, 'logical', operation])\n\n            # 8.5 Try a logical Or between the best bio_mask and the best back_mask images\n            if params['bio_mask'] is not None and params['back_mask'] is not None:\n                operation = {0: bio1, 1: back1, 'logical': 'Or'}\n                ProcessImage([self, params, 'logical', operation])\n\n            # 9. Order all saved features\n            self.combination_features = self.combination_features.iloc[:self.saved_csc_nb, :]\n            self.score_combination_features()\n            if params['is_first_image'] and params['blob_nb'] is not None:\n                distances = np.abs(self.combination_features['blob_nb'] - params['blob_nb'])\n                cc_efficiency_order = np.argsort(distances)\n            else:\n                cc_efficiency_order = np.argsort(self.combination_features['score'])\n                cc_efficiency_order = cc_efficiency_order.max() - cc_efficiency_order\n\n            # 7. Save and return a dictionary containing the selected color space combinations\n            # and their corresponding binary images\n            self.im_combinations = []\n            for saved_csc in cc_efficiency_order:\n                if len(self.saved_color_space_list[saved_csc]) &gt; 0:\n                    self.im_combinations.append({})\n                    self.im_combinations[len(self.im_combinations) - 1][\"csc\"] = {}\n                    self.im_combinations[len(self.im_combinations) - 1][\"csc\"]['logical'] = 'None'\n                    for k, v in self.saved_color_space_list[saved_csc].items():\n                        self.im_combinations[len(self.im_combinations) - 1][\"csc\"][k] = v\n                    self.im_combinations[len(self.im_combinations) - 1][\"binary_image\"] = self.saved_images_list[saved_csc]\n                    self.im_combinations[len(self.im_combinations) - 1][\"converted_image\"] = np.round(self.converted_images_list[\n                        saved_csc]).astype(np.uint8)\n                    self.im_combinations[len(self.im_combinations) - 1][\"shape_number\"] = int(self.combination_features['blob_nb'].iloc[saved_csc])\n                    self.im_combinations[len(self.im_combinations) - 1]['filter_spec']= params['filter_spec']\n            self.saved_color_space_list = []\n            del self.saved_images_list\n            del self.converted_images_list\n            del self.all_combined\n\n    def save_combination_features(self, process_i: object):\n        \"\"\"\n        Saves the combination features of a given processed image.\n\n        Args:\n            process_i (object): The processed image object containing various attributes\n                such as validated_shapes, image, csc_dict, unaltered_concomp_nb,\n                shape_number, total_area, stats, bio_mask, and back_mask.\n\n            Attributes:\n                processed image object\n                    validated_shapes (array-like): The validated shapes of the processed image.\n                    image (array-like): The image data.\n                    csc_dict (dict): Color space conversion dictionary\n        \"\"\"\n        if process_i.validated_shapes.any():\n            saved_csc_nb = self.saved_csc_nb\n            self.saved_csc_nb += 1\n            self.saved_images_list.append(process_i.validated_shapes)\n            self.converted_images_list.append(bracket_to_uint8_image_contrast(process_i.greyscale))\n            self.saved_color_space_list.append(process_i.csc_dict)\n            self.combination_features.iloc[saved_csc_nb, :] = process_i.fact\n\n    def score_combination_features(self):\n        for to_minimize in ['unaltered_blob_nb', 'blob_nb', 'area_std', 'width_std', 'height_std', 'back_sum', 'out_of_arenas']:\n            values = rankdata(self.combination_features[to_minimize], method='dense')\n            self.combination_features['score'] += values.max() - values\n        for to_maximize in ['bio_sum', 'in_arenas', 'common_with_ref']:\n            values = rankdata(self.combination_features[to_maximize], method='dense') - 1\n            self.combination_features['score'] += values\n\n    def update_current_images(self, current_combination_id: int):\n        \"\"\"\n        Update the current images based on a given combination ID.\n\n        This method updates two attributes of the instance: `image` and\n        `validated_shapes`. The `image` attribute is set to the value of the key\n        \"converted_image\" from a dictionary in `im_combinations` which is\n        indexed by the provided `current_combination_id`. Similarly, the\n        `validated_shapes` attribute is set to the value of the key \"binary_image\"\n        from the same dictionary.\n\n        Args:\n            current_combination_id (int): The ID of the combination whose\n                images should be set as the current ones.\n\n        \"\"\"\n        self.image = self.im_combinations[current_combination_id][\"converted_image\"]\n        self.validated_shapes = self.im_combinations[current_combination_id][\"binary_image\"]\n\n    def network_detection(self, arenas_mask: NDArray=None, pseudopod_min_size: int=50, csc_dict: dict=None, lighter_background: bool= None, bio_mask=None, back_mask=None):\n        \"\"\"\n        Network Detection Function\n\n        Perform network detection and pseudopod analysis on an image.\n\n        Parameters\n        ----------\n        arenas_mask : NDArray, optional\n            The mask indicating the arena regions in the image.\n        pseudopod_min_size : int, optional\n            The minimum size for pseudopods to be detected.\n        csc_dict : dict, optional\n            A dictionary containing color space conversion parameters. If None,\n            defaults to {'bgr': np.array((1, 1, 1), np.int8), 'logical': 'None'}\n        lighter_background : bool, optional\n            Whether the background is lighter or not\n        bio_mask : NDArray, optional\n            The mask for biological objects in the image.\n        back_mask : NDArray, optional\n            The background mask.\n\n        Notes\n        -----\n        This function modifies the object's state by setting `self.im_combinations`\n        with the results of network detection and pseudopod analysis.\n        \"\"\"\n        logging.info(f\"Start automatic detection of network(s) in the last image\")\n        if len(self.bgr.shape) == 3:\n            if csc_dict is None:\n                csc_dict = {'bgr': np.array((1, 1, 1), np.int8), 'logical': 'None'}\n            self._get_all_color_spaces()\n            # csc_dict = translate_dict(csc_dict)\n            # self.image = combine_color_spaces(csc_dict, self.all_c_spaces)\n            first_dict, second_dict, c_spaces = split_dict(csc_dict)\n            self.image, _, _, first_pc_vector = generate_color_space_combination(self.bgr, c_spaces, first_dict, second_dict, all_c_spaces=self.all_c_spaces)\n            # if first_pc_vector is not None:\n            #     csc_dict = {\"bgr\": first_pc_vector, \"logical\": 'None'}\n        greyscale = self.image\n        NetDet = NetworkDetection(greyscale, possibly_filled_pixels=arenas_mask)\n        NetDet.get_best_network_detection_method()\n        if lighter_background is None:\n            lighter_background = True\n            if arenas_mask.any() and not arenas_mask.all():\n                lighter_background = NetDet.greyscale_image[arenas_mask &gt; 0].mean() &lt; NetDet.greyscale_image[arenas_mask == 0].mean()\n        NetDet.detect_pseudopods(lighter_background, pseudopod_min_size=pseudopod_min_size, only_one_connected_component=False)\n        NetDet.merge_network_with_pseudopods()\n        cc_efficiency_order = np.argsort(NetDet.quality_metrics)\n        self.im_combinations = []\n        for _i in cc_efficiency_order:\n            res_i = NetDet.all_results[_i]\n            self.im_combinations.append({})\n            self.im_combinations[len(self.im_combinations) - 1][\"csc\"] = csc_dict\n            self.im_combinations[len(self.im_combinations) - 1][\"converted_image\"] = bracket_to_uint8_image_contrast(res_i['filtered'])\n            self.im_combinations[len(self.im_combinations) - 1][\"binary_image\"] = res_i['binary']\n            self.im_combinations[len(self.im_combinations) - 1]['filter_spec']= {'filter1_type': res_i['filter'], 'filter1_param': [np.min(res_i['sigmas']), np.max(res_i['sigmas'])], 'filter2_type': \"\", 'filter2_param': [1., 1.]}\n            self.im_combinations[len(self.im_combinations) - 1]['rolling_window']= res_i['rolling_window']\n\n    def get_crop_coordinates(self):\n        \"\"\"\n        Get the crop coordinates for image processing.\n\n        This function projects the image on both x and y axes to detect rows\n        and columns of arenas, calculates the boundaries for cropping,\n        and determines if the arenas are zigzagged.-\n\n        \"\"\"\n        logging.info(\"Project the image on the y axis to detect rows of arenas\")\n        self.y_boundaries, y_max_sum = self.projection_to_get_peaks_boundaries(axis=1)\n        logging.info(\"Project the image on the x axis to detect columns of arenas\")\n        self.x_boundaries, x_max_sum = self.projection_to_get_peaks_boundaries(axis=0)\n        logging.info(\"Get crop coordinates using the get_crop_coordinates method of OneImageAnalysis class\")\n        row_number = len(np.nonzero(self.y_boundaries)[0]) // 2\n        col_number = len(np.nonzero(self.x_boundaries)[0]) // 2\n        are_zigzag = None\n        if col_number &gt; 0 and row_number &gt; 0:\n            if (x_max_sum / col_number) * 2 &lt; (y_max_sum / row_number):\n                are_zigzag = \"columns\"\n            elif (x_max_sum / col_number) &gt; (y_max_sum / row_number) * 2:\n                are_zigzag = \"rows\"\n        # here automatically determine if are zigzag\n        x_boundary_number = (self.x_boundaries == 1).sum()\n        if x_boundary_number &gt; 1:\n            if x_boundary_number &lt; 4:\n                x_interval = np.absolute(np.max(np.diff(np.where(self.x_boundaries == 1)[0]))) // 2\n            else:\n                if are_zigzag == \"columns\":\n                    x_interval = np.absolute(np.max(np.diff(np.where(self.x_boundaries == 1)[0][::2]))) // 2\n                else:\n                    x_interval = np.absolute(np.max(np.diff(np.where(self.x_boundaries == 1)[0]))) // 2\n            cx_min = np.where(self.x_boundaries == - 1)[0][0] - x_interval.astype(int)\n            cx_max = np.where(self.x_boundaries == 1)[0][col_number - 1] + x_interval.astype(int)\n            if cx_min &lt; 0: cx_min = 0\n            if cx_max &gt; len(self.x_boundaries): cx_max = len(self.x_boundaries) - 1\n        else:\n            cx_min = 0\n            cx_max = len(self.x_boundaries)# - 1\n\n        y_boundary_number = (self.y_boundaries == 1).sum()\n        if y_boundary_number &gt; 1:\n            if y_boundary_number &lt; 4:\n                y_interval = np.absolute(np.max(np.diff(np.where(self.y_boundaries == 1)[0]))) // 2\n            else:\n                if are_zigzag == \"rows\":\n                    y_interval = np.absolute(np.max(np.diff(np.where(self.y_boundaries == 1)[0][::2]))) // 2\n                else:\n                    y_interval = np.absolute(np.max(np.diff(np.where(self.y_boundaries == 1)[0]))) // 2\n            cy_min = np.where(self.y_boundaries == - 1)[0][0] - y_interval.astype(int)\n            cy_max = np.where(self.y_boundaries == 1)[0][row_number - 1] + y_interval.astype(int)\n            if cy_min &lt; 0: cy_min = 0\n            if cy_max &gt; len(self.y_boundaries): cy_max = len(self.y_boundaries) - 1\n        else:\n            cy_min = 0\n            cy_max = len(self.y_boundaries)# - 1\n\n        self.crop_coord = [cy_min, cy_max, cx_min, cx_max]\n\n    def projection_to_get_peaks_boundaries(self, axis: int) -&gt; Tuple[NDArray, int]:\n        \"\"\"\n\n        Projection to get peaks' boundaries.\n\n        Calculate the projection of an array along a specified axis and\n        identify the boundaries of non-zero peaks.\n\n        Args:\n            axis: int,\n                The axis along which to calculate the projection and identify\n                peaks' boundaries.\n\n        Returns:\n            Tuple[NDArray, int]:\n                A tuple containing two elements: an array representing the slopes\n                of peaks' boundaries and an integer representing the maximum sum\n                along the specified axis.\n\n        \"\"\"\n        sums = np.sum(self.validated_shapes, axis)\n        slopes = np.greater(sums, 0)\n        slopes = np.append(0, np.diff(slopes))\n        coord = np.nonzero(slopes)[0]\n        for ci in np.arange(len(coord)):\n            if ci % 2 == 0:\n                slopes[coord[ci]] = - 1\n        return slopes, sums.max()\n\n    def automatically_crop(self, crop_coord):\n        \"\"\"\n        Automatically crops the image using the given crop coordinates.\n\n        This method crops various attributes of the image such as the main image,\n        binary image, and color spaces. It also updates internal states related to\n        cropping.\n\n        Args:\n            crop_coord (tuple): The coordinates for cropping in the format\n                (start_y, end_y, start_x, end_x), representing the bounding box region\n                to crop from the image.\n\n        \"\"\"\n        if not self.cropped and crop_coord is not None:\n            logging.info(\"Crop using the automatically_crop method of OneImageAnalysis class\")\n            self.cropped = True\n            self.image = self.image[crop_coord[0]:crop_coord[1], crop_coord[2]:crop_coord[3], ...]\n            self.bgr = deepcopy(self.bgr[crop_coord[0]:crop_coord[1], crop_coord[2]:crop_coord[3], ...])\n            self._get_all_color_spaces()\n            if self.im_combinations is not None:\n                for i in np.arange(len(self.im_combinations)):\n                    self.im_combinations[i][\"binary_image\"] = self.im_combinations[i][\"binary_image\"][crop_coord[0]:crop_coord[1], crop_coord[2]:crop_coord[3]]\n                    self.im_combinations[i][\"converted_image\"] = self.im_combinations[i][\"converted_image\"][crop_coord[0]:crop_coord[1], crop_coord[2]:crop_coord[3]]\n            self.binary_image = self.binary_image[crop_coord[0]:crop_coord[1], crop_coord[2]:crop_coord[3]]\n            if self.greyscale is not None:\n                self.greyscale = self.greyscale[crop_coord[0]:crop_coord[1], crop_coord[2]:crop_coord[3], ...]\n            if self.greyscale2 is not None:\n                self.greyscale2 = self.greyscale2[crop_coord[0]:crop_coord[1], crop_coord[2]:crop_coord[3], ...]\n            if self.image2 is not None:\n                self.image2 = self.image2[crop_coord[0]:crop_coord[1], crop_coord[2]:crop_coord[3], ...]\n            if self.binary_image2 is not None:\n                self.binary_image2 = self.binary_image2[crop_coord[0]:crop_coord[1], crop_coord[2]:crop_coord[3], ...]\n            if self.subtract_background is not None:\n                self.subtract_background = self.subtract_background[crop_coord[0]:crop_coord[1], crop_coord[2]:crop_coord[3], ...]\n            if self.subtract_background2 is not None:\n                self.subtract_background2 = self.subtract_background2[crop_coord[0]:crop_coord[1], crop_coord[2]:crop_coord[3], ...]\n            self.validated_shapes = self.validated_shapes[crop_coord[0]:crop_coord[1], crop_coord[2]:crop_coord[3]]\n\n            self.y_boundaries, y_max_sum = self.projection_to_get_peaks_boundaries(axis=1)\n            self.x_boundaries, x_max_sum = self.projection_to_get_peaks_boundaries(axis=0)\n</code></pre>"},{"location":"api/cellects/core/one_image_analysis/#cellects.core.one_image_analysis.OneImageAnalysis.adjust_to_drift_correction","title":"<code>adjust_to_drift_correction(logical)</code>","text":"<p>Adjust the image and binary image to correct for drift.</p> <p>This method applies a drift correction by dilating the binary image, calculating the mean value of the drifted region and applying it back to the image. After this, it applies Otsu's thresholding method to determine a new binary image and adjusts the second image if present. The logical operation specified is then applied to the binary images.</p> <p>Args:     logical (str): Logical operation ('Or', 'And', 'Xor') to apply to the binary         images.</p> Source code in <code>src/cellects/core/one_image_analysis.py</code> <pre><code>def adjust_to_drift_correction(self, logical: str):\n    \"\"\"\n    Adjust the image and binary image to correct for drift.\n\n    This method applies a drift correction by dilating the binary image, calculating\n    the mean value of the drifted region and applying it back to the image. After this,\n    it applies Otsu's thresholding method to determine a new binary image and adjusts\n    the second image if present. The logical operation specified is then applied to the\n    binary images.\n\n    Args:\n        logical (str): Logical operation ('Or', 'And', 'Xor') to apply to the binary\n            images.\"\"\"\n    if not self.drift_correction_already_adjusted:\n        self.drift_correction_already_adjusted = True\n\n        mask = cv2.dilate(self.binary_image, kernel=cross_33)\n        mask -= self.binary_image\n        mask = np.nonzero(mask)\n        drift_correction = np.mean(self.image[mask[0], mask[1]])\n        self.image[np.nonzero(self.binary_image)] = drift_correction\n        threshold = get_otsu_threshold(self.image)\n        binary = (self.image &gt; threshold)\n        self.binary_image = binary.astype(np.uint8)\n\n        if self.image2 is not None:\n            drift_correction2 = np.mean(self.image2[mask[0], mask[1]])\n            self.image2[np.nonzero(self.binary_image)] = drift_correction2\n            threshold = get_otsu_threshold(self.image2)\n            binary1 = (self.image2 &gt; threshold)\n            binary2 = np.logical_not(binary1)\n            if binary1.sum() &lt; binary2.sum():\n                binary = binary1\n            else:\n                binary = binary2\n            while np.any(binary * self.binary_image2) and threshold &gt; 1:\n                threshold -= 1\n                binary1 = (self.image2 &gt; threshold)\n                binary2 = np.logical_not(binary1)\n                if binary1.sum() &lt; binary2.sum():\n                    binary = binary1\n                else:\n                    binary = binary2\n            self.binary_image2 = binary.astype(np.uint8)\n            if logical == 'Or':\n                self.binary_image = np.logical_or(self.binary_image, self.binary_image2)\n            elif logical == 'And':\n                self.binary_image = np.logical_and(self.binary_image, self.binary_image2)\n            elif logical == 'Xor':\n                self.binary_image = np.logical_xor(self.binary_image, self.binary_image2)\n            self.binary_image = self.binary_image.astype(np.uint8)\n</code></pre>"},{"location":"api/cellects/core/one_image_analysis/#cellects.core.one_image_analysis.OneImageAnalysis.automatically_crop","title":"<code>automatically_crop(crop_coord)</code>","text":"<p>Automatically crops the image using the given crop coordinates.</p> <p>This method crops various attributes of the image such as the main image, binary image, and color spaces. It also updates internal states related to cropping.</p> <p>Args:     crop_coord (tuple): The coordinates for cropping in the format         (start_y, end_y, start_x, end_x), representing the bounding box region         to crop from the image.</p> Source code in <code>src/cellects/core/one_image_analysis.py</code> <pre><code>def automatically_crop(self, crop_coord):\n    \"\"\"\n    Automatically crops the image using the given crop coordinates.\n\n    This method crops various attributes of the image such as the main image,\n    binary image, and color spaces. It also updates internal states related to\n    cropping.\n\n    Args:\n        crop_coord (tuple): The coordinates for cropping in the format\n            (start_y, end_y, start_x, end_x), representing the bounding box region\n            to crop from the image.\n\n    \"\"\"\n    if not self.cropped and crop_coord is not None:\n        logging.info(\"Crop using the automatically_crop method of OneImageAnalysis class\")\n        self.cropped = True\n        self.image = self.image[crop_coord[0]:crop_coord[1], crop_coord[2]:crop_coord[3], ...]\n        self.bgr = deepcopy(self.bgr[crop_coord[0]:crop_coord[1], crop_coord[2]:crop_coord[3], ...])\n        self._get_all_color_spaces()\n        if self.im_combinations is not None:\n            for i in np.arange(len(self.im_combinations)):\n                self.im_combinations[i][\"binary_image\"] = self.im_combinations[i][\"binary_image\"][crop_coord[0]:crop_coord[1], crop_coord[2]:crop_coord[3]]\n                self.im_combinations[i][\"converted_image\"] = self.im_combinations[i][\"converted_image\"][crop_coord[0]:crop_coord[1], crop_coord[2]:crop_coord[3]]\n        self.binary_image = self.binary_image[crop_coord[0]:crop_coord[1], crop_coord[2]:crop_coord[3]]\n        if self.greyscale is not None:\n            self.greyscale = self.greyscale[crop_coord[0]:crop_coord[1], crop_coord[2]:crop_coord[3], ...]\n        if self.greyscale2 is not None:\n            self.greyscale2 = self.greyscale2[crop_coord[0]:crop_coord[1], crop_coord[2]:crop_coord[3], ...]\n        if self.image2 is not None:\n            self.image2 = self.image2[crop_coord[0]:crop_coord[1], crop_coord[2]:crop_coord[3], ...]\n        if self.binary_image2 is not None:\n            self.binary_image2 = self.binary_image2[crop_coord[0]:crop_coord[1], crop_coord[2]:crop_coord[3], ...]\n        if self.subtract_background is not None:\n            self.subtract_background = self.subtract_background[crop_coord[0]:crop_coord[1], crop_coord[2]:crop_coord[3], ...]\n        if self.subtract_background2 is not None:\n            self.subtract_background2 = self.subtract_background2[crop_coord[0]:crop_coord[1], crop_coord[2]:crop_coord[3], ...]\n        self.validated_shapes = self.validated_shapes[crop_coord[0]:crop_coord[1], crop_coord[2]:crop_coord[3]]\n\n        self.y_boundaries, y_max_sum = self.projection_to_get_peaks_boundaries(axis=1)\n        self.x_boundaries, x_max_sum = self.projection_to_get_peaks_boundaries(axis=0)\n</code></pre>"},{"location":"api/cellects/core/one_image_analysis/#cellects.core.one_image_analysis.OneImageAnalysis.check_if_image_border_attest_drift_correction","title":"<code>check_if_image_border_attest_drift_correction()</code>","text":"<p>Check if the given binary image requires border attenuation and drift correction.</p> <p>In order to determine the need for border attenuation or drift correction, this function evaluates the borders of a binary image. If any two opposite borders are fully black, it assumes that there is an issue requiring correction.</p> <p>Returns:     bool: True if border attenuation or drift correction is required, False otherwise.</p> Source code in <code>src/cellects/core/one_image_analysis.py</code> <pre><code>def check_if_image_border_attest_drift_correction(self) -&gt; bool:\n    \"\"\"\n    Check if the given binary image requires border attenuation and drift correction.\n\n    In order to determine the need for border attenuation or drift correction, this function\n    evaluates the borders of a binary image. If any two opposite borders are fully black,\n    it assumes that there is an issue requiring correction.\n\n    Returns:\n        bool: True if border attenuation or drift correction is required, False otherwise.\n\n    \"\"\"\n    t = np.all(self.binary_image[0, :])\n    b = np.all(self.binary_image[-1, :])\n    l = np.all(self.binary_image[:, 0])\n    r = np.all(self.binary_image[:, -1])\n    self.drift_mask_coord = None\n    if (t and b) or (t and r) or (t and l) or (t and r) or (b and l) or (b and r) or (l and r):\n        cc_nb, shapes = cv2.connectedComponents(self.binary_image)\n        if cc_nb &gt; 1:\n            if cc_nb == 2:\n                drift_mask_coord = np.nonzero(1 - self.binary_image)\n            else:\n                back = np.unique(np.concatenate((shapes[0, :], shapes[-1, :],  shapes[:, 0], shapes[:, -1]), axis=0))\n                drift_mask_coord = np.nonzero(np.logical_or(1 - self.binary_image, 1 - np.isin(shapes, back[back != 0])))\n            drift_mask_coord = (np.min(drift_mask_coord[0]), np.max(drift_mask_coord[0]) + 1,\n                                np.min(drift_mask_coord[1]), np.max(drift_mask_coord[1]) + 1)\n            self.drift_mask_coord = drift_mask_coord\n            return True\n        else:\n            return False\n    else:\n        return False\n</code></pre>"},{"location":"api/cellects/core/one_image_analysis/#cellects.core.one_image_analysis.OneImageAnalysis.convert_and_segment","title":"<code>convert_and_segment(c_space_dict, color_number=2, bio_mask=None, back_mask=None, subtract_background=None, subtract_background2=None, rolling_window_segmentation=None, lighter_background=None, allowed_window=None, filter_spec=None)</code>","text":"<p>Convert an image to grayscale and segment it based on specified parameters.</p> <p>This method converts the given color space dictionary into grayscale images, combines them with existing color spaces and performs segmentation. It has special handling for images that are already in grayscale.</p> <p>Args:</p> <ul> <li><code>c_space_dict</code> (dict): Dictionary containing color spaces.</li> <li><code>color_number</code> (int, optional): Number of colors to use in segmentation. Defaults to 2.</li> <li><code>bio_mask</code> (NDArray[np.uint8], optional): Biomask for segmentation. Defaults to None.</li> <li><code>back_mask</code> (NDArray[np.uint8], optional): Backmask for segmentation. Defaults to None.</li> <li><code>subtract_background</code> (NDArray, optional): Background to subtract. Defaults to None.</li> <li><code>subtract_background2</code> (NDArray, optional): Second background to subtract. Defaults to None.</li> <li>rolling_window_segmentation (dict, optional): Flag for grid segmentation. Defaults to None.</li> <li><code>lighter_background</code> (bool, optional): Flag for lighter background. Defaults to None.</li> <li><code>mask</code> (NDArray, optional): Additional mask for segmentation. Defaults to None.</li> <li><code>filter_spec</code> (dict, optional): Filter specifications. Defaults to None.</li> </ul> <p>Attributes:</p> <ul> <li><code>self.already_greyscale</code> (bool): Indicates whether the image is already greyscale.</li> <li><code>self.all_c_spaces</code> (list): List of color spaces.</li> </ul> Source code in <code>src/cellects/core/one_image_analysis.py</code> <pre><code>def convert_and_segment(self, c_space_dict: dict, color_number=2, bio_mask: NDArray[np.uint8]=None,\n                        back_mask: NDArray[np.uint8]=None, subtract_background: NDArray=None,\n                        subtract_background2: NDArray=None, rolling_window_segmentation: dict=None,\n                        lighter_background: bool=None,\n                        allowed_window: NDArray=None, filter_spec: dict=None):\n    \"\"\"\n    Convert an image to grayscale and segment it based on specified parameters.\n\n    This method converts the given color space dictionary into grayscale\n    images, combines them with existing color spaces and performs segmentation.\n    It has special handling for images that are already in grayscale.\n\n    **Args:**\n\n    - `c_space_dict` (dict): Dictionary containing color spaces.\n    - `color_number` (int, optional): Number of colors to use in segmentation. Defaults to 2.\n    - `bio_mask` (NDArray[np.uint8], optional): Biomask for segmentation. Defaults to None.\n    - `back_mask` (NDArray[np.uint8], optional): Backmask for segmentation. Defaults to None.\n    - `subtract_background` (NDArray, optional): Background to subtract. Defaults to None.\n    - `subtract_background2` (NDArray, optional): Second background to subtract. Defaults to None.\n    - rolling_window_segmentation (dict, optional): Flag for grid segmentation. Defaults to None.\n    - `lighter_background` (bool, optional): Flag for lighter background. Defaults to None.\n    - `mask` (NDArray, optional): Additional mask for segmentation. Defaults to None.\n    - `filter_spec` (dict, optional): Filter specifications. Defaults to None.\n\n    **Attributes:**\n\n    - `self.already_greyscale` (bool): Indicates whether the image is already greyscale.\n    - `self.all_c_spaces` (list): List of color spaces.\n\n    \"\"\"\n    if not self.already_greyscale:\n        first_dict, second_dict, c_spaces = split_dict(c_space_dict)\n        self.image, self.image2, all_c_spaces, self.first_pc_vector = generate_color_space_combination(self.bgr, c_spaces, first_dict, second_dict, subtract_background, subtract_background2)\n        if len(all_c_spaces) &gt; len(self.all_c_spaces):\n            self.all_c_spaces = all_c_spaces\n\n    self.segmentation(logical=c_space_dict['logical'], color_number=color_number, bio_mask=bio_mask,\n                      back_mask=back_mask, rolling_window_segmentation=rolling_window_segmentation,\n                      lighter_background=lighter_background, allowed_window=allowed_window, filter_spec=filter_spec)\n</code></pre>"},{"location":"api/cellects/core/one_image_analysis/#cellects.core.one_image_analysis.OneImageAnalysis.generate_subtract_background","title":"<code>generate_subtract_background(c_space_dict, drift_corrected=False)</code>","text":"<p>Generate a background-subtracted image using specified color space dictionary.</p> <p>This method first checks if color spaces have already been generated or if the image is greyscale. If not, it generates color spaces from the BGR image. It then converts and segments the image using the provided color space dictionary without grid segmentation. A disk-shaped structuring element is created and used to perform a morphological opening operation on the image, resulting in a background-subtracted version. If there is a second image (see Also: image2), the same operation is performed on it.</p> <p>Args:     c_space_dict (dict): Dictionary containing color space specifications         for the segmentation process.</p> <p>Attributes:     disk_size: Radius of the disk-shaped structuring element         used for morphological operations, calculated based on image dimensions.     subtract_background: Background-subtracted version of <code>image</code> obtained         after morphological operations with the disk-shaped structuring element.     subtract_background2: Background-subtracted version of <code>image2</code> obtained         after morphological operations with the disk-shaped structuring element,         if <code>image2</code> is present.</p> Source code in <code>src/cellects/core/one_image_analysis.py</code> <pre><code>def generate_subtract_background(self, c_space_dict: dict, drift_corrected: bool=False):\n    \"\"\"\n    Generate a background-subtracted image using specified color space dictionary.\n\n    This method first checks if color spaces have already been generated or\n    if the image is greyscale. If not, it generates color spaces from the BGR\n    image. It then converts and segments the image using the provided color space\n    dictionary without grid segmentation. A disk-shaped structuring element is\n    created and used to perform a morphological opening operation on the image,\n    resulting in a background-subtracted version. If there is a second image\n    (see Also: image2), the same operation is performed on it.\n\n    Args:\n        c_space_dict (dict): Dictionary containing color space specifications\n            for the segmentation process.\n\n    Attributes:\n        disk_size: Radius of the disk-shaped structuring element\n            used for morphological operations, calculated based on image dimensions.\n        subtract_background: Background-subtracted version of `image` obtained\n            after morphological operations with the disk-shaped structuring element.\n        subtract_background2: Background-subtracted version of `image2` obtained\n            after morphological operations with the disk-shaped structuring element,\n            if `image2` is present.\"\"\"\n    logging.info(\"Generate background using the generate_subtract_background method of OneImageAnalysis class\")\n    self._get_all_color_spaces()\n    if drift_corrected:\n        # self.adjust_to_drift_correction(c_space_dict['logical'])\n        self.check_if_image_border_attest_drift_correction()\n    self.convert_and_segment(c_space_dict, rolling_window_segmentation=None, allowed_window=self.drift_mask_coord)\n    disk_size = int(np.floor(np.sqrt(np.min(self.bgr.shape[:2])) / 2))\n    disk = create_ellipse(disk_size, disk_size, min_size=3).astype(np.uint8)\n    self.subtract_background = cv2.morphologyEx(self.image, cv2.MORPH_OPEN, disk)\n    if self.image2 is not None:\n        self.subtract_background2 = cv2.morphologyEx(self.image2, cv2.MORPH_OPEN, disk)\n</code></pre>"},{"location":"api/cellects/core/one_image_analysis/#cellects.core.one_image_analysis.OneImageAnalysis.get_crop_coordinates","title":"<code>get_crop_coordinates()</code>","text":"<p>Get the crop coordinates for image processing.</p> <p>This function projects the image on both x and y axes to detect rows and columns of arenas, calculates the boundaries for cropping, and determines if the arenas are zigzagged.-</p> Source code in <code>src/cellects/core/one_image_analysis.py</code> <pre><code>def get_crop_coordinates(self):\n    \"\"\"\n    Get the crop coordinates for image processing.\n\n    This function projects the image on both x and y axes to detect rows\n    and columns of arenas, calculates the boundaries for cropping,\n    and determines if the arenas are zigzagged.-\n\n    \"\"\"\n    logging.info(\"Project the image on the y axis to detect rows of arenas\")\n    self.y_boundaries, y_max_sum = self.projection_to_get_peaks_boundaries(axis=1)\n    logging.info(\"Project the image on the x axis to detect columns of arenas\")\n    self.x_boundaries, x_max_sum = self.projection_to_get_peaks_boundaries(axis=0)\n    logging.info(\"Get crop coordinates using the get_crop_coordinates method of OneImageAnalysis class\")\n    row_number = len(np.nonzero(self.y_boundaries)[0]) // 2\n    col_number = len(np.nonzero(self.x_boundaries)[0]) // 2\n    are_zigzag = None\n    if col_number &gt; 0 and row_number &gt; 0:\n        if (x_max_sum / col_number) * 2 &lt; (y_max_sum / row_number):\n            are_zigzag = \"columns\"\n        elif (x_max_sum / col_number) &gt; (y_max_sum / row_number) * 2:\n            are_zigzag = \"rows\"\n    # here automatically determine if are zigzag\n    x_boundary_number = (self.x_boundaries == 1).sum()\n    if x_boundary_number &gt; 1:\n        if x_boundary_number &lt; 4:\n            x_interval = np.absolute(np.max(np.diff(np.where(self.x_boundaries == 1)[0]))) // 2\n        else:\n            if are_zigzag == \"columns\":\n                x_interval = np.absolute(np.max(np.diff(np.where(self.x_boundaries == 1)[0][::2]))) // 2\n            else:\n                x_interval = np.absolute(np.max(np.diff(np.where(self.x_boundaries == 1)[0]))) // 2\n        cx_min = np.where(self.x_boundaries == - 1)[0][0] - x_interval.astype(int)\n        cx_max = np.where(self.x_boundaries == 1)[0][col_number - 1] + x_interval.astype(int)\n        if cx_min &lt; 0: cx_min = 0\n        if cx_max &gt; len(self.x_boundaries): cx_max = len(self.x_boundaries) - 1\n    else:\n        cx_min = 0\n        cx_max = len(self.x_boundaries)# - 1\n\n    y_boundary_number = (self.y_boundaries == 1).sum()\n    if y_boundary_number &gt; 1:\n        if y_boundary_number &lt; 4:\n            y_interval = np.absolute(np.max(np.diff(np.where(self.y_boundaries == 1)[0]))) // 2\n        else:\n            if are_zigzag == \"rows\":\n                y_interval = np.absolute(np.max(np.diff(np.where(self.y_boundaries == 1)[0][::2]))) // 2\n            else:\n                y_interval = np.absolute(np.max(np.diff(np.where(self.y_boundaries == 1)[0]))) // 2\n        cy_min = np.where(self.y_boundaries == - 1)[0][0] - y_interval.astype(int)\n        cy_max = np.where(self.y_boundaries == 1)[0][row_number - 1] + y_interval.astype(int)\n        if cy_min &lt; 0: cy_min = 0\n        if cy_max &gt; len(self.y_boundaries): cy_max = len(self.y_boundaries) - 1\n    else:\n        cy_min = 0\n        cy_max = len(self.y_boundaries)# - 1\n\n    self.crop_coord = [cy_min, cy_max, cx_min, cx_max]\n</code></pre>"},{"location":"api/cellects/core/one_image_analysis/#cellects.core.one_image_analysis.OneImageAnalysis.network_detection","title":"<code>network_detection(arenas_mask=None, pseudopod_min_size=50, csc_dict=None, lighter_background=None, bio_mask=None, back_mask=None)</code>","text":"<p>Network Detection Function</p> <p>Perform network detection and pseudopod analysis on an image.</p> <p>Parameters:</p> Name Type Description Default <code>arenas_mask</code> <code>NDArray</code> <p>The mask indicating the arena regions in the image.</p> <code>None</code> <code>pseudopod_min_size</code> <code>int</code> <p>The minimum size for pseudopods to be detected.</p> <code>50</code> <code>csc_dict</code> <code>dict</code> <p>A dictionary containing color space conversion parameters. If None, defaults to {'bgr': np.array((1, 1, 1), np.int8), 'logical': 'None'}</p> <code>None</code> <code>lighter_background</code> <code>bool</code> <p>Whether the background is lighter or not</p> <code>None</code> <code>bio_mask</code> <code>NDArray</code> <p>The mask for biological objects in the image.</p> <code>None</code> <code>back_mask</code> <code>NDArray</code> <p>The background mask.</p> <code>None</code> Notes <p>This function modifies the object's state by setting <code>self.im_combinations</code> with the results of network detection and pseudopod analysis.</p> Source code in <code>src/cellects/core/one_image_analysis.py</code> <pre><code>def network_detection(self, arenas_mask: NDArray=None, pseudopod_min_size: int=50, csc_dict: dict=None, lighter_background: bool= None, bio_mask=None, back_mask=None):\n    \"\"\"\n    Network Detection Function\n\n    Perform network detection and pseudopod analysis on an image.\n\n    Parameters\n    ----------\n    arenas_mask : NDArray, optional\n        The mask indicating the arena regions in the image.\n    pseudopod_min_size : int, optional\n        The minimum size for pseudopods to be detected.\n    csc_dict : dict, optional\n        A dictionary containing color space conversion parameters. If None,\n        defaults to {'bgr': np.array((1, 1, 1), np.int8), 'logical': 'None'}\n    lighter_background : bool, optional\n        Whether the background is lighter or not\n    bio_mask : NDArray, optional\n        The mask for biological objects in the image.\n    back_mask : NDArray, optional\n        The background mask.\n\n    Notes\n    -----\n    This function modifies the object's state by setting `self.im_combinations`\n    with the results of network detection and pseudopod analysis.\n    \"\"\"\n    logging.info(f\"Start automatic detection of network(s) in the last image\")\n    if len(self.bgr.shape) == 3:\n        if csc_dict is None:\n            csc_dict = {'bgr': np.array((1, 1, 1), np.int8), 'logical': 'None'}\n        self._get_all_color_spaces()\n        # csc_dict = translate_dict(csc_dict)\n        # self.image = combine_color_spaces(csc_dict, self.all_c_spaces)\n        first_dict, second_dict, c_spaces = split_dict(csc_dict)\n        self.image, _, _, first_pc_vector = generate_color_space_combination(self.bgr, c_spaces, first_dict, second_dict, all_c_spaces=self.all_c_spaces)\n        # if first_pc_vector is not None:\n        #     csc_dict = {\"bgr\": first_pc_vector, \"logical\": 'None'}\n    greyscale = self.image\n    NetDet = NetworkDetection(greyscale, possibly_filled_pixels=arenas_mask)\n    NetDet.get_best_network_detection_method()\n    if lighter_background is None:\n        lighter_background = True\n        if arenas_mask.any() and not arenas_mask.all():\n            lighter_background = NetDet.greyscale_image[arenas_mask &gt; 0].mean() &lt; NetDet.greyscale_image[arenas_mask == 0].mean()\n    NetDet.detect_pseudopods(lighter_background, pseudopod_min_size=pseudopod_min_size, only_one_connected_component=False)\n    NetDet.merge_network_with_pseudopods()\n    cc_efficiency_order = np.argsort(NetDet.quality_metrics)\n    self.im_combinations = []\n    for _i in cc_efficiency_order:\n        res_i = NetDet.all_results[_i]\n        self.im_combinations.append({})\n        self.im_combinations[len(self.im_combinations) - 1][\"csc\"] = csc_dict\n        self.im_combinations[len(self.im_combinations) - 1][\"converted_image\"] = bracket_to_uint8_image_contrast(res_i['filtered'])\n        self.im_combinations[len(self.im_combinations) - 1][\"binary_image\"] = res_i['binary']\n        self.im_combinations[len(self.im_combinations) - 1]['filter_spec']= {'filter1_type': res_i['filter'], 'filter1_param': [np.min(res_i['sigmas']), np.max(res_i['sigmas'])], 'filter2_type': \"\", 'filter2_param': [1., 1.]}\n        self.im_combinations[len(self.im_combinations) - 1]['rolling_window']= res_i['rolling_window']\n</code></pre>"},{"location":"api/cellects/core/one_image_analysis/#cellects.core.one_image_analysis.OneImageAnalysis.projection_to_get_peaks_boundaries","title":"<code>projection_to_get_peaks_boundaries(axis)</code>","text":"<p>Projection to get peaks' boundaries.</p> <p>Calculate the projection of an array along a specified axis and identify the boundaries of non-zero peaks.</p> <p>Args:     axis: int,         The axis along which to calculate the projection and identify         peaks' boundaries.</p> <p>Returns:     Tuple[NDArray, int]:         A tuple containing two elements: an array representing the slopes         of peaks' boundaries and an integer representing the maximum sum         along the specified axis.</p> Source code in <code>src/cellects/core/one_image_analysis.py</code> <pre><code>def projection_to_get_peaks_boundaries(self, axis: int) -&gt; Tuple[NDArray, int]:\n    \"\"\"\n\n    Projection to get peaks' boundaries.\n\n    Calculate the projection of an array along a specified axis and\n    identify the boundaries of non-zero peaks.\n\n    Args:\n        axis: int,\n            The axis along which to calculate the projection and identify\n            peaks' boundaries.\n\n    Returns:\n        Tuple[NDArray, int]:\n            A tuple containing two elements: an array representing the slopes\n            of peaks' boundaries and an integer representing the maximum sum\n            along the specified axis.\n\n    \"\"\"\n    sums = np.sum(self.validated_shapes, axis)\n    slopes = np.greater(sums, 0)\n    slopes = np.append(0, np.diff(slopes))\n    coord = np.nonzero(slopes)[0]\n    for ci in np.arange(len(coord)):\n        if ci % 2 == 0:\n            slopes[coord[ci]] = - 1\n    return slopes, sums.max()\n</code></pre>"},{"location":"api/cellects/core/one_image_analysis/#cellects.core.one_image_analysis.OneImageAnalysis.save_combination_features","title":"<code>save_combination_features(process_i)</code>","text":"<p>Saves the combination features of a given processed image.</p> <p>Args:     process_i (object): The processed image object containing various attributes         such as validated_shapes, image, csc_dict, unaltered_concomp_nb,         shape_number, total_area, stats, bio_mask, and back_mask.</p> <pre><code>Attributes:\n    processed image object\n        validated_shapes (array-like): The validated shapes of the processed image.\n        image (array-like): The image data.\n        csc_dict (dict): Color space conversion dictionary\n</code></pre> Source code in <code>src/cellects/core/one_image_analysis.py</code> <pre><code>def save_combination_features(self, process_i: object):\n    \"\"\"\n    Saves the combination features of a given processed image.\n\n    Args:\n        process_i (object): The processed image object containing various attributes\n            such as validated_shapes, image, csc_dict, unaltered_concomp_nb,\n            shape_number, total_area, stats, bio_mask, and back_mask.\n\n        Attributes:\n            processed image object\n                validated_shapes (array-like): The validated shapes of the processed image.\n                image (array-like): The image data.\n                csc_dict (dict): Color space conversion dictionary\n    \"\"\"\n    if process_i.validated_shapes.any():\n        saved_csc_nb = self.saved_csc_nb\n        self.saved_csc_nb += 1\n        self.saved_images_list.append(process_i.validated_shapes)\n        self.converted_images_list.append(bracket_to_uint8_image_contrast(process_i.greyscale))\n        self.saved_color_space_list.append(process_i.csc_dict)\n        self.combination_features.iloc[saved_csc_nb, :] = process_i.fact\n</code></pre>"},{"location":"api/cellects/core/one_image_analysis/#cellects.core.one_image_analysis.OneImageAnalysis.segmentation","title":"<code>segmentation(logical='None', color_number=2, bio_mask=None, back_mask=None, bio_label=None, bio_label2=None, rolling_window_segmentation=None, lighter_background=None, allowed_window=None, filter_spec=None)</code>","text":"<p>Implement segmentation on the image using various methods and parameters.</p> <p>Args:     logical (str): Logical operation to perform between two binary images.                    Options are 'Or', 'And', 'Xor'. Default is 'None'.     color_number (int): Number of colors to use in segmentation. Must be greater than 2                         for kmeans clustering. Default is 2.     bio_mask (NDArray[np.uint8]): Binary mask for biological areas. Default is None.     back_mask (NDArray[np.uint8]): Binary mask for background areas. Default is None.     bio_label (Any): Label for biological features. Default is None.     bio_label2 (Any): Secondary label for biological features. Default is None.     rolling_window_segmentation (dict): Whether to perform grid segmentation. Default is None.     lighter_background (bool): Indicates if the background is lighter than objects.                                Default is None.     allowed_window (Tuple): Mask to apply during segmentation. Default is None.     filter_spec (dict): Dictionary of filters to apply on the image before segmentation.</p> Source code in <code>src/cellects/core/one_image_analysis.py</code> <pre><code>def segmentation(self, logical: str='None', color_number: int=2, bio_mask: NDArray[np.uint8]=None,\n                 back_mask: NDArray[np.uint8]=None, bio_label=None, bio_label2=None,\n                 rolling_window_segmentation: dict=None, lighter_background: bool=None, allowed_window: Tuple=None,\n                 filter_spec: dict=None):\n    \"\"\"\n    Implement segmentation on the image using various methods and parameters.\n\n    Args:\n        logical (str): Logical operation to perform between two binary images.\n                       Options are 'Or', 'And', 'Xor'. Default is 'None'.\n        color_number (int): Number of colors to use in segmentation. Must be greater than 2\n                            for kmeans clustering. Default is 2.\n        bio_mask (NDArray[np.uint8]): Binary mask for biological areas. Default is None.\n        back_mask (NDArray[np.uint8]): Binary mask for background areas. Default is None.\n        bio_label (Any): Label for biological features. Default is None.\n        bio_label2 (Any): Secondary label for biological features. Default is None.\n        rolling_window_segmentation (dict): Whether to perform grid segmentation. Default is None.\n        lighter_background (bool): Indicates if the background is lighter than objects.\n                                   Default is None.\n        allowed_window (Tuple): Mask to apply during segmentation. Default is None.\n        filter_spec (dict): Dictionary of filters to apply on the image before segmentation.\n\n    \"\"\"\n    # 1. Check valid pixels for segmentation (e.g. when there is a drift correction)\n    if allowed_window is None:\n        min_y, max_y, min_x, max_x = 0, self.image.shape[0] + 1, 0, self.image.shape[1] + 1\n    else:\n        min_y, max_y, min_x, max_x = allowed_window\n    greyscale = self.image[min_y:max_y, min_x:max_x].copy()\n    # 2. Apply filter on the greyscale images\n    if filter_spec is not None and filter_spec[\"filter1_type\"] != \"\":\n        greyscale = apply_filter(greyscale, filter_spec[\"filter1_type\"], filter_spec[\"filter1_param\"])\n\n    greyscale2 = None\n    if logical != 'None':\n        greyscale2 = self.image2[min_y:max_y, min_x:max_x].copy()\n        if filter_spec is not None and filter_spec[\"filter2_type\"] != \"\":\n            greyscale2 = apply_filter(greyscale2, filter_spec[\"filter2_type\"], filter_spec[\"filter2_param\"])\n\n    # 3. Do one of the three segmentation algorithms: kmeans, otsu, windowed\n    if color_number &gt; 2:\n        binary_image, binary_image2, self.bio_label, self.bio_label2  = kmeans(greyscale, greyscale2, color_number, bio_mask, back_mask, logical, bio_label, bio_label2)\n    elif rolling_window_segmentation is not None and rolling_window_segmentation['do']:\n        binary_image = windowed_thresholding(greyscale, lighter_background, rolling_window_segmentation['side_len'],\n        rolling_window_segmentation['step'], rolling_window_segmentation['min_int_var'])\n    else:\n        binary_image = otsu_thresholding(greyscale)\n    if logical != 'None' and color_number == 2:\n        if rolling_window_segmentation is not None and rolling_window_segmentation['do']:\n            binary_image2 = windowed_thresholding(greyscale2, lighter_background, rolling_window_segmentation['side_len'],\n            rolling_window_segmentation['step'], rolling_window_segmentation['min_int_var'])\n        else:\n            binary_image2 = otsu_thresholding(greyscale2)\n\n    # 4. Use previous_binary_image to make sure that the specimens are labelled with ones and the background zeros\n    if self.previous_binary_image is not None:\n        previous_binary_image = self.previous_binary_image[min_y:max_y, min_x:max_x]\n        if not (binary_image * previous_binary_image).any() or (binary_image[0, :].all() and binary_image[-1, :].all() and binary_image[:, 0].all() and binary_image[:, -1].all()):\n            # if (binary_image * (1 - previous_binary_image)).sum() &gt; (binary_image * previous_binary_image).sum() + perimeter(binary_image):\n            # Ones of the binary image have more in common with the background than with the specimen\n            binary_image = 1 - binary_image\n        if logical != 'None':\n            if (binary_image2 * (1 - previous_binary_image)).sum() &gt; (binary_image2 * previous_binary_image).sum():\n                binary_image2 = 1 - binary_image2\n\n    # 5. Give back the image their original size and combine binary images (optional)\n    self.binary_image = np.zeros(self.image.shape, dtype=np.uint8)\n    self.binary_image[min_y:max_y, min_x:max_x] = binary_image\n    self.greyscale = np.zeros(self.image.shape, dtype=np.uint8)\n    self.greyscale[min_y:max_y, min_x:max_x] = greyscale\n    if logical != 'None':\n        self.binary_image2 = np.zeros(self.image.shape, dtype=np.uint8)\n        self.binary_image2[min_y:max_y, min_x:max_x] = binary_image2\n        self.greyscale2 = np.zeros(self.image.shape, dtype=np.uint8)\n        self.greyscale2[min_y:max_y, min_x:max_x] = greyscale2\n    if logical != 'None':\n        if logical == 'Or':\n            self.binary_image = np.logical_or(self.binary_image, self.binary_image2)\n        elif logical == 'And':\n            self.binary_image = np.logical_and(self.binary_image, self.binary_image2)\n        elif logical == 'Xor':\n            self.binary_image = np.logical_xor(self.binary_image, self.binary_image2)\n        self.binary_image = self.binary_image.astype(np.uint8)\n</code></pre>"},{"location":"api/cellects/core/one_image_analysis/#cellects.core.one_image_analysis.OneImageAnalysis.update_current_images","title":"<code>update_current_images(current_combination_id)</code>","text":"<p>Update the current images based on a given combination ID.</p> <p>This method updates two attributes of the instance: <code>image</code> and <code>validated_shapes</code>. The <code>image</code> attribute is set to the value of the key \"converted_image\" from a dictionary in <code>im_combinations</code> which is indexed by the provided <code>current_combination_id</code>. Similarly, the <code>validated_shapes</code> attribute is set to the value of the key \"binary_image\" from the same dictionary.</p> <p>Args:     current_combination_id (int): The ID of the combination whose         images should be set as the current ones.</p> Source code in <code>src/cellects/core/one_image_analysis.py</code> <pre><code>def update_current_images(self, current_combination_id: int):\n    \"\"\"\n    Update the current images based on a given combination ID.\n\n    This method updates two attributes of the instance: `image` and\n    `validated_shapes`. The `image` attribute is set to the value of the key\n    \"converted_image\" from a dictionary in `im_combinations` which is\n    indexed by the provided `current_combination_id`. Similarly, the\n    `validated_shapes` attribute is set to the value of the key \"binary_image\"\n    from the same dictionary.\n\n    Args:\n        current_combination_id (int): The ID of the combination whose\n            images should be set as the current ones.\n\n    \"\"\"\n    self.image = self.im_combinations[current_combination_id][\"converted_image\"]\n    self.validated_shapes = self.im_combinations[current_combination_id][\"binary_image\"]\n</code></pre>"},{"location":"api/cellects/core/program_organizer/","title":"<code>cellects.core.program_organizer</code>","text":""},{"location":"api/cellects/core/program_organizer/#cellects.core.program_organizer","title":"<code>cellects.core.program_organizer</code>","text":"<p>This file contains the class constituting the link between the graphical interface and the computations. First, Cellects analyze one image in order to get a color space combination maximizing the contrast between the specimens and the background. Second, Cellects automatically delineate each arena. Third, Cellects write one video for each arena. Fourth, Cellects segments the video and apply post-processing algorithms to improve the segmentation. Fifth, Cellects extract variables and store them in .csv files.</p>"},{"location":"api/cellects/core/program_organizer/#cellects.core.program_organizer.ProgramOrganizer","title":"<code>ProgramOrganizer</code>","text":"<p>Organizes and manages variables, configuration settings, and processing workflows for motion analysis in a Cellects project.</p> <p>This class maintains global state and analysis-specific data structures, handles file operations, processes image/video inputs, and generates output tables. It provides methods to load/save configurations, segment images, track objects across frames, and export results with metadata.</p> <p>Attributes:</p> Name Type Description <code>one_arena_done</code> <code>bool</code> <p>Flag indicating whether a single arena has been processed.</p> <code>reduce_image_dim</code> <code>bool</code> <p>Whether image dimensions should be reduced (e.g., from color to grayscale).</p> <code>first_exp_ready_to_run</code> <code>bool</code> <p>Indicates if the initial experiment setup is complete and ready for execution.</p> <code>data_to_save</code> <code>dict of {str: bool}</code> <p>Specifies which data types (first image, coordinates, EXIF) require saving.</p> <code>videos</code> <code>OneVideoPerBlob or None</code> <p>Video processing container instance.</p> <code>motion</code> <code>MotionAnalysis or None</code> <p>Motion tracking and analysis module.</p> <code>all</code> <code>dict</code> <p>Global configuration parameters for the entire workflow.</p> <code>vars</code> <code>dict</code> <p>Analysis-specific variables used by <code>MotionAnalysis</code>.</p> <code>first_im, last_im</code> <code>ndarray or None</code> <p>First and last images of the dataset for preprocessing.</p> <code>data_list</code> <code>list of str</code> <p>List of video/image file paths in the working directory.</p> <code>computed_video_options</code> <code>np.ndarray of bool</code> <p>Flags indicating which video processing options have been applied.</p> <code>one_row_per_arena, one_row_per_frame</code> <code>DataFrame or None</code> <p>Result tables for different levels of analysis (per arena, per frame, and oscillating clusters).</p> Methods: <p>save_variable_dict() : Save configuration dictionaries to file. load_variable_dict() : Load saved configuration or initialize defaults. look_for_data() : Discover video/image files in the working directory. update_folder_id(...) : Update folder-specific metadata based on file structure. ...</p> Source code in <code>src/cellects/core/program_organizer.py</code> <pre><code>class ProgramOrganizer:\n    \"\"\"\n    Organizes and manages variables, configuration settings, and processing workflows for motion analysis in a Cellects project.\n\n    This class maintains global state and analysis-specific data structures, handles file operations,\n    processes image/video inputs, and generates output tables. It provides methods to load/save configurations,\n    segment images, track objects across frames, and export results with metadata.\n\n    Attributes\n    ----------\n    one_arena_done : bool\n        Flag indicating whether a single arena has been processed.\n    reduce_image_dim : bool\n        Whether image dimensions should be reduced (e.g., from color to grayscale).\n    first_exp_ready_to_run : bool\n        Indicates if the initial experiment setup is complete and ready for execution.\n    data_to_save : dict of {str: bool}\n        Specifies which data types (first image, coordinates, EXIF) require saving.\n    videos : OneVideoPerBlob or None\n        Video processing container instance.\n    motion : MotionAnalysis or None\n        Motion tracking and analysis module.\n    all : dict\n        Global configuration parameters for the entire workflow.\n    vars : dict\n        Analysis-specific variables used by `MotionAnalysis`.\n    first_im, last_im : np.ndarray or None\n        First and last images of the dataset for preprocessing.\n    data_list : list of str\n        List of video/image file paths in the working directory.\n    computed_video_options : np.ndarray of bool\n        Flags indicating which video processing options have been applied.\n    one_row_per_arena, one_row_per_frame : pd.DataFrame or None\n        Result tables for different levels of analysis (per arena, per frame, and oscillating clusters).\n\n    Methods:\n    --------\n    save_variable_dict() : Save configuration dictionaries to file.\n    load_variable_dict() : Load saved configuration or initialize defaults.\n    look_for_data() : Discover video/image files in the working directory.\n    update_folder_id(...) : Update folder-specific metadata based on file structure.\n    ...\n\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n            This class stores all variables required for analysis as well as\n            methods to process it.\n            Global variables (i.e. that does not concern the MotionAnalysis)\n            are directly stored in self.\n            Variables used in the MotionAnalysis class are stored in a dict\n            called self.vars\n        \"\"\"\n        if os.path.isfile('PickleRick.pkl'):\n            os.remove('PickleRick.pkl')\n        if os.path.isfile('PickleRick0.pkl'):\n            os.remove('PickleRick0.pkl')\n        self.one_arena_done: bool = False\n        self.reduce_image_dim: bool = False\n        self.first_exp_ready_to_run: bool = False\n        self.data_to_save = {'first_image': False, 'exif': False, 'vars': False}\n        self.sample_number = 1\n        self.top = None\n        self.motion = None\n        self.analysis_instance = None\n        self.computed_video_options = np.zeros(5, bool)\n        self.vars = {}\n        self.all = {}\n        self.all['folder_list'] = []\n        self.vars['first_detection_frame'] = 0\n        self.first_im = None\n        self.last_im = None\n        self.vars['background_list'] = []\n        self.starting_blob_hsize_in_pixels = None\n        self.vars['first_move_threshold'] = None\n        self.vars['convert_for_origin'] = None\n        self.vars['convert_for_motion'] = None\n        self.current_combination_id = 0\n        self.data_list = []\n        self.one_row_per_arena = None\n        self.one_row_per_frame = None\n        self.not_analyzed_individuals = None\n        self.visualize: bool = True\n        self.network_shaped: bool = False\n\n    def update_variable_dict(self):\n        \"\"\"\n\n        Update the `all` and `vars` dictionaries with new data from `DefaultDicts`.\n\n        This method updates the `all` and `vars` dictionaries of the current object with\n        data from a new instance of `DefaultDicts`. It checks if any keys or descriptors\n        are missing and adds them accordingly.\n\n        Examples\n        --------\n        &gt;&gt;&gt; organizer = ProgramOrganizer()\n        &gt;&gt;&gt; organizer.update_variable_dict()\n        \"\"\"\n        dd = DefaultDicts()\n        all = len(dd.all) != len(self.all)\n        vars = len(dd.vars) != len(self.vars)\n        all_desc = not 'descriptors' in self.all or len(dd.all['descriptors']) != len(self.all['descriptors'])\n        vars_desc = not 'descriptors' in self.vars or len(dd.vars['descriptors']) != len(self.vars['descriptors'])\n        if all:\n            for key, val in dd.all.items():\n                if not key in self.all:\n                    self.all[key] = val\n        if vars:\n            for key, val in dd.vars.items():\n                if not key in self.vars:\n                    self.vars[key] = val\n        if all_desc:\n            for key, val in dd.all['descriptors'].items():\n                if not key in self.all['descriptors']:\n                    self.all['descriptors'][key] = val\n        if vars_desc:\n            for key, val in dd.vars['descriptors'].items():\n                if not key in self.vars['descriptors']:\n                    self.vars['descriptors'][key] = val\n        self._set_analyzed_individuals()\n\n    def save_variable_dict(self):\n        \"\"\"\n        Saves the configuration dictionaries (`self.all` and `self.vars`) to a pickle file.\n\n        If bio_mask or back_mask are not required for all folders, they are excluded from the saved data.\n\n        Notes\n        -----\n        This method is used to preserve state between Cellects sessions or restart scenarios.\n        \"\"\"\n        logging.info(\"Save the parameters dictionaries in the Cellects folder\")\n        self.all['vars'] = self.vars\n        all_vars = deepcopy(self.all)\n        if not self.all['keep_cell_and_back_for_all_folders']:\n            all_vars['bio_mask'] = None\n            all_vars['back_mask'] = None\n        pickle_rick = PickleRick(0)\n        pickle_rick.write_file(all_vars, ALL_VARS_PKL_FILE)\n\n    def load_variable_dict(self):\n        \"\"\"\n        Loads configuration dictionaries from a pickle file if available, otherwise initializes defaults.\n\n        Tries to load saved parameters. If the file doesn't exist or loading fails due to corruption,\n        default values are used instead (logging relevant warnings).\n\n        Raises\n        ------\n        FileNotFoundError\n            If no valid configuration file is found and default initialization fails.\n\n        Notes\n        -----\n        This method ensures robust operation by handling missing or corrupted configuration files gracefully.\n        \"\"\"\n        if os.path.isfile(ALL_VARS_PKL_FILE):\n            logging.info(\"Load the parameters from all_vars.pkl in the config of the Cellects folder\")\n            try:\n                with open(ALL_VARS_PKL_FILE, 'rb') as fileopen:\n                    self.all = pickle.load(fileopen)\n                self.vars = self.all['vars']\n                self.update_variable_dict()\n                logging.info(\"Success to load the parameters dictionaries from the Cellects folder\")\n            except Exception as exc:\n                logging.error(f\"Initialize default parameters because error: {exc}\")\n                default_dicts = DefaultDicts()\n                self.all = default_dicts.all\n                self.vars = default_dicts.vars\n        else:\n            logging.info(\"Initialize default parameters\")\n            default_dicts = DefaultDicts()\n            self.all = default_dicts.all\n            self.vars = default_dicts.vars\n        if self.all['cores'] == 1:\n            self.all['cores'] = os.cpu_count() - 1\n\n    def look_for_data(self):\n        \"\"\"\n        Discovers all relevant video/image data in the working directory.\n\n        Uses natural sorting to handle filenames with numeric suffixes. Validates file consistency and logs warnings\n        if filename patterns are inconsistent across folders.\n\n        Raises\n        ------\n        ValueError\n            If no files match the specified naming convention.\n\n        Notes\n        -----\n        This method assumes all data files follow a predictable pattern with numeric extensions. Use caution in\n        unpredictable directory structures where this may fail silently or produce incorrect results.\n\n        Examples\n        --------\n        &gt;&gt;&gt; organizer.look_for_data()\n        &gt;&gt;&gt; print(organizer.data_list)\n        ['/path/to/video1.avi', '/path/to/video2.avi']\n        \"\"\"\n        os.chdir(Path(self.all['global_pathway']))\n        logging.info(f\"Dir: {self.all['global_pathway']}\")\n        self.data_list = insensitive_glob(self.all['radical'] + '*' + self.all['extension'])  # Provides a list ordered by last modification date\n        self.all['folder_list'] = []\n        self.all['folder_number'] = 1\n        self.vars['first_detection_frame'] = 0\n        if len(self.data_list) &gt; 0:\n            self._sort_data_list()\n            self.sample_number = self.all['first_folder_sample_number']\n        else:\n            content = os.listdir()\n            for obj in content:\n                if not os.path.isfile(obj):\n                    data_list = insensitive_glob(obj + \"/\" + self.all['radical'] + '*' + self.all['extension'])\n                    if len(data_list) &gt; 0:\n                        self.all['folder_list'].append(obj)\n                        self.all['folder_number'] += 1\n            self.all['folder_list'] = np.sort(self.all['folder_list'])\n\n            if isinstance(self.all['sample_number_per_folder'], int) or len(self.all['sample_number_per_folder']) == 1:\n                self.all['sample_number_per_folder'] = np.repeat(self.all['sample_number_per_folder'],\n                                                              self.all['folder_number'])\n\n    def _sort_data_list(self):\n        \"\"\"\n        Sorts the data list using natural sorting.\n\n        Extended Description\n        --------------------\n        This function sorts the `data_list` attribute of an instance using the natsort library,\n        which is useful when filenames have a mixture of numbers and letters.\n        \"\"\"\n        if len(self.data_list) &gt; 0:\n            lengths = vectorized_len(self.data_list)\n            if len(lengths) &gt; 1 and np.max(np.diff(lengths)) &gt; np.log10(len(self.data_list)):\n                logging.error(f\"File names present strong variations and cannot be correctly sorted.\")\n            wrong_images = np.nonzero(np.char.startswith(self.data_list, \"Analysis efficiency, \", ))[0]\n            for w_im in wrong_images[::-1]:\n                self.data_list.pop(w_im)\n            self.data_list = natsort.natsorted(self.data_list)\n        if self.all['im_or_vid'] == 1:\n            self.vars['video_list'] = self.data_list\n        else:\n            self.vars['video_list'] = None\n\n    def update_folder_id(self, sample_number: int, folder_name: str=\"\"):\n        \"\"\"\n        Update the current working directory and data list based on the given sample number\n        and optional folder name.\n\n        Parameters\n        ----------\n        sample_number : int\n            The number of samples to analyze.\n        folder_name : str, optional\n            The name of the folder to change to. Default is an empty string.\n\n        Notes\n        -----\n        This function changes the current working directory to the specified folder name\n        and updates the data list based on the file names in that directory. It also performs\n        sorting of the data list and checks for strong variations in file names.\n\n        \"\"\"\n        os.chdir(Path(self.all['global_pathway']) / folder_name)\n        self.data_list = insensitive_glob(\n            self.all['radical'] + '*' + self.all['extension'])  # Provides a list ordered by last modification date\n        # Sorting is necessary when some modifications (like rotation) modified the last modification date\n        self._sort_data_list()\n        if self.all['im_or_vid'] == 1:\n            self.sample_number = sample_number\n        else:\n            self.vars['img_number'] = len(self.data_list)\n            self.sample_number = sample_number\n        if not 'analyzed_individuals' in self.vars:\n            self._set_analyzed_individuals()\n\n    def _set_analyzed_individuals(self):\n        \"\"\"\n        Set the analyzed individuals variable in the dataset.\n        \"\"\"\n        self.vars['analyzed_individuals'] = np.arange(self.sample_number) + 1\n        if self.not_analyzed_individuals is not None:\n            self.vars['analyzed_individuals'] = np.delete(self.vars['analyzed_individuals'],\n                                                       self.not_analyzed_individuals - 1)\n\n    def load_data_to_run_cellects_quickly(self):\n        \"\"\"\n        Load data from a pickle file and update the current state of the object.\n\n        Summarizes, loads, and validates data needed to run Cellects,\n        updating the object's state accordingly. If the necessary data\n        are not present or valid, it ensures the experiment is marked as\n        not ready to run.\n\n        Parameters\n        ----------\n        self : CellectsObject\n            The instance of the class (assumed to be a subclass of\n            CellectsObject) that this method belongs to.\n\n        Returns\n        -------\n        None\n\n        Notes\n        -----\n        This function relies on the presence of a pickle file 'Data to run Cellects quickly.pkl'.\n        It updates the state of various attributes based on the loaded data\n        and logs appropriate messages.\n        \"\"\"\n        self.analysis_instance = None\n        self.first_im = None\n        self.first_image = None\n        self.last_image = None\n        current_global_pathway = self.all['global_pathway']\n        folder_number = self.all['folder_number']\n        if folder_number &gt; 1:\n            folder_list = deepcopy(self.all['folder_list'])\n            sample_number_per_folder = deepcopy(self.all['sample_number_per_folder'])\n\n        if os.path.isfile('Data to run Cellects quickly.pkl'):\n            pickle_rick = PickleRick()\n            data_to_run_cellects_quickly = pickle_rick.read_file('Data to run Cellects quickly.pkl')\n            if data_to_run_cellects_quickly is None:\n                data_to_run_cellects_quickly = {}\n\n            if ('validated_shapes' in data_to_run_cellects_quickly) and ('all' in data_to_run_cellects_quickly) and ('bb_coord' in data_to_run_cellects_quickly['all']['vars']):\n                logging.info(\"Success to load Data to run Cellects quickly.pkl from the user chosen directory\")\n                self.all = data_to_run_cellects_quickly['all']\n                # If you want to add a new variable, first run an updated version of all_vars_dict,\n                # then put a breakpoint here and run the following + self.save_data_to_run_cellects_quickly() :\n                self.vars = self.all['vars']\n                self.update_variable_dict()\n                folder_changed = False\n                if current_global_pathway != self.all['global_pathway']:\n                    folder_changed = True\n                    logging.info(\n                        \"Although the folder is ready, it is not at the same place as it was during creation, updating\")\n                    self.all['global_pathway'] = current_global_pathway\n                if folder_number &gt; 1:\n                    self.all['global_pathway'] = current_global_pathway\n                    self.all['folder_list'] = folder_list\n                    self.all['folder_number'] = folder_number\n                    self.all['sample_number_per_folder'] = sample_number_per_folder\n\n                if len(self.data_list) == 0:\n                    self.look_for_data()\n                    if folder_changed and folder_number &gt; 1 and len(self.all['folder_list']) &gt; 0:\n                        self.update_folder_id(self.all['sample_number_per_folder'][0], self.all['folder_list'][0])\n                self.get_first_image()\n                self.get_last_image()\n                (ccy1, ccy2, ccx1, ccx2, self.top, self.bot, self.left, self.right) = data_to_run_cellects_quickly['all']['vars']['bb_coord']\n                if self.all['automatically_crop']:\n                    self.first_image.crop_coord = [ccy1, ccy2, ccx1, ccx2]\n                    logging.info(\"Crop first image\")\n                    self.first_image.automatically_crop(self.first_image.crop_coord)\n                    logging.info(\"Crop last image\")\n                    self.last_image.automatically_crop(self.first_image.crop_coord)\n                else:\n                    self.first_image.crop_coord = None\n                self.first_image.validated_shapes = data_to_run_cellects_quickly['validated_shapes']\n                self.first_image.im_combinations = []\n                self.current_combination_id = 0\n                self.first_image.im_combinations.append({})\n                self.first_image.im_combinations[self.current_combination_id]['csc'] = self.vars['convert_for_origin']\n                self.first_image.im_combinations[self.current_combination_id]['binary_image'] = self.first_image.validated_shapes\n                self.first_image.im_combinations[self.current_combination_id]['shape_number'] = data_to_run_cellects_quickly['shape_number']\n\n                self.first_exp_ready_to_run = True\n                if self.vars['subtract_background'] and len(self.vars['background_list']) == 0:\n                    self.first_exp_ready_to_run = False\n            else:\n                self.first_exp_ready_to_run = False\n        else:\n            self.first_exp_ready_to_run = False\n        if self.first_exp_ready_to_run:\n            logging.info(\"The current (or the first) folder is ready to run\")\n        else:\n            logging.info(\"The current (or the first) folder is not ready to run\")\n\n    def save_data_to_run_cellects_quickly(self, new_one_if_does_not_exist: bool=True):\n        \"\"\"\n        Save data to a pickled file if it does not exist or update existing data.\n\n        Parameters\n        ----------\n        new_one_if_does_not_exist : bool, optional\n            Whether to create a new data file if it does not already exist.\n            Default is True.\n\n        Notes\n        -----\n        This method logs various information about its operations and handles the writing of data to a pickled file.\n        \"\"\"\n        data_to_run_cellects_quickly = None\n        if os.path.isfile('Data to run Cellects quickly.pkl'):\n            logging.info(\"Update -Data to run Cellects quickly.pkl- in the user chosen directory\")\n            pickle_rick = PickleRick()\n            data_to_run_cellects_quickly = pickle_rick.read_file('Data to run Cellects quickly.pkl')\n            if data_to_run_cellects_quickly is None:\n                os.remove('Data to run Cellects quickly.pkl')\n                logging.error(\"Failed to load Data to run Cellects quickly.pkl before update. Remove pre existing.\")\n        else:\n            if new_one_if_does_not_exist:\n                logging.info(\"Create Data to run Cellects quickly.pkl in the user chosen directory\")\n                data_to_run_cellects_quickly = {}\n        if data_to_run_cellects_quickly is not None:\n            if self.data_to_save['first_image']:\n                data_to_run_cellects_quickly['validated_shapes'] = self.first_image.im_combinations[self.current_combination_id]['binary_image']\n                data_to_run_cellects_quickly['shape_number'] = self.first_image.im_combinations[self.current_combination_id]['shape_number']\n            if self.data_to_save['exif']:\n                self.vars['exif'] = self.extract_exif()\n            self.all['vars'] = self.vars\n            data_to_run_cellects_quickly['all'] = self.all\n            pickle_rick = PickleRick()\n            pickle_rick.write_file(data_to_run_cellects_quickly, 'Data to run Cellects quickly.pkl')\n\n    def list_coordinates(self):\n        \"\"\"\n        Summarize the coordinates of images and video.\n\n        Combine the crop coordinates from the first image with additional\n        coordinates for left, right, top, and bottom boundaries to form a list of\n        video coordinates. If the crop coordinates are not already set, initialize\n        them to cover the entire image.\n\n        Returns\n        -------\n        list of int\n            A list containing the coordinates [left, right, top, bottom] for video.\n\n        \"\"\"\n        if self.first_image.crop_coord is None:\n            self.first_image.crop_coord = [0, self.first_image.image.shape[0], 0, self.first_image.image.shape[1]]\n        self.vars['bb_coord'] = self.first_image.crop_coord + [self.top, self.bot, self.left, self.right]\n        self.all['overwrite_unaltered_videos'] = True\n\n    def get_first_image(self, first_im: NDArray=None, sample_number: int=None):\n        \"\"\"\n        Load and process the first image or frame from a video.\n\n        This method handles loading the first image or the first frame of a video\n        depending on whether the data is an image or a video. It performs necessary\n        preprocessing and initializes relevant attributes for subsequent analysis.\n        \"\"\"\n        if sample_number is not None:\n            self.sample_number = sample_number\n        self.reduce_image_dim = False\n        if first_im is not None:\n            self.first_im = first_im\n        else:\n            logging.info(\"Load first image\")\n            if self.all['im_or_vid'] == 1:\n                if self.analysis_instance is None:\n                    self.analysis_instance = video2numpy(self.data_list[0])\n                    self.sample_number = len(self.data_list)\n                    self.vars['img_number'] = self.analysis_instance.shape[0]\n                    self.first_im = self.analysis_instance[0, ...]\n                    self.vars['dims'] = self.analysis_instance.shape[:3]\n                else:\n                    self.first_im = self.analysis_instance[self.vars['first_detection_frame'], ...]\n\n            else:\n                self.vars['img_number'] = len(self.data_list)\n                self.all['raw_images'] = is_raw_image(self.data_list[0])\n                self.first_im = readim(self.data_list[self.vars['first_detection_frame']], self.all['raw_images'])\n                self.vars['dims'] = [self.vars['img_number'], self.first_im.shape[0], self.first_im.shape[1]]\n\n                if len(self.first_im.shape) == 3:\n                    if np.all(np.equal(self.first_im[:, :, 0], self.first_im[:, :, 1])) and np.all(\n                            np.equal(self.first_im[:, :, 1], self.first_im[:, :, 2])):\n                        self.reduce_image_dim = True\n                    if self.reduce_image_dim:\n                        self.first_im = self.first_im[:, :, 0]\n\n        self.first_image = OneImageAnalysis(self.first_im, self.sample_number)\n        self.vars['already_greyscale'] = self.first_image.already_greyscale\n        if self.vars['already_greyscale']:\n            self.vars[\"convert_for_origin\"] = {\"bgr\": np.array((1, 1, 1), dtype=np.uint8), \"logical\": \"None\"}\n            self.vars[\"convert_for_motion\"] = {\"bgr\": np.array((1, 1, 1), dtype=np.uint8), \"logical\": \"None\"}\n        if np.mean((np.mean(self.first_image.image[2, :, ...]), np.mean(self.first_image.image[-3, :, ...]), np.mean(self.first_image.image[:, 2, ...]), np.mean(self.first_image.image[:, -3, ...]))) &gt; 127:\n            self.vars['contour_color']: np.uint8 = 0\n        else:\n            self.vars['contour_color']: np.uint8 = 255\n        if self.vars['first_detection_frame'] &gt; 0:\n            self.vars['origin_state'] = 'invisible'\n\n    def get_last_image(self, last_im: NDArray=None):\n        \"\"\"\n\n        Load the last image from a video or image list and process it based on given parameters.\n\n        Parameters\n        ----------\n        last_im : NDArray, optional\n            The last image to be loaded. If not provided, the last image will be loaded from the data list.\n        \"\"\"\n        logging.info(\"Load last image\")\n        if last_im is not None:\n            self.last_im = last_im\n        else:\n            if self.all['im_or_vid'] == 1:\n                self.last_im = self.analysis_instance[-1, ...]\n            else:\n                is_landscape = self.first_image.image.shape[0] &lt; self.first_image.image.shape[1]\n                self.last_im = read_and_rotate(self.data_list[-1], self.first_im, self.all['raw_images'], is_landscape)\n                if self.reduce_image_dim:\n                    self.last_im = self.last_im[:, :, 0]\n        self.last_image = OneImageAnalysis(self.last_im)\n\n    def extract_exif(self):\n        \"\"\"\n        Extract EXIF data from image or video files.\n\n        Notes\n        -----\n        If `extract_time_interval` is True and unsuccessful, arbitrary time steps will be used.\n        Timings are normalized to minutes for consistency across different files.\n        \"\"\"\n        self.vars['time_step_is_arbitrary'] = True\n        if self.all['im_or_vid'] == 1:\n            if not 'dims' in self.vars:\n                self.vars['dims'] = self.analysis_instance.shape[:3]\n            timings = np.arange(self.vars['dims'][0])\n        else:\n            timings = np.arange(len(self.data_list))\n            if sys.platform.startswith('win'):\n                pathway = os.getcwd() + '\\\\'\n            else:\n                pathway = os.getcwd() + '/'\n            if not 'extract_time_interval' in self.all:\n                self.all['extract_time_interval'] = True\n            if self.all['extract_time_interval']:\n                self.vars['time_step'] = 1\n                try:\n                    timings = extract_time(self.data_list, pathway, self.all['raw_images'])\n                    timings = timings - timings[0]\n                    timings = timings / 60\n                    time_step = np.diff(timings)\n                    if len(time_step) &gt; 0:\n                        time_step = np.mean(time_step)\n                        digit_nb = 0\n                        for i in str(time_step):\n                            if i in {'.'}:\n                                pass\n                            elif i in {'0'}:\n                                digit_nb += 1\n                            else:\n                                break\n                        self.vars['time_step'] = np.round(time_step, digit_nb + 1)\n                        self.vars['time_step_is_arbitrary'] = False\n                except:\n                    pass\n            else:\n                timings = np.arange(0, len(self.data_list) * self.vars['time_step'], self.vars['time_step'])\n                self.vars['time_step_is_arbitrary'] = False\n        return timings\n\n    def fast_first_image_segmentation(self):\n        \"\"\"\n        Segment the first or subsequent image in a series for biological and background masks.\n\n        Notes\n        -----\n        This function processes the first or subsequent image in a sequence, applying biological and background masks,\n        segmenting the image, and updating internal data structures accordingly. The function is specific to handling\n        image sequences for biological analysis\n\n        \"\"\"\n        if not \"color_number\" in self.vars:\n            self.update_variable_dict()\n        if self.vars['convert_for_origin'] is None:\n            self.vars['convert_for_origin'] = {\"logical\": 'None', \"PCA\": np.ones(3, dtype=np.uint8)}\n        self.first_image.convert_and_segment(self.vars['convert_for_origin'], self.vars[\"color_number\"],\n                                             self.all[\"bio_mask\"], self.all[\"back_mask\"], subtract_background=None,\n                                             subtract_background2=None,\n                                             rolling_window_segmentation=self.vars[\"rolling_window_segmentation\"],\n                                             filter_spec=self.vars[\"filter_spec\"])\n        if not self.first_image.drift_correction_already_adjusted:\n            self.vars['drift_already_corrected'] = self.first_image.check_if_image_border_attest_drift_correction()\n            if self.vars['drift_already_corrected']:\n                logging.info(\"Cellects detected that the images have already been corrected for drift\")\n                self.first_image.convert_and_segment(self.vars['convert_for_origin'], self.vars[\"color_number\"],\n                                                     self.all[\"bio_mask\"], self.all[\"back_mask\"],\n                                                     subtract_background=None, subtract_background2=None,\n                                                     rolling_window_segmentation=self.vars[\"rolling_window_segmentation\"],\n                                                     filter_spec=self.vars[\"filter_spec\"],\n                                                     allowed_window=self.first_image.drift_mask_coord)\n\n        shapes_features = shape_selection(self.first_image.binary_image, true_shape_number=self.sample_number,\n                                          horizontal_size=self.starting_blob_hsize_in_pixels,\n                                          spot_shape=self.all['starting_blob_shape'],\n                                          several_blob_per_arena=self.vars['several_blob_per_arena'],\n                                          bio_mask=self.all[\"bio_mask\"], back_mask=self.all[\"back_mask\"])\n        self.first_image.validated_shapes, shape_number, stats, centroids = shapes_features\n        self.first_image.shape_number = shape_number\n        if self.first_image.im_combinations is None:\n            self.first_image.im_combinations = []\n        if len(self.first_image.im_combinations) == 0:\n            self.first_image.im_combinations.append({})\n        self.current_combination_id = np.min((self.current_combination_id, len(self.first_image.im_combinations) - 1))\n        self.first_image.im_combinations[self.current_combination_id]['csc'] = self.vars['convert_for_origin']\n        self.first_image.im_combinations[self.current_combination_id]['binary_image'] = self.first_image.validated_shapes\n        if self.first_image.greyscale is not None:\n            greyscale = self.first_image.greyscale\n        else:\n            greyscale = self.first_image.image\n        self.first_image.im_combinations[self.current_combination_id]['converted_image'] = bracket_to_uint8_image_contrast(greyscale)\n        self.first_image.im_combinations[self.current_combination_id]['shape_number'] = shape_number\n\n    def fast_last_image_segmentation(self, bio_mask: NDArray[np.uint8] = None, back_mask: NDArray[np.uint8] = None):\n        \"\"\"\n        Segment the first or subsequent image in a series for biological and background masks.\n\n        Parameters\n        ----------\n        bio_mask : NDArray[np.uint8], optional\n            The biological mask to be applied to the image.\n        back_mask : NDArray[np.uint8], optional\n            The background mask to be applied to the image.\n\n        Returns\n        -------\n        None\n\n        Notes\n        -----\n        This function processes the first or subsequent image in a sequence, applying biological and background masks,\n        segmenting the image, and updating internal data structures accordingly. The function is specific to handling\n        image sequences for biological analysis\n\n        \"\"\"\n        if self.vars['convert_for_motion'] is None:\n            self.vars['convert_for_motion'] = {\"logical\": 'None', \"PCA\": np.ones(3, dtype=np.uint8)}\n        self.cropping(is_first_image=False)\n        self.last_image.convert_and_segment(self.vars['convert_for_motion'], self.vars[\"color_number\"],\n                                            bio_mask, back_mask, self.first_image.subtract_background,\n                                            self.first_image.subtract_background2,\n                                            rolling_window_segmentation=self.vars[\"rolling_window_segmentation\"],\n                                            filter_spec=self.vars[\"filter_spec\"])\n        if self.vars['drift_already_corrected'] and not self.last_image.drift_correction_already_adjusted and not self.vars[\"rolling_window_segmentation\"]['do']:\n            self.last_image.check_if_image_border_attest_drift_correction()\n            self.last_image.convert_and_segment(self.vars['convert_for_motion'], self.vars[\"color_number\"],\n                                                bio_mask, back_mask, self.first_image.subtract_background,\n                                                self.first_image.subtract_background2,\n                                                allowed_window=self.last_image.drift_mask_coord,\n                                                filter_spec=self.vars[\"filter_spec\"])\n\n        if self.last_image.im_combinations is None:\n            self.last_image.im_combinations = []\n        if len(self.last_image.im_combinations) == 0:\n            self.last_image.im_combinations.append({})\n        self.current_combination_id = np.min((self.current_combination_id, len(self.last_image.im_combinations) - 1))\n        self.last_image.im_combinations[self.current_combination_id]['csc'] = self.vars['convert_for_motion']\n        self.last_image.im_combinations[self.current_combination_id]['binary_image'] = self.last_image.binary_image\n        if self.last_image.greyscale is not None:\n            greyscale = self.last_image.greyscale\n        else:\n            greyscale = self.last_image.image\n        self.last_image.im_combinations[self.current_combination_id]['converted_image'] = bracket_to_uint8_image_contrast(greyscale)\n\n    def save_user_masks(self, bio_mask=None, back_mask=None):\n        self.all[\"bio_mask\"] = None\n        self.all[\"back_mask\"] = None\n        if self.all['keep_cell_and_back_for_all_folders']:\n            self.all[\"bio_mask\"] = bio_mask\n            self.all[\"back_mask\"] = back_mask\n\n    def full_first_image_segmentation(self, first_param_known: bool, bio_mask: NDArray[np.uint8] = None, back_mask: NDArray[np.uint8] = None):\n        if bio_mask.any():\n            shape_nb, ordered_image = cv2.connectedComponents((bio_mask &gt; 0).astype(np.uint8))\n            shape_nb -= 1\n            bio_mask = np.nonzero(bio_mask)\n        else:\n            shape_nb = 0\n            bio_mask = None\n        if back_mask.any():\n            back_mask = np.nonzero(back_mask)\n        else:\n            back_mask = None\n        self.save_user_masks(bio_mask=bio_mask, back_mask=back_mask)\n        if self.visualize or len(self.first_im.shape) == 2:\n            if not first_param_known and self.all['scale_with_image_or_cells'] == 0 and self.all[\"set_spot_size\"]:\n                self.get_average_pixel_size()\n            else:\n                self.starting_blob_hsize_in_pixels = None\n            self.fast_first_image_segmentation()\n            if not self.vars['several_blob_per_arena'] and bio_mask is not None and shape_nb == self.sample_number and self.first_image.im_combinations[self.current_combination_id]['shape_number'] != self.sample_number:\n                self.first_image.im_combinations[self.current_combination_id]['shape_number'] = shape_nb\n                self.first_image.shape_number = shape_nb\n                self.first_image.validated_shapes = (ordered_image &gt; 0).astype(np.uint8)\n                self.first_image.im_combinations[self.current_combination_id]['binary_image'] = self.first_image.validated_shapes\n        else:\n\n            params = init_params()\n            params['is_first_image'] = True\n            params['blob_nb'] = self.sample_number\n            if self.vars[\"color_number\"] &gt; 2:\n                params['kmeans_clust_nb'] = self.vars[\"color_number\"]\n            params['bio_mask'] = self.all[\"bio_mask\"]\n            params['back_mask'] = self.all[\"back_mask\"]\n            params['filter_spec'] = self.vars[\"filter_spec\"]\n\n            if first_param_known:\n                if self.all['scale_with_image_or_cells'] == 0:\n                    self.get_average_pixel_size()\n                else:\n                    self.starting_blob_hsize_in_pixels = None\n                params['several_blob_per_arena'] = self.vars['several_blob_per_arena']\n                params['blob_shape'] = self.all['starting_blob_shape']\n                params['blob_size'] = self.starting_blob_hsize_in_pixels\n\n            self.first_image.find_color_space_combinations(params)\n\n    def full_last_image_segmentation(self, bio_mask: NDArray[np.uint8] = None, back_mask: NDArray[np.uint8] = None):\n        if bio_mask.any():\n            bio_mask = np.nonzero(bio_mask)\n        else:\n            bio_mask = None\n        if back_mask.any():\n            back_mask = np.nonzero(back_mask)\n        else:\n            back_mask = None\n        if self.last_im is None:\n            self.get_last_image()\n        self.cropping(False)\n        self.get_background_to_subtract()\n        if self.visualize or (len(self.first_im.shape) == 2 and not self.network_shaped):\n            self.fast_last_image_segmentation(bio_mask=bio_mask, back_mask=back_mask)\n        else:\n            arenas_mask = None\n            if self.all['are_gravity_centers_moving'] != 1:\n                cr = [self.top, self.bot, self.left, self.right]\n                arenas_mask = np.zeros_like(self.first_image.validated_shapes)\n                for _i in np.arange(len(self.vars['analyzed_individuals'])):\n                    if self.vars['arena_shape'] == 'circle':\n                        ellipse = create_ellipse(cr[1][_i] - cr[0][_i], cr[3][_i] - cr[2][_i])\n                        arenas_mask[cr[0][_i]: cr[1][_i], cr[2][_i]:cr[3][_i]] = ellipse\n                    else:\n                        arenas_mask[cr[0][_i]: cr[1][_i], cr[2][_i]:cr[3][_i]] = 1\n            if self.network_shaped:\n                self.last_image.network_detection(arenas_mask, csc_dict=self.vars[\"convert_for_motion\"], lighter_background=None, bio_mask=bio_mask, back_mask=back_mask)\n            else:\n                ref_image = self.first_image.validated_shapes\n                params = init_params()\n                params['is_first_image'] = False\n                params['several_blob_per_arena'] = self.vars['several_blob_per_arena']\n                params['blob_nb'] = self.sample_number\n                params['arenas_mask'] = arenas_mask\n                params['ref_image'] = ref_image\n                params['subtract_background'] = self.first_image.subtract_background\n                params['bio_mask'] = bio_mask\n                params['back_mask'] = back_mask\n                params['filter_spec'] = self.vars[\"filter_spec\"]\n\n                self.last_image.find_color_space_combinations(params)\n\n    def cropping(self, is_first_image: bool):\n        \"\"\"\n        Crops the image based on specified conditions and settings.\n\n        This method checks if drift correction has already been applied.\n        If the image is the first one and hasn't been cropped yet, it will attempt\n        to use pre-stored coordinates or compute new crop coordinates. If automatic\n        cropping is enabled, it will apply the cropping process.\n\n        Parameters\n        ----------\n        is_first_image : bool\n            Indicates whether the image being processed is the first one in the sequence.\n        \"\"\"\n        if not self.vars['drift_already_corrected']:\n            if is_first_image:\n                if not self.first_image.cropped:\n                    if (not self.all['overwrite_unaltered_videos'] and os.path.isfile('Data to run Cellects quickly.pkl')):\n                        self.first_image.get_crop_coordinates()\n                        pickle_rick = PickleRick()\n                        data_to_run_cellects_quickly = pickle_rick.read_file('Data to run Cellects quickly.pkl')\n                        if data_to_run_cellects_quickly is not None and 'bb_coord' in data_to_run_cellects_quickly['all']['vars']:\n                            logging.info(\"Get crop coordinates from Data to run Cellects quickly.pkl\")\n                            (ccy1, ccy2, ccx1, ccx2, self.top, self.bot, self.left, self.right) = \\\n                                data_to_run_cellects_quickly['all']['vars']['bb_coord']\n                            self.first_image.crop_coord = [ccy1, ccy2, ccx1, ccx2]\n                    else:\n                        self.first_image.get_crop_coordinates()\n                    if self.all['automatically_crop']:\n                        self.first_image.automatically_crop(self.first_image.crop_coord)\n                    else:\n                        self.first_image.crop_coord = None\n            else:\n                if not self.last_image.cropped and self.all['automatically_crop']:\n                    self.last_image.automatically_crop(self.first_image.crop_coord)\n\n    def get_average_pixel_size(self):\n        \"\"\"\n        Calculate the average pixel size and related variables.\n\n        Logs information about calculation steps, computes the average\n        pixel size based on image or cell scaling settings,\n        and sets initial thresholds for object detection.\n\n        Notes\n        -----\n        - The average pixel size is determined by either image dimensions or blob sizes.\n        - Thresholds for automatic detection are set based on configuration settings.\n\n        \"\"\"\n        logging.info(\"Getting average pixel size\")\n        (self.first_image.shape_number,\n            self.first_image.shapes,\n            self.first_image.stats,\n            centroids) = cv2.connectedComponentsWithStats(\n                self.first_image.validated_shapes,\n                connectivity=8)\n        self.first_image.shape_number -= 1\n        if self.all['scale_with_image_or_cells'] == 0:\n            self.vars['average_pixel_size'] = np.square(self.all['image_horizontal_size_in_mm'] /\n                                                        self.first_im.shape[1])\n        else:\n            if len(self.first_image.stats[1:, 2]) &gt; 0:\n                self.vars['average_pixel_size'] = np.square(self.all['starting_blob_hsize_in_mm'] /\n                                                            np.mean(self.first_image.stats[1:, 2]))\n            else:\n                self.vars['average_pixel_size'] = 1.\n                self.vars['output_in_mm'] = False\n\n        if self.all['set_spot_size']:\n            self.starting_blob_hsize_in_pixels = (self.all['starting_blob_hsize_in_mm'] /\n                                                  np.sqrt(self.vars['average_pixel_size']))\n        else:\n            self.starting_blob_hsize_in_pixels = None\n\n        if self.all['automatic_size_thresholding']:\n            self.vars['first_move_threshold'] = 10\n        else:\n            self.vars['first_move_threshold'] = np.round(self.all['first_move_threshold_in_mm\u00b2'] /\n                                                         self.vars['average_pixel_size']).astype(np.uint8)\n        logging.info(f\"The average pixel size is: {self.vars['average_pixel_size']} mm\u00b2\")\n\n    def get_background_to_subtract(self):\n        \"\"\"\n        Determine if background subtraction should be applied to the image.\n\n        Extended Description\n        --------------------\n        This function checks whether background subtraction should be applied.\n        It utilizes the 'subtract_background' flag and potentially converts\n        the image for motion estimation.\n\n        Parameters\n        ----------\n        self : object\n            The instance of the class containing this method.\n            Must have attributes `vars` and `first_image`.\n        \"\"\"\n        if self.vars['subtract_background']:\n            self.first_image.generate_subtract_background(self.vars['convert_for_motion'], self.vars['drift_already_corrected'])\n\n    def find_if_lighter_background(self):\n        \"\"\"\n        Determines whether the background is lighter or darker than the cells.\n\n        This function analyzes images to determine if their backgrounds are lighter\n        or darker relative to the cells, updating attributes accordingly for analysis and display purposes.\n\n\n        Notes\n        -----\n        This function modifies instance variables and does not return any value.\n        The analysis involves comparing mean pixel values in specific areas of the image.\n        \"\"\"\n        logging.info(\"Find if the background is lighter or darker than the cells\")\n        self.vars['lighter_background']: bool = True\n        self.vars['contour_color']: np.uint8 = 0\n        are_dicts_equal: bool = True\n        if self.vars['convert_for_origin'] is not None and self.vars['convert_for_origin'] is not None:\n            for key in self.vars['convert_for_origin'].keys():\n                are_dicts_equal = are_dicts_equal and np.all(key in self.vars['convert_for_motion'] and self.vars['convert_for_origin'][key] == self.vars['convert_for_motion'][key])\n\n            for key in self.vars['convert_for_motion'].keys():\n                are_dicts_equal = are_dicts_equal and np.all(key in self.vars['convert_for_origin'] and self.vars['convert_for_motion'][key] == self.vars['convert_for_origin'][key])\n        else:\n            self.vars['convert_for_origin'] = {\"logical\": 'None', \"PCA\": np.ones(3, dtype=np.uint8)}\n            are_dicts_equal = True\n        if are_dicts_equal:\n            if self.first_im is None:\n                self.get_first_image()\n                self.fast_first_image_segmentation()\n                self.cropping(is_first_image=True)\n            among = np.nonzero(self.first_image.validated_shapes)\n            not_among = np.nonzero(1 - self.first_image.validated_shapes)\n            # Use the converted image to tell if the background is lighter, for analysis purposes\n            if self.first_image.image[among[0], among[1]].mean() &gt; self.first_image.image[not_among[0], not_among[1]].mean():\n                self.vars['lighter_background'] = False\n            # Use the original image to tell if the background is lighter, for display purposes\n            if self.first_image.bgr[among[0], among[1], ...].mean() &gt; self.first_image.bgr[not_among[0], not_among[1], ...].mean():\n                self.vars['contour_color'] = 255\n        else:\n            if self.last_im is None:\n                self.get_last_image()\n                # self.cropping(is_first_image=False)\n                self.fast_last_image_segmentation()\n            if self.last_image.binary_image.sum() == 0:\n                self.fast_last_image_segmentation()\n            among = np.nonzero(self.last_image.binary_image)\n            not_among = np.nonzero(1 - self.last_image.binary_image)\n            # Use the converted image to tell if the background is lighter, for analysis purposes\n            if self.last_image.image[among[0], among[1]].mean() &gt; self.last_image.image[not_among[0], not_among[1]].mean():\n                self.vars['lighter_background'] = False\n            # Use the original image to tell if the background is lighter, for display purposes\n            if self.last_image.bgr[among[0], among[1], ...].mean() &gt; self.last_image.bgr[not_among[0], not_among[1], ...].mean():\n                self.vars['contour_color'] = 255\n        if self.vars['origin_state'] == \"invisible\":\n            binary_image = deepcopy(self.first_image.binary_image)\n            self.first_image.convert_and_segment(self.vars['convert_for_motion'], self.vars[\"color_number\"],\n                                                 None, None, subtract_background=None,\n                                                 subtract_background2=None,\n                                                 rolling_window_segmentation=self.vars['rolling_window_segmentation'],\n                                                 filter_spec=self.vars[\"filter_spec\"])\n            covered_values = self.first_image.image[np.nonzero(binary_image)]\n            self.vars['luminosity_threshold'] = 127\n            if len(covered_values) &gt; 0:\n                if self.vars['lighter_background']:\n                    if np.max(covered_values) &lt; 255:\n                        self.vars['luminosity_threshold'] = np.max(covered_values) + 1\n                else:\n                    if np.min(covered_values) &gt; 0:\n                        self.vars['luminosity_threshold'] = np.min(covered_values) - 1\n\n    def delineate_each_arena(self):\n        \"\"\"\n        Determine the coordinates of each arena for video analysis.\n\n        The function processes video frames to identify bounding boxes around\n        specimens and determines valid arenas for analysis. In case of existing data,\n        it uses previously computed coordinates if available and valid.\n\n        Returns\n        -------\n        analysis_status : dict\n            A dictionary containing flags and messages indicating the status of\n            the analysis.\n            - 'continue' (bool): Whether to continue processing.\n            - 'message' (str): Informational or error message.\n\n        Notes\n        -----\n        This function relies on the existence of certain attributes and variables\n        defined in the class instance.\n        \"\"\"\n        analysis_status = {\"continue\": True, \"message\": \"\"}\n        if not self.vars['several_blob_per_arena'] and (self.sample_number &gt; 1):\n            compute_get_bb: bool = True\n            if (not self.all['overwrite_unaltered_videos'] and os.path.isfile('Data to run Cellects quickly.pkl')):\n\n                pickle_rick = PickleRick()\n                data_to_run_cellects_quickly = pickle_rick.read_file('Data to run Cellects quickly.pkl')\n                if data_to_run_cellects_quickly is not None:\n                    if 'bb_coord' in data_to_run_cellects_quickly['all']['vars']:\n                        (ccy1, ccy2, ccx1, ccx2, self.top, self.bot, self.left, self.right) = \\\n                            data_to_run_cellects_quickly['all']['vars']['bb_coord']\n                        self.first_image.crop_coord = [ccy1, ccy2, ccx1, ccx2]\n                        if (self.first_image.image.shape[0] == (ccy2 - ccy1)) and (\n                                self.first_image.image.shape[1] == (ccx2 - ccx1)):  # maybe useless now\n                            logging.info(\"Get the coordinates of all arenas from Data to run Cellects quickly.pkl\")\n                            compute_get_bb = False\n\n            if compute_get_bb:\n                motion_list = None\n                if self.all['are_gravity_centers_moving']:\n                    motion_list = self._segment_blob_motion(sample_size=5)\n                self.get_bounding_boxes(are_gravity_centers_moving=self.all['are_gravity_centers_moving'] == 1,\n                    motion_list=motion_list, all_specimens_have_same_direction=self.all['all_specimens_have_same_direction'])\n\n                if np.any(self.ordered_stats[:, 4] &gt; 100 * np.median(self.ordered_stats[:, 4])):\n                    analysis_status['message'] = \"A specimen is at least 100 times larger: click previous and retry by specifying 'back' areas.\"\n                    analysis_status['continue'] = False\n                if np.any(self.ordered_stats[:, 4] &lt; 0.01 * np.median(self.ordered_stats[:, 4])):\n                    analysis_status['message'] = \"A specimen is at least 100 times smaller: click previous and retry by specifying 'back' areas.\"\n                    analysis_status['continue'] = False\n                del self.ordered_stats\n                logging.info(\n                    str(self.not_analyzed_individuals) + \" individuals are out of picture scope and cannot be analyzed\")\n\n        else:\n            self._whole_image_bounding_boxes()\n            self.sample_number = 1\n        self._set_analyzed_individuals()\n        self.vars['arena_coord'] = []\n        self.list_coordinates()\n        return analysis_status\n\n    def _segment_blob_motion(self, sample_size: int) -&gt; list:\n        \"\"\"\n        Segment blob motion from the data list at specified sample sizes.\n\n        Parameters\n        ----------\n        sample_size : int\n            Number of samples to take from the data list.\n\n        Returns\n        -------\n        list\n            List containing segmented binary images at sampled frames.\n\n        Notes\n        -----\n        This function uses numpy for handling array operations and assumes the presence of certain attributes in the object, namely `data_list`, `first_image`, and `vars`.\n\n        Examples\n        --------\n        &gt;&gt;&gt; motion_samples = _segment_blob_motion(10)\n        &gt;&gt;&gt; print(len(motion_samples))  # Expected output: 10\n        \"\"\"\n        motion_list = list()\n        if isinstance(self.data_list, list):\n            frame_number = len(self.data_list)\n        else:\n            frame_number = self.data_list.shape[0]\n        sample_numbers = np.floor(np.linspace(0, frame_number, sample_size)).astype(int)\n        if not 'lighter_background' in self.vars.keys():\n            self.find_if_lighter_background()\n        for frame_idx in np.arange(sample_size):\n            if frame_idx == 0:\n                motion_list.insert(frame_idx, self.first_image.validated_shapes)\n            else:\n                if isinstance(self.data_list[0], str):\n                    image = self.data_list[sample_numbers[frame_idx] - 1]\n                else:\n                    image = self.data_list[sample_numbers[frame_idx] - 1]\n                if isinstance(image, str):\n                    is_landscape = self.first_image.image.shape[0] &lt; self.first_image.image.shape[1]\n                    image = read_and_rotate(image, self.first_image.bgr, self.all['raw_images'],\n                                            is_landscape, self.first_image.crop_coord)\n                    # image = readim(image)\n                In = OneImageAnalysis(image)\n                if self.vars['drift_already_corrected']:\n                    In.check_if_image_border_attest_drift_correction()\n                    # In.adjust_to_drift_correction(self.vars['convert_for_motion']['logical'])\n                In.convert_and_segment(self.vars['convert_for_motion'], self.vars['color_number'], None, None,\n                                       self.first_image.subtract_background, self.first_image.subtract_background2,\n                                       self.vars['rolling_window_segmentation'], self.vars['lighter_background'],\n                                       allowed_window=In.drift_mask_coord, filter_spec=self.vars['filter_spec'])\n                motion_list.insert(frame_idx, In.binary_image)\n        return motion_list\n\n\n    def get_bounding_boxes(self, are_gravity_centers_moving: bool, motion_list: list=(), all_specimens_have_same_direction: bool=True, original_shape_hsize: int=None):\n        \"\"\"Get the coordinates of arenas using bounding boxes.\n\n        Parameters\n        ----------\n        are_gravity_centers_moving : bool\n            Flag indicating whether gravity centers are moving or not.\n        motion_list : list\n            List of motion information for the specimens.\n        all_specimens_have_same_direction : bool, optional\n            Flag indicating whether all specimens have the same direction,\n            by default True.\n        Notes\n        -----\n        This method uses various internal methods and variables to determine the bounding boxes.\n        \"\"\"\n        # 7) Create required empty arrays: especially the bounding box coordinates of each video\n        self.ordered_first_image = None\n        self.shapes_to_remove = None\n        if self.first_image.crop_coord is None:\n            self.first_image.get_crop_coordinates()\n\n        logging.info(\"Get the coordinates of all arenas using the get_bounding_boxes method of the VideoMaker class\")\n        if self.first_image.validated_shapes.any() and self.first_image.shape_number &gt; 0:\n            self.ordered_stats, ordered_centroids, self.ordered_first_image = rank_from_top_to_bottom_from_left_to_right(\n                self.first_image.validated_shapes, self.first_image.y_boundaries, get_ordered_image=True)\n            self.unchanged_ordered_fimg = deepcopy(self.ordered_first_image)\n            self.modif_validated_shapes = deepcopy(self.first_image.validated_shapes)\n            self.standard = - 1\n            counter = 0\n            while np.any(np.less(self.standard, 0)) and counter &lt; 20:\n                counter += 1\n                self.left = np.zeros(self.first_image.shape_number, dtype=np.int64)\n                self.right = np.repeat(self.modif_validated_shapes.shape[1], self.first_image.shape_number)\n                self.top = np.zeros(self.first_image.shape_number, dtype=np.int64)\n                self.bot = np.repeat(self.modif_validated_shapes.shape[0], self.first_image.shape_number)\n                if are_gravity_centers_moving:\n                    self.top, self.bot, self.left, self.right, self.ordered_first_image = get_bb_with_moving_centers(motion_list, all_specimens_have_same_direction,\n                                                     original_shape_hsize, self.first_image.validated_shapes,\n                                                     self.first_image.y_boundaries)\n                    new_ordered_first_image = np.zeros(self.ordered_first_image.shape, dtype=np.uint8)\n\n                    for i in np.arange(1, self.first_image.shape_number + 1):\n                        previous_shape = np.zeros(self.ordered_first_image.shape, dtype=np.uint8)\n                        previous_shape[np.nonzero(self.unchanged_ordered_fimg == i)] = 1\n                        new_potentials = np.zeros(self.ordered_first_image.shape, dtype=np.uint8)\n                        new_potentials[np.nonzero(self.ordered_first_image == i)] = 1\n                        new_potentials[np.nonzero(self.unchanged_ordered_fimg == i)] = 0\n\n                        pads = ProgressivelyAddDistantShapes(new_potentials, previous_shape, max_distance=2)\n                        pads.consider_shapes_sizes(min_shape_size=10)\n                        pads.connect_shapes(only_keep_connected_shapes=True, rank_connecting_pixels=False)\n                        new_ordered_first_image[np.nonzero(pads.expanded_shape)] = i\n                    self.ordered_first_image = new_ordered_first_image\n                    self.modif_validated_shapes = np.zeros(self.ordered_first_image.shape, dtype=np.uint8)\n                    self.modif_validated_shapes[np.nonzero(self.ordered_first_image)] = 1\n                    self.ordered_stats, ordered_centroids, self.ordered_first_image = rank_from_top_to_bottom_from_left_to_right(\n                        self.modif_validated_shapes, self.first_image.y_boundaries, get_ordered_image=True)\n                    self.top, self.bot, self.left, self.right = get_quick_bounding_boxes(self.modif_validated_shapes, self.ordered_first_image, self.ordered_stats)\n                else:\n                    self.top, self.bot, self.left, self.right = get_quick_bounding_boxes(self.modif_validated_shapes, self.ordered_first_image, self.ordered_stats)\n                self._standardize_video_sizes()\n            if counter == 20:\n                self.top[self.top &lt; 0] = 1\n                self.bot[self.bot &gt;= self.ordered_first_image.shape[0] - 1] = self.ordered_first_image.shape[0] - 2\n                self.left[self.left &lt; 0] = 1\n                self.right[self.right &gt;= self.ordered_first_image.shape[1] - 1] = self.ordered_first_image.shape[1] - 2\n            del self.ordered_first_image\n            del self.unchanged_ordered_fimg\n            del self.modif_validated_shapes\n            del self.standard\n            del self.shapes_to_remove\n            self.bot += 1\n            self.right += 1\n        else:\n            self._whole_image_bounding_boxes()\n\n    def _whole_image_bounding_boxes(self):\n        self.top, self.bot, self.left, self.right = np.array([0]), np.array([self.first_image.image.shape[0]]), np.array([0]), np.array([self.first_image.image.shape[1]])\n\n    def _standardize_video_sizes(self):\n        \"\"\"\n        Standardize video sizes by adjusting bounding boxes.\n\n        Extended Description\n        --------------------\n        This function adjusts the bounding boxes of detected shapes in a video frame.\n        It ensures that all bounding boxes are within the frame's boundaries and\n        standardizes their sizes to avoid issues with odd dimensions during video writing.\n\n        Returns\n        -------\n        None\n            The function modifies the following attributes of the class instance:\n\n        Attributes Modified\n        ------------------\n        standard : numpy.ndarray\n            Standardized bounding boxes.\n        shapes_to_remove : numpy.ndarray\n            Indices of shapes to be removed from the image.\n        modif_validated_shapes : numpy.ndarray\n            Modified validated shapes after removing out-of-picture areas.\n        ordered_stats : list of float\n            Updated order statistics for the shapes.\n        ordered_centroids : numpy.ndarray\n            Centroids of the ordered shapes.\n        ordered_first_image : numpy.ndarray\n            First image with updated order statistics and centroids.\n        first_image.shape_number : int\n            Updated number of shapes in the first image.\n        not_analyzed_individuals : numpy.ndarray\n            Indices of individuals not analyzed after modifications.\n\n        \"\"\"\n        distance_threshold_to_consider_an_arena_out_of_the_picture = None# in pixels, worked nicely with - 50\n\n        # The modifications allowing to not make videos of setups out of view, do not work for moving centers\n        y_diffs = self.bot - self.top\n        x_diffs = self.right - self.left\n        add_to_y = ((np.max(y_diffs) - y_diffs) / 2)\n        add_to_x = ((np.max(x_diffs) - x_diffs) / 2)\n        self.standard = np.zeros((len(self.top), 4), dtype=np.int64)\n        self.standard[:, 0] = self.top - np.uint8(np.floor(add_to_y))\n        self.standard[:, 1] = self.bot + np.uint8(np.ceil(add_to_y))\n        self.standard[:, 2] = self.left - np.uint8(np.floor(add_to_x))\n        self.standard[:, 3] = self.right + np.uint8(np.ceil(add_to_x))\n\n        # Monitor if one bounding box gets out of picture shape\n        out_of_pic = deepcopy(self.standard)\n        out_of_pic[:, 1] = self.ordered_first_image.shape[0] - out_of_pic[:, 1] - 1\n        out_of_pic[:, 3] = self.ordered_first_image.shape[1] - out_of_pic[:, 3] - 1\n\n        if distance_threshold_to_consider_an_arena_out_of_the_picture is None:\n            distance_threshold_to_consider_an_arena_out_of_the_picture = np.min(out_of_pic) - 1\n\n        # If it occurs at least one time, apply a correction, otherwise, continue and write videos\n        # If the overflow is strong, remove the corresponding individuals and remake bounding_box finding\n        if np.any(np.less(out_of_pic, distance_threshold_to_consider_an_arena_out_of_the_picture)):\n            # Remove shapes\n            self.standard = - 1\n            self.shapes_to_remove = np.nonzero(np.less(out_of_pic, - 20))[0]\n            for shape_i in self.shapes_to_remove:\n                self.ordered_first_image[self.ordered_first_image == (shape_i + 1)] = 0\n            self.modif_validated_shapes = np.zeros(self.ordered_first_image.shape, dtype=np.uint8)\n            self.modif_validated_shapes[np.nonzero(self.ordered_first_image)] = 1\n            self.ordered_stats, ordered_centroids, self.ordered_first_image = rank_from_top_to_bottom_from_left_to_right(\n                self.modif_validated_shapes, self.first_image.y_boundaries, get_ordered_image=True)\n\n            self.first_image.shape_number = self.first_image.shape_number - len(self.shapes_to_remove)\n            self.not_analyzed_individuals = np.unique(self.unchanged_ordered_fimg -\n                                                      (self.unchanged_ordered_fimg * self.modif_validated_shapes))[1:]\n\n        else:\n            # Reduce all box sizes if necessary and proceed\n            if np.any(np.less(out_of_pic, 0)):\n                # When the overflow is weak, remake standardization with lower \"add_to_y\" and \"add_to_x\"\n                overflow = np.nonzero(np.logical_and(np.less(out_of_pic, 0), np.greater_equal(out_of_pic, distance_threshold_to_consider_an_arena_out_of_the_picture)))[0]\n                # Look if overflow occurs on the y axis\n                if np.any(np.less(out_of_pic[overflow, :2], 0)):\n                    add_to_top_and_bot = np.min(out_of_pic[overflow, :2])\n                    self.standard[:, 0] = self.standard[:, 0] - add_to_top_and_bot\n                    self.standard[:, 1] = self.standard[:, 1] + add_to_top_and_bot\n                # Look if overflow occurs on the x axis\n                if np.any(np.less(out_of_pic[overflow, 2:], 0)):\n                    add_to_left_and_right = np.min(out_of_pic[overflow, 2:])\n                    self.standard[:, 2] = self.standard[:, 2] - add_to_left_and_right\n                    self.standard[:, 3] = self.standard[:, 3] + add_to_left_and_right\n            # If x or y sizes are odd, make them even :\n            # Don't know why, but opencv remove 1 to odd shapes when writing videos\n            if (self.standard[0, 1] - self.standard[0, 0]) % 2 != 0:\n                self.standard[:, 1] -= 1\n            if (self.standard[0, 3] - self.standard[0, 2]) % 2 != 0:\n                self.standard[:, 3] -= 1\n            self.top = self.standard[:, 0]\n            self.bot = self.standard[:, 1]\n            self.left = self.standard[:, 2]\n            self.right = self.standard[:, 3]\n\n    def get_origins_and_backgrounds_lists(self):\n        \"\"\"\n        Create origins and background lists for image processing.\n\n        Extended Description\n        --------------------\n        This method generates the origin and background lists by slicing the first image\n        and its background subtraction based on predefined boundaries. It handles cases where\n        the top, bottom, left, and right boundaries are not yet initialized.\n\n        Notes\n        -----\n        This method directly modifies the input image data. The `self.vars` dictionary is populated\n        with lists of sliced arrays from the first image and its background.\n\n        Attributes\n        ----------\n        self.vars : dict\n            Dictionary to store processed data.\n        self.first_image : ImageObject\n            The first image object containing validated shapes and background subtraction arrays.\n        \"\"\"\n        logging.info(\"Create origins and background lists\")\n        if self.top is None:\n            self._whole_image_bounding_boxes()\n\n        if not self.first_image.validated_shapes.any():\n            if self.vars['convert_for_motion'] is not None:\n                self.vars['convert_for_origin'] = self.vars['convert_for_motion']\n            self.fast_first_image_segmentation()\n        first_im = self.first_image.validated_shapes\n        self.vars['origin_list'] = []\n        self.vars['background_list'] = []\n        self.vars['background_list2'] = []\n        for rep in np.arange(len(self.vars['analyzed_individuals'])):\n            origin_coord = np.nonzero(first_im[self.top[rep]:self.bot[rep], self.left[rep]:self.right[rep]])\n            self.vars['origin_list'].append(origin_coord)\n        if self.vars['subtract_background']:\n            for rep in np.arange(len(self.vars['analyzed_individuals'])):\n                self.vars['background_list'].append(\n                    self.first_image.subtract_background[self.top[rep]:self.bot[rep], self.left[rep]:self.right[rep]])\n                if self.vars['convert_for_motion']['logical'] != 'None':\n                    self.vars['background_list2'].append(self.first_image.subtract_background2[self.top[rep]:\n                                                         self.bot[rep], self.left[rep]:self.right[rep]])\n\n    def complete_image_analysis(self):\n        if not self.visualize and len(self.last_image.im_combinations) &gt; 0:\n            self.last_image.binary_image = self.last_image.im_combinations[self.current_combination_id]['binary_image']\n            self.last_image.image = self.last_image.im_combinations[self.current_combination_id]['converted_image']\n        self.instantiate_tables()\n        if len(self.vars['exif']) &gt; 1:\n            self.vars['exif'] = self.vars['exif'][0]\n        if len(self.last_image.all_c_spaces) == 0:\n            self.last_image.all_c_spaces['bgr'] = self.last_image.bgr.copy()\n        if self.all['bio_mask'] is not None:\n            self.last_image.binary_image[self.all['bio_mask']] = 1\n        if self.all['back_mask'] is not None:\n            self.last_image.binary_image[self.all['back_mask']] = 0\n        for i, arena in enumerate(self.vars['analyzed_individuals']):\n            binary = self.last_image.binary_image[self.top[i]:self.bot[i], self.left[i]:self.right[i]]\n            efficiency_test = self.last_image.all_c_spaces['bgr'][self.top[i]:self.bot[i], self.left[i]:self.right[i], :]\n            if not self.vars['several_blob_per_arena']:\n                binary = keep_one_connected_component(binary)\n                one_row_per_frame = compute_one_descriptor_per_frame(binary[None, :, :],\n                                                                     arena,\n                                                                     self.vars['exif'],\n                                                                     self.vars['descriptors'],\n                                                                     self.vars['output_in_mm'],\n                                                                     self.vars['average_pixel_size'],\n                                                                     self.vars['do_fading'],\n                                                                     self.vars['save_coord_specimen'])\n                coord_network = None\n                coord_pseudopods = None\n                if self.vars['save_graph']:\n                    if coord_network is None:\n                        coord_network = np.array(np.nonzero(binary))\n                    extract_graph_dynamics(self.last_image.image[None, :, :], coord_network, arena,\n                                           0, None, coord_pseudopods)\n\n            else:\n                one_row_per_frame = compute_one_descriptor_per_colony(binary[None, :, :],\n                                                                      arena,\n                                                                      self.vars['exif'],\n                                                                      self.vars['descriptors'],\n                                                                      self.vars['output_in_mm'],\n                                                                      self.vars['average_pixel_size'],\n                                                                      self.vars['do_fading'],\n                                                                      self.vars['first_move_threshold'],\n                                                                      self.vars['save_coord_specimen'])\n            if self.vars['fractal_analysis']:\n                zoomed_binary, side_lengths = prepare_box_counting(binary,\n                                                                   min_mesh_side=self.vars[\n                                                                       'fractal_box_side_threshold'],\n                                                                   zoom_step=self.vars['fractal_zoom_step'],\n                                                                   contours=True)\n                box_counting_dimensions = box_counting_dimension(zoomed_binary, side_lengths)\n                one_row_per_frame[\"fractal_dimension\"] = box_counting_dimensions[0]\n                one_row_per_frame[\"fractal_box_nb\"] = box_counting_dimensions[1]\n                one_row_per_frame[\"fractal_r_value\"] = box_counting_dimensions[2]\n\n            one_descriptor_per_arena = {}\n            one_descriptor_per_arena[\"arena\"] = arena\n            one_descriptor_per_arena[\"first_move\"] = pd.NA\n            one_descriptor_per_arena[\"final_area\"] = binary.sum()\n            one_descriptor_per_arena[\"iso_digi_transi\"] = pd.NA\n            one_descriptor_per_arena[\"is_growth_isotropic\"] = pd.NA\n            self.update_one_row_per_arena(i, one_descriptor_per_arena)\n            self.update_one_row_per_frame(i * 1, (i + 1) * 1, one_row_per_frame)\n            contours = np.nonzero(get_contours(binary))\n            efficiency_test[contours[0], contours[1], :] = np.array((94, 0, 213), dtype=np.uint8)\n            self.add_analysis_visualization_to_first_and_last_images(i, efficiency_test, None)\n        self.save_tables(with_last_image=False)\n\n    def prepare_video_writing(self, img_list: list, min_ram_free: float, in_colors: bool=False, pathway: str=\"\"):\n        \"\"\"\n\n        Prepare the raw video (.npy) writing process for Cellects.\n\n        Parameters\n        ----------\n        img_list : list\n            List of images to be processed.\n        min_ram_free : float\n            Minimum amount of RAM in GB that should remain free.\n        in_colors : bool, optional\n            Whether the images are in color. Default is False.\n        pathway : str, optional\n            Path to save the video files. Default is an empty string.\n\n        Returns\n        -------\n        tuple\n            A tuple containing:\n            - bunch_nb: int, number of bunches needed for video writing.\n            - video_nb_per_bunch: int, number of videos per bunch.\n            - sizes: ndarray, dimensions of each video.\n            - video_bunch: list or ndarray, initialized video arrays.\n            - vid_names: list, names of the video files.\n            - rom_memory_required: None or float, required ROM memory.\n            - analysis_status: dict, status and message of the analysis process.\n            - remaining: int, remainder videos that do not fit in a complete bunch.\n\n        Notes\n        -----\n        - The function calculates necessary memory and ensures 10% extra to avoid issues.\n        - It checks for available RAM and adjusts the number of bunches accordingly.\n        - If using color images, memory requirements are tripled.\n\n        expected output depends on the provided images and RAM availability\n        \"\"\"\n        # 1) Create a list of video names\n        if self.not_analyzed_individuals is not None:\n            number_to_add = len(self.not_analyzed_individuals)\n        else:\n            number_to_add = 0\n        vid_names = list()\n        ind_i = 0\n        counter = 0\n        while ind_i &lt; (self.first_image.shape_number + number_to_add):\n            ind_i += 1\n            while np.any(np.isin(self.not_analyzed_individuals, ind_i)):\n                ind_i += 1\n            vid_names.append(pathway + \"ind_\" + str(ind_i) + \".npy\")\n            counter += 1\n        img_nb = len(img_list)\n\n        # 2) Create a table of the dimensions of each video\n        # Add 10% to the necessary memory to avoid problems\n        necessary_memory = img_nb * np.multiply((self.bot - self.top).astype(np.uint64), (self.right - self.left).astype(np.uint64)).sum() * 8 * 1.16415e-10\n        if in_colors:\n            sizes = np.column_stack(\n                (np.repeat(img_nb, self.first_image.shape_number), self.bot - self.top, self.right - self.left,\n                 np.repeat(3, self.first_image.shape_number)))\n            necessary_memory *= 3\n        else:\n            sizes = np.column_stack(\n                (np.repeat(img_nb, self.first_image.shape_number), self.bot - self.top, self.right - self.left))\n        use_list_of_vid = True\n        if np.all(sizes[0, :] == sizes):\n            use_list_of_vid = False\n        available_memory = (psutil.virtual_memory().available &gt;&gt; 30) - min_ram_free\n        if available_memory == 0:\n            analysis_status = {\"continue\": False, \"message\": \"There are not enough RAM available\"}\n            bunch_nb = 1\n        else:\n            bunch_nb = int(np.ceil(necessary_memory / available_memory))\n            if bunch_nb &gt; 1:\n                # The program will need twice the memory to create the second bunch.\n                bunch_nb = int(np.ceil(2 * necessary_memory / available_memory))\n\n        video_nb_per_bunch = np.floor(self.first_image.shape_number / bunch_nb).astype(np.uint8)\n        analysis_status = {\"continue\": True, \"message\": \"\"}\n        video_bunch = None\n        try:\n            if use_list_of_vid:\n                video_bunch = [np.zeros(sizes[i, :], dtype=np.uint8) for i in range(video_nb_per_bunch)]\n            else:\n                video_bunch = np.zeros(np.append(sizes[0, :], video_nb_per_bunch), dtype=np.uint8)\n        except ValueError as v_err:\n            analysis_status = {\"continue\": False, \"message\": \"Probably failed to detect the right cell(s) number, do the first image analysis manually.\"}\n            logging.error(f\"{analysis_status['message']} error is: {v_err}\")\n        # Check for available ROM memory\n        if (psutil.disk_usage('/')[2] &gt;&gt; 30) &lt; (necessary_memory + 2):\n            rom_memory_required = necessary_memory + 2\n        else:\n            rom_memory_required = None\n        remaining = self.first_image.shape_number % bunch_nb\n        if remaining &gt; 0:\n            bunch_nb += 1\n        is_landscape = self.first_image.image.shape[0] &lt; self.first_image.image.shape[1]\n        logging.info(f\"Cellects will start writing {self.first_image.shape_number} videos. Given available memory, it will do it in {bunch_nb} time(s)\")\n        return bunch_nb, video_nb_per_bunch, sizes, video_bunch, vid_names, rom_memory_required, analysis_status, remaining, use_list_of_vid, is_landscape\n\n\n\n    def update_output_list(self):\n        \"\"\"\n        Update the output list with various descriptors from the analysis results.\n\n        This method processes different types of descriptors and assigns them to\n        the `self.vars['descriptors']` dictionary. It handles special cases for\n        descriptors related to 'xy' dimensions and ensures that all relevant metrics\n        are stored in the output list.\n        \"\"\"\n        self.vars['descriptors'] = {}\n        for descriptor in self.all['descriptors'].keys():\n            if descriptor == 'standard_deviation_xy':\n                self.vars['descriptors']['standard_deviation_x'] = self.all['descriptors'][descriptor]\n                self.vars['descriptors']['standard_deviation_y'] = self.all['descriptors'][descriptor]\n            elif descriptor == 'skewness_xy':\n                self.vars['descriptors']['skewness_x'] = self.all['descriptors'][descriptor]\n                self.vars['descriptors']['skewness_y'] = self.all['descriptors'][descriptor]\n            elif descriptor == 'kurtosis_xy':\n                self.vars['descriptors']['kurtosis_x'] = self.all['descriptors'][descriptor]\n                self.vars['descriptors']['kurtosis_y'] = self.all['descriptors'][descriptor]\n            elif descriptor == 'major_axes_len_and_angle':\n                self.vars['descriptors']['major_axis_len'] = self.all['descriptors'][descriptor]\n                self.vars['descriptors']['minor_axis_len'] = self.all['descriptors'][descriptor]\n                self.vars['descriptors']['axes_orientation'] = self.all['descriptors'][descriptor]\n            else:\n                if np.isin(descriptor, list(from_shape_descriptors_class.keys())):\n\n                    self.vars['descriptors'][descriptor] = self.all['descriptors'][descriptor]\n        self.vars['descriptors']['newly_explored_area'] = self.vars['do_fading']\n\n    def update_available_core_nb(self, image_bit_number=256, video_bit_number=140):# video_bit_number=176\n        \"\"\"\n        Update available computation resources based on memory and processing constraints.\n\n        Parameters\n        ----------\n        image_bit_number : int, optional\n            Number of bits per image pixel (default is 256).\n        video_bit_number : int, optional\n            Number of bits per video frame pixel (default is 140).\n\n        Other Parameters\n        ----------------\n        lose_accuracy_to_save_memory : bool\n            Flag to reduce accuracy for memory savings.\n        convert_for_motion : dict\n            Conversion settings for motion analysis.\n        already_greyscale : bool\n            Flag indicating if the image is already greyscale.\n        save_coord_thickening_slimming : bool\n            Flag to save coordinates for thickening and slimming.\n        oscilacyto_analysis : bool\n            Flag indicating if oscilacyto analysis is enabled.\n        save_coord_network : bool\n            Flag to save coordinates for network analysis.\n\n        Returns\n        -------\n        float\n            Rounded absolute difference between available memory and necessary memory in GB.\n\n        Notes\n        -----\n        Performance considerations and limitations should be noted here if applicable.\n\n        \"\"\"\n        if self.vars['lose_accuracy_to_save_memory']:\n            video_bit_number -= 56\n        if self.vars['convert_for_motion']['logical'] != 'None':\n            video_bit_number += 64\n            if self.vars['lose_accuracy_to_save_memory']:\n                video_bit_number -= 56\n        if self.vars['already_greyscale']:\n            video_bit_number -= 64\n        if self.vars['save_coord_thickening_slimming'] or self.vars['oscilacyto_analysis']:\n            video_bit_number += 16\n            image_bit_number += 128\n        if self.vars['save_coord_network']:\n            video_bit_number += 8\n            image_bit_number += 64\n\n        if isinstance(self.bot, list):\n            one_image_memory = np.multiply((self.bot[0] - self.top[0]),\n                                        (self.right[0] - self.left[0])).max().astype(np.uint64)\n        else:\n            one_image_memory = np.multiply((self.bot - self.top).astype(np.uint64),\n                                        (self.right - self.left).astype(np.uint64)).max()\n        one_video_memory = self.vars['img_number'] * one_image_memory\n        necessary_memory = (one_image_memory * image_bit_number + one_video_memory * video_bit_number) * 1.16415e-10\n        available_memory = (virtual_memory().available &gt;&gt; 30) - self.vars['min_ram_free']\n        max_repeat_in_memory = (available_memory // necessary_memory).astype(np.uint16)\n        if max_repeat_in_memory &gt; 1:\n            max_repeat_in_memory = np.max(((available_memory // (2 * necessary_memory)).astype(np.uint16), 1))\n\n\n        self.cores = np.min((self.all['cores'], max_repeat_in_memory))\n        if self.cores &gt; self.sample_number:\n            self.cores = self.sample_number\n        return np.round(np.absolute(available_memory - necessary_memory), 3)\n\n\n    def update_one_row_per_arena(self, i: int, table_to_add):\n        \"\"\"\n        Update one row of the dataframe per arena.\n\n        Add a row to a DataFrame for each arena, based on the provided table_to_add. If no previous rows exist,\n        initialize a new DataFrame with zeros.\n\n        Parameters\n        ----------\n        i : int\n            Index of the arena to update.\n        table_to_add : dict\n            Dictionary containing values to add. Keys are column names, values are the data.\n\n        \"\"\"\n        if not self.vars['several_blob_per_arena']:\n            if self.one_row_per_arena is None:\n                self.one_row_per_arena = pd.DataFrame(np.zeros((len(self.vars['analyzed_individuals']), len(table_to_add)), dtype=float),\n                                            columns=table_to_add.keys())\n            self.one_row_per_arena.iloc[i, :] = table_to_add.values()\n\n\n    def update_one_row_per_frame(self, i: int, j: int, table_to_add):\n        \"\"\"\n        Update a range of rows in `self.one_row_per_frame` DataFrame with values from\n        `table_to_add`.\n\n        Parameters\n        ----------\n        i : int\n            The starting row index to update in `self.one_row_per_frame`.\n        j : int\n            The ending row index (exclusive) to update in `self.one_row_per_frame`.\n        table_to_add : dict\n            A dictionary where keys are column labels and values are lists or arrays of\n            data to insert into `self.one_row_per_frame`.\n        Notes\n        -----\n        Ensures that one row per arena is being updated. If `self.one_row_per_frame` is\n        None, it initializes a DataFrame to hold the data.\n        \"\"\"\n        if not self.vars['several_blob_per_arena']:\n            if self.one_row_per_frame is None:\n                self.one_row_per_frame = pd.DataFrame(index=range(len(self.vars['analyzed_individuals']) *\n                                                        self.vars['img_number']),\n                                            columns=table_to_add.keys())\n\n            self.one_row_per_frame.iloc[i:j, :] = table_to_add\n\n\n    def instantiate_tables(self):\n        \"\"\"\n        Update output list and prepare results tables and validation images.\n\n        Extended Description\n        --------------------\n        This method performs necessary preparations for processing image sequences,\n        including updating the output list and initializing key attributes required\n        for subsequent operations.\n\n        \"\"\"\n        self.update_output_list()\n        logging.info(\"Instantiate results tables and validation images\")\n        self.fractal_box_sizes = None\n        self.one_row_per_arena = None\n        self.one_row_per_frame = None\n        if self.vars['already_greyscale']:\n            if len(self.first_image.bgr.shape) == 2:\n                self.first_image.bgr = np.stack((self.first_image.bgr, self.first_image.bgr, self.first_image.bgr), axis=2).astype(np.uint8)\n            if len(self.last_image.bgr.shape) == 2:\n                self.last_image.bgr = np.stack((self.last_image.bgr, self.last_image.bgr, self.last_image.bgr), axis=2).astype(np.uint8)\n            self.vars[\"convert_for_motion\"] = {\"bgr\": np.array((1, 1, 1), dtype=np.uint8), \"logical\": \"None\"}\n\n    def add_analysis_visualization_to_first_and_last_images(self, i: int, first_visualization: NDArray, last_visualization: NDArray=None):\n        \"\"\"\n        Adds analysis visualizations to the first and last images of a sequence.\n\n        Parameters\n        ----------\n        i : int\n            Index of the image in the sequence.\n        first_visualization : NDArray[np.uint8]\n            The visualization to add to the first image.\n        last_visualization : NDArray[np.uint8]\n            The visualization to add to the last image.\n\n        Other Parameters\n        ----------------\n        vars : dict\n            Dictionary containing various parameters.\n        arena_shape : str, optional\n            The shape of the arena. Either 'circle' or other shapes.\n\n        Notes\n        -----\n        If `arena_shape` is 'circle', the visualization will be masked by an ellipse.\n\n        \"\"\"\n        minmax = (self.top[i], self.bot[i], self.left[i], self.right[i])\n        self.first_image.bgr = draw_img_with_mask(self.first_image.bgr, self.first_image.bgr.shape[:2], minmax,\n                                                  self.vars['arena_shape'], first_visualization)\n        if last_visualization is not None:\n            self.last_image.bgr = draw_img_with_mask(self.last_image.bgr, self.last_image.bgr.shape[:2], minmax,\n                                                      self.vars['arena_shape'], last_visualization)\n\n\n    def save_tables(self, with_last_image: bool=True):\n        \"\"\"\n        Exports analysis results to CSV files and saves visualization outputs.\n\n        Generates the following output:\n        - one_row_per_arena.csv, one_row_per_frame.csv : Tracking data per arena/frame.\n        - software_settings.csv : Full configuration settings for reproducibility.\n\n        Raises\n        ------\n        PermissionError\n            If any output file is already open in an external program (logged and re-raised).\n\n        Notes\n        -----\n        Ensure no exported CSV files are open while running this method to avoid permission errors. This\n        function will fail gracefully if the files cannot be overwritten.\n\n        \"\"\"\n        logging.info(\"Save results tables and validation images\")\n        if not self.vars['several_blob_per_arena']:\n            try:\n                self.one_row_per_arena.to_csv(\"one_row_per_arena.csv\", sep=\";\", index=False, lineterminator='\\n')\n                del self.one_row_per_arena\n            except PermissionError:\n                logging.error(\"Never let one_row_per_arena.csv open when Cellects runs\")\n            try:\n                self.one_row_per_frame.to_csv(\"one_row_per_frame.csv\", sep=\";\", index=False, lineterminator='\\n')\n                del self.one_row_per_frame\n            except PermissionError:\n                logging.error(\"Never let one_row_per_frame.csv open when Cellects runs\")\n        if self.all['extension'] == '.JPG':\n            extension = '.PNG'\n        else:\n            extension = '.JPG'\n        if with_last_image:\n            cv2.imwrite(f\"Analysis efficiency, last image{extension}\", self.last_image.bgr)\n        cv2.imwrite(\n            f\"Analysis efficiency, {np.ceil(self.vars['img_number'] / 10).astype(np.uint64)}th image{extension}\",\n            self.first_image.bgr)\n        software_settings = deepcopy(self.vars)\n        for key in ['descriptors', 'analyzed_individuals', 'exif', 'dims', 'origin_list', 'background_list', 'background_list2', 'descriptors', 'folder_list', 'sample_number_per_folder']:\n            software_settings.pop(key, None)\n        global_settings = deepcopy(self.all)\n        for key in ['analyzed_individuals', 'night_mode', 'expert_mode', 'is_auto', 'arena', 'video_option', 'compute_all_options', 'vars', 'dims', 'origin_list', 'background_list', 'background_list2', 'descriptors', 'folder_list', 'sample_number_per_folder']:\n            global_settings.pop(key, None)\n        software_settings.update(global_settings)\n        software_settings.pop('video_list', None)\n        software_settings = pd.DataFrame.from_dict(software_settings, columns=[\"Setting\"], orient='index')\n        try:\n            software_settings.to_csv(\"software_settings.csv\", sep=\";\")\n        except PermissionError:\n            logging.error(\"Never let software_settings.csv open when Cellects runs\")\n</code></pre>"},{"location":"api/cellects/core/program_organizer/#cellects.core.program_organizer.ProgramOrganizer.__init__","title":"<code>__init__()</code>","text":"<p>This class stores all variables required for analysis as well as methods to process it. Global variables (i.e. that does not concern the MotionAnalysis) are directly stored in self. Variables used in the MotionAnalysis class are stored in a dict called self.vars</p> Source code in <code>src/cellects/core/program_organizer.py</code> <pre><code>def __init__(self):\n    \"\"\"\n        This class stores all variables required for analysis as well as\n        methods to process it.\n        Global variables (i.e. that does not concern the MotionAnalysis)\n        are directly stored in self.\n        Variables used in the MotionAnalysis class are stored in a dict\n        called self.vars\n    \"\"\"\n    if os.path.isfile('PickleRick.pkl'):\n        os.remove('PickleRick.pkl')\n    if os.path.isfile('PickleRick0.pkl'):\n        os.remove('PickleRick0.pkl')\n    self.one_arena_done: bool = False\n    self.reduce_image_dim: bool = False\n    self.first_exp_ready_to_run: bool = False\n    self.data_to_save = {'first_image': False, 'exif': False, 'vars': False}\n    self.sample_number = 1\n    self.top = None\n    self.motion = None\n    self.analysis_instance = None\n    self.computed_video_options = np.zeros(5, bool)\n    self.vars = {}\n    self.all = {}\n    self.all['folder_list'] = []\n    self.vars['first_detection_frame'] = 0\n    self.first_im = None\n    self.last_im = None\n    self.vars['background_list'] = []\n    self.starting_blob_hsize_in_pixels = None\n    self.vars['first_move_threshold'] = None\n    self.vars['convert_for_origin'] = None\n    self.vars['convert_for_motion'] = None\n    self.current_combination_id = 0\n    self.data_list = []\n    self.one_row_per_arena = None\n    self.one_row_per_frame = None\n    self.not_analyzed_individuals = None\n    self.visualize: bool = True\n    self.network_shaped: bool = False\n</code></pre>"},{"location":"api/cellects/core/program_organizer/#cellects.core.program_organizer.ProgramOrganizer.add_analysis_visualization_to_first_and_last_images","title":"<code>add_analysis_visualization_to_first_and_last_images(i, first_visualization, last_visualization=None)</code>","text":"<p>Adds analysis visualizations to the first and last images of a sequence.</p> <p>Parameters:</p> Name Type Description Default <code>i</code> <code>int</code> <p>Index of the image in the sequence.</p> required <code>first_visualization</code> <code>NDArray[uint8]</code> <p>The visualization to add to the first image.</p> required <code>last_visualization</code> <code>NDArray[uint8]</code> <p>The visualization to add to the last image.</p> <code>None</code> <p>Other Parameters:</p> Name Type Description <code>vars</code> <code>dict</code> <p>Dictionary containing various parameters.</p> <code>arena_shape</code> <code>str</code> <p>The shape of the arena. Either 'circle' or other shapes.</p> Notes <p>If <code>arena_shape</code> is 'circle', the visualization will be masked by an ellipse.</p> Source code in <code>src/cellects/core/program_organizer.py</code> <pre><code>def add_analysis_visualization_to_first_and_last_images(self, i: int, first_visualization: NDArray, last_visualization: NDArray=None):\n    \"\"\"\n    Adds analysis visualizations to the first and last images of a sequence.\n\n    Parameters\n    ----------\n    i : int\n        Index of the image in the sequence.\n    first_visualization : NDArray[np.uint8]\n        The visualization to add to the first image.\n    last_visualization : NDArray[np.uint8]\n        The visualization to add to the last image.\n\n    Other Parameters\n    ----------------\n    vars : dict\n        Dictionary containing various parameters.\n    arena_shape : str, optional\n        The shape of the arena. Either 'circle' or other shapes.\n\n    Notes\n    -----\n    If `arena_shape` is 'circle', the visualization will be masked by an ellipse.\n\n    \"\"\"\n    minmax = (self.top[i], self.bot[i], self.left[i], self.right[i])\n    self.first_image.bgr = draw_img_with_mask(self.first_image.bgr, self.first_image.bgr.shape[:2], minmax,\n                                              self.vars['arena_shape'], first_visualization)\n    if last_visualization is not None:\n        self.last_image.bgr = draw_img_with_mask(self.last_image.bgr, self.last_image.bgr.shape[:2], minmax,\n                                                  self.vars['arena_shape'], last_visualization)\n</code></pre>"},{"location":"api/cellects/core/program_organizer/#cellects.core.program_organizer.ProgramOrganizer.cropping","title":"<code>cropping(is_first_image)</code>","text":"<p>Crops the image based on specified conditions and settings.</p> <p>This method checks if drift correction has already been applied. If the image is the first one and hasn't been cropped yet, it will attempt to use pre-stored coordinates or compute new crop coordinates. If automatic cropping is enabled, it will apply the cropping process.</p> <p>Parameters:</p> Name Type Description Default <code>is_first_image</code> <code>bool</code> <p>Indicates whether the image being processed is the first one in the sequence.</p> required Source code in <code>src/cellects/core/program_organizer.py</code> <pre><code>def cropping(self, is_first_image: bool):\n    \"\"\"\n    Crops the image based on specified conditions and settings.\n\n    This method checks if drift correction has already been applied.\n    If the image is the first one and hasn't been cropped yet, it will attempt\n    to use pre-stored coordinates or compute new crop coordinates. If automatic\n    cropping is enabled, it will apply the cropping process.\n\n    Parameters\n    ----------\n    is_first_image : bool\n        Indicates whether the image being processed is the first one in the sequence.\n    \"\"\"\n    if not self.vars['drift_already_corrected']:\n        if is_first_image:\n            if not self.first_image.cropped:\n                if (not self.all['overwrite_unaltered_videos'] and os.path.isfile('Data to run Cellects quickly.pkl')):\n                    self.first_image.get_crop_coordinates()\n                    pickle_rick = PickleRick()\n                    data_to_run_cellects_quickly = pickle_rick.read_file('Data to run Cellects quickly.pkl')\n                    if data_to_run_cellects_quickly is not None and 'bb_coord' in data_to_run_cellects_quickly['all']['vars']:\n                        logging.info(\"Get crop coordinates from Data to run Cellects quickly.pkl\")\n                        (ccy1, ccy2, ccx1, ccx2, self.top, self.bot, self.left, self.right) = \\\n                            data_to_run_cellects_quickly['all']['vars']['bb_coord']\n                        self.first_image.crop_coord = [ccy1, ccy2, ccx1, ccx2]\n                else:\n                    self.first_image.get_crop_coordinates()\n                if self.all['automatically_crop']:\n                    self.first_image.automatically_crop(self.first_image.crop_coord)\n                else:\n                    self.first_image.crop_coord = None\n        else:\n            if not self.last_image.cropped and self.all['automatically_crop']:\n                self.last_image.automatically_crop(self.first_image.crop_coord)\n</code></pre>"},{"location":"api/cellects/core/program_organizer/#cellects.core.program_organizer.ProgramOrganizer.delineate_each_arena","title":"<code>delineate_each_arena()</code>","text":"<p>Determine the coordinates of each arena for video analysis.</p> <p>The function processes video frames to identify bounding boxes around specimens and determines valid arenas for analysis. In case of existing data, it uses previously computed coordinates if available and valid.</p> <p>Returns:</p> Name Type Description <code>analysis_status</code> <code>dict</code> <p>A dictionary containing flags and messages indicating the status of the analysis. - 'continue' (bool): Whether to continue processing. - 'message' (str): Informational or error message.</p> Notes <p>This function relies on the existence of certain attributes and variables defined in the class instance.</p> Source code in <code>src/cellects/core/program_organizer.py</code> <pre><code>def delineate_each_arena(self):\n    \"\"\"\n    Determine the coordinates of each arena for video analysis.\n\n    The function processes video frames to identify bounding boxes around\n    specimens and determines valid arenas for analysis. In case of existing data,\n    it uses previously computed coordinates if available and valid.\n\n    Returns\n    -------\n    analysis_status : dict\n        A dictionary containing flags and messages indicating the status of\n        the analysis.\n        - 'continue' (bool): Whether to continue processing.\n        - 'message' (str): Informational or error message.\n\n    Notes\n    -----\n    This function relies on the existence of certain attributes and variables\n    defined in the class instance.\n    \"\"\"\n    analysis_status = {\"continue\": True, \"message\": \"\"}\n    if not self.vars['several_blob_per_arena'] and (self.sample_number &gt; 1):\n        compute_get_bb: bool = True\n        if (not self.all['overwrite_unaltered_videos'] and os.path.isfile('Data to run Cellects quickly.pkl')):\n\n            pickle_rick = PickleRick()\n            data_to_run_cellects_quickly = pickle_rick.read_file('Data to run Cellects quickly.pkl')\n            if data_to_run_cellects_quickly is not None:\n                if 'bb_coord' in data_to_run_cellects_quickly['all']['vars']:\n                    (ccy1, ccy2, ccx1, ccx2, self.top, self.bot, self.left, self.right) = \\\n                        data_to_run_cellects_quickly['all']['vars']['bb_coord']\n                    self.first_image.crop_coord = [ccy1, ccy2, ccx1, ccx2]\n                    if (self.first_image.image.shape[0] == (ccy2 - ccy1)) and (\n                            self.first_image.image.shape[1] == (ccx2 - ccx1)):  # maybe useless now\n                        logging.info(\"Get the coordinates of all arenas from Data to run Cellects quickly.pkl\")\n                        compute_get_bb = False\n\n        if compute_get_bb:\n            motion_list = None\n            if self.all['are_gravity_centers_moving']:\n                motion_list = self._segment_blob_motion(sample_size=5)\n            self.get_bounding_boxes(are_gravity_centers_moving=self.all['are_gravity_centers_moving'] == 1,\n                motion_list=motion_list, all_specimens_have_same_direction=self.all['all_specimens_have_same_direction'])\n\n            if np.any(self.ordered_stats[:, 4] &gt; 100 * np.median(self.ordered_stats[:, 4])):\n                analysis_status['message'] = \"A specimen is at least 100 times larger: click previous and retry by specifying 'back' areas.\"\n                analysis_status['continue'] = False\n            if np.any(self.ordered_stats[:, 4] &lt; 0.01 * np.median(self.ordered_stats[:, 4])):\n                analysis_status['message'] = \"A specimen is at least 100 times smaller: click previous and retry by specifying 'back' areas.\"\n                analysis_status['continue'] = False\n            del self.ordered_stats\n            logging.info(\n                str(self.not_analyzed_individuals) + \" individuals are out of picture scope and cannot be analyzed\")\n\n    else:\n        self._whole_image_bounding_boxes()\n        self.sample_number = 1\n    self._set_analyzed_individuals()\n    self.vars['arena_coord'] = []\n    self.list_coordinates()\n    return analysis_status\n</code></pre>"},{"location":"api/cellects/core/program_organizer/#cellects.core.program_organizer.ProgramOrganizer.extract_exif","title":"<code>extract_exif()</code>","text":"<p>Extract EXIF data from image or video files.</p> Notes <p>If <code>extract_time_interval</code> is True and unsuccessful, arbitrary time steps will be used. Timings are normalized to minutes for consistency across different files.</p> Source code in <code>src/cellects/core/program_organizer.py</code> <pre><code>def extract_exif(self):\n    \"\"\"\n    Extract EXIF data from image or video files.\n\n    Notes\n    -----\n    If `extract_time_interval` is True and unsuccessful, arbitrary time steps will be used.\n    Timings are normalized to minutes for consistency across different files.\n    \"\"\"\n    self.vars['time_step_is_arbitrary'] = True\n    if self.all['im_or_vid'] == 1:\n        if not 'dims' in self.vars:\n            self.vars['dims'] = self.analysis_instance.shape[:3]\n        timings = np.arange(self.vars['dims'][0])\n    else:\n        timings = np.arange(len(self.data_list))\n        if sys.platform.startswith('win'):\n            pathway = os.getcwd() + '\\\\'\n        else:\n            pathway = os.getcwd() + '/'\n        if not 'extract_time_interval' in self.all:\n            self.all['extract_time_interval'] = True\n        if self.all['extract_time_interval']:\n            self.vars['time_step'] = 1\n            try:\n                timings = extract_time(self.data_list, pathway, self.all['raw_images'])\n                timings = timings - timings[0]\n                timings = timings / 60\n                time_step = np.diff(timings)\n                if len(time_step) &gt; 0:\n                    time_step = np.mean(time_step)\n                    digit_nb = 0\n                    for i in str(time_step):\n                        if i in {'.'}:\n                            pass\n                        elif i in {'0'}:\n                            digit_nb += 1\n                        else:\n                            break\n                    self.vars['time_step'] = np.round(time_step, digit_nb + 1)\n                    self.vars['time_step_is_arbitrary'] = False\n            except:\n                pass\n        else:\n            timings = np.arange(0, len(self.data_list) * self.vars['time_step'], self.vars['time_step'])\n            self.vars['time_step_is_arbitrary'] = False\n    return timings\n</code></pre>"},{"location":"api/cellects/core/program_organizer/#cellects.core.program_organizer.ProgramOrganizer.fast_first_image_segmentation","title":"<code>fast_first_image_segmentation()</code>","text":"<p>Segment the first or subsequent image in a series for biological and background masks.</p> Notes <p>This function processes the first or subsequent image in a sequence, applying biological and background masks, segmenting the image, and updating internal data structures accordingly. The function is specific to handling image sequences for biological analysis</p> Source code in <code>src/cellects/core/program_organizer.py</code> <pre><code>def fast_first_image_segmentation(self):\n    \"\"\"\n    Segment the first or subsequent image in a series for biological and background masks.\n\n    Notes\n    -----\n    This function processes the first or subsequent image in a sequence, applying biological and background masks,\n    segmenting the image, and updating internal data structures accordingly. The function is specific to handling\n    image sequences for biological analysis\n\n    \"\"\"\n    if not \"color_number\" in self.vars:\n        self.update_variable_dict()\n    if self.vars['convert_for_origin'] is None:\n        self.vars['convert_for_origin'] = {\"logical\": 'None', \"PCA\": np.ones(3, dtype=np.uint8)}\n    self.first_image.convert_and_segment(self.vars['convert_for_origin'], self.vars[\"color_number\"],\n                                         self.all[\"bio_mask\"], self.all[\"back_mask\"], subtract_background=None,\n                                         subtract_background2=None,\n                                         rolling_window_segmentation=self.vars[\"rolling_window_segmentation\"],\n                                         filter_spec=self.vars[\"filter_spec\"])\n    if not self.first_image.drift_correction_already_adjusted:\n        self.vars['drift_already_corrected'] = self.first_image.check_if_image_border_attest_drift_correction()\n        if self.vars['drift_already_corrected']:\n            logging.info(\"Cellects detected that the images have already been corrected for drift\")\n            self.first_image.convert_and_segment(self.vars['convert_for_origin'], self.vars[\"color_number\"],\n                                                 self.all[\"bio_mask\"], self.all[\"back_mask\"],\n                                                 subtract_background=None, subtract_background2=None,\n                                                 rolling_window_segmentation=self.vars[\"rolling_window_segmentation\"],\n                                                 filter_spec=self.vars[\"filter_spec\"],\n                                                 allowed_window=self.first_image.drift_mask_coord)\n\n    shapes_features = shape_selection(self.first_image.binary_image, true_shape_number=self.sample_number,\n                                      horizontal_size=self.starting_blob_hsize_in_pixels,\n                                      spot_shape=self.all['starting_blob_shape'],\n                                      several_blob_per_arena=self.vars['several_blob_per_arena'],\n                                      bio_mask=self.all[\"bio_mask\"], back_mask=self.all[\"back_mask\"])\n    self.first_image.validated_shapes, shape_number, stats, centroids = shapes_features\n    self.first_image.shape_number = shape_number\n    if self.first_image.im_combinations is None:\n        self.first_image.im_combinations = []\n    if len(self.first_image.im_combinations) == 0:\n        self.first_image.im_combinations.append({})\n    self.current_combination_id = np.min((self.current_combination_id, len(self.first_image.im_combinations) - 1))\n    self.first_image.im_combinations[self.current_combination_id]['csc'] = self.vars['convert_for_origin']\n    self.first_image.im_combinations[self.current_combination_id]['binary_image'] = self.first_image.validated_shapes\n    if self.first_image.greyscale is not None:\n        greyscale = self.first_image.greyscale\n    else:\n        greyscale = self.first_image.image\n    self.first_image.im_combinations[self.current_combination_id]['converted_image'] = bracket_to_uint8_image_contrast(greyscale)\n    self.first_image.im_combinations[self.current_combination_id]['shape_number'] = shape_number\n</code></pre>"},{"location":"api/cellects/core/program_organizer/#cellects.core.program_organizer.ProgramOrganizer.fast_last_image_segmentation","title":"<code>fast_last_image_segmentation(bio_mask=None, back_mask=None)</code>","text":"<p>Segment the first or subsequent image in a series for biological and background masks.</p> <p>Parameters:</p> Name Type Description Default <code>bio_mask</code> <code>NDArray[uint8]</code> <p>The biological mask to be applied to the image.</p> <code>None</code> <code>back_mask</code> <code>NDArray[uint8]</code> <p>The background mask to be applied to the image.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> Notes <p>This function processes the first or subsequent image in a sequence, applying biological and background masks, segmenting the image, and updating internal data structures accordingly. The function is specific to handling image sequences for biological analysis</p> Source code in <code>src/cellects/core/program_organizer.py</code> <pre><code>def fast_last_image_segmentation(self, bio_mask: NDArray[np.uint8] = None, back_mask: NDArray[np.uint8] = None):\n    \"\"\"\n    Segment the first or subsequent image in a series for biological and background masks.\n\n    Parameters\n    ----------\n    bio_mask : NDArray[np.uint8], optional\n        The biological mask to be applied to the image.\n    back_mask : NDArray[np.uint8], optional\n        The background mask to be applied to the image.\n\n    Returns\n    -------\n    None\n\n    Notes\n    -----\n    This function processes the first or subsequent image in a sequence, applying biological and background masks,\n    segmenting the image, and updating internal data structures accordingly. The function is specific to handling\n    image sequences for biological analysis\n\n    \"\"\"\n    if self.vars['convert_for_motion'] is None:\n        self.vars['convert_for_motion'] = {\"logical\": 'None', \"PCA\": np.ones(3, dtype=np.uint8)}\n    self.cropping(is_first_image=False)\n    self.last_image.convert_and_segment(self.vars['convert_for_motion'], self.vars[\"color_number\"],\n                                        bio_mask, back_mask, self.first_image.subtract_background,\n                                        self.first_image.subtract_background2,\n                                        rolling_window_segmentation=self.vars[\"rolling_window_segmentation\"],\n                                        filter_spec=self.vars[\"filter_spec\"])\n    if self.vars['drift_already_corrected'] and not self.last_image.drift_correction_already_adjusted and not self.vars[\"rolling_window_segmentation\"]['do']:\n        self.last_image.check_if_image_border_attest_drift_correction()\n        self.last_image.convert_and_segment(self.vars['convert_for_motion'], self.vars[\"color_number\"],\n                                            bio_mask, back_mask, self.first_image.subtract_background,\n                                            self.first_image.subtract_background2,\n                                            allowed_window=self.last_image.drift_mask_coord,\n                                            filter_spec=self.vars[\"filter_spec\"])\n\n    if self.last_image.im_combinations is None:\n        self.last_image.im_combinations = []\n    if len(self.last_image.im_combinations) == 0:\n        self.last_image.im_combinations.append({})\n    self.current_combination_id = np.min((self.current_combination_id, len(self.last_image.im_combinations) - 1))\n    self.last_image.im_combinations[self.current_combination_id]['csc'] = self.vars['convert_for_motion']\n    self.last_image.im_combinations[self.current_combination_id]['binary_image'] = self.last_image.binary_image\n    if self.last_image.greyscale is not None:\n        greyscale = self.last_image.greyscale\n    else:\n        greyscale = self.last_image.image\n    self.last_image.im_combinations[self.current_combination_id]['converted_image'] = bracket_to_uint8_image_contrast(greyscale)\n</code></pre>"},{"location":"api/cellects/core/program_organizer/#cellects.core.program_organizer.ProgramOrganizer.find_if_lighter_background","title":"<code>find_if_lighter_background()</code>","text":"<p>Determines whether the background is lighter or darker than the cells.</p> <p>This function analyzes images to determine if their backgrounds are lighter or darker relative to the cells, updating attributes accordingly for analysis and display purposes.</p> Notes <p>This function modifies instance variables and does not return any value. The analysis involves comparing mean pixel values in specific areas of the image.</p> Source code in <code>src/cellects/core/program_organizer.py</code> <pre><code>def find_if_lighter_background(self):\n    \"\"\"\n    Determines whether the background is lighter or darker than the cells.\n\n    This function analyzes images to determine if their backgrounds are lighter\n    or darker relative to the cells, updating attributes accordingly for analysis and display purposes.\n\n\n    Notes\n    -----\n    This function modifies instance variables and does not return any value.\n    The analysis involves comparing mean pixel values in specific areas of the image.\n    \"\"\"\n    logging.info(\"Find if the background is lighter or darker than the cells\")\n    self.vars['lighter_background']: bool = True\n    self.vars['contour_color']: np.uint8 = 0\n    are_dicts_equal: bool = True\n    if self.vars['convert_for_origin'] is not None and self.vars['convert_for_origin'] is not None:\n        for key in self.vars['convert_for_origin'].keys():\n            are_dicts_equal = are_dicts_equal and np.all(key in self.vars['convert_for_motion'] and self.vars['convert_for_origin'][key] == self.vars['convert_for_motion'][key])\n\n        for key in self.vars['convert_for_motion'].keys():\n            are_dicts_equal = are_dicts_equal and np.all(key in self.vars['convert_for_origin'] and self.vars['convert_for_motion'][key] == self.vars['convert_for_origin'][key])\n    else:\n        self.vars['convert_for_origin'] = {\"logical\": 'None', \"PCA\": np.ones(3, dtype=np.uint8)}\n        are_dicts_equal = True\n    if are_dicts_equal:\n        if self.first_im is None:\n            self.get_first_image()\n            self.fast_first_image_segmentation()\n            self.cropping(is_first_image=True)\n        among = np.nonzero(self.first_image.validated_shapes)\n        not_among = np.nonzero(1 - self.first_image.validated_shapes)\n        # Use the converted image to tell if the background is lighter, for analysis purposes\n        if self.first_image.image[among[0], among[1]].mean() &gt; self.first_image.image[not_among[0], not_among[1]].mean():\n            self.vars['lighter_background'] = False\n        # Use the original image to tell if the background is lighter, for display purposes\n        if self.first_image.bgr[among[0], among[1], ...].mean() &gt; self.first_image.bgr[not_among[0], not_among[1], ...].mean():\n            self.vars['contour_color'] = 255\n    else:\n        if self.last_im is None:\n            self.get_last_image()\n            # self.cropping(is_first_image=False)\n            self.fast_last_image_segmentation()\n        if self.last_image.binary_image.sum() == 0:\n            self.fast_last_image_segmentation()\n        among = np.nonzero(self.last_image.binary_image)\n        not_among = np.nonzero(1 - self.last_image.binary_image)\n        # Use the converted image to tell if the background is lighter, for analysis purposes\n        if self.last_image.image[among[0], among[1]].mean() &gt; self.last_image.image[not_among[0], not_among[1]].mean():\n            self.vars['lighter_background'] = False\n        # Use the original image to tell if the background is lighter, for display purposes\n        if self.last_image.bgr[among[0], among[1], ...].mean() &gt; self.last_image.bgr[not_among[0], not_among[1], ...].mean():\n            self.vars['contour_color'] = 255\n    if self.vars['origin_state'] == \"invisible\":\n        binary_image = deepcopy(self.first_image.binary_image)\n        self.first_image.convert_and_segment(self.vars['convert_for_motion'], self.vars[\"color_number\"],\n                                             None, None, subtract_background=None,\n                                             subtract_background2=None,\n                                             rolling_window_segmentation=self.vars['rolling_window_segmentation'],\n                                             filter_spec=self.vars[\"filter_spec\"])\n        covered_values = self.first_image.image[np.nonzero(binary_image)]\n        self.vars['luminosity_threshold'] = 127\n        if len(covered_values) &gt; 0:\n            if self.vars['lighter_background']:\n                if np.max(covered_values) &lt; 255:\n                    self.vars['luminosity_threshold'] = np.max(covered_values) + 1\n            else:\n                if np.min(covered_values) &gt; 0:\n                    self.vars['luminosity_threshold'] = np.min(covered_values) - 1\n</code></pre>"},{"location":"api/cellects/core/program_organizer/#cellects.core.program_organizer.ProgramOrganizer.get_average_pixel_size","title":"<code>get_average_pixel_size()</code>","text":"<p>Calculate the average pixel size and related variables.</p> <p>Logs information about calculation steps, computes the average pixel size based on image or cell scaling settings, and sets initial thresholds for object detection.</p> Notes <ul> <li>The average pixel size is determined by either image dimensions or blob sizes.</li> <li>Thresholds for automatic detection are set based on configuration settings.</li> </ul> Source code in <code>src/cellects/core/program_organizer.py</code> <pre><code>def get_average_pixel_size(self):\n    \"\"\"\n    Calculate the average pixel size and related variables.\n\n    Logs information about calculation steps, computes the average\n    pixel size based on image or cell scaling settings,\n    and sets initial thresholds for object detection.\n\n    Notes\n    -----\n    - The average pixel size is determined by either image dimensions or blob sizes.\n    - Thresholds for automatic detection are set based on configuration settings.\n\n    \"\"\"\n    logging.info(\"Getting average pixel size\")\n    (self.first_image.shape_number,\n        self.first_image.shapes,\n        self.first_image.stats,\n        centroids) = cv2.connectedComponentsWithStats(\n            self.first_image.validated_shapes,\n            connectivity=8)\n    self.first_image.shape_number -= 1\n    if self.all['scale_with_image_or_cells'] == 0:\n        self.vars['average_pixel_size'] = np.square(self.all['image_horizontal_size_in_mm'] /\n                                                    self.first_im.shape[1])\n    else:\n        if len(self.first_image.stats[1:, 2]) &gt; 0:\n            self.vars['average_pixel_size'] = np.square(self.all['starting_blob_hsize_in_mm'] /\n                                                        np.mean(self.first_image.stats[1:, 2]))\n        else:\n            self.vars['average_pixel_size'] = 1.\n            self.vars['output_in_mm'] = False\n\n    if self.all['set_spot_size']:\n        self.starting_blob_hsize_in_pixels = (self.all['starting_blob_hsize_in_mm'] /\n                                              np.sqrt(self.vars['average_pixel_size']))\n    else:\n        self.starting_blob_hsize_in_pixels = None\n\n    if self.all['automatic_size_thresholding']:\n        self.vars['first_move_threshold'] = 10\n    else:\n        self.vars['first_move_threshold'] = np.round(self.all['first_move_threshold_in_mm\u00b2'] /\n                                                     self.vars['average_pixel_size']).astype(np.uint8)\n    logging.info(f\"The average pixel size is: {self.vars['average_pixel_size']} mm\u00b2\")\n</code></pre>"},{"location":"api/cellects/core/program_organizer/#cellects.core.program_organizer.ProgramOrganizer.get_background_to_subtract","title":"<code>get_background_to_subtract()</code>","text":"<p>Determine if background subtraction should be applied to the image.</p> Extended Description <p>This function checks whether background subtraction should be applied. It utilizes the 'subtract_background' flag and potentially converts the image for motion estimation.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>object</code> <p>The instance of the class containing this method. Must have attributes <code>vars</code> and <code>first_image</code>.</p> required Source code in <code>src/cellects/core/program_organizer.py</code> <pre><code>def get_background_to_subtract(self):\n    \"\"\"\n    Determine if background subtraction should be applied to the image.\n\n    Extended Description\n    --------------------\n    This function checks whether background subtraction should be applied.\n    It utilizes the 'subtract_background' flag and potentially converts\n    the image for motion estimation.\n\n    Parameters\n    ----------\n    self : object\n        The instance of the class containing this method.\n        Must have attributes `vars` and `first_image`.\n    \"\"\"\n    if self.vars['subtract_background']:\n        self.first_image.generate_subtract_background(self.vars['convert_for_motion'], self.vars['drift_already_corrected'])\n</code></pre>"},{"location":"api/cellects/core/program_organizer/#cellects.core.program_organizer.ProgramOrganizer.get_bounding_boxes","title":"<code>get_bounding_boxes(are_gravity_centers_moving, motion_list=(), all_specimens_have_same_direction=True, original_shape_hsize=None)</code>","text":"<p>Get the coordinates of arenas using bounding boxes.</p> <p>Parameters:</p> Name Type Description Default <code>are_gravity_centers_moving</code> <code>bool</code> <p>Flag indicating whether gravity centers are moving or not.</p> required <code>motion_list</code> <code>list</code> <p>List of motion information for the specimens.</p> <code>()</code> <code>all_specimens_have_same_direction</code> <code>bool</code> <p>Flag indicating whether all specimens have the same direction, by default True.</p> <code>True</code> Notes <p>This method uses various internal methods and variables to determine the bounding boxes.</p> Source code in <code>src/cellects/core/program_organizer.py</code> <pre><code>def get_bounding_boxes(self, are_gravity_centers_moving: bool, motion_list: list=(), all_specimens_have_same_direction: bool=True, original_shape_hsize: int=None):\n    \"\"\"Get the coordinates of arenas using bounding boxes.\n\n    Parameters\n    ----------\n    are_gravity_centers_moving : bool\n        Flag indicating whether gravity centers are moving or not.\n    motion_list : list\n        List of motion information for the specimens.\n    all_specimens_have_same_direction : bool, optional\n        Flag indicating whether all specimens have the same direction,\n        by default True.\n    Notes\n    -----\n    This method uses various internal methods and variables to determine the bounding boxes.\n    \"\"\"\n    # 7) Create required empty arrays: especially the bounding box coordinates of each video\n    self.ordered_first_image = None\n    self.shapes_to_remove = None\n    if self.first_image.crop_coord is None:\n        self.first_image.get_crop_coordinates()\n\n    logging.info(\"Get the coordinates of all arenas using the get_bounding_boxes method of the VideoMaker class\")\n    if self.first_image.validated_shapes.any() and self.first_image.shape_number &gt; 0:\n        self.ordered_stats, ordered_centroids, self.ordered_first_image = rank_from_top_to_bottom_from_left_to_right(\n            self.first_image.validated_shapes, self.first_image.y_boundaries, get_ordered_image=True)\n        self.unchanged_ordered_fimg = deepcopy(self.ordered_first_image)\n        self.modif_validated_shapes = deepcopy(self.first_image.validated_shapes)\n        self.standard = - 1\n        counter = 0\n        while np.any(np.less(self.standard, 0)) and counter &lt; 20:\n            counter += 1\n            self.left = np.zeros(self.first_image.shape_number, dtype=np.int64)\n            self.right = np.repeat(self.modif_validated_shapes.shape[1], self.first_image.shape_number)\n            self.top = np.zeros(self.first_image.shape_number, dtype=np.int64)\n            self.bot = np.repeat(self.modif_validated_shapes.shape[0], self.first_image.shape_number)\n            if are_gravity_centers_moving:\n                self.top, self.bot, self.left, self.right, self.ordered_first_image = get_bb_with_moving_centers(motion_list, all_specimens_have_same_direction,\n                                                 original_shape_hsize, self.first_image.validated_shapes,\n                                                 self.first_image.y_boundaries)\n                new_ordered_first_image = np.zeros(self.ordered_first_image.shape, dtype=np.uint8)\n\n                for i in np.arange(1, self.first_image.shape_number + 1):\n                    previous_shape = np.zeros(self.ordered_first_image.shape, dtype=np.uint8)\n                    previous_shape[np.nonzero(self.unchanged_ordered_fimg == i)] = 1\n                    new_potentials = np.zeros(self.ordered_first_image.shape, dtype=np.uint8)\n                    new_potentials[np.nonzero(self.ordered_first_image == i)] = 1\n                    new_potentials[np.nonzero(self.unchanged_ordered_fimg == i)] = 0\n\n                    pads = ProgressivelyAddDistantShapes(new_potentials, previous_shape, max_distance=2)\n                    pads.consider_shapes_sizes(min_shape_size=10)\n                    pads.connect_shapes(only_keep_connected_shapes=True, rank_connecting_pixels=False)\n                    new_ordered_first_image[np.nonzero(pads.expanded_shape)] = i\n                self.ordered_first_image = new_ordered_first_image\n                self.modif_validated_shapes = np.zeros(self.ordered_first_image.shape, dtype=np.uint8)\n                self.modif_validated_shapes[np.nonzero(self.ordered_first_image)] = 1\n                self.ordered_stats, ordered_centroids, self.ordered_first_image = rank_from_top_to_bottom_from_left_to_right(\n                    self.modif_validated_shapes, self.first_image.y_boundaries, get_ordered_image=True)\n                self.top, self.bot, self.left, self.right = get_quick_bounding_boxes(self.modif_validated_shapes, self.ordered_first_image, self.ordered_stats)\n            else:\n                self.top, self.bot, self.left, self.right = get_quick_bounding_boxes(self.modif_validated_shapes, self.ordered_first_image, self.ordered_stats)\n            self._standardize_video_sizes()\n        if counter == 20:\n            self.top[self.top &lt; 0] = 1\n            self.bot[self.bot &gt;= self.ordered_first_image.shape[0] - 1] = self.ordered_first_image.shape[0] - 2\n            self.left[self.left &lt; 0] = 1\n            self.right[self.right &gt;= self.ordered_first_image.shape[1] - 1] = self.ordered_first_image.shape[1] - 2\n        del self.ordered_first_image\n        del self.unchanged_ordered_fimg\n        del self.modif_validated_shapes\n        del self.standard\n        del self.shapes_to_remove\n        self.bot += 1\n        self.right += 1\n    else:\n        self._whole_image_bounding_boxes()\n</code></pre>"},{"location":"api/cellects/core/program_organizer/#cellects.core.program_organizer.ProgramOrganizer.get_first_image","title":"<code>get_first_image(first_im=None, sample_number=None)</code>","text":"<p>Load and process the first image or frame from a video.</p> <p>This method handles loading the first image or the first frame of a video depending on whether the data is an image or a video. It performs necessary preprocessing and initializes relevant attributes for subsequent analysis.</p> Source code in <code>src/cellects/core/program_organizer.py</code> <pre><code>def get_first_image(self, first_im: NDArray=None, sample_number: int=None):\n    \"\"\"\n    Load and process the first image or frame from a video.\n\n    This method handles loading the first image or the first frame of a video\n    depending on whether the data is an image or a video. It performs necessary\n    preprocessing and initializes relevant attributes for subsequent analysis.\n    \"\"\"\n    if sample_number is not None:\n        self.sample_number = sample_number\n    self.reduce_image_dim = False\n    if first_im is not None:\n        self.first_im = first_im\n    else:\n        logging.info(\"Load first image\")\n        if self.all['im_or_vid'] == 1:\n            if self.analysis_instance is None:\n                self.analysis_instance = video2numpy(self.data_list[0])\n                self.sample_number = len(self.data_list)\n                self.vars['img_number'] = self.analysis_instance.shape[0]\n                self.first_im = self.analysis_instance[0, ...]\n                self.vars['dims'] = self.analysis_instance.shape[:3]\n            else:\n                self.first_im = self.analysis_instance[self.vars['first_detection_frame'], ...]\n\n        else:\n            self.vars['img_number'] = len(self.data_list)\n            self.all['raw_images'] = is_raw_image(self.data_list[0])\n            self.first_im = readim(self.data_list[self.vars['first_detection_frame']], self.all['raw_images'])\n            self.vars['dims'] = [self.vars['img_number'], self.first_im.shape[0], self.first_im.shape[1]]\n\n            if len(self.first_im.shape) == 3:\n                if np.all(np.equal(self.first_im[:, :, 0], self.first_im[:, :, 1])) and np.all(\n                        np.equal(self.first_im[:, :, 1], self.first_im[:, :, 2])):\n                    self.reduce_image_dim = True\n                if self.reduce_image_dim:\n                    self.first_im = self.first_im[:, :, 0]\n\n    self.first_image = OneImageAnalysis(self.first_im, self.sample_number)\n    self.vars['already_greyscale'] = self.first_image.already_greyscale\n    if self.vars['already_greyscale']:\n        self.vars[\"convert_for_origin\"] = {\"bgr\": np.array((1, 1, 1), dtype=np.uint8), \"logical\": \"None\"}\n        self.vars[\"convert_for_motion\"] = {\"bgr\": np.array((1, 1, 1), dtype=np.uint8), \"logical\": \"None\"}\n    if np.mean((np.mean(self.first_image.image[2, :, ...]), np.mean(self.first_image.image[-3, :, ...]), np.mean(self.first_image.image[:, 2, ...]), np.mean(self.first_image.image[:, -3, ...]))) &gt; 127:\n        self.vars['contour_color']: np.uint8 = 0\n    else:\n        self.vars['contour_color']: np.uint8 = 255\n    if self.vars['first_detection_frame'] &gt; 0:\n        self.vars['origin_state'] = 'invisible'\n</code></pre>"},{"location":"api/cellects/core/program_organizer/#cellects.core.program_organizer.ProgramOrganizer.get_last_image","title":"<code>get_last_image(last_im=None)</code>","text":"<p>Load the last image from a video or image list and process it based on given parameters.</p> <p>Parameters:</p> Name Type Description Default <code>last_im</code> <code>NDArray</code> <p>The last image to be loaded. If not provided, the last image will be loaded from the data list.</p> <code>None</code> Source code in <code>src/cellects/core/program_organizer.py</code> <pre><code>def get_last_image(self, last_im: NDArray=None):\n    \"\"\"\n\n    Load the last image from a video or image list and process it based on given parameters.\n\n    Parameters\n    ----------\n    last_im : NDArray, optional\n        The last image to be loaded. If not provided, the last image will be loaded from the data list.\n    \"\"\"\n    logging.info(\"Load last image\")\n    if last_im is not None:\n        self.last_im = last_im\n    else:\n        if self.all['im_or_vid'] == 1:\n            self.last_im = self.analysis_instance[-1, ...]\n        else:\n            is_landscape = self.first_image.image.shape[0] &lt; self.first_image.image.shape[1]\n            self.last_im = read_and_rotate(self.data_list[-1], self.first_im, self.all['raw_images'], is_landscape)\n            if self.reduce_image_dim:\n                self.last_im = self.last_im[:, :, 0]\n    self.last_image = OneImageAnalysis(self.last_im)\n</code></pre>"},{"location":"api/cellects/core/program_organizer/#cellects.core.program_organizer.ProgramOrganizer.get_origins_and_backgrounds_lists","title":"<code>get_origins_and_backgrounds_lists()</code>","text":"<p>Create origins and background lists for image processing.</p> Extended Description <p>This method generates the origin and background lists by slicing the first image and its background subtraction based on predefined boundaries. It handles cases where the top, bottom, left, and right boundaries are not yet initialized.</p> Notes <p>This method directly modifies the input image data. The <code>self.vars</code> dictionary is populated with lists of sliced arrays from the first image and its background.</p> <p>Attributes:</p> Name Type Description <code>self.vars</code> <code>dict</code> <p>Dictionary to store processed data.</p> <code>self.first_image</code> <code>ImageObject</code> <p>The first image object containing validated shapes and background subtraction arrays.</p> Source code in <code>src/cellects/core/program_organizer.py</code> <pre><code>def get_origins_and_backgrounds_lists(self):\n    \"\"\"\n    Create origins and background lists for image processing.\n\n    Extended Description\n    --------------------\n    This method generates the origin and background lists by slicing the first image\n    and its background subtraction based on predefined boundaries. It handles cases where\n    the top, bottom, left, and right boundaries are not yet initialized.\n\n    Notes\n    -----\n    This method directly modifies the input image data. The `self.vars` dictionary is populated\n    with lists of sliced arrays from the first image and its background.\n\n    Attributes\n    ----------\n    self.vars : dict\n        Dictionary to store processed data.\n    self.first_image : ImageObject\n        The first image object containing validated shapes and background subtraction arrays.\n    \"\"\"\n    logging.info(\"Create origins and background lists\")\n    if self.top is None:\n        self._whole_image_bounding_boxes()\n\n    if not self.first_image.validated_shapes.any():\n        if self.vars['convert_for_motion'] is not None:\n            self.vars['convert_for_origin'] = self.vars['convert_for_motion']\n        self.fast_first_image_segmentation()\n    first_im = self.first_image.validated_shapes\n    self.vars['origin_list'] = []\n    self.vars['background_list'] = []\n    self.vars['background_list2'] = []\n    for rep in np.arange(len(self.vars['analyzed_individuals'])):\n        origin_coord = np.nonzero(first_im[self.top[rep]:self.bot[rep], self.left[rep]:self.right[rep]])\n        self.vars['origin_list'].append(origin_coord)\n    if self.vars['subtract_background']:\n        for rep in np.arange(len(self.vars['analyzed_individuals'])):\n            self.vars['background_list'].append(\n                self.first_image.subtract_background[self.top[rep]:self.bot[rep], self.left[rep]:self.right[rep]])\n            if self.vars['convert_for_motion']['logical'] != 'None':\n                self.vars['background_list2'].append(self.first_image.subtract_background2[self.top[rep]:\n                                                     self.bot[rep], self.left[rep]:self.right[rep]])\n</code></pre>"},{"location":"api/cellects/core/program_organizer/#cellects.core.program_organizer.ProgramOrganizer.instantiate_tables","title":"<code>instantiate_tables()</code>","text":"<p>Update output list and prepare results tables and validation images.</p> Extended Description <p>This method performs necessary preparations for processing image sequences, including updating the output list and initializing key attributes required for subsequent operations.</p> Source code in <code>src/cellects/core/program_organizer.py</code> <pre><code>def instantiate_tables(self):\n    \"\"\"\n    Update output list and prepare results tables and validation images.\n\n    Extended Description\n    --------------------\n    This method performs necessary preparations for processing image sequences,\n    including updating the output list and initializing key attributes required\n    for subsequent operations.\n\n    \"\"\"\n    self.update_output_list()\n    logging.info(\"Instantiate results tables and validation images\")\n    self.fractal_box_sizes = None\n    self.one_row_per_arena = None\n    self.one_row_per_frame = None\n    if self.vars['already_greyscale']:\n        if len(self.first_image.bgr.shape) == 2:\n            self.first_image.bgr = np.stack((self.first_image.bgr, self.first_image.bgr, self.first_image.bgr), axis=2).astype(np.uint8)\n        if len(self.last_image.bgr.shape) == 2:\n            self.last_image.bgr = np.stack((self.last_image.bgr, self.last_image.bgr, self.last_image.bgr), axis=2).astype(np.uint8)\n        self.vars[\"convert_for_motion\"] = {\"bgr\": np.array((1, 1, 1), dtype=np.uint8), \"logical\": \"None\"}\n</code></pre>"},{"location":"api/cellects/core/program_organizer/#cellects.core.program_organizer.ProgramOrganizer.list_coordinates","title":"<code>list_coordinates()</code>","text":"<p>Summarize the coordinates of images and video.</p> <p>Combine the crop coordinates from the first image with additional coordinates for left, right, top, and bottom boundaries to form a list of video coordinates. If the crop coordinates are not already set, initialize them to cover the entire image.</p> <p>Returns:</p> Type Description <code>list of int</code> <p>A list containing the coordinates [left, right, top, bottom] for video.</p> Source code in <code>src/cellects/core/program_organizer.py</code> <pre><code>def list_coordinates(self):\n    \"\"\"\n    Summarize the coordinates of images and video.\n\n    Combine the crop coordinates from the first image with additional\n    coordinates for left, right, top, and bottom boundaries to form a list of\n    video coordinates. If the crop coordinates are not already set, initialize\n    them to cover the entire image.\n\n    Returns\n    -------\n    list of int\n        A list containing the coordinates [left, right, top, bottom] for video.\n\n    \"\"\"\n    if self.first_image.crop_coord is None:\n        self.first_image.crop_coord = [0, self.first_image.image.shape[0], 0, self.first_image.image.shape[1]]\n    self.vars['bb_coord'] = self.first_image.crop_coord + [self.top, self.bot, self.left, self.right]\n    self.all['overwrite_unaltered_videos'] = True\n</code></pre>"},{"location":"api/cellects/core/program_organizer/#cellects.core.program_organizer.ProgramOrganizer.load_data_to_run_cellects_quickly","title":"<code>load_data_to_run_cellects_quickly()</code>","text":"<p>Load data from a pickle file and update the current state of the object.</p> <p>Summarizes, loads, and validates data needed to run Cellects, updating the object's state accordingly. If the necessary data are not present or valid, it ensures the experiment is marked as not ready to run.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>CellectsObject</code> <p>The instance of the class (assumed to be a subclass of CellectsObject) that this method belongs to.</p> required <p>Returns:</p> Type Description <code>None</code> Notes <p>This function relies on the presence of a pickle file 'Data to run Cellects quickly.pkl'. It updates the state of various attributes based on the loaded data and logs appropriate messages.</p> Source code in <code>src/cellects/core/program_organizer.py</code> <pre><code>def load_data_to_run_cellects_quickly(self):\n    \"\"\"\n    Load data from a pickle file and update the current state of the object.\n\n    Summarizes, loads, and validates data needed to run Cellects,\n    updating the object's state accordingly. If the necessary data\n    are not present or valid, it ensures the experiment is marked as\n    not ready to run.\n\n    Parameters\n    ----------\n    self : CellectsObject\n        The instance of the class (assumed to be a subclass of\n        CellectsObject) that this method belongs to.\n\n    Returns\n    -------\n    None\n\n    Notes\n    -----\n    This function relies on the presence of a pickle file 'Data to run Cellects quickly.pkl'.\n    It updates the state of various attributes based on the loaded data\n    and logs appropriate messages.\n    \"\"\"\n    self.analysis_instance = None\n    self.first_im = None\n    self.first_image = None\n    self.last_image = None\n    current_global_pathway = self.all['global_pathway']\n    folder_number = self.all['folder_number']\n    if folder_number &gt; 1:\n        folder_list = deepcopy(self.all['folder_list'])\n        sample_number_per_folder = deepcopy(self.all['sample_number_per_folder'])\n\n    if os.path.isfile('Data to run Cellects quickly.pkl'):\n        pickle_rick = PickleRick()\n        data_to_run_cellects_quickly = pickle_rick.read_file('Data to run Cellects quickly.pkl')\n        if data_to_run_cellects_quickly is None:\n            data_to_run_cellects_quickly = {}\n\n        if ('validated_shapes' in data_to_run_cellects_quickly) and ('all' in data_to_run_cellects_quickly) and ('bb_coord' in data_to_run_cellects_quickly['all']['vars']):\n            logging.info(\"Success to load Data to run Cellects quickly.pkl from the user chosen directory\")\n            self.all = data_to_run_cellects_quickly['all']\n            # If you want to add a new variable, first run an updated version of all_vars_dict,\n            # then put a breakpoint here and run the following + self.save_data_to_run_cellects_quickly() :\n            self.vars = self.all['vars']\n            self.update_variable_dict()\n            folder_changed = False\n            if current_global_pathway != self.all['global_pathway']:\n                folder_changed = True\n                logging.info(\n                    \"Although the folder is ready, it is not at the same place as it was during creation, updating\")\n                self.all['global_pathway'] = current_global_pathway\n            if folder_number &gt; 1:\n                self.all['global_pathway'] = current_global_pathway\n                self.all['folder_list'] = folder_list\n                self.all['folder_number'] = folder_number\n                self.all['sample_number_per_folder'] = sample_number_per_folder\n\n            if len(self.data_list) == 0:\n                self.look_for_data()\n                if folder_changed and folder_number &gt; 1 and len(self.all['folder_list']) &gt; 0:\n                    self.update_folder_id(self.all['sample_number_per_folder'][0], self.all['folder_list'][0])\n            self.get_first_image()\n            self.get_last_image()\n            (ccy1, ccy2, ccx1, ccx2, self.top, self.bot, self.left, self.right) = data_to_run_cellects_quickly['all']['vars']['bb_coord']\n            if self.all['automatically_crop']:\n                self.first_image.crop_coord = [ccy1, ccy2, ccx1, ccx2]\n                logging.info(\"Crop first image\")\n                self.first_image.automatically_crop(self.first_image.crop_coord)\n                logging.info(\"Crop last image\")\n                self.last_image.automatically_crop(self.first_image.crop_coord)\n            else:\n                self.first_image.crop_coord = None\n            self.first_image.validated_shapes = data_to_run_cellects_quickly['validated_shapes']\n            self.first_image.im_combinations = []\n            self.current_combination_id = 0\n            self.first_image.im_combinations.append({})\n            self.first_image.im_combinations[self.current_combination_id]['csc'] = self.vars['convert_for_origin']\n            self.first_image.im_combinations[self.current_combination_id]['binary_image'] = self.first_image.validated_shapes\n            self.first_image.im_combinations[self.current_combination_id]['shape_number'] = data_to_run_cellects_quickly['shape_number']\n\n            self.first_exp_ready_to_run = True\n            if self.vars['subtract_background'] and len(self.vars['background_list']) == 0:\n                self.first_exp_ready_to_run = False\n        else:\n            self.first_exp_ready_to_run = False\n    else:\n        self.first_exp_ready_to_run = False\n    if self.first_exp_ready_to_run:\n        logging.info(\"The current (or the first) folder is ready to run\")\n    else:\n        logging.info(\"The current (or the first) folder is not ready to run\")\n</code></pre>"},{"location":"api/cellects/core/program_organizer/#cellects.core.program_organizer.ProgramOrganizer.load_variable_dict","title":"<code>load_variable_dict()</code>","text":"<p>Loads configuration dictionaries from a pickle file if available, otherwise initializes defaults.</p> <p>Tries to load saved parameters. If the file doesn't exist or loading fails due to corruption, default values are used instead (logging relevant warnings).</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If no valid configuration file is found and default initialization fails.</p> Notes <p>This method ensures robust operation by handling missing or corrupted configuration files gracefully.</p> Source code in <code>src/cellects/core/program_organizer.py</code> <pre><code>def load_variable_dict(self):\n    \"\"\"\n    Loads configuration dictionaries from a pickle file if available, otherwise initializes defaults.\n\n    Tries to load saved parameters. If the file doesn't exist or loading fails due to corruption,\n    default values are used instead (logging relevant warnings).\n\n    Raises\n    ------\n    FileNotFoundError\n        If no valid configuration file is found and default initialization fails.\n\n    Notes\n    -----\n    This method ensures robust operation by handling missing or corrupted configuration files gracefully.\n    \"\"\"\n    if os.path.isfile(ALL_VARS_PKL_FILE):\n        logging.info(\"Load the parameters from all_vars.pkl in the config of the Cellects folder\")\n        try:\n            with open(ALL_VARS_PKL_FILE, 'rb') as fileopen:\n                self.all = pickle.load(fileopen)\n            self.vars = self.all['vars']\n            self.update_variable_dict()\n            logging.info(\"Success to load the parameters dictionaries from the Cellects folder\")\n        except Exception as exc:\n            logging.error(f\"Initialize default parameters because error: {exc}\")\n            default_dicts = DefaultDicts()\n            self.all = default_dicts.all\n            self.vars = default_dicts.vars\n    else:\n        logging.info(\"Initialize default parameters\")\n        default_dicts = DefaultDicts()\n        self.all = default_dicts.all\n        self.vars = default_dicts.vars\n    if self.all['cores'] == 1:\n        self.all['cores'] = os.cpu_count() - 1\n</code></pre>"},{"location":"api/cellects/core/program_organizer/#cellects.core.program_organizer.ProgramOrganizer.look_for_data","title":"<code>look_for_data()</code>","text":"<p>Discovers all relevant video/image data in the working directory.</p> <p>Uses natural sorting to handle filenames with numeric suffixes. Validates file consistency and logs warnings if filename patterns are inconsistent across folders.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no files match the specified naming convention.</p> Notes <p>This method assumes all data files follow a predictable pattern with numeric extensions. Use caution in unpredictable directory structures where this may fail silently or produce incorrect results.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; organizer.look_for_data()\n&gt;&gt;&gt; print(organizer.data_list)\n['/path/to/video1.avi', '/path/to/video2.avi']\n</code></pre> Source code in <code>src/cellects/core/program_organizer.py</code> <pre><code>def look_for_data(self):\n    \"\"\"\n    Discovers all relevant video/image data in the working directory.\n\n    Uses natural sorting to handle filenames with numeric suffixes. Validates file consistency and logs warnings\n    if filename patterns are inconsistent across folders.\n\n    Raises\n    ------\n    ValueError\n        If no files match the specified naming convention.\n\n    Notes\n    -----\n    This method assumes all data files follow a predictable pattern with numeric extensions. Use caution in\n    unpredictable directory structures where this may fail silently or produce incorrect results.\n\n    Examples\n    --------\n    &gt;&gt;&gt; organizer.look_for_data()\n    &gt;&gt;&gt; print(organizer.data_list)\n    ['/path/to/video1.avi', '/path/to/video2.avi']\n    \"\"\"\n    os.chdir(Path(self.all['global_pathway']))\n    logging.info(f\"Dir: {self.all['global_pathway']}\")\n    self.data_list = insensitive_glob(self.all['radical'] + '*' + self.all['extension'])  # Provides a list ordered by last modification date\n    self.all['folder_list'] = []\n    self.all['folder_number'] = 1\n    self.vars['first_detection_frame'] = 0\n    if len(self.data_list) &gt; 0:\n        self._sort_data_list()\n        self.sample_number = self.all['first_folder_sample_number']\n    else:\n        content = os.listdir()\n        for obj in content:\n            if not os.path.isfile(obj):\n                data_list = insensitive_glob(obj + \"/\" + self.all['radical'] + '*' + self.all['extension'])\n                if len(data_list) &gt; 0:\n                    self.all['folder_list'].append(obj)\n                    self.all['folder_number'] += 1\n        self.all['folder_list'] = np.sort(self.all['folder_list'])\n\n        if isinstance(self.all['sample_number_per_folder'], int) or len(self.all['sample_number_per_folder']) == 1:\n            self.all['sample_number_per_folder'] = np.repeat(self.all['sample_number_per_folder'],\n                                                          self.all['folder_number'])\n</code></pre>"},{"location":"api/cellects/core/program_organizer/#cellects.core.program_organizer.ProgramOrganizer.prepare_video_writing","title":"<code>prepare_video_writing(img_list, min_ram_free, in_colors=False, pathway='')</code>","text":"<p>Prepare the raw video (.npy) writing process for Cellects.</p> <p>Parameters:</p> Name Type Description Default <code>img_list</code> <code>list</code> <p>List of images to be processed.</p> required <code>min_ram_free</code> <code>float</code> <p>Minimum amount of RAM in GB that should remain free.</p> required <code>in_colors</code> <code>bool</code> <p>Whether the images are in color. Default is False.</p> <code>False</code> <code>pathway</code> <code>str</code> <p>Path to save the video files. Default is an empty string.</p> <code>''</code> <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing: - bunch_nb: int, number of bunches needed for video writing. - video_nb_per_bunch: int, number of videos per bunch. - sizes: ndarray, dimensions of each video. - video_bunch: list or ndarray, initialized video arrays. - vid_names: list, names of the video files. - rom_memory_required: None or float, required ROM memory. - analysis_status: dict, status and message of the analysis process. - remaining: int, remainder videos that do not fit in a complete bunch.</p> Notes <ul> <li>The function calculates necessary memory and ensures 10% extra to avoid issues.</li> <li>It checks for available RAM and adjusts the number of bunches accordingly.</li> <li>If using color images, memory requirements are tripled.</li> </ul> <p>expected output depends on the provided images and RAM availability</p> Source code in <code>src/cellects/core/program_organizer.py</code> <pre><code>def prepare_video_writing(self, img_list: list, min_ram_free: float, in_colors: bool=False, pathway: str=\"\"):\n    \"\"\"\n\n    Prepare the raw video (.npy) writing process for Cellects.\n\n    Parameters\n    ----------\n    img_list : list\n        List of images to be processed.\n    min_ram_free : float\n        Minimum amount of RAM in GB that should remain free.\n    in_colors : bool, optional\n        Whether the images are in color. Default is False.\n    pathway : str, optional\n        Path to save the video files. Default is an empty string.\n\n    Returns\n    -------\n    tuple\n        A tuple containing:\n        - bunch_nb: int, number of bunches needed for video writing.\n        - video_nb_per_bunch: int, number of videos per bunch.\n        - sizes: ndarray, dimensions of each video.\n        - video_bunch: list or ndarray, initialized video arrays.\n        - vid_names: list, names of the video files.\n        - rom_memory_required: None or float, required ROM memory.\n        - analysis_status: dict, status and message of the analysis process.\n        - remaining: int, remainder videos that do not fit in a complete bunch.\n\n    Notes\n    -----\n    - The function calculates necessary memory and ensures 10% extra to avoid issues.\n    - It checks for available RAM and adjusts the number of bunches accordingly.\n    - If using color images, memory requirements are tripled.\n\n    expected output depends on the provided images and RAM availability\n    \"\"\"\n    # 1) Create a list of video names\n    if self.not_analyzed_individuals is not None:\n        number_to_add = len(self.not_analyzed_individuals)\n    else:\n        number_to_add = 0\n    vid_names = list()\n    ind_i = 0\n    counter = 0\n    while ind_i &lt; (self.first_image.shape_number + number_to_add):\n        ind_i += 1\n        while np.any(np.isin(self.not_analyzed_individuals, ind_i)):\n            ind_i += 1\n        vid_names.append(pathway + \"ind_\" + str(ind_i) + \".npy\")\n        counter += 1\n    img_nb = len(img_list)\n\n    # 2) Create a table of the dimensions of each video\n    # Add 10% to the necessary memory to avoid problems\n    necessary_memory = img_nb * np.multiply((self.bot - self.top).astype(np.uint64), (self.right - self.left).astype(np.uint64)).sum() * 8 * 1.16415e-10\n    if in_colors:\n        sizes = np.column_stack(\n            (np.repeat(img_nb, self.first_image.shape_number), self.bot - self.top, self.right - self.left,\n             np.repeat(3, self.first_image.shape_number)))\n        necessary_memory *= 3\n    else:\n        sizes = np.column_stack(\n            (np.repeat(img_nb, self.first_image.shape_number), self.bot - self.top, self.right - self.left))\n    use_list_of_vid = True\n    if np.all(sizes[0, :] == sizes):\n        use_list_of_vid = False\n    available_memory = (psutil.virtual_memory().available &gt;&gt; 30) - min_ram_free\n    if available_memory == 0:\n        analysis_status = {\"continue\": False, \"message\": \"There are not enough RAM available\"}\n        bunch_nb = 1\n    else:\n        bunch_nb = int(np.ceil(necessary_memory / available_memory))\n        if bunch_nb &gt; 1:\n            # The program will need twice the memory to create the second bunch.\n            bunch_nb = int(np.ceil(2 * necessary_memory / available_memory))\n\n    video_nb_per_bunch = np.floor(self.first_image.shape_number / bunch_nb).astype(np.uint8)\n    analysis_status = {\"continue\": True, \"message\": \"\"}\n    video_bunch = None\n    try:\n        if use_list_of_vid:\n            video_bunch = [np.zeros(sizes[i, :], dtype=np.uint8) for i in range(video_nb_per_bunch)]\n        else:\n            video_bunch = np.zeros(np.append(sizes[0, :], video_nb_per_bunch), dtype=np.uint8)\n    except ValueError as v_err:\n        analysis_status = {\"continue\": False, \"message\": \"Probably failed to detect the right cell(s) number, do the first image analysis manually.\"}\n        logging.error(f\"{analysis_status['message']} error is: {v_err}\")\n    # Check for available ROM memory\n    if (psutil.disk_usage('/')[2] &gt;&gt; 30) &lt; (necessary_memory + 2):\n        rom_memory_required = necessary_memory + 2\n    else:\n        rom_memory_required = None\n    remaining = self.first_image.shape_number % bunch_nb\n    if remaining &gt; 0:\n        bunch_nb += 1\n    is_landscape = self.first_image.image.shape[0] &lt; self.first_image.image.shape[1]\n    logging.info(f\"Cellects will start writing {self.first_image.shape_number} videos. Given available memory, it will do it in {bunch_nb} time(s)\")\n    return bunch_nb, video_nb_per_bunch, sizes, video_bunch, vid_names, rom_memory_required, analysis_status, remaining, use_list_of_vid, is_landscape\n</code></pre>"},{"location":"api/cellects/core/program_organizer/#cellects.core.program_organizer.ProgramOrganizer.save_data_to_run_cellects_quickly","title":"<code>save_data_to_run_cellects_quickly(new_one_if_does_not_exist=True)</code>","text":"<p>Save data to a pickled file if it does not exist or update existing data.</p> <p>Parameters:</p> Name Type Description Default <code>new_one_if_does_not_exist</code> <code>bool</code> <p>Whether to create a new data file if it does not already exist. Default is True.</p> <code>True</code> Notes <p>This method logs various information about its operations and handles the writing of data to a pickled file.</p> Source code in <code>src/cellects/core/program_organizer.py</code> <pre><code>def save_data_to_run_cellects_quickly(self, new_one_if_does_not_exist: bool=True):\n    \"\"\"\n    Save data to a pickled file if it does not exist or update existing data.\n\n    Parameters\n    ----------\n    new_one_if_does_not_exist : bool, optional\n        Whether to create a new data file if it does not already exist.\n        Default is True.\n\n    Notes\n    -----\n    This method logs various information about its operations and handles the writing of data to a pickled file.\n    \"\"\"\n    data_to_run_cellects_quickly = None\n    if os.path.isfile('Data to run Cellects quickly.pkl'):\n        logging.info(\"Update -Data to run Cellects quickly.pkl- in the user chosen directory\")\n        pickle_rick = PickleRick()\n        data_to_run_cellects_quickly = pickle_rick.read_file('Data to run Cellects quickly.pkl')\n        if data_to_run_cellects_quickly is None:\n            os.remove('Data to run Cellects quickly.pkl')\n            logging.error(\"Failed to load Data to run Cellects quickly.pkl before update. Remove pre existing.\")\n    else:\n        if new_one_if_does_not_exist:\n            logging.info(\"Create Data to run Cellects quickly.pkl in the user chosen directory\")\n            data_to_run_cellects_quickly = {}\n    if data_to_run_cellects_quickly is not None:\n        if self.data_to_save['first_image']:\n            data_to_run_cellects_quickly['validated_shapes'] = self.first_image.im_combinations[self.current_combination_id]['binary_image']\n            data_to_run_cellects_quickly['shape_number'] = self.first_image.im_combinations[self.current_combination_id]['shape_number']\n        if self.data_to_save['exif']:\n            self.vars['exif'] = self.extract_exif()\n        self.all['vars'] = self.vars\n        data_to_run_cellects_quickly['all'] = self.all\n        pickle_rick = PickleRick()\n        pickle_rick.write_file(data_to_run_cellects_quickly, 'Data to run Cellects quickly.pkl')\n</code></pre>"},{"location":"api/cellects/core/program_organizer/#cellects.core.program_organizer.ProgramOrganizer.save_tables","title":"<code>save_tables(with_last_image=True)</code>","text":"<p>Exports analysis results to CSV files and saves visualization outputs.</p> <p>Generates the following output: - one_row_per_arena.csv, one_row_per_frame.csv : Tracking data per arena/frame. - software_settings.csv : Full configuration settings for reproducibility.</p> <p>Raises:</p> Type Description <code>PermissionError</code> <p>If any output file is already open in an external program (logged and re-raised).</p> Notes <p>Ensure no exported CSV files are open while running this method to avoid permission errors. This function will fail gracefully if the files cannot be overwritten.</p> Source code in <code>src/cellects/core/program_organizer.py</code> <pre><code>def save_tables(self, with_last_image: bool=True):\n    \"\"\"\n    Exports analysis results to CSV files and saves visualization outputs.\n\n    Generates the following output:\n    - one_row_per_arena.csv, one_row_per_frame.csv : Tracking data per arena/frame.\n    - software_settings.csv : Full configuration settings for reproducibility.\n\n    Raises\n    ------\n    PermissionError\n        If any output file is already open in an external program (logged and re-raised).\n\n    Notes\n    -----\n    Ensure no exported CSV files are open while running this method to avoid permission errors. This\n    function will fail gracefully if the files cannot be overwritten.\n\n    \"\"\"\n    logging.info(\"Save results tables and validation images\")\n    if not self.vars['several_blob_per_arena']:\n        try:\n            self.one_row_per_arena.to_csv(\"one_row_per_arena.csv\", sep=\";\", index=False, lineterminator='\\n')\n            del self.one_row_per_arena\n        except PermissionError:\n            logging.error(\"Never let one_row_per_arena.csv open when Cellects runs\")\n        try:\n            self.one_row_per_frame.to_csv(\"one_row_per_frame.csv\", sep=\";\", index=False, lineterminator='\\n')\n            del self.one_row_per_frame\n        except PermissionError:\n            logging.error(\"Never let one_row_per_frame.csv open when Cellects runs\")\n    if self.all['extension'] == '.JPG':\n        extension = '.PNG'\n    else:\n        extension = '.JPG'\n    if with_last_image:\n        cv2.imwrite(f\"Analysis efficiency, last image{extension}\", self.last_image.bgr)\n    cv2.imwrite(\n        f\"Analysis efficiency, {np.ceil(self.vars['img_number'] / 10).astype(np.uint64)}th image{extension}\",\n        self.first_image.bgr)\n    software_settings = deepcopy(self.vars)\n    for key in ['descriptors', 'analyzed_individuals', 'exif', 'dims', 'origin_list', 'background_list', 'background_list2', 'descriptors', 'folder_list', 'sample_number_per_folder']:\n        software_settings.pop(key, None)\n    global_settings = deepcopy(self.all)\n    for key in ['analyzed_individuals', 'night_mode', 'expert_mode', 'is_auto', 'arena', 'video_option', 'compute_all_options', 'vars', 'dims', 'origin_list', 'background_list', 'background_list2', 'descriptors', 'folder_list', 'sample_number_per_folder']:\n        global_settings.pop(key, None)\n    software_settings.update(global_settings)\n    software_settings.pop('video_list', None)\n    software_settings = pd.DataFrame.from_dict(software_settings, columns=[\"Setting\"], orient='index')\n    try:\n        software_settings.to_csv(\"software_settings.csv\", sep=\";\")\n    except PermissionError:\n        logging.error(\"Never let software_settings.csv open when Cellects runs\")\n</code></pre>"},{"location":"api/cellects/core/program_organizer/#cellects.core.program_organizer.ProgramOrganizer.save_variable_dict","title":"<code>save_variable_dict()</code>","text":"<p>Saves the configuration dictionaries (<code>self.all</code> and <code>self.vars</code>) to a pickle file.</p> <p>If bio_mask or back_mask are not required for all folders, they are excluded from the saved data.</p> Notes <p>This method is used to preserve state between Cellects sessions or restart scenarios.</p> Source code in <code>src/cellects/core/program_organizer.py</code> <pre><code>def save_variable_dict(self):\n    \"\"\"\n    Saves the configuration dictionaries (`self.all` and `self.vars`) to a pickle file.\n\n    If bio_mask or back_mask are not required for all folders, they are excluded from the saved data.\n\n    Notes\n    -----\n    This method is used to preserve state between Cellects sessions or restart scenarios.\n    \"\"\"\n    logging.info(\"Save the parameters dictionaries in the Cellects folder\")\n    self.all['vars'] = self.vars\n    all_vars = deepcopy(self.all)\n    if not self.all['keep_cell_and_back_for_all_folders']:\n        all_vars['bio_mask'] = None\n        all_vars['back_mask'] = None\n    pickle_rick = PickleRick(0)\n    pickle_rick.write_file(all_vars, ALL_VARS_PKL_FILE)\n</code></pre>"},{"location":"api/cellects/core/program_organizer/#cellects.core.program_organizer.ProgramOrganizer.update_available_core_nb","title":"<code>update_available_core_nb(image_bit_number=256, video_bit_number=140)</code>","text":"<p>Update available computation resources based on memory and processing constraints.</p> <p>Parameters:</p> Name Type Description Default <code>image_bit_number</code> <code>int</code> <p>Number of bits per image pixel (default is 256).</p> <code>256</code> <code>video_bit_number</code> <code>int</code> <p>Number of bits per video frame pixel (default is 140).</p> <code>140</code> <p>Other Parameters:</p> Name Type Description <code>lose_accuracy_to_save_memory</code> <code>bool</code> <p>Flag to reduce accuracy for memory savings.</p> <code>convert_for_motion</code> <code>dict</code> <p>Conversion settings for motion analysis.</p> <code>already_greyscale</code> <code>bool</code> <p>Flag indicating if the image is already greyscale.</p> <code>save_coord_thickening_slimming</code> <code>bool</code> <p>Flag to save coordinates for thickening and slimming.</p> <code>oscilacyto_analysis</code> <code>bool</code> <p>Flag indicating if oscilacyto analysis is enabled.</p> <code>save_coord_network</code> <code>bool</code> <p>Flag to save coordinates for network analysis.</p> <p>Returns:</p> Type Description <code>float</code> <p>Rounded absolute difference between available memory and necessary memory in GB.</p> Notes <p>Performance considerations and limitations should be noted here if applicable.</p> Source code in <code>src/cellects/core/program_organizer.py</code> <pre><code>def update_available_core_nb(self, image_bit_number=256, video_bit_number=140):# video_bit_number=176\n    \"\"\"\n    Update available computation resources based on memory and processing constraints.\n\n    Parameters\n    ----------\n    image_bit_number : int, optional\n        Number of bits per image pixel (default is 256).\n    video_bit_number : int, optional\n        Number of bits per video frame pixel (default is 140).\n\n    Other Parameters\n    ----------------\n    lose_accuracy_to_save_memory : bool\n        Flag to reduce accuracy for memory savings.\n    convert_for_motion : dict\n        Conversion settings for motion analysis.\n    already_greyscale : bool\n        Flag indicating if the image is already greyscale.\n    save_coord_thickening_slimming : bool\n        Flag to save coordinates for thickening and slimming.\n    oscilacyto_analysis : bool\n        Flag indicating if oscilacyto analysis is enabled.\n    save_coord_network : bool\n        Flag to save coordinates for network analysis.\n\n    Returns\n    -------\n    float\n        Rounded absolute difference between available memory and necessary memory in GB.\n\n    Notes\n    -----\n    Performance considerations and limitations should be noted here if applicable.\n\n    \"\"\"\n    if self.vars['lose_accuracy_to_save_memory']:\n        video_bit_number -= 56\n    if self.vars['convert_for_motion']['logical'] != 'None':\n        video_bit_number += 64\n        if self.vars['lose_accuracy_to_save_memory']:\n            video_bit_number -= 56\n    if self.vars['already_greyscale']:\n        video_bit_number -= 64\n    if self.vars['save_coord_thickening_slimming'] or self.vars['oscilacyto_analysis']:\n        video_bit_number += 16\n        image_bit_number += 128\n    if self.vars['save_coord_network']:\n        video_bit_number += 8\n        image_bit_number += 64\n\n    if isinstance(self.bot, list):\n        one_image_memory = np.multiply((self.bot[0] - self.top[0]),\n                                    (self.right[0] - self.left[0])).max().astype(np.uint64)\n    else:\n        one_image_memory = np.multiply((self.bot - self.top).astype(np.uint64),\n                                    (self.right - self.left).astype(np.uint64)).max()\n    one_video_memory = self.vars['img_number'] * one_image_memory\n    necessary_memory = (one_image_memory * image_bit_number + one_video_memory * video_bit_number) * 1.16415e-10\n    available_memory = (virtual_memory().available &gt;&gt; 30) - self.vars['min_ram_free']\n    max_repeat_in_memory = (available_memory // necessary_memory).astype(np.uint16)\n    if max_repeat_in_memory &gt; 1:\n        max_repeat_in_memory = np.max(((available_memory // (2 * necessary_memory)).astype(np.uint16), 1))\n\n\n    self.cores = np.min((self.all['cores'], max_repeat_in_memory))\n    if self.cores &gt; self.sample_number:\n        self.cores = self.sample_number\n    return np.round(np.absolute(available_memory - necessary_memory), 3)\n</code></pre>"},{"location":"api/cellects/core/program_organizer/#cellects.core.program_organizer.ProgramOrganizer.update_folder_id","title":"<code>update_folder_id(sample_number, folder_name='')</code>","text":"<p>Update the current working directory and data list based on the given sample number and optional folder name.</p> <p>Parameters:</p> Name Type Description Default <code>sample_number</code> <code>int</code> <p>The number of samples to analyze.</p> required <code>folder_name</code> <code>str</code> <p>The name of the folder to change to. Default is an empty string.</p> <code>''</code> Notes <p>This function changes the current working directory to the specified folder name and updates the data list based on the file names in that directory. It also performs sorting of the data list and checks for strong variations in file names.</p> Source code in <code>src/cellects/core/program_organizer.py</code> <pre><code>def update_folder_id(self, sample_number: int, folder_name: str=\"\"):\n    \"\"\"\n    Update the current working directory and data list based on the given sample number\n    and optional folder name.\n\n    Parameters\n    ----------\n    sample_number : int\n        The number of samples to analyze.\n    folder_name : str, optional\n        The name of the folder to change to. Default is an empty string.\n\n    Notes\n    -----\n    This function changes the current working directory to the specified folder name\n    and updates the data list based on the file names in that directory. It also performs\n    sorting of the data list and checks for strong variations in file names.\n\n    \"\"\"\n    os.chdir(Path(self.all['global_pathway']) / folder_name)\n    self.data_list = insensitive_glob(\n        self.all['radical'] + '*' + self.all['extension'])  # Provides a list ordered by last modification date\n    # Sorting is necessary when some modifications (like rotation) modified the last modification date\n    self._sort_data_list()\n    if self.all['im_or_vid'] == 1:\n        self.sample_number = sample_number\n    else:\n        self.vars['img_number'] = len(self.data_list)\n        self.sample_number = sample_number\n    if not 'analyzed_individuals' in self.vars:\n        self._set_analyzed_individuals()\n</code></pre>"},{"location":"api/cellects/core/program_organizer/#cellects.core.program_organizer.ProgramOrganizer.update_one_row_per_arena","title":"<code>update_one_row_per_arena(i, table_to_add)</code>","text":"<p>Update one row of the dataframe per arena.</p> <p>Add a row to a DataFrame for each arena, based on the provided table_to_add. If no previous rows exist, initialize a new DataFrame with zeros.</p> <p>Parameters:</p> Name Type Description Default <code>i</code> <code>int</code> <p>Index of the arena to update.</p> required <code>table_to_add</code> <code>dict</code> <p>Dictionary containing values to add. Keys are column names, values are the data.</p> required Source code in <code>src/cellects/core/program_organizer.py</code> <pre><code>def update_one_row_per_arena(self, i: int, table_to_add):\n    \"\"\"\n    Update one row of the dataframe per arena.\n\n    Add a row to a DataFrame for each arena, based on the provided table_to_add. If no previous rows exist,\n    initialize a new DataFrame with zeros.\n\n    Parameters\n    ----------\n    i : int\n        Index of the arena to update.\n    table_to_add : dict\n        Dictionary containing values to add. Keys are column names, values are the data.\n\n    \"\"\"\n    if not self.vars['several_blob_per_arena']:\n        if self.one_row_per_arena is None:\n            self.one_row_per_arena = pd.DataFrame(np.zeros((len(self.vars['analyzed_individuals']), len(table_to_add)), dtype=float),\n                                        columns=table_to_add.keys())\n        self.one_row_per_arena.iloc[i, :] = table_to_add.values()\n</code></pre>"},{"location":"api/cellects/core/program_organizer/#cellects.core.program_organizer.ProgramOrganizer.update_one_row_per_frame","title":"<code>update_one_row_per_frame(i, j, table_to_add)</code>","text":"<p>Update a range of rows in <code>self.one_row_per_frame</code> DataFrame with values from <code>table_to_add</code>.</p> <p>Parameters:</p> Name Type Description Default <code>i</code> <code>int</code> <p>The starting row index to update in <code>self.one_row_per_frame</code>.</p> required <code>j</code> <code>int</code> <p>The ending row index (exclusive) to update in <code>self.one_row_per_frame</code>.</p> required <code>table_to_add</code> <code>dict</code> <p>A dictionary where keys are column labels and values are lists or arrays of data to insert into <code>self.one_row_per_frame</code>.</p> required Notes <p>Ensures that one row per arena is being updated. If <code>self.one_row_per_frame</code> is None, it initializes a DataFrame to hold the data.</p> Source code in <code>src/cellects/core/program_organizer.py</code> <pre><code>def update_one_row_per_frame(self, i: int, j: int, table_to_add):\n    \"\"\"\n    Update a range of rows in `self.one_row_per_frame` DataFrame with values from\n    `table_to_add`.\n\n    Parameters\n    ----------\n    i : int\n        The starting row index to update in `self.one_row_per_frame`.\n    j : int\n        The ending row index (exclusive) to update in `self.one_row_per_frame`.\n    table_to_add : dict\n        A dictionary where keys are column labels and values are lists or arrays of\n        data to insert into `self.one_row_per_frame`.\n    Notes\n    -----\n    Ensures that one row per arena is being updated. If `self.one_row_per_frame` is\n    None, it initializes a DataFrame to hold the data.\n    \"\"\"\n    if not self.vars['several_blob_per_arena']:\n        if self.one_row_per_frame is None:\n            self.one_row_per_frame = pd.DataFrame(index=range(len(self.vars['analyzed_individuals']) *\n                                                    self.vars['img_number']),\n                                        columns=table_to_add.keys())\n\n        self.one_row_per_frame.iloc[i:j, :] = table_to_add\n</code></pre>"},{"location":"api/cellects/core/program_organizer/#cellects.core.program_organizer.ProgramOrganizer.update_output_list","title":"<code>update_output_list()</code>","text":"<p>Update the output list with various descriptors from the analysis results.</p> <p>This method processes different types of descriptors and assigns them to the <code>self.vars['descriptors']</code> dictionary. It handles special cases for descriptors related to 'xy' dimensions and ensures that all relevant metrics are stored in the output list.</p> Source code in <code>src/cellects/core/program_organizer.py</code> <pre><code>def update_output_list(self):\n    \"\"\"\n    Update the output list with various descriptors from the analysis results.\n\n    This method processes different types of descriptors and assigns them to\n    the `self.vars['descriptors']` dictionary. It handles special cases for\n    descriptors related to 'xy' dimensions and ensures that all relevant metrics\n    are stored in the output list.\n    \"\"\"\n    self.vars['descriptors'] = {}\n    for descriptor in self.all['descriptors'].keys():\n        if descriptor == 'standard_deviation_xy':\n            self.vars['descriptors']['standard_deviation_x'] = self.all['descriptors'][descriptor]\n            self.vars['descriptors']['standard_deviation_y'] = self.all['descriptors'][descriptor]\n        elif descriptor == 'skewness_xy':\n            self.vars['descriptors']['skewness_x'] = self.all['descriptors'][descriptor]\n            self.vars['descriptors']['skewness_y'] = self.all['descriptors'][descriptor]\n        elif descriptor == 'kurtosis_xy':\n            self.vars['descriptors']['kurtosis_x'] = self.all['descriptors'][descriptor]\n            self.vars['descriptors']['kurtosis_y'] = self.all['descriptors'][descriptor]\n        elif descriptor == 'major_axes_len_and_angle':\n            self.vars['descriptors']['major_axis_len'] = self.all['descriptors'][descriptor]\n            self.vars['descriptors']['minor_axis_len'] = self.all['descriptors'][descriptor]\n            self.vars['descriptors']['axes_orientation'] = self.all['descriptors'][descriptor]\n        else:\n            if np.isin(descriptor, list(from_shape_descriptors_class.keys())):\n\n                self.vars['descriptors'][descriptor] = self.all['descriptors'][descriptor]\n    self.vars['descriptors']['newly_explored_area'] = self.vars['do_fading']\n</code></pre>"},{"location":"api/cellects/core/program_organizer/#cellects.core.program_organizer.ProgramOrganizer.update_variable_dict","title":"<code>update_variable_dict()</code>","text":"<p>Update the <code>all</code> and <code>vars</code> dictionaries with new data from <code>DefaultDicts</code>.</p> <p>This method updates the <code>all</code> and <code>vars</code> dictionaries of the current object with data from a new instance of <code>DefaultDicts</code>. It checks if any keys or descriptors are missing and adds them accordingly.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; organizer = ProgramOrganizer()\n&gt;&gt;&gt; organizer.update_variable_dict()\n</code></pre> Source code in <code>src/cellects/core/program_organizer.py</code> <pre><code>def update_variable_dict(self):\n    \"\"\"\n\n    Update the `all` and `vars` dictionaries with new data from `DefaultDicts`.\n\n    This method updates the `all` and `vars` dictionaries of the current object with\n    data from a new instance of `DefaultDicts`. It checks if any keys or descriptors\n    are missing and adds them accordingly.\n\n    Examples\n    --------\n    &gt;&gt;&gt; organizer = ProgramOrganizer()\n    &gt;&gt;&gt; organizer.update_variable_dict()\n    \"\"\"\n    dd = DefaultDicts()\n    all = len(dd.all) != len(self.all)\n    vars = len(dd.vars) != len(self.vars)\n    all_desc = not 'descriptors' in self.all or len(dd.all['descriptors']) != len(self.all['descriptors'])\n    vars_desc = not 'descriptors' in self.vars or len(dd.vars['descriptors']) != len(self.vars['descriptors'])\n    if all:\n        for key, val in dd.all.items():\n            if not key in self.all:\n                self.all[key] = val\n    if vars:\n        for key, val in dd.vars.items():\n            if not key in self.vars:\n                self.vars[key] = val\n    if all_desc:\n        for key, val in dd.all['descriptors'].items():\n            if not key in self.all['descriptors']:\n                self.all['descriptors'][key] = val\n    if vars_desc:\n        for key, val in dd.vars['descriptors'].items():\n            if not key in self.vars['descriptors']:\n                self.vars['descriptors'][key] = val\n    self._set_analyzed_individuals()\n</code></pre>"},{"location":"api/cellects/core/script_based_run/","title":"<code>cellects.core.script_based_run</code>","text":""},{"location":"api/cellects/core/script_based_run/#cellects.core.script_based_run","title":"<code>cellects.core.script_based_run</code>","text":"<p>This file contains lines to run Cellects without user interface</p>"},{"location":"api/cellects/gui/","title":"<code>cellects.gui</code>","text":""},{"location":"api/cellects/gui/#cellects.gui","title":"<code>cellects.gui</code>","text":""},{"location":"api/cellects/gui/advanced_parameters/","title":"<code>cellects.gui.advanced_parameters</code>","text":""},{"location":"api/cellects/gui/advanced_parameters/#cellects.gui.advanced_parameters","title":"<code>cellects.gui.advanced_parameters</code>","text":"<p>GUI module implementing the Advanced Parameters configuration window for Cellects.</p> <p>This module provides an interactive dialog allowing users to configure advanced image analysis and processing settings. The UI organizes parameter controls into categorized boxes: general parameters, cell detection rules, spatiotemporal scaling, computer resources, video saving options, and color space conversion (CSC) settings. It maintains user preferences in both RAM and persistent storage via \"Ok\" button click.</p> <p>Main Components AdvancedParameters : QWidget subclass for advanced parameter configuration window</p> <p>Notes Uses QThread for background operations to maintain UI responsiveness during parameter saving.</p>"},{"location":"api/cellects/gui/advanced_parameters/#cellects.gui.advanced_parameters.AdvancedParameters","title":"<code>AdvancedParameters</code>","text":"<p>               Bases: <code>WindowType</code></p> <p>This class creates the Advanced Parameters window. In the app, it is accessible from the first and the Video tracking window. It allows the user to fill in some parameters stored in the directory po.all (in RAM) and in all_vars.pkl (in ROM). Clicking \"Ok\" save the directory in RAM and in ROM.</p> Source code in <code>src/cellects/gui/advanced_parameters.py</code> <pre><code>class AdvancedParameters(WindowType):\n    \"\"\"\n        This class creates the Advanced Parameters window.\n        In the app, it is accessible from the first and the Video tracking window. It allows the user to fill in\n        some parameters stored in the directory po.all (in RAM) and in all_vars.pkl (in ROM).\n        Clicking \"Ok\" save the directory in RAM and in ROM.\n    \"\"\"\n    def __init__(self, parent, night_mode):\n        \"\"\"\n        Initialize the AdvancedParameters window with a parent widget and night mode setting.\n\n        Parameters\n        ----------\n        parent : QWidget\n            The parent widget to which this window will be attached.\n        night_mode : bool\n            A boolean indicating whether the night mode should be enabled.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from PySide6 import QtWidgets\n        &gt;&gt;&gt; from cellects.gui.cellects import CellectsMainWidget\n        &gt;&gt;&gt; from cellects.gui.advanced_parameters import AdvancedParameters\n        &gt;&gt;&gt; import sys\n        &gt;&gt;&gt; app = QtWidgets.QApplication([])\n        &gt;&gt;&gt; parent = CellectsMainWidget()\n        &gt;&gt;&gt; session = AdvancedParameters(parent, False)\n        &gt;&gt;&gt; session.true_init()\n        &gt;&gt;&gt; parent.insertWidget(0, session)\n        &gt;&gt;&gt; parent.show()\n        &gt;&gt;&gt; sys.exit(app.exec())\n        \"\"\"\n        super().__init__(parent, night_mode)\n\n        self.setParent(parent)\n        try:\n            self.true_init()\n        except KeyError:\n            default_dicts = DefaultDicts()\n            self.parent().po.all = default_dicts.all\n            self.parent().po.vars = default_dicts.vars\n            self.true_init()\n\n    def true_init(self):\n        \"\"\"\n        Initialize the AdvancedParameters window.\n\n        This method sets up the layout and widgets for the AdvancedParameters window,\n        including scroll areas, layouts, and various UI components for configuring\n        advanced parameters, including 'Cancel' and 'Ok' buttons.\n\n        Notes\n        -----\n        This method assumes that the parent widget has a 'po' attribute with specific settings and variables.\n        \"\"\"\n        logging.info(\"Initialize AdvancedParameters window\")\n        self.layout = QtWidgets.QVBoxLayout()\n\n        self.left_scroll_table = QtWidgets.QScrollArea()  #   # Scroll Area which contains the widgets, set as the centralWidget\n        self.left_scroll_table.setSizePolicy(QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.MinimumExpanding)\n        self.left_scroll_table.setMinimumHeight(150)\n        self.left_scroll_table.setFrameShape(QtWidgets.QFrame.NoFrame)\n        self.left_scroll_table.setVerticalScrollBarPolicy(QtCore.Qt.ScrollBarAsNeeded)\n        self.left_scroll_table.setHorizontalScrollBarPolicy(QtCore.Qt.ScrollBarAsNeeded)\n\n        self.left_col_layout = QtWidgets.QVBoxLayout()\n        self.right_col_layout = QtWidgets.QVBoxLayout()\n        self.left_col_widget = QtWidgets.QWidget()\n        self.right_col_widget = QtWidgets.QWidget()\n        # Create the main Title\n        self.title = FixedText('Advanced parameters', police=30, night_mode=self.parent().po.all['night_mode'])\n        self.title.setAlignment(QtCore.Qt.AlignHCenter)\n        # Create the main layout\n        self.layout.addWidget(self.title)\n        # Create the stylesheet for the boxes allowing to categorize advanced parameters.\n        boxstylesheet = \\\n            \".QWidget {\\n\" \\\n            + \"border: 1px solid black;\\n\" \\\n            + \"border-radius: 20px;\\n\" \\\n            + \"}\"\n\n\n        # I/ First box: General parameters\n        # I/A/ Title\n        self.general_param_box_label = FixedText('General parameters:', tip=\"\",\n                                         night_mode=self.parent().po.all['night_mode'])\n        self.left_col_layout.addWidget(self.general_param_box_label)\n        # I/B/ Create the box\n        self.general_param_box_layout = QtWidgets.QGridLayout()\n        self.general_param_box_widget = QtWidgets.QWidget()\n        self.general_param_box_widget.setStyleSheet(boxstylesheet)\n        # I/C/ Create widgets\n        self.automatically_crop = Checkbox(self.parent().po.all['automatically_crop'])\n        self.automatically_crop_label = FixedText(AP[\"Crop_images\"][\"label\"], tip=AP[\"Crop_images\"][\"tips\"],\n                                                  night_mode=self.parent().po.all['night_mode'])\n\n        self.subtract_background = Checkbox(self.parent().po.vars['subtract_background'])\n        self.subtract_background.stateChanged.connect(self.subtract_background_check)\n        self.subtract_background_label = FixedText(AP[\"Subtract_background\"][\"label\"], tip=AP[\"Subtract_background\"][\"tips\"], night_mode=self.parent().po.all['night_mode'])\n\n        self.keep_cell_and_back_for_all_folders = Checkbox(self.parent().po.all['keep_cell_and_back_for_all_folders'])\n        self.keep_cell_and_back_for_all_folders_label = FixedText(AP[\"Keep_drawings\"][\"label\"],\n                                               tip=AP[\"Keep_drawings\"][\"tips\"],\n                                               night_mode=self.parent().po.all['night_mode'])\n\n        self.correct_errors_around_initial = Checkbox(self.parent().po.vars['correct_errors_around_initial'])\n        self.correct_errors_around_initial_label = FixedText(AP[\"Correct_errors_around_initial\"][\"label\"],\n                                               tip=AP[\"Correct_errors_around_initial\"][\"tips\"],\n                                               night_mode=self.parent().po.all['night_mode'])\n\n        self.prevent_fast_growth_near_periphery = Checkbox(self.parent().po.vars['prevent_fast_growth_near_periphery'])\n        self.prevent_fast_growth_near_periphery_label = FixedText(AP[\"Prevent_fast_growth_near_periphery\"][\"label\"],\n                                               tip=AP[\"Prevent_fast_growth_near_periphery\"][\"tips\"],\n                                               night_mode=self.parent().po.all['night_mode'])\n\n        self.prevent_fast_growth_near_periphery.stateChanged.connect(self.prevent_fast_growth_near_periphery_check)\n        self.periphery_width = Spinbox(min=1, max=1000, val=self.parent().po.vars['periphery_width'],\n                                            decimals=0, night_mode=self.parent().po.all['night_mode'])\n        self.periphery_width_label = FixedText('Periphery width',\n                                               tip=\"The width, in pixels, of the arena s border designated as the peripheral region\",\n                                               night_mode=self.parent().po.all['night_mode'])\n        self.max_periphery_growth = Spinbox(min=1, max=1000000, val=self.parent().po.vars['max_periphery_growth'],\n                                            decimals=0, night_mode=self.parent().po.all['night_mode'])\n        self.max_periphery_growth_label = FixedText('Max periphery growth',\n                                               tip=\"The maximum detectable size (in pixels) of a shape in a single frame near the periphery of the arena.\\nLarger shapes will be considered as noise.\",\n                                               night_mode=self.parent().po.all['night_mode'])\n        self.prevent_fast_growth_near_periphery_check()\n\n\n        # I/D/ Arrange widgets in the box\n        self.general_param_box_layout.addWidget(self.automatically_crop, 0, 0)\n        self.general_param_box_layout.addWidget(self.automatically_crop_label, 0, 1)\n        self.general_param_box_layout.addWidget(self.subtract_background, 1, 0)\n        self.general_param_box_layout.addWidget(self.subtract_background_label, 1, 1)\n        self.general_param_box_layout.addWidget(self.keep_cell_and_back_for_all_folders, 2, 0)\n        self.general_param_box_layout.addWidget(self.keep_cell_and_back_for_all_folders_label, 2, 1)\n        self.general_param_box_layout.addWidget(self.correct_errors_around_initial, 3, 0)\n        self.general_param_box_layout.addWidget(self.correct_errors_around_initial_label, 3, 1)\n        self.general_param_box_layout.addWidget(self.prevent_fast_growth_near_periphery, 4, 0)\n        self.general_param_box_layout.addWidget(self.prevent_fast_growth_near_periphery_label, 4, 1)\n        self.general_param_box_layout.addWidget(self.periphery_width, 5, 0)\n        self.general_param_box_layout.addWidget(self.periphery_width_label, 5, 1)\n        self.general_param_box_layout.addWidget(self.max_periphery_growth, 6, 0)\n        self.general_param_box_layout.addWidget(self.max_periphery_growth_label, 6, 1)\n        self.general_param_box_widget.setLayout(self.general_param_box_layout)\n        self.left_col_layout.addWidget(self.general_param_box_widget)\n\n        # II/ Second box: One cell/colony per arena\n        # II/A/ Title\n        self.one_per_arena_label = FixedText('One cell/colony per arena parameters:', tip=\"\",\n                                            night_mode=self.parent().po.all['night_mode'])\n        self.left_col_layout.addWidget(self.one_per_arena_label)\n        # II/B/ Create the box\n        self.one_per_arena_box_layout = QtWidgets.QGridLayout()\n        self.one_per_arena_box_widget = QtWidgets.QWidget()\n        self.one_per_arena_box_widget.setStyleSheet(boxstylesheet)\n\n        # II/C/ Create widgets\n        self.all_specimens_have_same_direction = Checkbox(self.parent().po.all['all_specimens_have_same_direction'])\n        # self.all_specimens_have_same_direction.stateChanged.connect(self.all_specimens_have_same_direction_changed)\n        self.all_specimens_have_same_direction_label = FixedText(AP[\"Specimens_have_same_direction\"][\"label\"],\n                                                         tip=AP[\"Specimens_have_same_direction\"][\"tips\"],\n                                                         night_mode=self.parent().po.all['night_mode'])\n\n\n        connect_distant_shape = self.parent().po.all['connect_distant_shape_during_segmentation']\n        self.connect_distant_shape_during_segmentation = Checkbox(connect_distant_shape)\n        self.connect_distant_shape_during_segmentation.stateChanged.connect(self.do_distant_shape_int_changed)\n        self.connect_distant_shape_label = FixedText(AP[\"Connect_distant_shapes\"][\"label\"],\n                                                         tip=AP[\"Connect_distant_shapes\"][\"tips\"],\n                                                         night_mode=self.parent().po.all['night_mode'])\n        self.detection_range_factor = Spinbox(min=0, max=1000000,\n                                                      val=self.parent().po.vars['detection_range_factor'],\n                                                      night_mode=self.parent().po.all['night_mode'])\n        self.detection_range_factor_label = FixedText('Detection range factor:',\n                                                              tip=\"From 1 to 10, increase the allowed distance from original shape(s) to connect distant shapes\",\n                                                              night_mode=self.parent().po.all['night_mode'])\n\n        # Connect distant shape algo:\n        do_use_max_size = self.parent().po.vars['max_size_for_connection'] is not None and connect_distant_shape\n        do_use_min_size = self.parent().po.vars['min_size_for_connection'] is not None and connect_distant_shape\n        self.use_max_size = Checkbox(do_use_max_size, night_mode=self.parent().po.all['night_mode'])\n        self.use_min_size = Checkbox(do_use_min_size, night_mode=self.parent().po.all['night_mode'])\n        self.use_max_size.stateChanged.connect(self.use_max_size_changed)\n        self.use_min_size.stateChanged.connect(self.use_min_size_changed)\n\n        self.use_max_size_label = FixedText('Use max size as a threshold',\n                                            tip=\"To decide whether distant shapes should get connected\",\n                                            night_mode=self.parent().po.all['night_mode'])\n        self.use_min_size_label = FixedText('Use min size as a threshold',\n                                            tip=\"To decide whether distant shapes should get connected\",\n                                            night_mode=self.parent().po.all['night_mode'])\n        self.max_size_for_connection_label = FixedText('Max (pixels):', night_mode=self.parent().po.all['night_mode'])\n        self.min_size_for_connection_label = FixedText('Min (pixels):', night_mode=self.parent().po.all['night_mode'])\n        if do_use_max_size:\n            self.max_size_for_connection = Spinbox(min=0, max=1000000,\n                                                  val=self.parent().po.vars['max_size_for_connection'],\n                                                  night_mode=self.parent().po.all['night_mode'])\n        else:\n            self.max_size_for_connection = Spinbox(min=0, max=1000000, val=50,\n                                                  night_mode=self.parent().po.all['night_mode'])\n        if do_use_min_size:\n            self.min_size_for_connection = Spinbox(min=0, max=1000000,\n                                                  val=self.parent().po.vars['min_size_for_connection'],\n                                                  night_mode=self.parent().po.all['night_mode'])\n        else:\n            self.min_size_for_connection = Spinbox(min=0, max=1000000, val=0,\n                                                  night_mode=self.parent().po.all['night_mode'])\n\n        self.use_min_size.setStyleSheet(\"QCheckBox::indicator {width: 12px;height: 12px;background-color: transparent;\"\n                            \"border-radius: 5px;border-style: solid;border-width: 1px;\"\n                            \"border-color: rgb(100,100,100);}\"\n                            \"QCheckBox::indicator:checked {background-color: rgb(70,130,180);}\"\n                            \"QCheckBox:checked, QCheckBox::indicator:checked {border-color: black black white white;}\"\n                            \"QCheckBox:checked {background-color: transparent;}\"\n                            \"QCheckBox:margin-left {100%}\"\n                            \"QCheckBox:margin-right {0%}\")\n        self.min_size_for_connection_label.setAlignment(QtCore.Qt.AlignRight)\n        self.use_max_size.setStyleSheet(\"QCheckBox::indicator {width: 12px;height: 12px;background-color: transparent;\"\n                            \"border-radius: 5px;border-style: solid;border-width: 1px;\"\n                            \"border-color: rgb(100,100,100);}\"\n                            \"QCheckBox::indicator:checked {background-color: rgb(70,130,180);}\"\n                            \"QCheckBox:checked, QCheckBox::indicator:checked {border-color: black black white white;}\"\n                            \"QCheckBox:checked {background-color: transparent;}\"\n                            \"QCheckBox:margin-left {100%}\"\n                            \"QCheckBox:margin-right {0%}\")\n        self.max_size_for_connection_label.setAlignment(QtCore.Qt.AlignRight)\n\n        # II/D/ Arrange widgets in the box\n        curr_box_row = 0\n        self.one_per_arena_box_layout.addWidget(self.connect_distant_shape_during_segmentation, curr_box_row, 0)\n        self.one_per_arena_box_layout.addWidget(self.connect_distant_shape_label, curr_box_row, 1)\n        curr_box_row += 1\n        self.one_per_arena_box_layout.addWidget(self.detection_range_factor, curr_box_row, 0)\n        self.one_per_arena_box_layout.addWidget(self.detection_range_factor_label, curr_box_row, 1)\n        curr_box_row += 1\n        self.one_per_arena_box_layout.addWidget(self.use_min_size, curr_box_row, 0)\n        self.one_per_arena_box_layout.addWidget(self.use_min_size_label, curr_box_row, 1)\n        curr_box_row += 1\n        self.one_per_arena_box_layout.addWidget(self.min_size_for_connection_label, curr_box_row, 0)\n        self.one_per_arena_box_layout.addWidget(self.min_size_for_connection, curr_box_row, 1)\n        curr_box_row += 1\n        self.one_per_arena_box_layout.addWidget(self.use_max_size, curr_box_row, 0)\n        self.one_per_arena_box_layout.addWidget(self.use_max_size_label, curr_box_row, 1)\n        curr_box_row += 1\n        self.one_per_arena_box_layout.addWidget(self.max_size_for_connection_label, curr_box_row, 0)\n        self.one_per_arena_box_layout.addWidget(self.max_size_for_connection, curr_box_row, 1)\n        curr_box_row += 1\n        self.one_per_arena_box_layout.addWidget(self.all_specimens_have_same_direction, curr_box_row, 0)\n        self.one_per_arena_box_layout.addWidget(self.all_specimens_have_same_direction_label, curr_box_row, 1)\n        curr_box_row += 1\n\n        self.one_per_arena_box_widget.setLayout(self.one_per_arena_box_layout)\n        self.left_col_layout.addWidget(self.one_per_arena_box_widget)\n\n        # III/ Third box: Appearing cell/colony\n        # III/A/ Title\n        self.appearing_cell_label = FixedText('Appearing cell/colony parameters:', tip=\"\",\n                                              night_mode=self.parent().po.all['night_mode'])\n\n        self.left_col_layout.addWidget(self.appearing_cell_label)\n        # III/B/ Create the box\n        self.appearing_cell_box_layout = QtWidgets.QGridLayout()\n        self.appearing_cell_box_widget = QtWidgets.QWidget()\n        self.appearing_cell_box_widget.setStyleSheet(boxstylesheet)\n\n        # III/C/ Create widgets\n        self.first_move_threshold = Spinbox(min=0, max=1000000, val=self.parent().po.all['first_move_threshold_in_mm\u00b2'],\n                                            decimals=6, night_mode=self.parent().po.all['night_mode'])\n        self.first_move_threshold_label = FixedText('Minimal size to detect a cell/colony',\n                                                    tip=\"In mm\u00b2. All appearing cell/colony lesser than this value will be considered as noise\",\n                                                    night_mode=self.parent().po.all['night_mode'])\n        self.do_automatic_size_thresholding = Checkbox(self.parent().po.all['automatic_size_thresholding'])\n        self.do_automatic_size_thresholding_label = FixedText(AP[\"Appearance_size_threshold\"][\"label\"],\n                                                              tip=AP[\"Appearance_size_threshold\"][\"tips\"],\n                                                              night_mode=self.parent().po.all['night_mode'])\n        self.do_automatic_size_thresholding.stateChanged.connect(self.do_automatic_size_thresholding_changed)\n        self.appearing_selection = Combobox([\"largest\", \"most_central\"], night_mode=self.parent().po.all['night_mode'])\n        self.appearing_selection_label = FixedText(AP[\"Appearance_detection_method\"][\"label\"],\n                                                   tip=AP[\"Appearance_detection_method\"][\"tips\"],\n                                                   night_mode=self.parent().po.all['night_mode'])\n        self.appearing_selection.setCurrentText(self.parent().po.vars['appearance_detection_method'])\n        self.appearing_selection.setFixedWidth(190)\n\n        # III/D/ Arrange widgets in the box\n        curr_box_row = 0\n        self.appearing_cell_box_layout.addWidget(self.do_automatic_size_thresholding, curr_box_row, 0)\n        self.appearing_cell_box_layout.addWidget(self.do_automatic_size_thresholding_label, curr_box_row, 1)\n        curr_box_row += 1\n        self.appearing_cell_box_layout.addWidget(self.first_move_threshold, curr_box_row, 0)\n        self.appearing_cell_box_layout.addWidget(self.first_move_threshold_label, curr_box_row, 1)\n        curr_box_row += 1\n        self.appearing_cell_box_layout.addWidget(self.appearing_selection, curr_box_row, 0)\n        self.appearing_cell_box_layout.addWidget(self.appearing_selection_label, curr_box_row, 1)\n        curr_box_row += 1\n\n        self.appearing_cell_box_widget.setLayout(self.appearing_cell_box_layout)\n        self.left_col_layout.addWidget(self.appearing_cell_box_widget)\n\n        # V/ Fifth box: Network detection parameters:#\n        # IV/A/ Title\n        self.rolling_window_s_label = FixedText(IAW[\"Rolling_window_segmentation\"][\"label\"] + ': (auto if checked)',\n                                                tip=IAW[\"Rolling_window_segmentation\"][\"tips\"],\n                                              night_mode=self.parent().po.all['night_mode'])\n        self.left_col_layout.addWidget(self.rolling_window_s_label)\n        self.rolling_window_s_layout = QtWidgets.QGridLayout()\n        self.rolling_window_s_widget = QtWidgets.QWidget()\n        self.rolling_window_s_widget.setStyleSheet(boxstylesheet)\n        self.mesh_side_length_cb = Checkbox(self.parent().po.all['auto_mesh_side_length'],\n                                        night_mode=self.parent().po.all['night_mode'])\n        self.mesh_side_length_cb.stateChanged.connect(self.mesh_side_length_cb_changed)\n        self.mesh_side_length_label = FixedText(AP[\"Mesh_side_length\"][\"label\"], tip=AP[\"Mesh_side_length\"][\"tips\"],\n                                                night_mode=self.parent().po.all['night_mode'])\n        if self.parent().po.vars['rolling_window_segmentation']['side_len'] is None:\n            self.mesh_side_length = Spinbox(min=0, max=1000000, val=4, decimals=0,\n                                            night_mode=self.parent().po.all['night_mode'])\n            self.mesh_side_length.setVisible(False)\n        else:\n            self.mesh_side_length = Spinbox(min=0, max=1000000, val=self.parent().po.vars['rolling_window_segmentation']['side_len'], decimals=0,\n                                            night_mode=self.parent().po.all['night_mode'])\n\n\n        self.mesh_step_length_cb = Checkbox(self.parent().po.all['auto_mesh_step_length'],\n                                        night_mode=self.parent().po.all['night_mode'])\n        self.mesh_step_length_cb.stateChanged.connect(self.mesh_step_length_cb_changed)\n        self.mesh_step_length_label = FixedText(AP[\"Mesh_step_length\"][\"label\"], tip=AP[\"Mesh_step_length\"][\"tips\"],\n                                                night_mode=self.parent().po.all['night_mode'])\n        if self.parent().po.vars['rolling_window_segmentation']['side_len'] is None:\n            self.mesh_step_length = Spinbox(min=0, max=1000, val=2, decimals=0,\n                                            night_mode=self.parent().po.all['night_mode'])\n            self.mesh_step_length.setVisible(False)\n        else:\n            self.mesh_step_length = Spinbox(min=0, max=1000, val=self.parent().po.vars['rolling_window_segmentation']['step'], decimals=0,\n                                            night_mode=self.parent().po.all['night_mode'])\n\n\n        self.mesh_min_int_var_cb = Checkbox(self.parent().po.all['auto_mesh_min_int_var'],\n                                        night_mode=self.parent().po.all['night_mode'])\n        self.mesh_min_int_var_cb.stateChanged.connect(self.mesh_min_int_var_cb_changed)\n        if self.parent().po.vars['rolling_window_segmentation']['side_len'] is None:\n            self.mesh_min_int_var = Spinbox(min=0, max=1000, val=2, decimals=0,\n                                            night_mode=self.parent().po.all['night_mode'])\n            self.mesh_min_int_var.setVisible(False)\n        else:\n            self.mesh_min_int_var = Spinbox(min=0, max=1000, val=self.parent().po.vars['rolling_window_segmentation']['min_int_var'], decimals=0,\n                                            night_mode=self.parent().po.all['night_mode'])\n        self.mesh_min_int_var_label = FixedText(AP[\"Mesh_minimal_intensity_variation\"][\"label\"],\n                                                tip=AP[\"Mesh_minimal_intensity_variation\"][\"tips\"],\n                                                night_mode=self.parent().po.all['night_mode'])\n        self.rolling_window_s_layout.addWidget(self.mesh_side_length_cb, 0, 0)\n        self.rolling_window_s_layout.addWidget(self.mesh_side_length_label, 0, 1)\n        self.rolling_window_s_layout.addWidget(self.mesh_side_length, 0, 2)\n        self.rolling_window_s_layout.addWidget(self.mesh_step_length_cb, 1, 0)\n        self.rolling_window_s_layout.addWidget(self.mesh_step_length_label, 1, 1)\n        self.rolling_window_s_layout.addWidget(self.mesh_step_length, 1, 2)\n        self.rolling_window_s_layout.addWidget(self.mesh_min_int_var_cb, 2, 0)\n        self.rolling_window_s_layout.addWidget(self.mesh_min_int_var_label, 2, 1)\n        self.rolling_window_s_layout.addWidget(self.mesh_min_int_var, 2, 2)\n        self.rolling_window_s_widget.setLayout(self.rolling_window_s_layout)\n        self.left_col_layout.addWidget(self.rolling_window_s_widget)\n\n        # IV/ Fourth box: Oscillation period:\n        # IV/A/ Title\n        self.oscillation_label = FixedText('Oscillatory parameters:', tip=\"\",\n                                              night_mode=self.parent().po.all['night_mode'])\n        self.left_col_layout.addWidget(self.oscillation_label)\n\n        self.oscillation_period_layout = QtWidgets.QGridLayout()\n        self.oscillation_period_widget = QtWidgets.QWidget()\n        self.oscillation_period_widget.setStyleSheet(boxstylesheet)\n\n        self.oscillation_period = Spinbox(min=0, max=10000, val=self.parent().po.vars['expected_oscillation_period'], decimals=2,\n                                          night_mode=self.parent().po.all['night_mode'])\n        self.oscillation_period_label = FixedText(AP[\"Expected_oscillation_period\"][\"label\"],\n                                                  tip=AP[\"Expected_oscillation_period\"][\"tips\"],\n                                                  night_mode=self.parent().po.all['night_mode'])\n\n        self.minimal_oscillating_cluster_size = Spinbox(min=1, max=1000000000, decimals=0, val=self.parent().po.vars['minimal_oscillating_cluster_size'],\n                                          night_mode=self.parent().po.all['night_mode'])\n        self.minimal_oscillating_cluster_size_label = FixedText(AP[\"Minimal_oscillating_cluster_size\"][\"label\"],\n                                                  tip=AP[\"Minimal_oscillating_cluster_size\"][\"tips\"],\n                                                  night_mode=self.parent().po.all['night_mode'])\n\n        self.oscillation_period_layout.addWidget(self.oscillation_period, 0, 0)\n        self.oscillation_period_layout.addWidget(self.oscillation_period_label, 0, 1)\n        self.oscillation_period_layout.addWidget(self.minimal_oscillating_cluster_size, 1, 0)\n        self.oscillation_period_layout.addWidget(self.minimal_oscillating_cluster_size_label, 1, 1)\n\n        self.oscillation_period_widget.setLayout(self.oscillation_period_layout)\n        self.left_col_layout.addWidget(self.oscillation_period_widget)\n\n        # I/ First box: Scales\n        # I/A/ Title\n        self.right_scroll_table = QtWidgets.QScrollArea()   # Scroll Area which contains the widgets, set as the centralWidget\n        self.right_scroll_table.setSizePolicy(QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.MinimumExpanding)\n        self.right_scroll_table.setMinimumHeight(150)#self.parent().im_max_height - 100\n        self.right_scroll_table.setFrameShape(QtWidgets.QFrame.NoFrame)\n        self.right_scroll_table.setVerticalScrollBarPolicy(QtCore.Qt.ScrollBarAsNeeded)\n        self.right_scroll_table.setHorizontalScrollBarPolicy(QtCore.Qt.ScrollBarAsNeeded)\n        self.scale_box_label = FixedText(AP[\"Spatio_temporal_scaling\"][\"label\"] + ':',\n                                         tip=AP[\"Spatio_temporal_scaling\"][\"tips\"],\n                                         night_mode=self.parent().po.all['night_mode'])\n        self.right_col_layout.addWidget(self.scale_box_label)\n\n        # I/B/ Create the box\n        self.scale_box_layout = QtWidgets.QGridLayout()\n        self.scale_box_widget = QtWidgets.QWidget()\n        self.scale_box_widget.setStyleSheet(boxstylesheet)\n\n        # I/C/ Create widgets\n        self.extract_time = Checkbox(self.parent().po.all['extract_time_interval'])\n        self.extract_time.clicked.connect(self.extract_time_is_clicked)\n        self.time_step = Spinbox(min=0, max=100000, val=self.parent().po.vars['time_step'], decimals=3,\n                                 night_mode=self.parent().po.all['night_mode'])\n        self.time_step.setFixedWidth(60)\n        if self.parent().po.all['extract_time_interval']:\n            self.time_step.setVisible(False)\n            self.time_step_label = FixedText('Automatically extract time interval between images',\n                                         tip=\"Uses the exif data of the images (if available), to extract these intervals\\nOtherwise, default time interval is 1 min\",\n                                         night_mode=self.parent().po.all['night_mode'])\n        else:\n            self.time_step_label = FixedText('Set the time interval between images',\n                                         tip=\"In minutes\",\n                                         night_mode=self.parent().po.all['night_mode'])\n        self.pixels_to_mm = Checkbox(self.parent().po.vars['output_in_mm'])\n        self.pixels_to_mm_label = FixedText('Convert areas and distances from pixels to mm',\n                                            tip=\"Check if you want output variables to be in mm\\nUncheck if you want output variables to be in pixels\",\n                                            night_mode=self.parent().po.all['night_mode'])\n\n        # I/D/ Arrange widgets in the box\n        self.scale_box_layout.addWidget(self.extract_time, 0, 0)\n        self.scale_box_layout.addWidget(self.time_step_label, 0, 1)\n        self.scale_box_layout.addWidget(self.time_step, 0, 2)\n        self.scale_box_layout.addWidget(self.pixels_to_mm, 2, 0)\n        self.scale_box_layout.addWidget(self.pixels_to_mm_label, 2, 1)\n        self.scale_box_widget.setLayout(self.scale_box_layout)\n        self.right_col_layout.addWidget(self.scale_box_widget)\n\n        # IV/ Fourth box: Computer resources\n        # IV/A/ Title\n        self.resources_label = FixedText('Computer resources:', tip=\"\",\n                                            night_mode=self.parent().po.all['night_mode'])\n        self.right_col_layout.addWidget(self.resources_label)\n\n        # IV/B/ Create the box\n        self.resources_box_layout = QtWidgets.QGridLayout()\n        self.resources_box_widget = QtWidgets.QWidget()\n        self.resources_box_widget.setStyleSheet(boxstylesheet)\n\n        # IV/C/ Create widgets\n        self.do_multiprocessing = Checkbox(self.parent().po.all['do_multiprocessing'])\n        self.do_multiprocessing_label = FixedText(AP[\"Parallel_analysis\"][\"label\"], tip=AP[\"Parallel_analysis\"][\"tips\"],\n                                                  night_mode=self.parent().po.all['night_mode'])\n        self.do_multiprocessing.stateChanged.connect(self.do_multiprocessing_is_clicked)\n        self.max_core_nb = Spinbox(min=0, max=256, val=self.parent().po.all['cores'],\n                                   night_mode=self.parent().po.all['night_mode'])\n        self.max_core_nb_label = FixedText(AP[\"Proc_max_core_nb\"][\"label\"],\n                                           tip=AP[\"Proc_max_core_nb\"][\"tips\"],\n                                           night_mode=self.parent().po.all['night_mode'])\n        self.min_memory_left = Spinbox(min=0, max=1024, val=self.parent().po.vars['min_ram_free'], decimals=1,\n                                       night_mode=self.parent().po.all['night_mode'])\n        self.min_memory_left_label = FixedText(AP[\"Minimal_RAM_let_free\"][\"label\"],\n                                                tip=AP[\"Minimal_RAM_let_free\"][\"tips\"],\n                                               night_mode=self.parent().po.all['night_mode'])\n\n        self.lose_accuracy_to_save_memory = Checkbox(self.parent().po.vars['lose_accuracy_to_save_memory'])\n        self.lose_accuracy_to_save_memory_label = FixedText(AP[\"Lose_accuracy_to_save_RAM\"][\"label\"],\n                                                  tip=AP[\"Lose_accuracy_to_save_RAM\"][\"tips\"],\n                                                  night_mode=self.parent().po.all['night_mode'])\n\n        # IV/D/ Arrange widgets in the box\n        self.resources_box_layout.addWidget(self.do_multiprocessing, 0, 0)\n        self.resources_box_layout.addWidget(self.do_multiprocessing_label, 0, 1)\n        self.resources_box_layout.addWidget(self.max_core_nb, 1, 0)\n        self.resources_box_layout.addWidget(self.max_core_nb_label, 1, 1)\n        self.resources_box_layout.addWidget(self.min_memory_left, 2, 0)\n        self.resources_box_layout.addWidget(self.min_memory_left_label, 2, 1)\n        self.resources_box_layout.addWidget(self.lose_accuracy_to_save_memory, 3, 0)\n        self.resources_box_layout.addWidget(self.lose_accuracy_to_save_memory_label, 3, 1)\n        self.resources_box_widget.setLayout(self.resources_box_layout)\n        self.right_col_layout.addWidget(self.resources_box_widget)\n\n        # V/ Fifth box: Video saving\n        # V/A/ Title\n        self.video_saving_label = FixedText('Video saving:', tip=\"\",\n                                         night_mode=self.parent().po.all['night_mode'])\n        self.right_col_layout.addWidget(self.video_saving_label)\n\n        # V/B/ Create the box\n        self.video_saving_layout = QtWidgets.QGridLayout()\n        self.video_saving_widget = QtWidgets.QWidget()\n        self.video_saving_widget.setStyleSheet(boxstylesheet)\n\n        # V/C/ Create widgets\n        self.video_fps = Spinbox(min=0, max=10000, val=self.parent().po.vars['video_fps'], decimals=2,\n                                 night_mode=self.parent().po.all['night_mode'])\n        self.video_fps_label = FixedText(AP[\"Video_fps\"][\"label\"], tip=AP[\"Video_fps\"][\"tips\"],\n                                         night_mode=self.parent().po.all['night_mode'])\n        self.keep_unaltered_videos = Checkbox(self.parent().po.vars['keep_unaltered_videos'])\n        self.keep_unaltered_videos_label = FixedText(AP[\"Keep_unaltered_videos\"][\"label\"],\n                                                     tip=AP[\"Keep_unaltered_videos\"][\"tips\"],\n                                                     night_mode=self.parent().po.all['night_mode'])\n        self.save_processed_videos = Checkbox(self.parent().po.vars['save_processed_videos'])\n        self.save_processed_videos_label = FixedText(AP[\"Save_processed_videos\"][\"label\"],\n                                                     tip=AP[\"Save_processed_videos\"][\"tips\"],\n                                                     night_mode=self.parent().po.all['night_mode'])\n\n        # V/D/ Arrange widgets in the box\n        curr_box_row = 0\n        self.video_saving_layout.addWidget(self.video_fps, curr_box_row, 0)\n        self.video_saving_layout.addWidget(self.video_fps_label, curr_box_row, 1)\n        curr_box_row += 1\n        self.video_saving_layout.addWidget(self.keep_unaltered_videos, curr_box_row, 0)\n        self.video_saving_layout.addWidget(self.keep_unaltered_videos_label, curr_box_row, 1)\n        curr_box_row += 1\n        self.video_saving_layout.addWidget(self.save_processed_videos, curr_box_row, 0)\n        self.video_saving_layout.addWidget(self.save_processed_videos_label, curr_box_row, 1)\n        curr_box_row += 1\n        self.video_saving_widget.setLayout(self.video_saving_layout)\n        self.right_col_layout.addWidget(self.video_saving_widget)\n\n        # VII/ Seventh box: csc\n        # VII/A/ Title\n        # VII/C/ Create widgets\n        self.generate_csc_editing()\n        # VII/D/ Arrange widgets in the box\n        self.right_col_layout.addWidget(self.edit_widget)\n\n        # VIII/ Finalize layout and add the night mode option and the ok button\n        self.left_col_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.Maximum, QtWidgets.QSizePolicy.MinimumExpanding))\n        self.right_col_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.Maximum, QtWidgets.QSizePolicy.MinimumExpanding))\n        self.left_col_widget.setLayout(self.left_col_layout)\n        self.right_col_widget.setLayout(self.right_col_layout)\n        self.central_widget = QtWidgets.QWidget()\n        self.central_layout = QtWidgets.QHBoxLayout()\n        self.central_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n        self.left_scroll_table.setWidget(self.left_col_widget)\n        self.left_scroll_table.setWidgetResizable(True)\n        self.left_scroll_table.setParent(self.central_widget)\n        self.central_layout.addWidget(self.left_scroll_table)\n        self.central_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n        self.right_scroll_table.setWidget(self.right_col_widget)\n        self.right_scroll_table.setWidgetResizable(True)\n        self.right_scroll_table.setParent(self.central_widget)\n        self.central_layout.addWidget(self.right_scroll_table)\n        self.central_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n        self.central_widget.setLayout(self.central_layout)\n        self.layout.addWidget(self.central_widget)\n\n        # Last row\n        self.last_row_layout = QtWidgets.QHBoxLayout()\n        self.last_row_widget = QtWidgets.QWidget()\n        self.night_mode_cb = Checkbox(self.parent().po.all['night_mode'])\n        self.night_mode_cb.clicked.connect(self.night_mode_is_clicked)\n        self.night_mode_label = FixedText(AP[\"Night_mode\"][\"label\"], tip=AP[\"Night_mode\"][\"tips\"],\n                                          night_mode=self.parent().po.all['night_mode'])\n        self.reset_all_settings = PButton(AP[\"Reset_all_settings\"][\"label\"], tip=AP[\"Reset_all_settings\"][\"tips\"],\n                                          night_mode=self.parent().po.all['night_mode'])\n        self.reset_all_settings.clicked.connect(self.reset_all_settings_is_clicked)\n        self.message = FixedText('', night_mode=self.parent().po.all['night_mode'])\n        self.cancel = PButton('Cancel', night_mode=self.parent().po.all['night_mode'])\n        self.cancel.clicked.connect(self.cancel_is_clicked)\n        self.ok = PButton('Ok', night_mode=self.parent().po.all['night_mode'])\n        self.ok.clicked.connect(self.ok_is_clicked)\n        self.last_row_layout.addWidget(self.night_mode_cb)\n        self.last_row_layout.addWidget(self.night_mode_label)\n        self.last_row_layout.addWidget(self.reset_all_settings)\n        self.last_row_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n        self.last_row_layout.addWidget(self.message)\n        self.last_row_layout.addWidget(self.cancel)\n        self.last_row_layout.addWidget(self.ok)\n        self.last_row_widget.setLayout(self.last_row_layout)\n        self.layout.addWidget(self.last_row_widget)\n\n        self.setLayout(self.layout)\n\n    def display_conditionally_visible_widgets(self):\n        \"\"\"\n        Conditionally displays widgets based on various settings within the parent object.\n\n        This function controls the visibility of several UI elements based on the\n        values in the parent object's `all` dictionary and `vars` dictionary. It ensures\n        that only relevant widgets are shown to the user, depending on the current settings.\n        \"\"\"\n        self.max_core_nb.setVisible(self.parent().po.all['do_multiprocessing'])\n        self.max_core_nb_label.setVisible(self.parent().po.all['do_multiprocessing'])\n        self.first_move_threshold.setVisible(not self.parent().po.all['automatic_size_thresholding'])\n        self.first_move_threshold_label.setVisible(not self.parent().po.all['automatic_size_thresholding'])\n        connect_distant_shape = self.parent().po.all['connect_distant_shape_during_segmentation']\n        do_use_max_size = self.parent().po.vars['max_size_for_connection'] is not None and connect_distant_shape\n        do_use_min_size = self.parent().po.vars['min_size_for_connection'] is not None and connect_distant_shape\n        self.detection_range_factor.setVisible(connect_distant_shape)\n        self.detection_range_factor_label.setVisible(connect_distant_shape)\n        self.use_max_size.setVisible(connect_distant_shape)\n        self.use_min_size.setVisible(connect_distant_shape)\n        self.use_max_size_label.setVisible(connect_distant_shape)\n        self.use_min_size_label.setVisible(connect_distant_shape)\n\n        self.max_size_for_connection.setVisible(do_use_max_size)\n        self.max_size_for_connection_label.setVisible(do_use_max_size)\n        self.min_size_for_connection.setVisible(do_use_min_size)\n        self.min_size_for_connection_label.setVisible(do_use_min_size)\n        self.display_more_than_two_colors_option()\n\n        if self.parent().po.vars['convert_for_motion'] is not None:\n            self.update_csc_editing_display()\n        else:\n            self.row1[0].setCurrentIndex(4)\n            self.row1[3].setValue(1)\n            self.row21[0].setCurrentIndex(0)\n            self.row21[3].setValue(0)\n\n    def subtract_background_check(self):\n        \"\"\"\n        Handles the logic for using background subtraction or not during image segmentation.\n        \"\"\"\n        self.parent().po.motion = None\n        if self.subtract_background.isChecked():\n            self.parent().po.first_exp_ready_to_run = False\n\n    def prevent_fast_growth_near_periphery_check(self):\n        \"\"\"\n        Handles the logic for using a special algorithm on growth near the periphery during video segmentation.\n        \"\"\"\n        checked_status = self.prevent_fast_growth_near_periphery.isChecked()\n        self.periphery_width.setVisible(checked_status)\n        self.periphery_width_label.setVisible(checked_status)\n        self.max_periphery_growth.setVisible(checked_status)\n        self.max_periphery_growth_label.setVisible(checked_status)\n\n    def do_automatic_size_thresholding_changed(self):\n        \"\"\"\n        This function toggles the visibility of `first_move_threshold` and\n        `first_move_threshold_label` UI elements based on whether the\n        `do_automatic_size_thresholding` checkbox is checked or not.\n        \"\"\"\n        self.first_move_threshold.setVisible(not self.do_automatic_size_thresholding.isChecked())\n        self.first_move_threshold_label.setVisible(not self.do_automatic_size_thresholding.isChecked())\n\n    def mesh_side_length_cb_changed(self):\n        self.mesh_side_length.setVisible(self.mesh_side_length_cb.isChecked())\n\n    def mesh_step_length_cb_changed(self):\n        self.mesh_step_length.setVisible(self.mesh_step_length_cb.isChecked())\n\n    def mesh_min_int_var_cb_changed(self):\n        self.mesh_min_int_var.setVisible(self.mesh_min_int_var_cb.isChecked())\n\n    def extract_time_is_clicked(self):\n        \"\"\"\n        Toggle the visibility of time_step_label and update its text/tooltip based on\n        whether extract_time is checked.\n        \"\"\"\n        self.time_step.setVisible(not self.extract_time.isChecked())\n        if self.extract_time.isChecked():\n            self.time_step_label.setText(\"Automatically extract time interval between images\")\n            self.time_step_label.setToolTip(\"Uses the exif data of the images (if available), to extract these intervals\\nOtherwise, default time interval is 1 min\")\n        else:\n            self.time_step_label.setText(\"Set the time interval between images\")\n            self.time_step_label.setToolTip(\"In minutes\")\n\n    def do_multiprocessing_is_clicked(self):\n        \"\"\"\n        Update the visibility of `max_core_nb` and `max_core_nb_label` based on the checkbox state of `do_multiprocessing`.\n        \"\"\"\n        self.max_core_nb.setVisible(self.do_multiprocessing.isChecked())\n        self.max_core_nb_label.setVisible(self.do_multiprocessing.isChecked())\n\n    def do_distant_shape_int_changed(self):\n        \"\"\"\n        Toggles the visibility of widgets based the use of an algorithm allowing to connect distant shapes\n        during segmentation.\n        \"\"\"\n        do_distant_shape_int = self.connect_distant_shape_during_segmentation.isChecked()\n        self.detection_range_factor.setVisible(do_distant_shape_int)\n        if do_distant_shape_int:\n            self.detection_range_factor.setValue(2)\n        self.detection_range_factor_label.setVisible(do_distant_shape_int)\n        self.use_max_size.setVisible(do_distant_shape_int)\n        self.use_min_size.setVisible(do_distant_shape_int)\n        self.use_max_size_label.setVisible(do_distant_shape_int)\n        self.use_min_size_label.setVisible(do_distant_shape_int)\n        do_use_max_size = do_distant_shape_int and self.use_max_size.isChecked()\n        self.max_size_for_connection.setVisible(do_use_max_size)\n        self.max_size_for_connection_label.setVisible(do_use_max_size)\n        do_use_min_size = do_distant_shape_int and self.use_min_size.isChecked()\n        self.min_size_for_connection.setVisible(do_use_min_size)\n        self.min_size_for_connection_label.setVisible(do_use_min_size)\n\n    def use_max_size_changed(self):\n        \"\"\"\n        Toggles the visibility of max size input fields based on checkbox state.\n        \"\"\"\n        do_use_max_size = self.use_max_size.isChecked()\n        self.max_size_for_connection.setVisible(do_use_max_size)\n        self.max_size_for_connection_label.setVisible(do_use_max_size)\n        if do_use_max_size:\n            self.max_size_for_connection.setValue(300)\n\n    def use_min_size_changed(self):\n        \"\"\"\n        Updates the visibility and value of UI elements based on whether a checkbox is checked.\n        \"\"\"\n        do_use_min_size = self.use_min_size.isChecked()\n        self.min_size_for_connection.setVisible(do_use_min_size)\n        self.min_size_for_connection_label.setVisible(do_use_min_size)\n        if do_use_min_size:\n            self.min_size_for_connection.setValue(30)\n\n    def generate_csc_editing(self):\n        \"\"\"\n        Generate CSC Editing Layout\n\n        Creates and configures the layout for Color Space Combination (CSC) editing in the video analysis window,\n        initializing widgets and connecting signals to slots for dynamic UI handling.\n        \"\"\"\n        self.edit_widget = QtWidgets.QWidget()\n        self.edit_layout = QtWidgets.QVBoxLayout()\n\n        # 2) Titles\n        self.video_csc_label = FixedText(AP[\"Csc_for_video_analysis\"][\"label\"] + ':',\n                                         tip=AP[\"Csc_for_video_analysis\"][\"tips\"],\n                                         night_mode=self.parent().po.all['night_mode'])\n        self.video_csc_label.setFixedHeight(30)\n        self.edit_layout.addWidget(self.video_csc_label)\n        self.both_csc_widget = QtWidgets.QWidget()\n        self.both_csc_layout = QtWidgets.QHBoxLayout()\n\n        # 3) First CSC\n        self.first_csc_widget = QtWidgets.QWidget()\n        self.first_csc_layout = QtWidgets.QGridLayout()\n        self.row1 = self.one_csc_editing()\n        self.row1[4].clicked.connect(self.display_row2)\n        self.row2 = self.one_csc_editing()\n        self.row2[4].clicked.connect(self.display_row3)\n        self.row3 = self.one_csc_editing()# Second CSC\n        self.logical_operator_between_combination_result = Combobox([\"None\", \"Or\", \"And\", \"Xor\"],\n                                                                    night_mode=self.parent().po.all['night_mode'])\n        self.logical_operator_between_combination_result.setCurrentText(self.parent().po.vars['convert_for_motion']['logical'])\n        self.logical_operator_between_combination_result.currentTextChanged.connect(self.logical_op_changed)\n        self.logical_operator_between_combination_result.setFixedWidth(100)\n        self.logical_operator_label = FixedText(IAW[\"Logical_operator\"][\"label\"], halign='c', tip=IAW[\"Logical_operator\"][\"tips\"],\n                                                night_mode=self.parent().po.all['night_mode'])\n        self.row21 = self.one_csc_editing()\n        self.row21[4].clicked.connect(self.display_row22)\n        self.row22 = self.one_csc_editing()\n        self.row22[4].clicked.connect(self.display_row23)\n        self.row23 = self.one_csc_editing()\n        for i in range(5):\n            self.first_csc_layout.addWidget(self.row1[i], 0, i, 1, 1)\n            self.first_csc_layout.addWidget(self.row2[i], 1, i, 1, 1)\n            self.first_csc_layout.addWidget(self.row3[i], 2, i, 1, 1)\n            self.row1[i].setVisible(False)\n            self.row2[i].setVisible(False)\n            self.row3[i].setVisible(False)\n        self.first_csc_layout.setHorizontalSpacing(0)\n        self.first_csc_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum), 0, 5, 3, 1)\n        self.first_csc_widget.setLayout(self.first_csc_layout)\n        self.both_csc_layout.addWidget(self.first_csc_widget)\n\n        # 5) Second CSC\n        self.second_csc_widget = QtWidgets.QWidget()\n        self.second_csc_layout = QtWidgets.QGridLayout()\n        for i in range(5):\n            self.second_csc_layout.addWidget(self.row21[i], 0, i, 1, 1)\n            self.second_csc_layout.addWidget(self.row22[i], 1, i, 1, 1)\n            self.second_csc_layout.addWidget(self.row23[i], 2, i, 1, 1)\n            self.row21[i].setVisible(False)\n            self.row22[i].setVisible(False)\n            self.row23[i].setVisible(False)\n        self.second_csc_layout.setHorizontalSpacing(0)\n        self.second_csc_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum), 0, 5, 3, 1)\n        self.second_csc_widget.setLayout(self.second_csc_layout)\n        self.both_csc_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n        self.both_csc_layout.addWidget(self.second_csc_widget)\n        self.both_csc_widget.setLayout(self.both_csc_layout)\n        self.edit_layout.addWidget(self.both_csc_widget)\n\n        # 4) logical_operator\n        self.logical_op_widget = QtWidgets.QWidget()\n        self.logical_op_widget.setFixedHeight(30)\n        self.logical_op_layout = QtWidgets.QHBoxLayout()\n        self.logical_op_layout.addWidget(self.logical_operator_label)\n        self.logical_op_layout.addWidget(self.logical_operator_between_combination_result)\n        self.logical_op_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n        self.logical_operator_between_combination_result.setVisible(False)\n        self.logical_operator_label.setVisible(False)\n        self.logical_op_widget.setLayout(self.logical_op_layout)\n        self.logical_op_widget.setFixedHeight(50)\n        self.edit_layout.addWidget(self.logical_op_widget)\n\n        # 6) Open the more_than_2_colors row layout\n        self.more_than_2_colors_widget = QtWidgets.QWidget()\n        self.more_than_2_colors_layout = QtWidgets.QHBoxLayout()\n        self.more_than_two_colors = Checkbox(self.parent().po.all[\"more_than_two_colors\"])\n        self.more_than_two_colors.setStyleSheet(\"QCheckBox::indicator {width: 12px;height: 12px;background-color: transparent;\"\n                            \"border-radius: 5px;border-style: solid;border-width: 1px;\"\n                            \"border-color: rgb(100,100,100);}\"\n                            \"QCheckBox::indicator:checked {background-color: rgb(70,130,180);}\"\n                            \"QCheckBox:checked, QCheckBox::indicator:checked {border-color: black black white white;}\"\n                            \"QCheckBox:checked {background-color: transparent;}\"\n                            \"QCheckBox:margin-left {0%}\"\n                            \"QCheckBox:margin-right {-10%}\")\n        self.more_than_two_colors.stateChanged.connect(self.display_more_than_two_colors_option)\n\n        self.more_than_two_colors_label = FixedText(IAW[\"Kmeans\"][\"label\"],\n                                                    tip=IAW[\"Kmeans\"][\"tips\"], night_mode=self.parent().po.all['night_mode'])\n        self.more_than_two_colors_label.setSizePolicy(QtWidgets.QSizePolicy.Maximum, QtWidgets.QSizePolicy.Maximum)\n        self.more_than_two_colors_label.setAlignment(QtCore.Qt.AlignLeft)\n        self.distinct_colors_number = Spinbox(min=2, max=5, val=self.parent().po.vars[\"color_number\"], night_mode=self.parent().po.all['night_mode'])\n        self.more_than_2_colors_layout.addWidget(self.more_than_two_colors)\n        self.more_than_2_colors_layout.addWidget(self.more_than_two_colors_label)\n        self.more_than_2_colors_layout.addWidget(self.distinct_colors_number)\n        self.more_than_2_colors_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n        self.more_than_2_colors_widget.setLayout(self.more_than_2_colors_layout)\n        self.more_than_2_colors_widget.setFixedHeight(50)\n        self.edit_layout.addWidget(self.more_than_2_colors_widget)\n        self.edit_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.Maximum, QtWidgets.QSizePolicy.MinimumExpanding))\n        self.edit_widget.setLayout(self.edit_layout)\n\n    def one_csc_editing(self):\n        \"\"\"\n        Creates a list of widgets for color space editing.\n\n        Returns\n        -------\n        widget_list : List[QtWidgets.QWidget]\n            A list containing a Combobox, three Spinboxes, and a PButton.\n\n        Notes\n        -----\n        The Combobox widget allows selection from predefined color spaces,\n        the Spinboxes are for editing numerical values, and the PButton is\n        for adding new entries.\n        \"\"\"\n        widget_list = []\n        widget_list.insert(0, Combobox([\"None\", \"bgr\", \"hsv\", \"hls\", \"lab\", \"luv\", \"yuv\"],\n                                       night_mode=self.parent().po.all['night_mode']))\n        widget_list[0].setSizePolicy(QtWidgets.QSizePolicy.Maximum, QtWidgets.QSizePolicy.Maximum)\n        widget_list[0].setFixedWidth(100)\n        for i in [1, 2, 3]:\n            widget_list.insert(i, Spinbox(min=-126, max=126, val=0, night_mode=self.parent().po.all['night_mode']))\n            widget_list[i].setFixedWidth(45)\n        widget_list.insert(i + 1, PButton(\"+\", night_mode=self.parent().po.all['night_mode']))\n\n        return widget_list\n\n    def logical_op_changed(self):\n        \"\"\"\n        Update the visibility and values of UI components based on the logical operator selection.\n        \"\"\"\n        # show = self.logical_operator_between_combination_result.currentText() != 'None'\n        if self.logical_operator_between_combination_result.currentText() == 'None':\n            self.row21[0].setVisible(False)\n            self.row21[0].setCurrentIndex(0)\n            for i1 in [1, 2, 3]:\n                self.row21[i1].setVisible(False)\n                self.row21[i1].setValue(0)\n            self.row21[i1 + 1].setVisible(False)\n\n            self.row22[0].setVisible(False)\n            self.row22[0].setCurrentIndex(0)\n            for i1 in [1, 2, 3]:\n                self.row22[i1].setVisible(False)\n                self.row22[i1].setValue(0)\n            self.row22[i1 + 1].setVisible(False)\n\n            self.row23[0].setVisible(False)\n            self.row23[0].setCurrentIndex(0)\n            for i1 in [1, 2, 3]:\n                self.row23[i1].setVisible(False)\n                self.row23[i1].setValue(0)\n            self.row23[i1 + 1].setVisible(False)\n        else:\n            self.row21[0].setVisible(True)\n            for i1 in [1, 2, 3]:\n                self.row21[i1].setVisible(True)\n            self.row21[i1 + 1].setVisible(True)\n\n    def display_logical_operator(self):\n        \"\"\"\n        Display logical operator components in the user interface.\n        \"\"\"\n        self.logical_operator_between_combination_result.setVisible(True)\n        self.logical_operator_label.setVisible(True)\n\n    def display_row2(self):\n        \"\"\"\n        Display or hide the second row of the csc editing widgets.\n        \"\"\"\n        self.row1[4].setVisible(False)\n        for i in range(5):\n            self.row2[i].setVisible(True)\n        self.display_logical_operator()\n\n    def display_row3(self):\n        \"\"\"\n        Display or hide the third row of the csc editing widgets.\n        \"\"\"\n        self.row2[4].setVisible(False)\n        for i in range(4):\n            self.row3[i].setVisible(True)\n        self.display_logical_operator()\n\n    def display_row22(self):\n        \"\"\"\n        Display or hide the second row (for the second image segmentation pipeline) of the csc editing widgets.\n        \"\"\"\n        self.row21[4].setVisible(False)\n        for i in range(5):\n            self.row22[i].setVisible(True)\n        self.display_logical_operator()\n\n    def display_row23(self):\n        \"\"\"\n        Display or hide the third row (for the second image segmentation pipeline) of the csc editing widgets.\n        \"\"\"\n        self.row22[4].setVisible(False)\n        for i in range(4):\n            self.row23[i].setVisible(True)\n        self.display_logical_operator()\n\n    def update_csc_editing_display(self):\n        \"\"\"\n        Update the color space conversion (CSC) editing display.\n\n        This method updates the visibility and values of UI elements related to color\n        space conversions based on the current state of `self.csc_dict`. It handles\n        the display logic for different color spaces and their combinations, ensuring\n        that the UI reflects the current configuration accurately.\n        \"\"\"\n        c_space_order = [\"None\", \"bgr\", \"hsv\", \"hls\", \"lab\", \"luv\", \"yuv\"]\n        remaining_c_spaces = []\n        row_number1 = 0\n        row_number2 = 0\n        if \"PCA\" in self.parent().po.vars['convert_for_motion'].keys():\n            self.row1[0].setCurrentIndex(0)\n            self.row1[0].setVisible(True)\n            for i in range(1, 4):\n                self.row1[i].setVisible(False)\n        else:\n            for i, (k, v) in enumerate(self.parent().po.vars['convert_for_motion'].items()):\n                if k != \"logical\":\n                    if k[-1] != \"2\":\n                        if row_number1 == 0:\n                            row_to_change = self.row1\n                        elif row_number1 == 1:\n                            row_to_change = self.row2\n                        elif row_number1 == 2:\n                            row_to_change = self.row3\n                        else:\n                            remaining_c_spaces.append(k + \" \" + str(v))\n                        row_number1 += 1\n                        current_row_number = row_number1\n                    else:\n                        if row_number2 == 0:\n                            row_to_change = self.row21\n                        elif row_number2 == 1:\n                            row_to_change = self.row22\n                        elif row_number2 == 2:\n                            row_to_change = self.row23\n                        else:\n                            remaining_c_spaces.append(k + \" \" + str(v))\n                        row_number2 += 1\n                        current_row_number = row_number2\n                        k = k[:-1]\n                    if current_row_number &lt;= 3:\n                        row_to_change[0].setCurrentIndex(np.nonzero(np.isin(c_space_order, k))[0][0])\n                        row_to_change[0].setVisible(True)\n                        for i1, i2 in zip([1, 2, 3], [0, 1, 2]):\n                            row_to_change[i1].setValue(v[i2])\n                            row_to_change[i1].setVisible(True)\n                        if current_row_number &lt; 3:\n                            row_to_change[i1 + 1].setVisible(True)\n\n        # If not all color space combinations are filled, put None and 0 in boxes\n        if row_number1 &lt; 3:\n            self.row3[0].setVisible(False)\n            self.row3[0].setCurrentIndex(0)\n            for i1 in [1, 2, 3]:\n                self.row3[i1].setVisible(False)\n                self.row3[i1].setValue(0)\n            if row_number1 &lt; 2:\n                self.row2[0].setVisible(False)\n                self.row2[0].setCurrentIndex(0)\n                for i1 in [1, 2, 3]:\n                    self.row2[i1].setVisible(False)\n                    self.row2[i1].setValue(0)\n                self.row2[i1 + 1].setVisible(False)\n\n        self.row1[4].setVisible(row_number1 == 1)\n        self.row2[4].setVisible(row_number1 == 2)\n        self.row21[4].setVisible(row_number2 == 1)\n        self.row22[4].setVisible(row_number2 == 2)\n        if row_number2 &gt; 0:\n            self.logical_operator_between_combination_result.setCurrentText(self.parent().po.vars['convert_for_motion']['logical'])\n        if row_number2 == 0:\n            self.logical_operator_between_combination_result.setCurrentText('None')\n            self.logical_operator_between_combination_result.setVisible(False)\n            self.logical_operator_label.setVisible(False)\n            self.row21[0].setVisible(False)\n            self.row21[0].setCurrentIndex(0)\n            for i1 in [1, 2, 3]:\n                self.row21[i1].setVisible(False)\n                self.row21[i1].setValue(0)\n            self.row21[i1 + 1].setVisible(False)\n\n        self.logical_operator_between_combination_result.setVisible((row_number2 &gt; 0))\n        self.logical_operator_label.setVisible((row_number2 &gt; 0))\n\n        if row_number2 &lt; 3:\n            self.row23[0].setVisible(False)\n            self.row23[0].setCurrentIndex(0)\n            for i1 in [1, 2, 3]:\n                self.row23[i1].setVisible(False)\n                self.row23[i1].setValue(0)\n            self.row23[i1 + 1].setVisible(False)\n            self.row22[4].setVisible(False)\n            if row_number2 &lt; 2:\n                self.row22[0].setVisible(False)\n                self.row22[0].setCurrentIndex(0)\n                for i1 in [1, 2, 3]:\n                    self.row22[i1].setVisible(False)\n                    self.row22[i1].setValue(0)\n                self.row22[i1 + 1].setVisible(False)\n\n    def save_user_defined_csc(self):\n        \"\"\"\n        Save user-defined combination of color spaces and channels.\n        \"\"\"\n        self.parent().po.vars['convert_for_motion'] = {}\n        spaces = np.array((self.row1[0].currentText(), self.row2[0].currentText(), self.row3[0].currentText()))\n        channels = np.array(\n            ((self.row1[1].value(), self.row1[2].value(), self.row1[3].value()),\n             (self.row2[1].value(), self.row2[2].value(), self.row2[3].value()),\n             (self.row3[1].value(), self.row3[2].value(), self.row3[3].value()),\n             (self.row21[1].value(), self.row21[2].value(), self.row21[3].value()),\n             (self.row22[1].value(), self.row22[2].value(), self.row22[3].value()),\n             (self.row23[1].value(), self.row23[2].value(), self.row23[3].value())),\n            dtype=np.int8)\n        if self.logical_operator_between_combination_result.currentText() != 'None':\n            spaces = np.concatenate((spaces, np.array((\n                        self.row21[0].currentText() + \"2\", self.row22[0].currentText() + \"2\",\n                        self.row23[0].currentText() + \"2\"))))\n            channels = np.concatenate((channels, np.array(((self.row21[1].value(), self.row21[2].value(), self.row21[3].value()),\n             (self.row22[1].value(), self.row22[2].value(), self.row22[3].value()),\n             (self.row23[1].value(), self.row23[2].value(), self.row23[3].value())),\n             dtype=np.int8)))\n            self.parent().po.vars['convert_for_motion']['logical'] = self.logical_operator_between_combination_result.currentText()\n        else:\n            self.parent().po.vars['convert_for_motion']['logical'] = 'None'\n        if not np.all(spaces == \"None\"):\n            for i, space in enumerate(spaces):\n                if space != \"None\" and space != \"None2\":\n                    if np.any(channels[i, :] &lt; 0.):\n                        channels[i, :] + channels[i, :].min()\n                    self.parent().po.vars['convert_for_motion'][space] = channels[i, :]\n        if len(self.parent().po.vars['convert_for_motion']) == 1 or channels.sum() == 0:\n            self.csc_dict_is_empty = True\n        else:\n            self.csc_dict_is_empty = False\n\n        self.parent().po.all[\"more_than_two_colors\"] = self.more_than_two_colors.isChecked()\n        if self.more_than_two_colors.isChecked():\n            self.parent().po.vars[\"color_number\"] = int(self.distinct_colors_number.value())\n            self.parent().videoanalysiswindow.select_option.setVisible(True)\n            self.parent().videoanalysiswindow.select_option_label.setVisible(True)\n\n    def display_more_than_two_colors_option(self):\n        \"\"\"\n        Display the More Than Two Colors Options\n\n        This method manages the visibility and state of UI elements related to selecting\n        more than two colors for displaying biological masks in advanced mode.\n        \"\"\"\n        if self.more_than_two_colors.isChecked():\n            self.distinct_colors_number.setVisible(True)\n            self.more_than_two_colors_label.setText(\"How many distinct colors?\")\n            self.distinct_colors_number.setValue(3)\n        else:\n            self.more_than_two_colors_label.setText(\"Heterogeneous background\")\n            self.distinct_colors_number.setVisible(False)\n            self.distinct_colors_number.setValue(2)\n\n    def night_mode_is_clicked(self):\n        \"\"\" Triggered when night_mode_cb check status changes\"\"\"\n        self.parent().po.all['night_mode'] = self.night_mode_cb.isChecked()\n        self.message.setText('Close and restart Cellects to apply night or light mode')\n        self.message.setStyleSheet(\"color: rgb(230, 145, 18)\")\n\n    def reset_all_settings_is_clicked(self):\n        \"\"\"\n        Reset All Settings on Click\n\n        Resets the application settings to their default state by removing specific pickle files and saving new default dictionaries.\n\n        Notes\n        -----\n        - This function removes specific pickle files to reset settings.\n        - The function changes the current working directory temporarily.\n        \"\"\"\n        if os.path.isfile('Data to run Cellects quickly.pkl'):\n            os.remove('Data to run Cellects quickly.pkl')\n        if os.path.isfile('PickleRick.pkl'):\n            os.remove('PickleRick.pkl')\n        if os.path.isfile('PickleRick0.pkl'):\n            os.remove('PickleRick0.pkl')\n        if os.path.isfile(Path(CELLECTS_DIR.parent / 'PickleRick.pkl')):\n            os.remove(Path(CELLECTS_DIR.parent / 'PickleRick.pkl'))\n        if os.path.isfile(Path(CELLECTS_DIR.parent / 'PickleRick0.pkl')):\n            os.remove(Path(CELLECTS_DIR.parent / 'PickleRick0.pkl'))\n        if os.path.isfile(Path(CONFIG_DIR / 'PickleRick1.pkl')):\n            os.remove(Path(CONFIG_DIR / 'PickleRick1.pkl'))\n        current_dir = os.getcwd()\n        os.chdir(CONFIG_DIR)\n        DefaultDicts().save_as_pkl(self.parent().po)\n        os.chdir(current_dir)\n        self.message.setText('Close and restart Cellects to apply the settings reset')\n        self.message.setStyleSheet(\"color: rgb(230, 145, 18)\")\n\n    def cancel_is_clicked(self):\n        \"\"\"\n        Instead of saving the widgets values to the saved states, use the saved states to fill in the widgets.\n\n        This function updates the state of several checkboxes based on saved variables\n        and descriptors. It also changes the active widget to either the first or third\n        widget depending on a condition.\n        \"\"\"\n        self.automatically_crop.setChecked(self.parent().po.all['automatically_crop'])\n        self.subtract_background.setChecked(self.parent().po.vars['subtract_background'])\n        self.keep_cell_and_back_for_all_folders.setChecked(self.parent().po.all['keep_cell_and_back_for_all_folders'])\n        self.correct_errors_around_initial.setChecked(self.parent().po.vars['correct_errors_around_initial'])\n        self.prevent_fast_growth_near_periphery.setChecked(self.parent().po.vars['prevent_fast_growth_near_periphery'])\n        self.periphery_width.setValue(self.parent().po.vars['periphery_width'])\n        self.max_periphery_growth.setValue(self.parent().po.vars['max_periphery_growth'])\n\n\n        self.first_move_threshold.setValue(self.parent().po.all['first_move_threshold_in_mm\u00b2'])\n        self.pixels_to_mm.setChecked(self.parent().po.vars['output_in_mm'])\n        self.do_automatic_size_thresholding.setChecked(self.parent().po.all['automatic_size_thresholding'])\n        self.appearing_selection.setCurrentText(self.parent().po.vars['appearance_detection_method'])\n        self.oscillation_period.setValue(self.parent().po.vars['expected_oscillation_period'])\n        self.minimal_oscillating_cluster_size.setValue(self.parent().po.vars['minimal_oscillating_cluster_size'])\n\n        self.do_multiprocessing.setChecked(self.parent().po.all['do_multiprocessing'])\n        self.max_core_nb.setValue(self.parent().po.all['cores'])\n        self.min_memory_left.setValue(self.parent().po.vars['min_ram_free'])\n        self.lose_accuracy_to_save_memory.setChecked(self.parent().po.vars['lose_accuracy_to_save_memory'])\n        self.video_fps.setValue(self.parent().po.vars['video_fps'])\n        self.keep_unaltered_videos.setChecked(self.parent().po.vars['keep_unaltered_videos'])\n        self.save_processed_videos.setChecked(self.parent().po.vars['save_processed_videos'])\n        self.time_step.setValue(self.parent().po.vars['time_step'])\n        # self.parent().po.all['overwrite_cellects_data'] = self.overwrite_cellects_data.isChecked()\n\n        self.connect_distant_shape_during_segmentation.setChecked(self.parent().po.all['connect_distant_shape_during_segmentation'])\n        do_use_max_size = self.parent().po.vars['max_size_for_connection'] is not None and self.parent().po.all['connect_distant_shape_during_segmentation']\n        do_use_min_size = self.parent().po.vars['min_size_for_connection'] is not None and self.parent().po.all['connect_distant_shape_during_segmentation']\n        self.use_max_size.setChecked(do_use_max_size)\n        self.use_min_size.setChecked(do_use_min_size)\n        if do_use_max_size:\n            self.max_size_for_connection.setValue(self.parent().po.vars['max_size_for_connection'])\n        else:\n            self.max_size_for_connection.setValue(50)\n        if do_use_min_size:\n            self.min_size_for_connection.setValue(self.parent().po.vars['min_size_for_connection'])\n        else:\n            self.min_size_for_connection.setValue(0)\n\n        self.detection_range_factor.setValue(self.parent().po.vars['detection_range_factor'])\n        self.all_specimens_have_same_direction.setChecked(self.parent().po.all['all_specimens_have_same_direction'])\n        self.update_csc_editing_display()\n\n        if self.parent().last_is_first:\n            self.parent().change_widget(0) # FirstWidget\n        else:\n            self.parent().change_widget(3) # ImageAnalysisWindow ThirdWidget\n\n    def ok_is_clicked(self):\n        \"\"\"\n        Updates the parent object's processing options with the current state of various UI elements.\n\n        Summary\n        -------\n        Saves the current state of UI components to the parent object's processing options dictionary.\n\n        Extended Description\n        --------------------\n        This method iterates through various UI components such as checkboxes, sliders,\n        and dropdowns to save their current state into the parent object's processing\n        options variables. This allows the software to retain user preferences across\n        sessions and ensures that all settings are correctly applied before processing.\n        \"\"\"\n        self.parent().po.all['automatically_crop'] = self.automatically_crop.isChecked()\n        self.parent().po.vars['subtract_background'] = self.subtract_background.isChecked()\n        self.parent().po.all['keep_cell_and_back_for_all_folders'] = self.keep_cell_and_back_for_all_folders.isChecked()\n        self.parent().po.vars['correct_errors_around_initial'] = self.correct_errors_around_initial.isChecked()\n        self.parent().po.vars['prevent_fast_growth_near_periphery'] = self.prevent_fast_growth_near_periphery.isChecked()\n        self.parent().po.vars['periphery_width'] = int(self.periphery_width.value())\n        self.parent().po.vars['max_periphery_growth'] = int(self.max_periphery_growth.value())\n\n        self.parent().po.all['first_move_threshold_in_mm\u00b2'] = self.first_move_threshold.value()\n        self.parent().po.vars['output_in_mm'] = self.pixels_to_mm.isChecked()\n        self.parent().po.all['automatic_size_thresholding'] = self.do_automatic_size_thresholding.isChecked()\n        self.parent().po.vars['appearance_detection_method'] = self.appearing_selection.currentText()\n\n        self.parent().po.all['auto_mesh_step_length'] = self.mesh_step_length_cb.isChecked()\n        if self.parent().po.all['auto_mesh_step_length']:\n            self.parent().po.vars['rolling_window_segmentation']['step'] = None\n        else:\n            self.parent().po.vars['rolling_window_segmentation']['step'] = int(self.mesh_step_length.value())\n\n        self.parent().po.all['auto_mesh_side_length'] = self.mesh_side_length_cb.isChecked()\n        if self.parent().po.all['auto_mesh_side_length']:\n            self.parent().po.vars['rolling_window_segmentation']['side_len'] = None\n        else:\n            self.parent().po.vars['rolling_window_segmentation']['side_len'] = int(self.mesh_side_length.value())\n\n        self.parent().po.all['auto_mesh_min_int_var'] = self.mesh_min_int_var_cb.isChecked()\n        if self.parent().po.all['auto_mesh_min_int_var']:\n            self.parent().po.vars['rolling_window_segmentation']['min_int_var'] = None\n        else:\n            self.parent().po.vars['rolling_window_segmentation']['min_int_var'] = int(self.mesh_min_int_var.value())\n\n        self.parent().po.vars['expected_oscillation_period'] = self.oscillation_period.value()\n        self.parent().po.vars['minimal_oscillating_cluster_size'] = int(self.minimal_oscillating_cluster_size.value())\n\n        self.parent().po.all['do_multiprocessing'] = self.do_multiprocessing.isChecked()\n        self.parent().po.all['cores'] = np.uint8(self.max_core_nb.value())\n        self.parent().po.vars['min_ram_free'] = self.min_memory_left.value()\n        self.parent().po.vars['lose_accuracy_to_save_memory'] = self.lose_accuracy_to_save_memory.isChecked()\n        self.parent().po.vars['video_fps'] = float(self.video_fps.value())\n        self.parent().po.vars['keep_unaltered_videos'] = self.keep_unaltered_videos.isChecked()\n        self.parent().po.vars['save_processed_videos'] = self.save_processed_videos.isChecked()\n        self.parent().po.all['extract_time_interval'] = self.extract_time.isChecked()\n        self.parent().po.vars['time_step'] = float(self.time_step.value())\n\n        do_distant_shape_int = self.connect_distant_shape_during_segmentation.isChecked()\n        self.parent().po.all['connect_distant_shape_during_segmentation'] = do_distant_shape_int\n        if do_distant_shape_int:\n            self.parent().po.vars['detection_range_factor'] = int(\n                np.round(self.detection_range_factor.value()))\n        else:\n            self.parent().po.vars['detection_range_factor'] = 0\n        if self.use_max_size.isChecked():\n            self.parent().po.vars['max_size_for_connection'] = int(np.round(self.max_size_for_connection.value()))\n        else:\n            self.parent().po.vars['max_size_for_connection'] = None\n        if self.use_min_size.isChecked():\n            self.parent().po.vars['min_size_for_connection'] = int(np.round(self.min_size_for_connection.value()))\n        else:\n            self.parent().po.vars['min_size_for_connection'] = None\n\n        self.parent().po.all['all_specimens_have_same_direction'] = self.all_specimens_have_same_direction.isChecked()\n\n        previous_csc = deepcopy(self.parent().po.vars['convert_for_motion'])\n        self.save_user_defined_csc()\n        if self.parent().po.first_exp_ready_to_run:\n            are_dicts_equal: bool = True\n            for key in previous_csc.keys():\n                if key != 'logical':\n                    are_dicts_equal = are_dicts_equal and np.all(key in self.parent().po.vars['convert_for_motion'] and previous_csc[key] == self.parent().po.vars['convert_for_motion'][key])\n            for key in self.parent().po.vars['convert_for_motion'].keys():\n                if key != 'logical':\n                    are_dicts_equal = are_dicts_equal and np.all(\n                        key in previous_csc and self.parent().po.vars['convert_for_motion'][key] ==\n                        previous_csc[key])\n            if not are_dicts_equal:\n                self.parent().po.find_if_lighter_background()\n\n        if not self.parent().thread_dict['SaveAllVars'].isRunning():\n            self.parent().thread_dict['SaveAllVars'].start()\n\n        if self.parent().last_is_first:\n            self.parent().change_widget(0) # FirstWidget\n        else:\n            self.parent().change_widget(3) # ImageAnalysisWindow ThirdWidget\n</code></pre>"},{"location":"api/cellects/gui/advanced_parameters/#cellects.gui.advanced_parameters.AdvancedParameters.__init__","title":"<code>__init__(parent, night_mode)</code>","text":"<p>Initialize the AdvancedParameters window with a parent widget and night mode setting.</p> <p>Parameters:</p> Name Type Description Default <code>parent</code> <code>QWidget</code> <p>The parent widget to which this window will be attached.</p> required <code>night_mode</code> <code>bool</code> <p>A boolean indicating whether the night mode should be enabled.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from PySide6 import QtWidgets\n&gt;&gt;&gt; from cellects.gui.cellects import CellectsMainWidget\n&gt;&gt;&gt; from cellects.gui.advanced_parameters import AdvancedParameters\n&gt;&gt;&gt; import sys\n&gt;&gt;&gt; app = QtWidgets.QApplication([])\n&gt;&gt;&gt; parent = CellectsMainWidget()\n&gt;&gt;&gt; session = AdvancedParameters(parent, False)\n&gt;&gt;&gt; session.true_init()\n&gt;&gt;&gt; parent.insertWidget(0, session)\n&gt;&gt;&gt; parent.show()\n&gt;&gt;&gt; sys.exit(app.exec())\n</code></pre> Source code in <code>src/cellects/gui/advanced_parameters.py</code> <pre><code>def __init__(self, parent, night_mode):\n    \"\"\"\n    Initialize the AdvancedParameters window with a parent widget and night mode setting.\n\n    Parameters\n    ----------\n    parent : QWidget\n        The parent widget to which this window will be attached.\n    night_mode : bool\n        A boolean indicating whether the night mode should be enabled.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from PySide6 import QtWidgets\n    &gt;&gt;&gt; from cellects.gui.cellects import CellectsMainWidget\n    &gt;&gt;&gt; from cellects.gui.advanced_parameters import AdvancedParameters\n    &gt;&gt;&gt; import sys\n    &gt;&gt;&gt; app = QtWidgets.QApplication([])\n    &gt;&gt;&gt; parent = CellectsMainWidget()\n    &gt;&gt;&gt; session = AdvancedParameters(parent, False)\n    &gt;&gt;&gt; session.true_init()\n    &gt;&gt;&gt; parent.insertWidget(0, session)\n    &gt;&gt;&gt; parent.show()\n    &gt;&gt;&gt; sys.exit(app.exec())\n    \"\"\"\n    super().__init__(parent, night_mode)\n\n    self.setParent(parent)\n    try:\n        self.true_init()\n    except KeyError:\n        default_dicts = DefaultDicts()\n        self.parent().po.all = default_dicts.all\n        self.parent().po.vars = default_dicts.vars\n        self.true_init()\n</code></pre>"},{"location":"api/cellects/gui/advanced_parameters/#cellects.gui.advanced_parameters.AdvancedParameters.cancel_is_clicked","title":"<code>cancel_is_clicked()</code>","text":"<p>Instead of saving the widgets values to the saved states, use the saved states to fill in the widgets.</p> <p>This function updates the state of several checkboxes based on saved variables and descriptors. It also changes the active widget to either the first or third widget depending on a condition.</p> Source code in <code>src/cellects/gui/advanced_parameters.py</code> <pre><code>def cancel_is_clicked(self):\n    \"\"\"\n    Instead of saving the widgets values to the saved states, use the saved states to fill in the widgets.\n\n    This function updates the state of several checkboxes based on saved variables\n    and descriptors. It also changes the active widget to either the first or third\n    widget depending on a condition.\n    \"\"\"\n    self.automatically_crop.setChecked(self.parent().po.all['automatically_crop'])\n    self.subtract_background.setChecked(self.parent().po.vars['subtract_background'])\n    self.keep_cell_and_back_for_all_folders.setChecked(self.parent().po.all['keep_cell_and_back_for_all_folders'])\n    self.correct_errors_around_initial.setChecked(self.parent().po.vars['correct_errors_around_initial'])\n    self.prevent_fast_growth_near_periphery.setChecked(self.parent().po.vars['prevent_fast_growth_near_periphery'])\n    self.periphery_width.setValue(self.parent().po.vars['periphery_width'])\n    self.max_periphery_growth.setValue(self.parent().po.vars['max_periphery_growth'])\n\n\n    self.first_move_threshold.setValue(self.parent().po.all['first_move_threshold_in_mm\u00b2'])\n    self.pixels_to_mm.setChecked(self.parent().po.vars['output_in_mm'])\n    self.do_automatic_size_thresholding.setChecked(self.parent().po.all['automatic_size_thresholding'])\n    self.appearing_selection.setCurrentText(self.parent().po.vars['appearance_detection_method'])\n    self.oscillation_period.setValue(self.parent().po.vars['expected_oscillation_period'])\n    self.minimal_oscillating_cluster_size.setValue(self.parent().po.vars['minimal_oscillating_cluster_size'])\n\n    self.do_multiprocessing.setChecked(self.parent().po.all['do_multiprocessing'])\n    self.max_core_nb.setValue(self.parent().po.all['cores'])\n    self.min_memory_left.setValue(self.parent().po.vars['min_ram_free'])\n    self.lose_accuracy_to_save_memory.setChecked(self.parent().po.vars['lose_accuracy_to_save_memory'])\n    self.video_fps.setValue(self.parent().po.vars['video_fps'])\n    self.keep_unaltered_videos.setChecked(self.parent().po.vars['keep_unaltered_videos'])\n    self.save_processed_videos.setChecked(self.parent().po.vars['save_processed_videos'])\n    self.time_step.setValue(self.parent().po.vars['time_step'])\n    # self.parent().po.all['overwrite_cellects_data'] = self.overwrite_cellects_data.isChecked()\n\n    self.connect_distant_shape_during_segmentation.setChecked(self.parent().po.all['connect_distant_shape_during_segmentation'])\n    do_use_max_size = self.parent().po.vars['max_size_for_connection'] is not None and self.parent().po.all['connect_distant_shape_during_segmentation']\n    do_use_min_size = self.parent().po.vars['min_size_for_connection'] is not None and self.parent().po.all['connect_distant_shape_during_segmentation']\n    self.use_max_size.setChecked(do_use_max_size)\n    self.use_min_size.setChecked(do_use_min_size)\n    if do_use_max_size:\n        self.max_size_for_connection.setValue(self.parent().po.vars['max_size_for_connection'])\n    else:\n        self.max_size_for_connection.setValue(50)\n    if do_use_min_size:\n        self.min_size_for_connection.setValue(self.parent().po.vars['min_size_for_connection'])\n    else:\n        self.min_size_for_connection.setValue(0)\n\n    self.detection_range_factor.setValue(self.parent().po.vars['detection_range_factor'])\n    self.all_specimens_have_same_direction.setChecked(self.parent().po.all['all_specimens_have_same_direction'])\n    self.update_csc_editing_display()\n\n    if self.parent().last_is_first:\n        self.parent().change_widget(0) # FirstWidget\n    else:\n        self.parent().change_widget(3) # ImageAnalysisWindow ThirdWidget\n</code></pre>"},{"location":"api/cellects/gui/advanced_parameters/#cellects.gui.advanced_parameters.AdvancedParameters.display_conditionally_visible_widgets","title":"<code>display_conditionally_visible_widgets()</code>","text":"<p>Conditionally displays widgets based on various settings within the parent object.</p> <p>This function controls the visibility of several UI elements based on the values in the parent object's <code>all</code> dictionary and <code>vars</code> dictionary. It ensures that only relevant widgets are shown to the user, depending on the current settings.</p> Source code in <code>src/cellects/gui/advanced_parameters.py</code> <pre><code>def display_conditionally_visible_widgets(self):\n    \"\"\"\n    Conditionally displays widgets based on various settings within the parent object.\n\n    This function controls the visibility of several UI elements based on the\n    values in the parent object's `all` dictionary and `vars` dictionary. It ensures\n    that only relevant widgets are shown to the user, depending on the current settings.\n    \"\"\"\n    self.max_core_nb.setVisible(self.parent().po.all['do_multiprocessing'])\n    self.max_core_nb_label.setVisible(self.parent().po.all['do_multiprocessing'])\n    self.first_move_threshold.setVisible(not self.parent().po.all['automatic_size_thresholding'])\n    self.first_move_threshold_label.setVisible(not self.parent().po.all['automatic_size_thresholding'])\n    connect_distant_shape = self.parent().po.all['connect_distant_shape_during_segmentation']\n    do_use_max_size = self.parent().po.vars['max_size_for_connection'] is not None and connect_distant_shape\n    do_use_min_size = self.parent().po.vars['min_size_for_connection'] is not None and connect_distant_shape\n    self.detection_range_factor.setVisible(connect_distant_shape)\n    self.detection_range_factor_label.setVisible(connect_distant_shape)\n    self.use_max_size.setVisible(connect_distant_shape)\n    self.use_min_size.setVisible(connect_distant_shape)\n    self.use_max_size_label.setVisible(connect_distant_shape)\n    self.use_min_size_label.setVisible(connect_distant_shape)\n\n    self.max_size_for_connection.setVisible(do_use_max_size)\n    self.max_size_for_connection_label.setVisible(do_use_max_size)\n    self.min_size_for_connection.setVisible(do_use_min_size)\n    self.min_size_for_connection_label.setVisible(do_use_min_size)\n    self.display_more_than_two_colors_option()\n\n    if self.parent().po.vars['convert_for_motion'] is not None:\n        self.update_csc_editing_display()\n    else:\n        self.row1[0].setCurrentIndex(4)\n        self.row1[3].setValue(1)\n        self.row21[0].setCurrentIndex(0)\n        self.row21[3].setValue(0)\n</code></pre>"},{"location":"api/cellects/gui/advanced_parameters/#cellects.gui.advanced_parameters.AdvancedParameters.display_logical_operator","title":"<code>display_logical_operator()</code>","text":"<p>Display logical operator components in the user interface.</p> Source code in <code>src/cellects/gui/advanced_parameters.py</code> <pre><code>def display_logical_operator(self):\n    \"\"\"\n    Display logical operator components in the user interface.\n    \"\"\"\n    self.logical_operator_between_combination_result.setVisible(True)\n    self.logical_operator_label.setVisible(True)\n</code></pre>"},{"location":"api/cellects/gui/advanced_parameters/#cellects.gui.advanced_parameters.AdvancedParameters.display_more_than_two_colors_option","title":"<code>display_more_than_two_colors_option()</code>","text":"<p>Display the More Than Two Colors Options</p> <p>This method manages the visibility and state of UI elements related to selecting more than two colors for displaying biological masks in advanced mode.</p> Source code in <code>src/cellects/gui/advanced_parameters.py</code> <pre><code>def display_more_than_two_colors_option(self):\n    \"\"\"\n    Display the More Than Two Colors Options\n\n    This method manages the visibility and state of UI elements related to selecting\n    more than two colors for displaying biological masks in advanced mode.\n    \"\"\"\n    if self.more_than_two_colors.isChecked():\n        self.distinct_colors_number.setVisible(True)\n        self.more_than_two_colors_label.setText(\"How many distinct colors?\")\n        self.distinct_colors_number.setValue(3)\n    else:\n        self.more_than_two_colors_label.setText(\"Heterogeneous background\")\n        self.distinct_colors_number.setVisible(False)\n        self.distinct_colors_number.setValue(2)\n</code></pre>"},{"location":"api/cellects/gui/advanced_parameters/#cellects.gui.advanced_parameters.AdvancedParameters.display_row2","title":"<code>display_row2()</code>","text":"<p>Display or hide the second row of the csc editing widgets.</p> Source code in <code>src/cellects/gui/advanced_parameters.py</code> <pre><code>def display_row2(self):\n    \"\"\"\n    Display or hide the second row of the csc editing widgets.\n    \"\"\"\n    self.row1[4].setVisible(False)\n    for i in range(5):\n        self.row2[i].setVisible(True)\n    self.display_logical_operator()\n</code></pre>"},{"location":"api/cellects/gui/advanced_parameters/#cellects.gui.advanced_parameters.AdvancedParameters.display_row22","title":"<code>display_row22()</code>","text":"<p>Display or hide the second row (for the second image segmentation pipeline) of the csc editing widgets.</p> Source code in <code>src/cellects/gui/advanced_parameters.py</code> <pre><code>def display_row22(self):\n    \"\"\"\n    Display or hide the second row (for the second image segmentation pipeline) of the csc editing widgets.\n    \"\"\"\n    self.row21[4].setVisible(False)\n    for i in range(5):\n        self.row22[i].setVisible(True)\n    self.display_logical_operator()\n</code></pre>"},{"location":"api/cellects/gui/advanced_parameters/#cellects.gui.advanced_parameters.AdvancedParameters.display_row23","title":"<code>display_row23()</code>","text":"<p>Display or hide the third row (for the second image segmentation pipeline) of the csc editing widgets.</p> Source code in <code>src/cellects/gui/advanced_parameters.py</code> <pre><code>def display_row23(self):\n    \"\"\"\n    Display or hide the third row (for the second image segmentation pipeline) of the csc editing widgets.\n    \"\"\"\n    self.row22[4].setVisible(False)\n    for i in range(4):\n        self.row23[i].setVisible(True)\n    self.display_logical_operator()\n</code></pre>"},{"location":"api/cellects/gui/advanced_parameters/#cellects.gui.advanced_parameters.AdvancedParameters.display_row3","title":"<code>display_row3()</code>","text":"<p>Display or hide the third row of the csc editing widgets.</p> Source code in <code>src/cellects/gui/advanced_parameters.py</code> <pre><code>def display_row3(self):\n    \"\"\"\n    Display or hide the third row of the csc editing widgets.\n    \"\"\"\n    self.row2[4].setVisible(False)\n    for i in range(4):\n        self.row3[i].setVisible(True)\n    self.display_logical_operator()\n</code></pre>"},{"location":"api/cellects/gui/advanced_parameters/#cellects.gui.advanced_parameters.AdvancedParameters.do_automatic_size_thresholding_changed","title":"<code>do_automatic_size_thresholding_changed()</code>","text":"<p>This function toggles the visibility of <code>first_move_threshold</code> and <code>first_move_threshold_label</code> UI elements based on whether the <code>do_automatic_size_thresholding</code> checkbox is checked or not.</p> Source code in <code>src/cellects/gui/advanced_parameters.py</code> <pre><code>def do_automatic_size_thresholding_changed(self):\n    \"\"\"\n    This function toggles the visibility of `first_move_threshold` and\n    `first_move_threshold_label` UI elements based on whether the\n    `do_automatic_size_thresholding` checkbox is checked or not.\n    \"\"\"\n    self.first_move_threshold.setVisible(not self.do_automatic_size_thresholding.isChecked())\n    self.first_move_threshold_label.setVisible(not self.do_automatic_size_thresholding.isChecked())\n</code></pre>"},{"location":"api/cellects/gui/advanced_parameters/#cellects.gui.advanced_parameters.AdvancedParameters.do_distant_shape_int_changed","title":"<code>do_distant_shape_int_changed()</code>","text":"<p>Toggles the visibility of widgets based the use of an algorithm allowing to connect distant shapes during segmentation.</p> Source code in <code>src/cellects/gui/advanced_parameters.py</code> <pre><code>def do_distant_shape_int_changed(self):\n    \"\"\"\n    Toggles the visibility of widgets based the use of an algorithm allowing to connect distant shapes\n    during segmentation.\n    \"\"\"\n    do_distant_shape_int = self.connect_distant_shape_during_segmentation.isChecked()\n    self.detection_range_factor.setVisible(do_distant_shape_int)\n    if do_distant_shape_int:\n        self.detection_range_factor.setValue(2)\n    self.detection_range_factor_label.setVisible(do_distant_shape_int)\n    self.use_max_size.setVisible(do_distant_shape_int)\n    self.use_min_size.setVisible(do_distant_shape_int)\n    self.use_max_size_label.setVisible(do_distant_shape_int)\n    self.use_min_size_label.setVisible(do_distant_shape_int)\n    do_use_max_size = do_distant_shape_int and self.use_max_size.isChecked()\n    self.max_size_for_connection.setVisible(do_use_max_size)\n    self.max_size_for_connection_label.setVisible(do_use_max_size)\n    do_use_min_size = do_distant_shape_int and self.use_min_size.isChecked()\n    self.min_size_for_connection.setVisible(do_use_min_size)\n    self.min_size_for_connection_label.setVisible(do_use_min_size)\n</code></pre>"},{"location":"api/cellects/gui/advanced_parameters/#cellects.gui.advanced_parameters.AdvancedParameters.do_multiprocessing_is_clicked","title":"<code>do_multiprocessing_is_clicked()</code>","text":"<p>Update the visibility of <code>max_core_nb</code> and <code>max_core_nb_label</code> based on the checkbox state of <code>do_multiprocessing</code>.</p> Source code in <code>src/cellects/gui/advanced_parameters.py</code> <pre><code>def do_multiprocessing_is_clicked(self):\n    \"\"\"\n    Update the visibility of `max_core_nb` and `max_core_nb_label` based on the checkbox state of `do_multiprocessing`.\n    \"\"\"\n    self.max_core_nb.setVisible(self.do_multiprocessing.isChecked())\n    self.max_core_nb_label.setVisible(self.do_multiprocessing.isChecked())\n</code></pre>"},{"location":"api/cellects/gui/advanced_parameters/#cellects.gui.advanced_parameters.AdvancedParameters.extract_time_is_clicked","title":"<code>extract_time_is_clicked()</code>","text":"<p>Toggle the visibility of time_step_label and update its text/tooltip based on whether extract_time is checked.</p> Source code in <code>src/cellects/gui/advanced_parameters.py</code> <pre><code>def extract_time_is_clicked(self):\n    \"\"\"\n    Toggle the visibility of time_step_label and update its text/tooltip based on\n    whether extract_time is checked.\n    \"\"\"\n    self.time_step.setVisible(not self.extract_time.isChecked())\n    if self.extract_time.isChecked():\n        self.time_step_label.setText(\"Automatically extract time interval between images\")\n        self.time_step_label.setToolTip(\"Uses the exif data of the images (if available), to extract these intervals\\nOtherwise, default time interval is 1 min\")\n    else:\n        self.time_step_label.setText(\"Set the time interval between images\")\n        self.time_step_label.setToolTip(\"In minutes\")\n</code></pre>"},{"location":"api/cellects/gui/advanced_parameters/#cellects.gui.advanced_parameters.AdvancedParameters.generate_csc_editing","title":"<code>generate_csc_editing()</code>","text":"<p>Generate CSC Editing Layout</p> <p>Creates and configures the layout for Color Space Combination (CSC) editing in the video analysis window, initializing widgets and connecting signals to slots for dynamic UI handling.</p> Source code in <code>src/cellects/gui/advanced_parameters.py</code> <pre><code>def generate_csc_editing(self):\n    \"\"\"\n    Generate CSC Editing Layout\n\n    Creates and configures the layout for Color Space Combination (CSC) editing in the video analysis window,\n    initializing widgets and connecting signals to slots for dynamic UI handling.\n    \"\"\"\n    self.edit_widget = QtWidgets.QWidget()\n    self.edit_layout = QtWidgets.QVBoxLayout()\n\n    # 2) Titles\n    self.video_csc_label = FixedText(AP[\"Csc_for_video_analysis\"][\"label\"] + ':',\n                                     tip=AP[\"Csc_for_video_analysis\"][\"tips\"],\n                                     night_mode=self.parent().po.all['night_mode'])\n    self.video_csc_label.setFixedHeight(30)\n    self.edit_layout.addWidget(self.video_csc_label)\n    self.both_csc_widget = QtWidgets.QWidget()\n    self.both_csc_layout = QtWidgets.QHBoxLayout()\n\n    # 3) First CSC\n    self.first_csc_widget = QtWidgets.QWidget()\n    self.first_csc_layout = QtWidgets.QGridLayout()\n    self.row1 = self.one_csc_editing()\n    self.row1[4].clicked.connect(self.display_row2)\n    self.row2 = self.one_csc_editing()\n    self.row2[4].clicked.connect(self.display_row3)\n    self.row3 = self.one_csc_editing()# Second CSC\n    self.logical_operator_between_combination_result = Combobox([\"None\", \"Or\", \"And\", \"Xor\"],\n                                                                night_mode=self.parent().po.all['night_mode'])\n    self.logical_operator_between_combination_result.setCurrentText(self.parent().po.vars['convert_for_motion']['logical'])\n    self.logical_operator_between_combination_result.currentTextChanged.connect(self.logical_op_changed)\n    self.logical_operator_between_combination_result.setFixedWidth(100)\n    self.logical_operator_label = FixedText(IAW[\"Logical_operator\"][\"label\"], halign='c', tip=IAW[\"Logical_operator\"][\"tips\"],\n                                            night_mode=self.parent().po.all['night_mode'])\n    self.row21 = self.one_csc_editing()\n    self.row21[4].clicked.connect(self.display_row22)\n    self.row22 = self.one_csc_editing()\n    self.row22[4].clicked.connect(self.display_row23)\n    self.row23 = self.one_csc_editing()\n    for i in range(5):\n        self.first_csc_layout.addWidget(self.row1[i], 0, i, 1, 1)\n        self.first_csc_layout.addWidget(self.row2[i], 1, i, 1, 1)\n        self.first_csc_layout.addWidget(self.row3[i], 2, i, 1, 1)\n        self.row1[i].setVisible(False)\n        self.row2[i].setVisible(False)\n        self.row3[i].setVisible(False)\n    self.first_csc_layout.setHorizontalSpacing(0)\n    self.first_csc_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum), 0, 5, 3, 1)\n    self.first_csc_widget.setLayout(self.first_csc_layout)\n    self.both_csc_layout.addWidget(self.first_csc_widget)\n\n    # 5) Second CSC\n    self.second_csc_widget = QtWidgets.QWidget()\n    self.second_csc_layout = QtWidgets.QGridLayout()\n    for i in range(5):\n        self.second_csc_layout.addWidget(self.row21[i], 0, i, 1, 1)\n        self.second_csc_layout.addWidget(self.row22[i], 1, i, 1, 1)\n        self.second_csc_layout.addWidget(self.row23[i], 2, i, 1, 1)\n        self.row21[i].setVisible(False)\n        self.row22[i].setVisible(False)\n        self.row23[i].setVisible(False)\n    self.second_csc_layout.setHorizontalSpacing(0)\n    self.second_csc_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum), 0, 5, 3, 1)\n    self.second_csc_widget.setLayout(self.second_csc_layout)\n    self.both_csc_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n    self.both_csc_layout.addWidget(self.second_csc_widget)\n    self.both_csc_widget.setLayout(self.both_csc_layout)\n    self.edit_layout.addWidget(self.both_csc_widget)\n\n    # 4) logical_operator\n    self.logical_op_widget = QtWidgets.QWidget()\n    self.logical_op_widget.setFixedHeight(30)\n    self.logical_op_layout = QtWidgets.QHBoxLayout()\n    self.logical_op_layout.addWidget(self.logical_operator_label)\n    self.logical_op_layout.addWidget(self.logical_operator_between_combination_result)\n    self.logical_op_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n    self.logical_operator_between_combination_result.setVisible(False)\n    self.logical_operator_label.setVisible(False)\n    self.logical_op_widget.setLayout(self.logical_op_layout)\n    self.logical_op_widget.setFixedHeight(50)\n    self.edit_layout.addWidget(self.logical_op_widget)\n\n    # 6) Open the more_than_2_colors row layout\n    self.more_than_2_colors_widget = QtWidgets.QWidget()\n    self.more_than_2_colors_layout = QtWidgets.QHBoxLayout()\n    self.more_than_two_colors = Checkbox(self.parent().po.all[\"more_than_two_colors\"])\n    self.more_than_two_colors.setStyleSheet(\"QCheckBox::indicator {width: 12px;height: 12px;background-color: transparent;\"\n                        \"border-radius: 5px;border-style: solid;border-width: 1px;\"\n                        \"border-color: rgb(100,100,100);}\"\n                        \"QCheckBox::indicator:checked {background-color: rgb(70,130,180);}\"\n                        \"QCheckBox:checked, QCheckBox::indicator:checked {border-color: black black white white;}\"\n                        \"QCheckBox:checked {background-color: transparent;}\"\n                        \"QCheckBox:margin-left {0%}\"\n                        \"QCheckBox:margin-right {-10%}\")\n    self.more_than_two_colors.stateChanged.connect(self.display_more_than_two_colors_option)\n\n    self.more_than_two_colors_label = FixedText(IAW[\"Kmeans\"][\"label\"],\n                                                tip=IAW[\"Kmeans\"][\"tips\"], night_mode=self.parent().po.all['night_mode'])\n    self.more_than_two_colors_label.setSizePolicy(QtWidgets.QSizePolicy.Maximum, QtWidgets.QSizePolicy.Maximum)\n    self.more_than_two_colors_label.setAlignment(QtCore.Qt.AlignLeft)\n    self.distinct_colors_number = Spinbox(min=2, max=5, val=self.parent().po.vars[\"color_number\"], night_mode=self.parent().po.all['night_mode'])\n    self.more_than_2_colors_layout.addWidget(self.more_than_two_colors)\n    self.more_than_2_colors_layout.addWidget(self.more_than_two_colors_label)\n    self.more_than_2_colors_layout.addWidget(self.distinct_colors_number)\n    self.more_than_2_colors_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n    self.more_than_2_colors_widget.setLayout(self.more_than_2_colors_layout)\n    self.more_than_2_colors_widget.setFixedHeight(50)\n    self.edit_layout.addWidget(self.more_than_2_colors_widget)\n    self.edit_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.Maximum, QtWidgets.QSizePolicy.MinimumExpanding))\n    self.edit_widget.setLayout(self.edit_layout)\n</code></pre>"},{"location":"api/cellects/gui/advanced_parameters/#cellects.gui.advanced_parameters.AdvancedParameters.logical_op_changed","title":"<code>logical_op_changed()</code>","text":"<p>Update the visibility and values of UI components based on the logical operator selection.</p> Source code in <code>src/cellects/gui/advanced_parameters.py</code> <pre><code>def logical_op_changed(self):\n    \"\"\"\n    Update the visibility and values of UI components based on the logical operator selection.\n    \"\"\"\n    # show = self.logical_operator_between_combination_result.currentText() != 'None'\n    if self.logical_operator_between_combination_result.currentText() == 'None':\n        self.row21[0].setVisible(False)\n        self.row21[0].setCurrentIndex(0)\n        for i1 in [1, 2, 3]:\n            self.row21[i1].setVisible(False)\n            self.row21[i1].setValue(0)\n        self.row21[i1 + 1].setVisible(False)\n\n        self.row22[0].setVisible(False)\n        self.row22[0].setCurrentIndex(0)\n        for i1 in [1, 2, 3]:\n            self.row22[i1].setVisible(False)\n            self.row22[i1].setValue(0)\n        self.row22[i1 + 1].setVisible(False)\n\n        self.row23[0].setVisible(False)\n        self.row23[0].setCurrentIndex(0)\n        for i1 in [1, 2, 3]:\n            self.row23[i1].setVisible(False)\n            self.row23[i1].setValue(0)\n        self.row23[i1 + 1].setVisible(False)\n    else:\n        self.row21[0].setVisible(True)\n        for i1 in [1, 2, 3]:\n            self.row21[i1].setVisible(True)\n        self.row21[i1 + 1].setVisible(True)\n</code></pre>"},{"location":"api/cellects/gui/advanced_parameters/#cellects.gui.advanced_parameters.AdvancedParameters.night_mode_is_clicked","title":"<code>night_mode_is_clicked()</code>","text":"<p>Triggered when night_mode_cb check status changes</p> Source code in <code>src/cellects/gui/advanced_parameters.py</code> <pre><code>def night_mode_is_clicked(self):\n    \"\"\" Triggered when night_mode_cb check status changes\"\"\"\n    self.parent().po.all['night_mode'] = self.night_mode_cb.isChecked()\n    self.message.setText('Close and restart Cellects to apply night or light mode')\n    self.message.setStyleSheet(\"color: rgb(230, 145, 18)\")\n</code></pre>"},{"location":"api/cellects/gui/advanced_parameters/#cellects.gui.advanced_parameters.AdvancedParameters.ok_is_clicked","title":"<code>ok_is_clicked()</code>","text":"<p>Updates the parent object's processing options with the current state of various UI elements.</p> Summary <p>Saves the current state of UI components to the parent object's processing options dictionary.</p> Extended Description <p>This method iterates through various UI components such as checkboxes, sliders, and dropdowns to save their current state into the parent object's processing options variables. This allows the software to retain user preferences across sessions and ensures that all settings are correctly applied before processing.</p> Source code in <code>src/cellects/gui/advanced_parameters.py</code> <pre><code>def ok_is_clicked(self):\n    \"\"\"\n    Updates the parent object's processing options with the current state of various UI elements.\n\n    Summary\n    -------\n    Saves the current state of UI components to the parent object's processing options dictionary.\n\n    Extended Description\n    --------------------\n    This method iterates through various UI components such as checkboxes, sliders,\n    and dropdowns to save their current state into the parent object's processing\n    options variables. This allows the software to retain user preferences across\n    sessions and ensures that all settings are correctly applied before processing.\n    \"\"\"\n    self.parent().po.all['automatically_crop'] = self.automatically_crop.isChecked()\n    self.parent().po.vars['subtract_background'] = self.subtract_background.isChecked()\n    self.parent().po.all['keep_cell_and_back_for_all_folders'] = self.keep_cell_and_back_for_all_folders.isChecked()\n    self.parent().po.vars['correct_errors_around_initial'] = self.correct_errors_around_initial.isChecked()\n    self.parent().po.vars['prevent_fast_growth_near_periphery'] = self.prevent_fast_growth_near_periphery.isChecked()\n    self.parent().po.vars['periphery_width'] = int(self.periphery_width.value())\n    self.parent().po.vars['max_periphery_growth'] = int(self.max_periphery_growth.value())\n\n    self.parent().po.all['first_move_threshold_in_mm\u00b2'] = self.first_move_threshold.value()\n    self.parent().po.vars['output_in_mm'] = self.pixels_to_mm.isChecked()\n    self.parent().po.all['automatic_size_thresholding'] = self.do_automatic_size_thresholding.isChecked()\n    self.parent().po.vars['appearance_detection_method'] = self.appearing_selection.currentText()\n\n    self.parent().po.all['auto_mesh_step_length'] = self.mesh_step_length_cb.isChecked()\n    if self.parent().po.all['auto_mesh_step_length']:\n        self.parent().po.vars['rolling_window_segmentation']['step'] = None\n    else:\n        self.parent().po.vars['rolling_window_segmentation']['step'] = int(self.mesh_step_length.value())\n\n    self.parent().po.all['auto_mesh_side_length'] = self.mesh_side_length_cb.isChecked()\n    if self.parent().po.all['auto_mesh_side_length']:\n        self.parent().po.vars['rolling_window_segmentation']['side_len'] = None\n    else:\n        self.parent().po.vars['rolling_window_segmentation']['side_len'] = int(self.mesh_side_length.value())\n\n    self.parent().po.all['auto_mesh_min_int_var'] = self.mesh_min_int_var_cb.isChecked()\n    if self.parent().po.all['auto_mesh_min_int_var']:\n        self.parent().po.vars['rolling_window_segmentation']['min_int_var'] = None\n    else:\n        self.parent().po.vars['rolling_window_segmentation']['min_int_var'] = int(self.mesh_min_int_var.value())\n\n    self.parent().po.vars['expected_oscillation_period'] = self.oscillation_period.value()\n    self.parent().po.vars['minimal_oscillating_cluster_size'] = int(self.minimal_oscillating_cluster_size.value())\n\n    self.parent().po.all['do_multiprocessing'] = self.do_multiprocessing.isChecked()\n    self.parent().po.all['cores'] = np.uint8(self.max_core_nb.value())\n    self.parent().po.vars['min_ram_free'] = self.min_memory_left.value()\n    self.parent().po.vars['lose_accuracy_to_save_memory'] = self.lose_accuracy_to_save_memory.isChecked()\n    self.parent().po.vars['video_fps'] = float(self.video_fps.value())\n    self.parent().po.vars['keep_unaltered_videos'] = self.keep_unaltered_videos.isChecked()\n    self.parent().po.vars['save_processed_videos'] = self.save_processed_videos.isChecked()\n    self.parent().po.all['extract_time_interval'] = self.extract_time.isChecked()\n    self.parent().po.vars['time_step'] = float(self.time_step.value())\n\n    do_distant_shape_int = self.connect_distant_shape_during_segmentation.isChecked()\n    self.parent().po.all['connect_distant_shape_during_segmentation'] = do_distant_shape_int\n    if do_distant_shape_int:\n        self.parent().po.vars['detection_range_factor'] = int(\n            np.round(self.detection_range_factor.value()))\n    else:\n        self.parent().po.vars['detection_range_factor'] = 0\n    if self.use_max_size.isChecked():\n        self.parent().po.vars['max_size_for_connection'] = int(np.round(self.max_size_for_connection.value()))\n    else:\n        self.parent().po.vars['max_size_for_connection'] = None\n    if self.use_min_size.isChecked():\n        self.parent().po.vars['min_size_for_connection'] = int(np.round(self.min_size_for_connection.value()))\n    else:\n        self.parent().po.vars['min_size_for_connection'] = None\n\n    self.parent().po.all['all_specimens_have_same_direction'] = self.all_specimens_have_same_direction.isChecked()\n\n    previous_csc = deepcopy(self.parent().po.vars['convert_for_motion'])\n    self.save_user_defined_csc()\n    if self.parent().po.first_exp_ready_to_run:\n        are_dicts_equal: bool = True\n        for key in previous_csc.keys():\n            if key != 'logical':\n                are_dicts_equal = are_dicts_equal and np.all(key in self.parent().po.vars['convert_for_motion'] and previous_csc[key] == self.parent().po.vars['convert_for_motion'][key])\n        for key in self.parent().po.vars['convert_for_motion'].keys():\n            if key != 'logical':\n                are_dicts_equal = are_dicts_equal and np.all(\n                    key in previous_csc and self.parent().po.vars['convert_for_motion'][key] ==\n                    previous_csc[key])\n        if not are_dicts_equal:\n            self.parent().po.find_if_lighter_background()\n\n    if not self.parent().thread_dict['SaveAllVars'].isRunning():\n        self.parent().thread_dict['SaveAllVars'].start()\n\n    if self.parent().last_is_first:\n        self.parent().change_widget(0) # FirstWidget\n    else:\n        self.parent().change_widget(3) # ImageAnalysisWindow ThirdWidget\n</code></pre>"},{"location":"api/cellects/gui/advanced_parameters/#cellects.gui.advanced_parameters.AdvancedParameters.one_csc_editing","title":"<code>one_csc_editing()</code>","text":"<p>Creates a list of widgets for color space editing.</p> <p>Returns:</p> Name Type Description <code>widget_list</code> <code>List[QWidget]</code> <p>A list containing a Combobox, three Spinboxes, and a PButton.</p> Notes <p>The Combobox widget allows selection from predefined color spaces, the Spinboxes are for editing numerical values, and the PButton is for adding new entries.</p> Source code in <code>src/cellects/gui/advanced_parameters.py</code> <pre><code>def one_csc_editing(self):\n    \"\"\"\n    Creates a list of widgets for color space editing.\n\n    Returns\n    -------\n    widget_list : List[QtWidgets.QWidget]\n        A list containing a Combobox, three Spinboxes, and a PButton.\n\n    Notes\n    -----\n    The Combobox widget allows selection from predefined color spaces,\n    the Spinboxes are for editing numerical values, and the PButton is\n    for adding new entries.\n    \"\"\"\n    widget_list = []\n    widget_list.insert(0, Combobox([\"None\", \"bgr\", \"hsv\", \"hls\", \"lab\", \"luv\", \"yuv\"],\n                                   night_mode=self.parent().po.all['night_mode']))\n    widget_list[0].setSizePolicy(QtWidgets.QSizePolicy.Maximum, QtWidgets.QSizePolicy.Maximum)\n    widget_list[0].setFixedWidth(100)\n    for i in [1, 2, 3]:\n        widget_list.insert(i, Spinbox(min=-126, max=126, val=0, night_mode=self.parent().po.all['night_mode']))\n        widget_list[i].setFixedWidth(45)\n    widget_list.insert(i + 1, PButton(\"+\", night_mode=self.parent().po.all['night_mode']))\n\n    return widget_list\n</code></pre>"},{"location":"api/cellects/gui/advanced_parameters/#cellects.gui.advanced_parameters.AdvancedParameters.prevent_fast_growth_near_periphery_check","title":"<code>prevent_fast_growth_near_periphery_check()</code>","text":"<p>Handles the logic for using a special algorithm on growth near the periphery during video segmentation.</p> Source code in <code>src/cellects/gui/advanced_parameters.py</code> <pre><code>def prevent_fast_growth_near_periphery_check(self):\n    \"\"\"\n    Handles the logic for using a special algorithm on growth near the periphery during video segmentation.\n    \"\"\"\n    checked_status = self.prevent_fast_growth_near_periphery.isChecked()\n    self.periphery_width.setVisible(checked_status)\n    self.periphery_width_label.setVisible(checked_status)\n    self.max_periphery_growth.setVisible(checked_status)\n    self.max_periphery_growth_label.setVisible(checked_status)\n</code></pre>"},{"location":"api/cellects/gui/advanced_parameters/#cellects.gui.advanced_parameters.AdvancedParameters.reset_all_settings_is_clicked","title":"<code>reset_all_settings_is_clicked()</code>","text":"<p>Reset All Settings on Click</p> <p>Resets the application settings to their default state by removing specific pickle files and saving new default dictionaries.</p> Notes <ul> <li>This function removes specific pickle files to reset settings.</li> <li>The function changes the current working directory temporarily.</li> </ul> Source code in <code>src/cellects/gui/advanced_parameters.py</code> <pre><code>def reset_all_settings_is_clicked(self):\n    \"\"\"\n    Reset All Settings on Click\n\n    Resets the application settings to their default state by removing specific pickle files and saving new default dictionaries.\n\n    Notes\n    -----\n    - This function removes specific pickle files to reset settings.\n    - The function changes the current working directory temporarily.\n    \"\"\"\n    if os.path.isfile('Data to run Cellects quickly.pkl'):\n        os.remove('Data to run Cellects quickly.pkl')\n    if os.path.isfile('PickleRick.pkl'):\n        os.remove('PickleRick.pkl')\n    if os.path.isfile('PickleRick0.pkl'):\n        os.remove('PickleRick0.pkl')\n    if os.path.isfile(Path(CELLECTS_DIR.parent / 'PickleRick.pkl')):\n        os.remove(Path(CELLECTS_DIR.parent / 'PickleRick.pkl'))\n    if os.path.isfile(Path(CELLECTS_DIR.parent / 'PickleRick0.pkl')):\n        os.remove(Path(CELLECTS_DIR.parent / 'PickleRick0.pkl'))\n    if os.path.isfile(Path(CONFIG_DIR / 'PickleRick1.pkl')):\n        os.remove(Path(CONFIG_DIR / 'PickleRick1.pkl'))\n    current_dir = os.getcwd()\n    os.chdir(CONFIG_DIR)\n    DefaultDicts().save_as_pkl(self.parent().po)\n    os.chdir(current_dir)\n    self.message.setText('Close and restart Cellects to apply the settings reset')\n    self.message.setStyleSheet(\"color: rgb(230, 145, 18)\")\n</code></pre>"},{"location":"api/cellects/gui/advanced_parameters/#cellects.gui.advanced_parameters.AdvancedParameters.save_user_defined_csc","title":"<code>save_user_defined_csc()</code>","text":"<p>Save user-defined combination of color spaces and channels.</p> Source code in <code>src/cellects/gui/advanced_parameters.py</code> <pre><code>def save_user_defined_csc(self):\n    \"\"\"\n    Save user-defined combination of color spaces and channels.\n    \"\"\"\n    self.parent().po.vars['convert_for_motion'] = {}\n    spaces = np.array((self.row1[0].currentText(), self.row2[0].currentText(), self.row3[0].currentText()))\n    channels = np.array(\n        ((self.row1[1].value(), self.row1[2].value(), self.row1[3].value()),\n         (self.row2[1].value(), self.row2[2].value(), self.row2[3].value()),\n         (self.row3[1].value(), self.row3[2].value(), self.row3[3].value()),\n         (self.row21[1].value(), self.row21[2].value(), self.row21[3].value()),\n         (self.row22[1].value(), self.row22[2].value(), self.row22[3].value()),\n         (self.row23[1].value(), self.row23[2].value(), self.row23[3].value())),\n        dtype=np.int8)\n    if self.logical_operator_between_combination_result.currentText() != 'None':\n        spaces = np.concatenate((spaces, np.array((\n                    self.row21[0].currentText() + \"2\", self.row22[0].currentText() + \"2\",\n                    self.row23[0].currentText() + \"2\"))))\n        channels = np.concatenate((channels, np.array(((self.row21[1].value(), self.row21[2].value(), self.row21[3].value()),\n         (self.row22[1].value(), self.row22[2].value(), self.row22[3].value()),\n         (self.row23[1].value(), self.row23[2].value(), self.row23[3].value())),\n         dtype=np.int8)))\n        self.parent().po.vars['convert_for_motion']['logical'] = self.logical_operator_between_combination_result.currentText()\n    else:\n        self.parent().po.vars['convert_for_motion']['logical'] = 'None'\n    if not np.all(spaces == \"None\"):\n        for i, space in enumerate(spaces):\n            if space != \"None\" and space != \"None2\":\n                if np.any(channels[i, :] &lt; 0.):\n                    channels[i, :] + channels[i, :].min()\n                self.parent().po.vars['convert_for_motion'][space] = channels[i, :]\n    if len(self.parent().po.vars['convert_for_motion']) == 1 or channels.sum() == 0:\n        self.csc_dict_is_empty = True\n    else:\n        self.csc_dict_is_empty = False\n\n    self.parent().po.all[\"more_than_two_colors\"] = self.more_than_two_colors.isChecked()\n    if self.more_than_two_colors.isChecked():\n        self.parent().po.vars[\"color_number\"] = int(self.distinct_colors_number.value())\n        self.parent().videoanalysiswindow.select_option.setVisible(True)\n        self.parent().videoanalysiswindow.select_option_label.setVisible(True)\n</code></pre>"},{"location":"api/cellects/gui/advanced_parameters/#cellects.gui.advanced_parameters.AdvancedParameters.subtract_background_check","title":"<code>subtract_background_check()</code>","text":"<p>Handles the logic for using background subtraction or not during image segmentation.</p> Source code in <code>src/cellects/gui/advanced_parameters.py</code> <pre><code>def subtract_background_check(self):\n    \"\"\"\n    Handles the logic for using background subtraction or not during image segmentation.\n    \"\"\"\n    self.parent().po.motion = None\n    if self.subtract_background.isChecked():\n        self.parent().po.first_exp_ready_to_run = False\n</code></pre>"},{"location":"api/cellects/gui/advanced_parameters/#cellects.gui.advanced_parameters.AdvancedParameters.true_init","title":"<code>true_init()</code>","text":"<p>Initialize the AdvancedParameters window.</p> <p>This method sets up the layout and widgets for the AdvancedParameters window, including scroll areas, layouts, and various UI components for configuring advanced parameters, including 'Cancel' and 'Ok' buttons.</p> Notes <p>This method assumes that the parent widget has a 'po' attribute with specific settings and variables.</p> Source code in <code>src/cellects/gui/advanced_parameters.py</code> <pre><code>def true_init(self):\n    \"\"\"\n    Initialize the AdvancedParameters window.\n\n    This method sets up the layout and widgets for the AdvancedParameters window,\n    including scroll areas, layouts, and various UI components for configuring\n    advanced parameters, including 'Cancel' and 'Ok' buttons.\n\n    Notes\n    -----\n    This method assumes that the parent widget has a 'po' attribute with specific settings and variables.\n    \"\"\"\n    logging.info(\"Initialize AdvancedParameters window\")\n    self.layout = QtWidgets.QVBoxLayout()\n\n    self.left_scroll_table = QtWidgets.QScrollArea()  #   # Scroll Area which contains the widgets, set as the centralWidget\n    self.left_scroll_table.setSizePolicy(QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.MinimumExpanding)\n    self.left_scroll_table.setMinimumHeight(150)\n    self.left_scroll_table.setFrameShape(QtWidgets.QFrame.NoFrame)\n    self.left_scroll_table.setVerticalScrollBarPolicy(QtCore.Qt.ScrollBarAsNeeded)\n    self.left_scroll_table.setHorizontalScrollBarPolicy(QtCore.Qt.ScrollBarAsNeeded)\n\n    self.left_col_layout = QtWidgets.QVBoxLayout()\n    self.right_col_layout = QtWidgets.QVBoxLayout()\n    self.left_col_widget = QtWidgets.QWidget()\n    self.right_col_widget = QtWidgets.QWidget()\n    # Create the main Title\n    self.title = FixedText('Advanced parameters', police=30, night_mode=self.parent().po.all['night_mode'])\n    self.title.setAlignment(QtCore.Qt.AlignHCenter)\n    # Create the main layout\n    self.layout.addWidget(self.title)\n    # Create the stylesheet for the boxes allowing to categorize advanced parameters.\n    boxstylesheet = \\\n        \".QWidget {\\n\" \\\n        + \"border: 1px solid black;\\n\" \\\n        + \"border-radius: 20px;\\n\" \\\n        + \"}\"\n\n\n    # I/ First box: General parameters\n    # I/A/ Title\n    self.general_param_box_label = FixedText('General parameters:', tip=\"\",\n                                     night_mode=self.parent().po.all['night_mode'])\n    self.left_col_layout.addWidget(self.general_param_box_label)\n    # I/B/ Create the box\n    self.general_param_box_layout = QtWidgets.QGridLayout()\n    self.general_param_box_widget = QtWidgets.QWidget()\n    self.general_param_box_widget.setStyleSheet(boxstylesheet)\n    # I/C/ Create widgets\n    self.automatically_crop = Checkbox(self.parent().po.all['automatically_crop'])\n    self.automatically_crop_label = FixedText(AP[\"Crop_images\"][\"label\"], tip=AP[\"Crop_images\"][\"tips\"],\n                                              night_mode=self.parent().po.all['night_mode'])\n\n    self.subtract_background = Checkbox(self.parent().po.vars['subtract_background'])\n    self.subtract_background.stateChanged.connect(self.subtract_background_check)\n    self.subtract_background_label = FixedText(AP[\"Subtract_background\"][\"label\"], tip=AP[\"Subtract_background\"][\"tips\"], night_mode=self.parent().po.all['night_mode'])\n\n    self.keep_cell_and_back_for_all_folders = Checkbox(self.parent().po.all['keep_cell_and_back_for_all_folders'])\n    self.keep_cell_and_back_for_all_folders_label = FixedText(AP[\"Keep_drawings\"][\"label\"],\n                                           tip=AP[\"Keep_drawings\"][\"tips\"],\n                                           night_mode=self.parent().po.all['night_mode'])\n\n    self.correct_errors_around_initial = Checkbox(self.parent().po.vars['correct_errors_around_initial'])\n    self.correct_errors_around_initial_label = FixedText(AP[\"Correct_errors_around_initial\"][\"label\"],\n                                           tip=AP[\"Correct_errors_around_initial\"][\"tips\"],\n                                           night_mode=self.parent().po.all['night_mode'])\n\n    self.prevent_fast_growth_near_periphery = Checkbox(self.parent().po.vars['prevent_fast_growth_near_periphery'])\n    self.prevent_fast_growth_near_periphery_label = FixedText(AP[\"Prevent_fast_growth_near_periphery\"][\"label\"],\n                                           tip=AP[\"Prevent_fast_growth_near_periphery\"][\"tips\"],\n                                           night_mode=self.parent().po.all['night_mode'])\n\n    self.prevent_fast_growth_near_periphery.stateChanged.connect(self.prevent_fast_growth_near_periphery_check)\n    self.periphery_width = Spinbox(min=1, max=1000, val=self.parent().po.vars['periphery_width'],\n                                        decimals=0, night_mode=self.parent().po.all['night_mode'])\n    self.periphery_width_label = FixedText('Periphery width',\n                                           tip=\"The width, in pixels, of the arena s border designated as the peripheral region\",\n                                           night_mode=self.parent().po.all['night_mode'])\n    self.max_periphery_growth = Spinbox(min=1, max=1000000, val=self.parent().po.vars['max_periphery_growth'],\n                                        decimals=0, night_mode=self.parent().po.all['night_mode'])\n    self.max_periphery_growth_label = FixedText('Max periphery growth',\n                                           tip=\"The maximum detectable size (in pixels) of a shape in a single frame near the periphery of the arena.\\nLarger shapes will be considered as noise.\",\n                                           night_mode=self.parent().po.all['night_mode'])\n    self.prevent_fast_growth_near_periphery_check()\n\n\n    # I/D/ Arrange widgets in the box\n    self.general_param_box_layout.addWidget(self.automatically_crop, 0, 0)\n    self.general_param_box_layout.addWidget(self.automatically_crop_label, 0, 1)\n    self.general_param_box_layout.addWidget(self.subtract_background, 1, 0)\n    self.general_param_box_layout.addWidget(self.subtract_background_label, 1, 1)\n    self.general_param_box_layout.addWidget(self.keep_cell_and_back_for_all_folders, 2, 0)\n    self.general_param_box_layout.addWidget(self.keep_cell_and_back_for_all_folders_label, 2, 1)\n    self.general_param_box_layout.addWidget(self.correct_errors_around_initial, 3, 0)\n    self.general_param_box_layout.addWidget(self.correct_errors_around_initial_label, 3, 1)\n    self.general_param_box_layout.addWidget(self.prevent_fast_growth_near_periphery, 4, 0)\n    self.general_param_box_layout.addWidget(self.prevent_fast_growth_near_periphery_label, 4, 1)\n    self.general_param_box_layout.addWidget(self.periphery_width, 5, 0)\n    self.general_param_box_layout.addWidget(self.periphery_width_label, 5, 1)\n    self.general_param_box_layout.addWidget(self.max_periphery_growth, 6, 0)\n    self.general_param_box_layout.addWidget(self.max_periphery_growth_label, 6, 1)\n    self.general_param_box_widget.setLayout(self.general_param_box_layout)\n    self.left_col_layout.addWidget(self.general_param_box_widget)\n\n    # II/ Second box: One cell/colony per arena\n    # II/A/ Title\n    self.one_per_arena_label = FixedText('One cell/colony per arena parameters:', tip=\"\",\n                                        night_mode=self.parent().po.all['night_mode'])\n    self.left_col_layout.addWidget(self.one_per_arena_label)\n    # II/B/ Create the box\n    self.one_per_arena_box_layout = QtWidgets.QGridLayout()\n    self.one_per_arena_box_widget = QtWidgets.QWidget()\n    self.one_per_arena_box_widget.setStyleSheet(boxstylesheet)\n\n    # II/C/ Create widgets\n    self.all_specimens_have_same_direction = Checkbox(self.parent().po.all['all_specimens_have_same_direction'])\n    # self.all_specimens_have_same_direction.stateChanged.connect(self.all_specimens_have_same_direction_changed)\n    self.all_specimens_have_same_direction_label = FixedText(AP[\"Specimens_have_same_direction\"][\"label\"],\n                                                     tip=AP[\"Specimens_have_same_direction\"][\"tips\"],\n                                                     night_mode=self.parent().po.all['night_mode'])\n\n\n    connect_distant_shape = self.parent().po.all['connect_distant_shape_during_segmentation']\n    self.connect_distant_shape_during_segmentation = Checkbox(connect_distant_shape)\n    self.connect_distant_shape_during_segmentation.stateChanged.connect(self.do_distant_shape_int_changed)\n    self.connect_distant_shape_label = FixedText(AP[\"Connect_distant_shapes\"][\"label\"],\n                                                     tip=AP[\"Connect_distant_shapes\"][\"tips\"],\n                                                     night_mode=self.parent().po.all['night_mode'])\n    self.detection_range_factor = Spinbox(min=0, max=1000000,\n                                                  val=self.parent().po.vars['detection_range_factor'],\n                                                  night_mode=self.parent().po.all['night_mode'])\n    self.detection_range_factor_label = FixedText('Detection range factor:',\n                                                          tip=\"From 1 to 10, increase the allowed distance from original shape(s) to connect distant shapes\",\n                                                          night_mode=self.parent().po.all['night_mode'])\n\n    # Connect distant shape algo:\n    do_use_max_size = self.parent().po.vars['max_size_for_connection'] is not None and connect_distant_shape\n    do_use_min_size = self.parent().po.vars['min_size_for_connection'] is not None and connect_distant_shape\n    self.use_max_size = Checkbox(do_use_max_size, night_mode=self.parent().po.all['night_mode'])\n    self.use_min_size = Checkbox(do_use_min_size, night_mode=self.parent().po.all['night_mode'])\n    self.use_max_size.stateChanged.connect(self.use_max_size_changed)\n    self.use_min_size.stateChanged.connect(self.use_min_size_changed)\n\n    self.use_max_size_label = FixedText('Use max size as a threshold',\n                                        tip=\"To decide whether distant shapes should get connected\",\n                                        night_mode=self.parent().po.all['night_mode'])\n    self.use_min_size_label = FixedText('Use min size as a threshold',\n                                        tip=\"To decide whether distant shapes should get connected\",\n                                        night_mode=self.parent().po.all['night_mode'])\n    self.max_size_for_connection_label = FixedText('Max (pixels):', night_mode=self.parent().po.all['night_mode'])\n    self.min_size_for_connection_label = FixedText('Min (pixels):', night_mode=self.parent().po.all['night_mode'])\n    if do_use_max_size:\n        self.max_size_for_connection = Spinbox(min=0, max=1000000,\n                                              val=self.parent().po.vars['max_size_for_connection'],\n                                              night_mode=self.parent().po.all['night_mode'])\n    else:\n        self.max_size_for_connection = Spinbox(min=0, max=1000000, val=50,\n                                              night_mode=self.parent().po.all['night_mode'])\n    if do_use_min_size:\n        self.min_size_for_connection = Spinbox(min=0, max=1000000,\n                                              val=self.parent().po.vars['min_size_for_connection'],\n                                              night_mode=self.parent().po.all['night_mode'])\n    else:\n        self.min_size_for_connection = Spinbox(min=0, max=1000000, val=0,\n                                              night_mode=self.parent().po.all['night_mode'])\n\n    self.use_min_size.setStyleSheet(\"QCheckBox::indicator {width: 12px;height: 12px;background-color: transparent;\"\n                        \"border-radius: 5px;border-style: solid;border-width: 1px;\"\n                        \"border-color: rgb(100,100,100);}\"\n                        \"QCheckBox::indicator:checked {background-color: rgb(70,130,180);}\"\n                        \"QCheckBox:checked, QCheckBox::indicator:checked {border-color: black black white white;}\"\n                        \"QCheckBox:checked {background-color: transparent;}\"\n                        \"QCheckBox:margin-left {100%}\"\n                        \"QCheckBox:margin-right {0%}\")\n    self.min_size_for_connection_label.setAlignment(QtCore.Qt.AlignRight)\n    self.use_max_size.setStyleSheet(\"QCheckBox::indicator {width: 12px;height: 12px;background-color: transparent;\"\n                        \"border-radius: 5px;border-style: solid;border-width: 1px;\"\n                        \"border-color: rgb(100,100,100);}\"\n                        \"QCheckBox::indicator:checked {background-color: rgb(70,130,180);}\"\n                        \"QCheckBox:checked, QCheckBox::indicator:checked {border-color: black black white white;}\"\n                        \"QCheckBox:checked {background-color: transparent;}\"\n                        \"QCheckBox:margin-left {100%}\"\n                        \"QCheckBox:margin-right {0%}\")\n    self.max_size_for_connection_label.setAlignment(QtCore.Qt.AlignRight)\n\n    # II/D/ Arrange widgets in the box\n    curr_box_row = 0\n    self.one_per_arena_box_layout.addWidget(self.connect_distant_shape_during_segmentation, curr_box_row, 0)\n    self.one_per_arena_box_layout.addWidget(self.connect_distant_shape_label, curr_box_row, 1)\n    curr_box_row += 1\n    self.one_per_arena_box_layout.addWidget(self.detection_range_factor, curr_box_row, 0)\n    self.one_per_arena_box_layout.addWidget(self.detection_range_factor_label, curr_box_row, 1)\n    curr_box_row += 1\n    self.one_per_arena_box_layout.addWidget(self.use_min_size, curr_box_row, 0)\n    self.one_per_arena_box_layout.addWidget(self.use_min_size_label, curr_box_row, 1)\n    curr_box_row += 1\n    self.one_per_arena_box_layout.addWidget(self.min_size_for_connection_label, curr_box_row, 0)\n    self.one_per_arena_box_layout.addWidget(self.min_size_for_connection, curr_box_row, 1)\n    curr_box_row += 1\n    self.one_per_arena_box_layout.addWidget(self.use_max_size, curr_box_row, 0)\n    self.one_per_arena_box_layout.addWidget(self.use_max_size_label, curr_box_row, 1)\n    curr_box_row += 1\n    self.one_per_arena_box_layout.addWidget(self.max_size_for_connection_label, curr_box_row, 0)\n    self.one_per_arena_box_layout.addWidget(self.max_size_for_connection, curr_box_row, 1)\n    curr_box_row += 1\n    self.one_per_arena_box_layout.addWidget(self.all_specimens_have_same_direction, curr_box_row, 0)\n    self.one_per_arena_box_layout.addWidget(self.all_specimens_have_same_direction_label, curr_box_row, 1)\n    curr_box_row += 1\n\n    self.one_per_arena_box_widget.setLayout(self.one_per_arena_box_layout)\n    self.left_col_layout.addWidget(self.one_per_arena_box_widget)\n\n    # III/ Third box: Appearing cell/colony\n    # III/A/ Title\n    self.appearing_cell_label = FixedText('Appearing cell/colony parameters:', tip=\"\",\n                                          night_mode=self.parent().po.all['night_mode'])\n\n    self.left_col_layout.addWidget(self.appearing_cell_label)\n    # III/B/ Create the box\n    self.appearing_cell_box_layout = QtWidgets.QGridLayout()\n    self.appearing_cell_box_widget = QtWidgets.QWidget()\n    self.appearing_cell_box_widget.setStyleSheet(boxstylesheet)\n\n    # III/C/ Create widgets\n    self.first_move_threshold = Spinbox(min=0, max=1000000, val=self.parent().po.all['first_move_threshold_in_mm\u00b2'],\n                                        decimals=6, night_mode=self.parent().po.all['night_mode'])\n    self.first_move_threshold_label = FixedText('Minimal size to detect a cell/colony',\n                                                tip=\"In mm\u00b2. All appearing cell/colony lesser than this value will be considered as noise\",\n                                                night_mode=self.parent().po.all['night_mode'])\n    self.do_automatic_size_thresholding = Checkbox(self.parent().po.all['automatic_size_thresholding'])\n    self.do_automatic_size_thresholding_label = FixedText(AP[\"Appearance_size_threshold\"][\"label\"],\n                                                          tip=AP[\"Appearance_size_threshold\"][\"tips\"],\n                                                          night_mode=self.parent().po.all['night_mode'])\n    self.do_automatic_size_thresholding.stateChanged.connect(self.do_automatic_size_thresholding_changed)\n    self.appearing_selection = Combobox([\"largest\", \"most_central\"], night_mode=self.parent().po.all['night_mode'])\n    self.appearing_selection_label = FixedText(AP[\"Appearance_detection_method\"][\"label\"],\n                                               tip=AP[\"Appearance_detection_method\"][\"tips\"],\n                                               night_mode=self.parent().po.all['night_mode'])\n    self.appearing_selection.setCurrentText(self.parent().po.vars['appearance_detection_method'])\n    self.appearing_selection.setFixedWidth(190)\n\n    # III/D/ Arrange widgets in the box\n    curr_box_row = 0\n    self.appearing_cell_box_layout.addWidget(self.do_automatic_size_thresholding, curr_box_row, 0)\n    self.appearing_cell_box_layout.addWidget(self.do_automatic_size_thresholding_label, curr_box_row, 1)\n    curr_box_row += 1\n    self.appearing_cell_box_layout.addWidget(self.first_move_threshold, curr_box_row, 0)\n    self.appearing_cell_box_layout.addWidget(self.first_move_threshold_label, curr_box_row, 1)\n    curr_box_row += 1\n    self.appearing_cell_box_layout.addWidget(self.appearing_selection, curr_box_row, 0)\n    self.appearing_cell_box_layout.addWidget(self.appearing_selection_label, curr_box_row, 1)\n    curr_box_row += 1\n\n    self.appearing_cell_box_widget.setLayout(self.appearing_cell_box_layout)\n    self.left_col_layout.addWidget(self.appearing_cell_box_widget)\n\n    # V/ Fifth box: Network detection parameters:#\n    # IV/A/ Title\n    self.rolling_window_s_label = FixedText(IAW[\"Rolling_window_segmentation\"][\"label\"] + ': (auto if checked)',\n                                            tip=IAW[\"Rolling_window_segmentation\"][\"tips\"],\n                                          night_mode=self.parent().po.all['night_mode'])\n    self.left_col_layout.addWidget(self.rolling_window_s_label)\n    self.rolling_window_s_layout = QtWidgets.QGridLayout()\n    self.rolling_window_s_widget = QtWidgets.QWidget()\n    self.rolling_window_s_widget.setStyleSheet(boxstylesheet)\n    self.mesh_side_length_cb = Checkbox(self.parent().po.all['auto_mesh_side_length'],\n                                    night_mode=self.parent().po.all['night_mode'])\n    self.mesh_side_length_cb.stateChanged.connect(self.mesh_side_length_cb_changed)\n    self.mesh_side_length_label = FixedText(AP[\"Mesh_side_length\"][\"label\"], tip=AP[\"Mesh_side_length\"][\"tips\"],\n                                            night_mode=self.parent().po.all['night_mode'])\n    if self.parent().po.vars['rolling_window_segmentation']['side_len'] is None:\n        self.mesh_side_length = Spinbox(min=0, max=1000000, val=4, decimals=0,\n                                        night_mode=self.parent().po.all['night_mode'])\n        self.mesh_side_length.setVisible(False)\n    else:\n        self.mesh_side_length = Spinbox(min=0, max=1000000, val=self.parent().po.vars['rolling_window_segmentation']['side_len'], decimals=0,\n                                        night_mode=self.parent().po.all['night_mode'])\n\n\n    self.mesh_step_length_cb = Checkbox(self.parent().po.all['auto_mesh_step_length'],\n                                    night_mode=self.parent().po.all['night_mode'])\n    self.mesh_step_length_cb.stateChanged.connect(self.mesh_step_length_cb_changed)\n    self.mesh_step_length_label = FixedText(AP[\"Mesh_step_length\"][\"label\"], tip=AP[\"Mesh_step_length\"][\"tips\"],\n                                            night_mode=self.parent().po.all['night_mode'])\n    if self.parent().po.vars['rolling_window_segmentation']['side_len'] is None:\n        self.mesh_step_length = Spinbox(min=0, max=1000, val=2, decimals=0,\n                                        night_mode=self.parent().po.all['night_mode'])\n        self.mesh_step_length.setVisible(False)\n    else:\n        self.mesh_step_length = Spinbox(min=0, max=1000, val=self.parent().po.vars['rolling_window_segmentation']['step'], decimals=0,\n                                        night_mode=self.parent().po.all['night_mode'])\n\n\n    self.mesh_min_int_var_cb = Checkbox(self.parent().po.all['auto_mesh_min_int_var'],\n                                    night_mode=self.parent().po.all['night_mode'])\n    self.mesh_min_int_var_cb.stateChanged.connect(self.mesh_min_int_var_cb_changed)\n    if self.parent().po.vars['rolling_window_segmentation']['side_len'] is None:\n        self.mesh_min_int_var = Spinbox(min=0, max=1000, val=2, decimals=0,\n                                        night_mode=self.parent().po.all['night_mode'])\n        self.mesh_min_int_var.setVisible(False)\n    else:\n        self.mesh_min_int_var = Spinbox(min=0, max=1000, val=self.parent().po.vars['rolling_window_segmentation']['min_int_var'], decimals=0,\n                                        night_mode=self.parent().po.all['night_mode'])\n    self.mesh_min_int_var_label = FixedText(AP[\"Mesh_minimal_intensity_variation\"][\"label\"],\n                                            tip=AP[\"Mesh_minimal_intensity_variation\"][\"tips\"],\n                                            night_mode=self.parent().po.all['night_mode'])\n    self.rolling_window_s_layout.addWidget(self.mesh_side_length_cb, 0, 0)\n    self.rolling_window_s_layout.addWidget(self.mesh_side_length_label, 0, 1)\n    self.rolling_window_s_layout.addWidget(self.mesh_side_length, 0, 2)\n    self.rolling_window_s_layout.addWidget(self.mesh_step_length_cb, 1, 0)\n    self.rolling_window_s_layout.addWidget(self.mesh_step_length_label, 1, 1)\n    self.rolling_window_s_layout.addWidget(self.mesh_step_length, 1, 2)\n    self.rolling_window_s_layout.addWidget(self.mesh_min_int_var_cb, 2, 0)\n    self.rolling_window_s_layout.addWidget(self.mesh_min_int_var_label, 2, 1)\n    self.rolling_window_s_layout.addWidget(self.mesh_min_int_var, 2, 2)\n    self.rolling_window_s_widget.setLayout(self.rolling_window_s_layout)\n    self.left_col_layout.addWidget(self.rolling_window_s_widget)\n\n    # IV/ Fourth box: Oscillation period:\n    # IV/A/ Title\n    self.oscillation_label = FixedText('Oscillatory parameters:', tip=\"\",\n                                          night_mode=self.parent().po.all['night_mode'])\n    self.left_col_layout.addWidget(self.oscillation_label)\n\n    self.oscillation_period_layout = QtWidgets.QGridLayout()\n    self.oscillation_period_widget = QtWidgets.QWidget()\n    self.oscillation_period_widget.setStyleSheet(boxstylesheet)\n\n    self.oscillation_period = Spinbox(min=0, max=10000, val=self.parent().po.vars['expected_oscillation_period'], decimals=2,\n                                      night_mode=self.parent().po.all['night_mode'])\n    self.oscillation_period_label = FixedText(AP[\"Expected_oscillation_period\"][\"label\"],\n                                              tip=AP[\"Expected_oscillation_period\"][\"tips\"],\n                                              night_mode=self.parent().po.all['night_mode'])\n\n    self.minimal_oscillating_cluster_size = Spinbox(min=1, max=1000000000, decimals=0, val=self.parent().po.vars['minimal_oscillating_cluster_size'],\n                                      night_mode=self.parent().po.all['night_mode'])\n    self.minimal_oscillating_cluster_size_label = FixedText(AP[\"Minimal_oscillating_cluster_size\"][\"label\"],\n                                              tip=AP[\"Minimal_oscillating_cluster_size\"][\"tips\"],\n                                              night_mode=self.parent().po.all['night_mode'])\n\n    self.oscillation_period_layout.addWidget(self.oscillation_period, 0, 0)\n    self.oscillation_period_layout.addWidget(self.oscillation_period_label, 0, 1)\n    self.oscillation_period_layout.addWidget(self.minimal_oscillating_cluster_size, 1, 0)\n    self.oscillation_period_layout.addWidget(self.minimal_oscillating_cluster_size_label, 1, 1)\n\n    self.oscillation_period_widget.setLayout(self.oscillation_period_layout)\n    self.left_col_layout.addWidget(self.oscillation_period_widget)\n\n    # I/ First box: Scales\n    # I/A/ Title\n    self.right_scroll_table = QtWidgets.QScrollArea()   # Scroll Area which contains the widgets, set as the centralWidget\n    self.right_scroll_table.setSizePolicy(QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.MinimumExpanding)\n    self.right_scroll_table.setMinimumHeight(150)#self.parent().im_max_height - 100\n    self.right_scroll_table.setFrameShape(QtWidgets.QFrame.NoFrame)\n    self.right_scroll_table.setVerticalScrollBarPolicy(QtCore.Qt.ScrollBarAsNeeded)\n    self.right_scroll_table.setHorizontalScrollBarPolicy(QtCore.Qt.ScrollBarAsNeeded)\n    self.scale_box_label = FixedText(AP[\"Spatio_temporal_scaling\"][\"label\"] + ':',\n                                     tip=AP[\"Spatio_temporal_scaling\"][\"tips\"],\n                                     night_mode=self.parent().po.all['night_mode'])\n    self.right_col_layout.addWidget(self.scale_box_label)\n\n    # I/B/ Create the box\n    self.scale_box_layout = QtWidgets.QGridLayout()\n    self.scale_box_widget = QtWidgets.QWidget()\n    self.scale_box_widget.setStyleSheet(boxstylesheet)\n\n    # I/C/ Create widgets\n    self.extract_time = Checkbox(self.parent().po.all['extract_time_interval'])\n    self.extract_time.clicked.connect(self.extract_time_is_clicked)\n    self.time_step = Spinbox(min=0, max=100000, val=self.parent().po.vars['time_step'], decimals=3,\n                             night_mode=self.parent().po.all['night_mode'])\n    self.time_step.setFixedWidth(60)\n    if self.parent().po.all['extract_time_interval']:\n        self.time_step.setVisible(False)\n        self.time_step_label = FixedText('Automatically extract time interval between images',\n                                     tip=\"Uses the exif data of the images (if available), to extract these intervals\\nOtherwise, default time interval is 1 min\",\n                                     night_mode=self.parent().po.all['night_mode'])\n    else:\n        self.time_step_label = FixedText('Set the time interval between images',\n                                     tip=\"In minutes\",\n                                     night_mode=self.parent().po.all['night_mode'])\n    self.pixels_to_mm = Checkbox(self.parent().po.vars['output_in_mm'])\n    self.pixels_to_mm_label = FixedText('Convert areas and distances from pixels to mm',\n                                        tip=\"Check if you want output variables to be in mm\\nUncheck if you want output variables to be in pixels\",\n                                        night_mode=self.parent().po.all['night_mode'])\n\n    # I/D/ Arrange widgets in the box\n    self.scale_box_layout.addWidget(self.extract_time, 0, 0)\n    self.scale_box_layout.addWidget(self.time_step_label, 0, 1)\n    self.scale_box_layout.addWidget(self.time_step, 0, 2)\n    self.scale_box_layout.addWidget(self.pixels_to_mm, 2, 0)\n    self.scale_box_layout.addWidget(self.pixels_to_mm_label, 2, 1)\n    self.scale_box_widget.setLayout(self.scale_box_layout)\n    self.right_col_layout.addWidget(self.scale_box_widget)\n\n    # IV/ Fourth box: Computer resources\n    # IV/A/ Title\n    self.resources_label = FixedText('Computer resources:', tip=\"\",\n                                        night_mode=self.parent().po.all['night_mode'])\n    self.right_col_layout.addWidget(self.resources_label)\n\n    # IV/B/ Create the box\n    self.resources_box_layout = QtWidgets.QGridLayout()\n    self.resources_box_widget = QtWidgets.QWidget()\n    self.resources_box_widget.setStyleSheet(boxstylesheet)\n\n    # IV/C/ Create widgets\n    self.do_multiprocessing = Checkbox(self.parent().po.all['do_multiprocessing'])\n    self.do_multiprocessing_label = FixedText(AP[\"Parallel_analysis\"][\"label\"], tip=AP[\"Parallel_analysis\"][\"tips\"],\n                                              night_mode=self.parent().po.all['night_mode'])\n    self.do_multiprocessing.stateChanged.connect(self.do_multiprocessing_is_clicked)\n    self.max_core_nb = Spinbox(min=0, max=256, val=self.parent().po.all['cores'],\n                               night_mode=self.parent().po.all['night_mode'])\n    self.max_core_nb_label = FixedText(AP[\"Proc_max_core_nb\"][\"label\"],\n                                       tip=AP[\"Proc_max_core_nb\"][\"tips\"],\n                                       night_mode=self.parent().po.all['night_mode'])\n    self.min_memory_left = Spinbox(min=0, max=1024, val=self.parent().po.vars['min_ram_free'], decimals=1,\n                                   night_mode=self.parent().po.all['night_mode'])\n    self.min_memory_left_label = FixedText(AP[\"Minimal_RAM_let_free\"][\"label\"],\n                                            tip=AP[\"Minimal_RAM_let_free\"][\"tips\"],\n                                           night_mode=self.parent().po.all['night_mode'])\n\n    self.lose_accuracy_to_save_memory = Checkbox(self.parent().po.vars['lose_accuracy_to_save_memory'])\n    self.lose_accuracy_to_save_memory_label = FixedText(AP[\"Lose_accuracy_to_save_RAM\"][\"label\"],\n                                              tip=AP[\"Lose_accuracy_to_save_RAM\"][\"tips\"],\n                                              night_mode=self.parent().po.all['night_mode'])\n\n    # IV/D/ Arrange widgets in the box\n    self.resources_box_layout.addWidget(self.do_multiprocessing, 0, 0)\n    self.resources_box_layout.addWidget(self.do_multiprocessing_label, 0, 1)\n    self.resources_box_layout.addWidget(self.max_core_nb, 1, 0)\n    self.resources_box_layout.addWidget(self.max_core_nb_label, 1, 1)\n    self.resources_box_layout.addWidget(self.min_memory_left, 2, 0)\n    self.resources_box_layout.addWidget(self.min_memory_left_label, 2, 1)\n    self.resources_box_layout.addWidget(self.lose_accuracy_to_save_memory, 3, 0)\n    self.resources_box_layout.addWidget(self.lose_accuracy_to_save_memory_label, 3, 1)\n    self.resources_box_widget.setLayout(self.resources_box_layout)\n    self.right_col_layout.addWidget(self.resources_box_widget)\n\n    # V/ Fifth box: Video saving\n    # V/A/ Title\n    self.video_saving_label = FixedText('Video saving:', tip=\"\",\n                                     night_mode=self.parent().po.all['night_mode'])\n    self.right_col_layout.addWidget(self.video_saving_label)\n\n    # V/B/ Create the box\n    self.video_saving_layout = QtWidgets.QGridLayout()\n    self.video_saving_widget = QtWidgets.QWidget()\n    self.video_saving_widget.setStyleSheet(boxstylesheet)\n\n    # V/C/ Create widgets\n    self.video_fps = Spinbox(min=0, max=10000, val=self.parent().po.vars['video_fps'], decimals=2,\n                             night_mode=self.parent().po.all['night_mode'])\n    self.video_fps_label = FixedText(AP[\"Video_fps\"][\"label\"], tip=AP[\"Video_fps\"][\"tips\"],\n                                     night_mode=self.parent().po.all['night_mode'])\n    self.keep_unaltered_videos = Checkbox(self.parent().po.vars['keep_unaltered_videos'])\n    self.keep_unaltered_videos_label = FixedText(AP[\"Keep_unaltered_videos\"][\"label\"],\n                                                 tip=AP[\"Keep_unaltered_videos\"][\"tips\"],\n                                                 night_mode=self.parent().po.all['night_mode'])\n    self.save_processed_videos = Checkbox(self.parent().po.vars['save_processed_videos'])\n    self.save_processed_videos_label = FixedText(AP[\"Save_processed_videos\"][\"label\"],\n                                                 tip=AP[\"Save_processed_videos\"][\"tips\"],\n                                                 night_mode=self.parent().po.all['night_mode'])\n\n    # V/D/ Arrange widgets in the box\n    curr_box_row = 0\n    self.video_saving_layout.addWidget(self.video_fps, curr_box_row, 0)\n    self.video_saving_layout.addWidget(self.video_fps_label, curr_box_row, 1)\n    curr_box_row += 1\n    self.video_saving_layout.addWidget(self.keep_unaltered_videos, curr_box_row, 0)\n    self.video_saving_layout.addWidget(self.keep_unaltered_videos_label, curr_box_row, 1)\n    curr_box_row += 1\n    self.video_saving_layout.addWidget(self.save_processed_videos, curr_box_row, 0)\n    self.video_saving_layout.addWidget(self.save_processed_videos_label, curr_box_row, 1)\n    curr_box_row += 1\n    self.video_saving_widget.setLayout(self.video_saving_layout)\n    self.right_col_layout.addWidget(self.video_saving_widget)\n\n    # VII/ Seventh box: csc\n    # VII/A/ Title\n    # VII/C/ Create widgets\n    self.generate_csc_editing()\n    # VII/D/ Arrange widgets in the box\n    self.right_col_layout.addWidget(self.edit_widget)\n\n    # VIII/ Finalize layout and add the night mode option and the ok button\n    self.left_col_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.Maximum, QtWidgets.QSizePolicy.MinimumExpanding))\n    self.right_col_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.Maximum, QtWidgets.QSizePolicy.MinimumExpanding))\n    self.left_col_widget.setLayout(self.left_col_layout)\n    self.right_col_widget.setLayout(self.right_col_layout)\n    self.central_widget = QtWidgets.QWidget()\n    self.central_layout = QtWidgets.QHBoxLayout()\n    self.central_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n    self.left_scroll_table.setWidget(self.left_col_widget)\n    self.left_scroll_table.setWidgetResizable(True)\n    self.left_scroll_table.setParent(self.central_widget)\n    self.central_layout.addWidget(self.left_scroll_table)\n    self.central_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n    self.right_scroll_table.setWidget(self.right_col_widget)\n    self.right_scroll_table.setWidgetResizable(True)\n    self.right_scroll_table.setParent(self.central_widget)\n    self.central_layout.addWidget(self.right_scroll_table)\n    self.central_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n    self.central_widget.setLayout(self.central_layout)\n    self.layout.addWidget(self.central_widget)\n\n    # Last row\n    self.last_row_layout = QtWidgets.QHBoxLayout()\n    self.last_row_widget = QtWidgets.QWidget()\n    self.night_mode_cb = Checkbox(self.parent().po.all['night_mode'])\n    self.night_mode_cb.clicked.connect(self.night_mode_is_clicked)\n    self.night_mode_label = FixedText(AP[\"Night_mode\"][\"label\"], tip=AP[\"Night_mode\"][\"tips\"],\n                                      night_mode=self.parent().po.all['night_mode'])\n    self.reset_all_settings = PButton(AP[\"Reset_all_settings\"][\"label\"], tip=AP[\"Reset_all_settings\"][\"tips\"],\n                                      night_mode=self.parent().po.all['night_mode'])\n    self.reset_all_settings.clicked.connect(self.reset_all_settings_is_clicked)\n    self.message = FixedText('', night_mode=self.parent().po.all['night_mode'])\n    self.cancel = PButton('Cancel', night_mode=self.parent().po.all['night_mode'])\n    self.cancel.clicked.connect(self.cancel_is_clicked)\n    self.ok = PButton('Ok', night_mode=self.parent().po.all['night_mode'])\n    self.ok.clicked.connect(self.ok_is_clicked)\n    self.last_row_layout.addWidget(self.night_mode_cb)\n    self.last_row_layout.addWidget(self.night_mode_label)\n    self.last_row_layout.addWidget(self.reset_all_settings)\n    self.last_row_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n    self.last_row_layout.addWidget(self.message)\n    self.last_row_layout.addWidget(self.cancel)\n    self.last_row_layout.addWidget(self.ok)\n    self.last_row_widget.setLayout(self.last_row_layout)\n    self.layout.addWidget(self.last_row_widget)\n\n    self.setLayout(self.layout)\n</code></pre>"},{"location":"api/cellects/gui/advanced_parameters/#cellects.gui.advanced_parameters.AdvancedParameters.update_csc_editing_display","title":"<code>update_csc_editing_display()</code>","text":"<p>Update the color space conversion (CSC) editing display.</p> <p>This method updates the visibility and values of UI elements related to color space conversions based on the current state of <code>self.csc_dict</code>. It handles the display logic for different color spaces and their combinations, ensuring that the UI reflects the current configuration accurately.</p> Source code in <code>src/cellects/gui/advanced_parameters.py</code> <pre><code>def update_csc_editing_display(self):\n    \"\"\"\n    Update the color space conversion (CSC) editing display.\n\n    This method updates the visibility and values of UI elements related to color\n    space conversions based on the current state of `self.csc_dict`. It handles\n    the display logic for different color spaces and their combinations, ensuring\n    that the UI reflects the current configuration accurately.\n    \"\"\"\n    c_space_order = [\"None\", \"bgr\", \"hsv\", \"hls\", \"lab\", \"luv\", \"yuv\"]\n    remaining_c_spaces = []\n    row_number1 = 0\n    row_number2 = 0\n    if \"PCA\" in self.parent().po.vars['convert_for_motion'].keys():\n        self.row1[0].setCurrentIndex(0)\n        self.row1[0].setVisible(True)\n        for i in range(1, 4):\n            self.row1[i].setVisible(False)\n    else:\n        for i, (k, v) in enumerate(self.parent().po.vars['convert_for_motion'].items()):\n            if k != \"logical\":\n                if k[-1] != \"2\":\n                    if row_number1 == 0:\n                        row_to_change = self.row1\n                    elif row_number1 == 1:\n                        row_to_change = self.row2\n                    elif row_number1 == 2:\n                        row_to_change = self.row3\n                    else:\n                        remaining_c_spaces.append(k + \" \" + str(v))\n                    row_number1 += 1\n                    current_row_number = row_number1\n                else:\n                    if row_number2 == 0:\n                        row_to_change = self.row21\n                    elif row_number2 == 1:\n                        row_to_change = self.row22\n                    elif row_number2 == 2:\n                        row_to_change = self.row23\n                    else:\n                        remaining_c_spaces.append(k + \" \" + str(v))\n                    row_number2 += 1\n                    current_row_number = row_number2\n                    k = k[:-1]\n                if current_row_number &lt;= 3:\n                    row_to_change[0].setCurrentIndex(np.nonzero(np.isin(c_space_order, k))[0][0])\n                    row_to_change[0].setVisible(True)\n                    for i1, i2 in zip([1, 2, 3], [0, 1, 2]):\n                        row_to_change[i1].setValue(v[i2])\n                        row_to_change[i1].setVisible(True)\n                    if current_row_number &lt; 3:\n                        row_to_change[i1 + 1].setVisible(True)\n\n    # If not all color space combinations are filled, put None and 0 in boxes\n    if row_number1 &lt; 3:\n        self.row3[0].setVisible(False)\n        self.row3[0].setCurrentIndex(0)\n        for i1 in [1, 2, 3]:\n            self.row3[i1].setVisible(False)\n            self.row3[i1].setValue(0)\n        if row_number1 &lt; 2:\n            self.row2[0].setVisible(False)\n            self.row2[0].setCurrentIndex(0)\n            for i1 in [1, 2, 3]:\n                self.row2[i1].setVisible(False)\n                self.row2[i1].setValue(0)\n            self.row2[i1 + 1].setVisible(False)\n\n    self.row1[4].setVisible(row_number1 == 1)\n    self.row2[4].setVisible(row_number1 == 2)\n    self.row21[4].setVisible(row_number2 == 1)\n    self.row22[4].setVisible(row_number2 == 2)\n    if row_number2 &gt; 0:\n        self.logical_operator_between_combination_result.setCurrentText(self.parent().po.vars['convert_for_motion']['logical'])\n    if row_number2 == 0:\n        self.logical_operator_between_combination_result.setCurrentText('None')\n        self.logical_operator_between_combination_result.setVisible(False)\n        self.logical_operator_label.setVisible(False)\n        self.row21[0].setVisible(False)\n        self.row21[0].setCurrentIndex(0)\n        for i1 in [1, 2, 3]:\n            self.row21[i1].setVisible(False)\n            self.row21[i1].setValue(0)\n        self.row21[i1 + 1].setVisible(False)\n\n    self.logical_operator_between_combination_result.setVisible((row_number2 &gt; 0))\n    self.logical_operator_label.setVisible((row_number2 &gt; 0))\n\n    if row_number2 &lt; 3:\n        self.row23[0].setVisible(False)\n        self.row23[0].setCurrentIndex(0)\n        for i1 in [1, 2, 3]:\n            self.row23[i1].setVisible(False)\n            self.row23[i1].setValue(0)\n        self.row23[i1 + 1].setVisible(False)\n        self.row22[4].setVisible(False)\n        if row_number2 &lt; 2:\n            self.row22[0].setVisible(False)\n            self.row22[0].setCurrentIndex(0)\n            for i1 in [1, 2, 3]:\n                self.row22[i1].setVisible(False)\n                self.row22[i1].setValue(0)\n            self.row22[i1 + 1].setVisible(False)\n</code></pre>"},{"location":"api/cellects/gui/advanced_parameters/#cellects.gui.advanced_parameters.AdvancedParameters.use_max_size_changed","title":"<code>use_max_size_changed()</code>","text":"<p>Toggles the visibility of max size input fields based on checkbox state.</p> Source code in <code>src/cellects/gui/advanced_parameters.py</code> <pre><code>def use_max_size_changed(self):\n    \"\"\"\n    Toggles the visibility of max size input fields based on checkbox state.\n    \"\"\"\n    do_use_max_size = self.use_max_size.isChecked()\n    self.max_size_for_connection.setVisible(do_use_max_size)\n    self.max_size_for_connection_label.setVisible(do_use_max_size)\n    if do_use_max_size:\n        self.max_size_for_connection.setValue(300)\n</code></pre>"},{"location":"api/cellects/gui/advanced_parameters/#cellects.gui.advanced_parameters.AdvancedParameters.use_min_size_changed","title":"<code>use_min_size_changed()</code>","text":"<p>Updates the visibility and value of UI elements based on whether a checkbox is checked.</p> Source code in <code>src/cellects/gui/advanced_parameters.py</code> <pre><code>def use_min_size_changed(self):\n    \"\"\"\n    Updates the visibility and value of UI elements based on whether a checkbox is checked.\n    \"\"\"\n    do_use_min_size = self.use_min_size.isChecked()\n    self.min_size_for_connection.setVisible(do_use_min_size)\n    self.min_size_for_connection_label.setVisible(do_use_min_size)\n    if do_use_min_size:\n        self.min_size_for_connection.setValue(30)\n</code></pre>"},{"location":"api/cellects/gui/cellects/","title":"<code>cellects.gui.cellects</code>","text":""},{"location":"api/cellects/gui/cellects/#cellects.gui.cellects","title":"<code>cellects.gui.cellects</code>","text":"<p>Main window for Cellects application managing stacked workflow steps.</p> <p>This module implements a user-assisted image analysis workflow using a QStackedWidget to navigate between configuration  and analysis phases. It provides windows for data setup, image/video processing, output requirements, and  advanced parameters. Automatic algorithm suggestions are offered at each step while allowing full user customization.  Uses SaveAllVarsThread in background operations to maintain UI responsiveness.</p> <p>Main Components CellectsMainWidget : Central stacked widget managing workflow navigation</p> <p>Notes Uses QThread for background operations to maintain UI responsiveness.</p>"},{"location":"api/cellects/gui/cellects/#cellects.gui.cellects.CellectsMainWidget","title":"<code>CellectsMainWidget</code>","text":"<p>               Bases: <code>QStackedWidget</code></p> <p>Main widget: this is the main window of the Cellects application.</p> Source code in <code>src/cellects/gui/cellects.py</code> <pre><code>class CellectsMainWidget(QtWidgets.QStackedWidget):\n    \"\"\" Main widget: this is the main window of the Cellects application. \"\"\"\n\n    def __init__(self):\n        \"\"\"\n\n        Initializes the Cellects application window and sets up initial state.\n\n        Sets the title, dimensions, and default values for various attributes\n        required to manage the GUI's state and display settings.\n        Initializes a ProgramOrganizer object and loads its variable dictionary.\n\n        Attributes\n        ----------\n        pre_processing_done : bool\n            Indicates whether pre-processing has been completed.\n        last_is_first : bool\n            Tracks if the last operation was the first in sequence.\n        last_tab : str\n            The most recently accessed tab name (default: \"data_specifications\").\n        screen_height : int\n            Height of the monitor in pixels.\n        screen_width : int\n            Width of the monitor in pixels.\n        im_max_width : int\n            Maximum width allowed for displayed images (default: 570).\n        im_max_height : int\n            Maximum height allowed for displayed images (default: 266).\n        image_window_width_diff : int\n            Difference in width between image window and max image size.\n        image_window_height_diff : int\n            Difference in height between image window and max image size.\n        image_to_display : ndarray\n            Placeholder for the image to be displayed (initialized as zeros).\n        i : int\n            Counter or index used in the application.\n        po : ProgramOrganizer\n            Instance managing the organization and variables of the program.\n        \"\"\"\n        super().__init__()\n\n        self.setWindowTitle('Cellects')\n        self.pre_processing_done: bool = False\n        self.last_is_first: bool = True\n        self.last_tab: str = \"data_specifications\"\n        self.pre_processing_done: bool = False\n        self.screen_height = get_monitors()[0].height\n        self.screen_width = get_monitors()[0].width\n        self.im_max_width = 570\n        self.im_max_height = 266\n        self.image_window_width_diff = 1380 - 570\n        self.image_window_height_diff = 750 - 266\n        self.image_to_display = np.zeros((self.im_max_height, self.im_max_width, 3), np.uint8)\n        self.i = 1\n        self.po = ProgramOrganizer()\n\n        self.po.load_variable_dict()\n        self.resize(1380, 750)\n\n    def instantiate(self):\n        \"\"\"\n        Initiates the Cellects application by setting up the main window and starting various threads.\n\n        Extended Description\n        ---------------------\n        This method is responsible for initializing the Cellects application. It sets up the main window, creates necessary widgets, and starts the required threads for background operations.\n\n        Other Parameters\n        ----------------\n        night_mode : bool, optional\n            Indicates whether the application should run in night mode. This parameter is managed by another part of\n            the code and should not be set directly.\n        \"\"\"\n        logging.info(\"Instantiate Cellects\")\n        self.firstwindow = FirstWindow(\n            self,\n            night_mode=self.po.all['night_mode'])\n        self.insertWidget(0, self.firstwindow)\n\n        self.instantiate_widgets()\n\n        self.thread_dict = {}\n        self.thread_dict['SaveAllVars'] = SaveAllVarsThread(self)\n        self.change_widget(0)\n        self.center()\n\n    def instantiate_widgets(self, several_folder_included: bool=True):\n        \"\"\"\n        Instantiate various windows for the application's GUI.\n\n        This function configures the main GUI windows for image and video analysis,\n        output requirements, and advanced parameters.\n\n        Parameters\n        ----------\n        several_folder_included: bool, optional\n            A flag to determine whether the `IfSeveralFoldersWindow` should be instantiated. Default is `True`.\n        \"\"\"\n        logging.info(\"Widgets are instantiating\")\n        if several_folder_included:\n            self.ifseveralfolderswindow = IfSeveralFoldersWindow(self, night_mode=self.po.all['night_mode'])\n            self.insertWidget(1, self.ifseveralfolderswindow)\n        self.imageanalysiswindow = ImageAnalysisWindow(self, night_mode=self.po.all['night_mode'])\n        self.insertWidget(2, self.imageanalysiswindow)\n\n        self.videoanalysiswindow = VideoAnalysisWindow(self, night_mode=self.po.all['night_mode'])\n        self.insertWidget(3, self.videoanalysiswindow)\n\n        self.requiredoutputwindow = RequiredOutput(self, night_mode=self.po.all['night_mode'])\n        self.insertWidget(4, self.requiredoutputwindow)\n\n        self.advancedparameterswindow = AdvancedParameters(self, night_mode=self.po.all['night_mode'])\n        self.insertWidget(5, self.advancedparameterswindow)\n\n\n    def update_widget(self, idx: int, widget_to_call):\n        \"\"\" Update widget at its position (idx) in the stack \"\"\"\n        self.insertWidget(idx, widget_to_call)\n\n    def change_widget(self, idx: int):\n        \"\"\" Display a widget using its position (idx) in the stack \"\"\"\n        self.setCurrentIndex(idx)  # Index that new widget\n        self.updateGeometry()\n        self.currentWidget().setVisible(True)\n        if idx == 3 or idx == 5:\n            self.currentWidget().display_conditionally_visible_widgets()\n\n    def center(self):\n        \"\"\"\n        Centers the window on the screen.\n\n        Moves the window to the center of the available screen geometry.\n        Allows users to always see the application's windows in a consistent\n        position, regardless of screen resolution or window size.\n        \"\"\"\n        qr = self.frameGeometry()\n        # cp = QtWidgets.QDesktopWidget().availableGeometry().center()  # PyQt 5\n        cp = QtGui.QGuiApplication.primaryScreen().availableGeometry().center()  # Pyside 6\n        qr.moveCenter(cp)\n        self.move(qr.topLeft())\n\n    def closeEvent(self, event):\n        \"\"\"\n        Close the application window and handle cleanup.\n\n        Parameters\n        ----------\n        event : QCloseEvent\n            The close event that triggered this function.\n\n        Notes\n        -----\n        This function does not return any value and is intended for event\n        handling purposes only.\n        \"\"\"\n        reply = QtWidgets.QMessageBox.question(\n            self,\n            'Closing Cellects',\n            'Are you sure you want to exit?',\n            QtWidgets.QMessageBox.Yes | QtWidgets.QMessageBox.No,\n            QtWidgets.QMessageBox.No)\n\n        if reply == QtWidgets.QMessageBox.Yes:\n            for _, thread in self.imageanalysiswindow.thread_dict.items():\n                thread.wait()\n            for _, thread  in self.ifseveralfolderswindow.thread_dict.items():\n                thread.wait()\n            for _, thread  in self.videoanalysiswindow.thread_dict.items():\n                thread.wait()\n            for _, thread  in self.firstwindow.thread_dict.items():\n                thread.wait()\n            logging.info(\"Closing main window.\")\n            event.accept()\n        else:\n            event.ignore()\n</code></pre>"},{"location":"api/cellects/gui/cellects/#cellects.gui.cellects.CellectsMainWidget.__init__","title":"<code>__init__()</code>","text":"<p>Initializes the Cellects application window and sets up initial state.</p> <p>Sets the title, dimensions, and default values for various attributes required to manage the GUI's state and display settings. Initializes a ProgramOrganizer object and loads its variable dictionary.</p> <p>Attributes:</p> Name Type Description <code>pre_processing_done</code> <code>bool</code> <p>Indicates whether pre-processing has been completed.</p> <code>last_is_first</code> <code>bool</code> <p>Tracks if the last operation was the first in sequence.</p> <code>last_tab</code> <code>str</code> <p>The most recently accessed tab name (default: \"data_specifications\").</p> <code>screen_height</code> <code>int</code> <p>Height of the monitor in pixels.</p> <code>screen_width</code> <code>int</code> <p>Width of the monitor in pixels.</p> <code>im_max_width</code> <code>int</code> <p>Maximum width allowed for displayed images (default: 570).</p> <code>im_max_height</code> <code>int</code> <p>Maximum height allowed for displayed images (default: 266).</p> <code>image_window_width_diff</code> <code>int</code> <p>Difference in width between image window and max image size.</p> <code>image_window_height_diff</code> <code>int</code> <p>Difference in height between image window and max image size.</p> <code>image_to_display</code> <code>ndarray</code> <p>Placeholder for the image to be displayed (initialized as zeros).</p> <code>i</code> <code>int</code> <p>Counter or index used in the application.</p> <code>po</code> <code>ProgramOrganizer</code> <p>Instance managing the organization and variables of the program.</p> Source code in <code>src/cellects/gui/cellects.py</code> <pre><code>def __init__(self):\n    \"\"\"\n\n    Initializes the Cellects application window and sets up initial state.\n\n    Sets the title, dimensions, and default values for various attributes\n    required to manage the GUI's state and display settings.\n    Initializes a ProgramOrganizer object and loads its variable dictionary.\n\n    Attributes\n    ----------\n    pre_processing_done : bool\n        Indicates whether pre-processing has been completed.\n    last_is_first : bool\n        Tracks if the last operation was the first in sequence.\n    last_tab : str\n        The most recently accessed tab name (default: \"data_specifications\").\n    screen_height : int\n        Height of the monitor in pixels.\n    screen_width : int\n        Width of the monitor in pixels.\n    im_max_width : int\n        Maximum width allowed for displayed images (default: 570).\n    im_max_height : int\n        Maximum height allowed for displayed images (default: 266).\n    image_window_width_diff : int\n        Difference in width between image window and max image size.\n    image_window_height_diff : int\n        Difference in height between image window and max image size.\n    image_to_display : ndarray\n        Placeholder for the image to be displayed (initialized as zeros).\n    i : int\n        Counter or index used in the application.\n    po : ProgramOrganizer\n        Instance managing the organization and variables of the program.\n    \"\"\"\n    super().__init__()\n\n    self.setWindowTitle('Cellects')\n    self.pre_processing_done: bool = False\n    self.last_is_first: bool = True\n    self.last_tab: str = \"data_specifications\"\n    self.pre_processing_done: bool = False\n    self.screen_height = get_monitors()[0].height\n    self.screen_width = get_monitors()[0].width\n    self.im_max_width = 570\n    self.im_max_height = 266\n    self.image_window_width_diff = 1380 - 570\n    self.image_window_height_diff = 750 - 266\n    self.image_to_display = np.zeros((self.im_max_height, self.im_max_width, 3), np.uint8)\n    self.i = 1\n    self.po = ProgramOrganizer()\n\n    self.po.load_variable_dict()\n    self.resize(1380, 750)\n</code></pre>"},{"location":"api/cellects/gui/cellects/#cellects.gui.cellects.CellectsMainWidget.center","title":"<code>center()</code>","text":"<p>Centers the window on the screen.</p> <p>Moves the window to the center of the available screen geometry. Allows users to always see the application's windows in a consistent position, regardless of screen resolution or window size.</p> Source code in <code>src/cellects/gui/cellects.py</code> <pre><code>def center(self):\n    \"\"\"\n    Centers the window on the screen.\n\n    Moves the window to the center of the available screen geometry.\n    Allows users to always see the application's windows in a consistent\n    position, regardless of screen resolution or window size.\n    \"\"\"\n    qr = self.frameGeometry()\n    # cp = QtWidgets.QDesktopWidget().availableGeometry().center()  # PyQt 5\n    cp = QtGui.QGuiApplication.primaryScreen().availableGeometry().center()  # Pyside 6\n    qr.moveCenter(cp)\n    self.move(qr.topLeft())\n</code></pre>"},{"location":"api/cellects/gui/cellects/#cellects.gui.cellects.CellectsMainWidget.change_widget","title":"<code>change_widget(idx)</code>","text":"<p>Display a widget using its position (idx) in the stack</p> Source code in <code>src/cellects/gui/cellects.py</code> <pre><code>def change_widget(self, idx: int):\n    \"\"\" Display a widget using its position (idx) in the stack \"\"\"\n    self.setCurrentIndex(idx)  # Index that new widget\n    self.updateGeometry()\n    self.currentWidget().setVisible(True)\n    if idx == 3 or idx == 5:\n        self.currentWidget().display_conditionally_visible_widgets()\n</code></pre>"},{"location":"api/cellects/gui/cellects/#cellects.gui.cellects.CellectsMainWidget.closeEvent","title":"<code>closeEvent(event)</code>","text":"<p>Close the application window and handle cleanup.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>QCloseEvent</code> <p>The close event that triggered this function.</p> required Notes <p>This function does not return any value and is intended for event handling purposes only.</p> Source code in <code>src/cellects/gui/cellects.py</code> <pre><code>def closeEvent(self, event):\n    \"\"\"\n    Close the application window and handle cleanup.\n\n    Parameters\n    ----------\n    event : QCloseEvent\n        The close event that triggered this function.\n\n    Notes\n    -----\n    This function does not return any value and is intended for event\n    handling purposes only.\n    \"\"\"\n    reply = QtWidgets.QMessageBox.question(\n        self,\n        'Closing Cellects',\n        'Are you sure you want to exit?',\n        QtWidgets.QMessageBox.Yes | QtWidgets.QMessageBox.No,\n        QtWidgets.QMessageBox.No)\n\n    if reply == QtWidgets.QMessageBox.Yes:\n        for _, thread in self.imageanalysiswindow.thread_dict.items():\n            thread.wait()\n        for _, thread  in self.ifseveralfolderswindow.thread_dict.items():\n            thread.wait()\n        for _, thread  in self.videoanalysiswindow.thread_dict.items():\n            thread.wait()\n        for _, thread  in self.firstwindow.thread_dict.items():\n            thread.wait()\n        logging.info(\"Closing main window.\")\n        event.accept()\n    else:\n        event.ignore()\n</code></pre>"},{"location":"api/cellects/gui/cellects/#cellects.gui.cellects.CellectsMainWidget.instantiate","title":"<code>instantiate()</code>","text":"<p>Initiates the Cellects application by setting up the main window and starting various threads.</p> Extended Description <p>This method is responsible for initializing the Cellects application. It sets up the main window, creates necessary widgets, and starts the required threads for background operations.</p> <p>Other Parameters:</p> Name Type Description <code>night_mode</code> <code>bool</code> <p>Indicates whether the application should run in night mode. This parameter is managed by another part of the code and should not be set directly.</p> Source code in <code>src/cellects/gui/cellects.py</code> <pre><code>def instantiate(self):\n    \"\"\"\n    Initiates the Cellects application by setting up the main window and starting various threads.\n\n    Extended Description\n    ---------------------\n    This method is responsible for initializing the Cellects application. It sets up the main window, creates necessary widgets, and starts the required threads for background operations.\n\n    Other Parameters\n    ----------------\n    night_mode : bool, optional\n        Indicates whether the application should run in night mode. This parameter is managed by another part of\n        the code and should not be set directly.\n    \"\"\"\n    logging.info(\"Instantiate Cellects\")\n    self.firstwindow = FirstWindow(\n        self,\n        night_mode=self.po.all['night_mode'])\n    self.insertWidget(0, self.firstwindow)\n\n    self.instantiate_widgets()\n\n    self.thread_dict = {}\n    self.thread_dict['SaveAllVars'] = SaveAllVarsThread(self)\n    self.change_widget(0)\n    self.center()\n</code></pre>"},{"location":"api/cellects/gui/cellects/#cellects.gui.cellects.CellectsMainWidget.instantiate_widgets","title":"<code>instantiate_widgets(several_folder_included=True)</code>","text":"<p>Instantiate various windows for the application's GUI.</p> <p>This function configures the main GUI windows for image and video analysis, output requirements, and advanced parameters.</p> <p>Parameters:</p> Name Type Description Default <code>several_folder_included</code> <code>bool</code> <p>A flag to determine whether the <code>IfSeveralFoldersWindow</code> should be instantiated. Default is <code>True</code>.</p> <code>True</code> Source code in <code>src/cellects/gui/cellects.py</code> <pre><code>def instantiate_widgets(self, several_folder_included: bool=True):\n    \"\"\"\n    Instantiate various windows for the application's GUI.\n\n    This function configures the main GUI windows for image and video analysis,\n    output requirements, and advanced parameters.\n\n    Parameters\n    ----------\n    several_folder_included: bool, optional\n        A flag to determine whether the `IfSeveralFoldersWindow` should be instantiated. Default is `True`.\n    \"\"\"\n    logging.info(\"Widgets are instantiating\")\n    if several_folder_included:\n        self.ifseveralfolderswindow = IfSeveralFoldersWindow(self, night_mode=self.po.all['night_mode'])\n        self.insertWidget(1, self.ifseveralfolderswindow)\n    self.imageanalysiswindow = ImageAnalysisWindow(self, night_mode=self.po.all['night_mode'])\n    self.insertWidget(2, self.imageanalysiswindow)\n\n    self.videoanalysiswindow = VideoAnalysisWindow(self, night_mode=self.po.all['night_mode'])\n    self.insertWidget(3, self.videoanalysiswindow)\n\n    self.requiredoutputwindow = RequiredOutput(self, night_mode=self.po.all['night_mode'])\n    self.insertWidget(4, self.requiredoutputwindow)\n\n    self.advancedparameterswindow = AdvancedParameters(self, night_mode=self.po.all['night_mode'])\n    self.insertWidget(5, self.advancedparameterswindow)\n</code></pre>"},{"location":"api/cellects/gui/cellects/#cellects.gui.cellects.CellectsMainWidget.update_widget","title":"<code>update_widget(idx, widget_to_call)</code>","text":"<p>Update widget at its position (idx) in the stack</p> Source code in <code>src/cellects/gui/cellects.py</code> <pre><code>def update_widget(self, idx: int, widget_to_call):\n    \"\"\" Update widget at its position (idx) in the stack \"\"\"\n    self.insertWidget(idx, widget_to_call)\n</code></pre>"},{"location":"api/cellects/gui/custom_widgets/","title":"<code>cellects.gui.custom_widgets</code>","text":""},{"location":"api/cellects/gui/custom_widgets/#cellects.gui.custom_widgets","title":"<code>cellects.gui.custom_widgets</code>","text":"<p>This module contains all modified/simplified widgets from PySide6 It is made to be easier to use and to be consistant in terms of colors and sizes.</p>"},{"location":"api/cellects/gui/custom_widgets/#cellects.gui.custom_widgets.MainTabsWidget","title":"<code>MainTabsWidget</code>","text":"<p>               Bases: <code>QPushButton</code></p> <p>A custom QPushButton that mimics an explorer tab appearance.</p> <p>Features: - Customizable text - Night mode support - Three states: not_in_use (grey border), in_use (black border), not_usable (grey text) - Tooltip support for not_usable state</p> Source code in <code>src/cellects/gui/custom_widgets.py</code> <pre><code>class MainTabsWidget(QtWidgets.QPushButton):\n    \"\"\"\n    A custom QPushButton that mimics an explorer tab appearance.\n\n    Features:\n    - Customizable text\n    - Night mode support\n    - Three states: not_in_use (grey border), in_use (black border), not_usable (grey text)\n    - Tooltip support for not_usable state\n    \"\"\"\n\n    def __init__(self, text=\"\", night_mode=False, parent=None):\n        super().__init__()\n\n        self.setText(text)\n        self.state = \"not_in_use\"  # States: \"not_in_use\", \"in_use\", \"not_usable\"\n        self.setFont(buttonfont)\n\n        # Set basic properties\n        self.setSizePolicy(QtWidgets.QSizePolicy.Minimum, QtWidgets.QSizePolicy.Fixed)\n        self.setFixedHeight(35)\n        self.setCursor(QtCore.Qt.CursorShape.PointingHandCursor)\n\n        self.night_mode_switch(night_mode)\n\n\n    def night_mode_switch(self, night_mode):\n        self._night_mode = night_mode\n        self.update_style()\n\n    def update_style(self):\n        \"\"\"Update the widget's stylesheet\u00b5\"\"\"\n\n        if self.state == \"not_usable\":\n            tab_text_color = \"#888888\"\n            self.setCursor(QtCore.Qt.CursorShape.ForbiddenCursor)\n        else:\n            if self._night_mode:\n                tab_text_color = night_text_Color\n            else:\n                tab_text_color = textColor\n            if self.state == \"not_in_use\":\n                self.setCursor(QtCore.Qt.CursorShape.PointingHandCursor)\n        if self.state == \"in_use\":\n            border_width = 2\n            if self._night_mode:\n                tab_border = f\"{border_width}px solid #adadad\"\n            else:\n                tab_border = f\"{border_width}px solid #323241\"\n        else:\n            border_width = 1\n            if self._night_mode:\n                tab_border = f\"{border_width}px solid #323241\"\n            else:\n                tab_border = f\"{border_width}px solid #adadad\"\n\n        if self._night_mode:\n            style = {\"buttoncolor\": night_button_color, \"buttontextColor\": tab_text_color, \"border\": tab_border,\n                     \"font_family\": tabfont, \"font_size\": \"22pt\", #\"font_weight\": \"bold\",\n                     \"border-top-color\": \"#323241\", \"border-right-color\": \"#323241\", # invisible\n                     \"border-top-left-radius\": \"10\", \"border-bottom-left-radius\": \"1\"}\n        else:\n            style = {\"buttoncolor\": buttoncolor, \"buttontextColor\": tab_text_color, \"border\": tab_border,\n                     \"font_family\": tabfont, \"font_size\": \"22pt\", #\"font_weight\": \"bold\",\n                     \"border-top-color\": \"#ffffff\", \"border-right-color\": \"#ffffff\", # invisible\n                     \"border-top-left-radius\": \"10\", \"border-bottom-left-radius\": \"1\"\n                     }\n        self.setStyleSheet(\n            \"background-color: %s; color: %s; border: %s; font-family: %s; font-size: %s;  border-top-color: %s; border-right-color: %s; border-top-left-radius: %s; border-bottom-left-radius: %s\" % tuple(style.values()))\n\n\n    def set_in_use(self):\n        \"\"\"Set the tab to 'in_use' state with black border.\"\"\"\n        self.state = \"in_use\"\n        self.setToolTip(\"\")  # Clear any tooltip\n        self.update_style()\n\n    def set_not_in_use(self):\n        \"\"\"Set the tab to 'not_in_use' state with grey border.\"\"\"\n        self.state = \"not_in_use\"\n        self.setToolTip(\"\")  # Clear any tooltip\n        self.update_style()\n\n    def set_not_usable(self, tooltip_text=\"This tab is not usable\"):\n        \"\"\"\n        Set the tab to 'not_usable' state with grey text.\n\n        Args:\n            tooltip_text (str): Custom tooltip text to show when hovering\n        \"\"\"\n        self.state = \"not_usable\"\n        self.setToolTip(tooltip_text)\n        self.update_style()\n\n    def get_state(self):\n        \"\"\"Get the current state of the tab.\"\"\"\n        return self.state\n\n    def is_night_mode(self):\n        \"\"\"Check if night mode is enabled.\"\"\"\n        return self._night_mode\n</code></pre>"},{"location":"api/cellects/gui/custom_widgets/#cellects.gui.custom_widgets.MainTabsWidget.get_state","title":"<code>get_state()</code>","text":"<p>Get the current state of the tab.</p> Source code in <code>src/cellects/gui/custom_widgets.py</code> <pre><code>def get_state(self):\n    \"\"\"Get the current state of the tab.\"\"\"\n    return self.state\n</code></pre>"},{"location":"api/cellects/gui/custom_widgets/#cellects.gui.custom_widgets.MainTabsWidget.is_night_mode","title":"<code>is_night_mode()</code>","text":"<p>Check if night mode is enabled.</p> Source code in <code>src/cellects/gui/custom_widgets.py</code> <pre><code>def is_night_mode(self):\n    \"\"\"Check if night mode is enabled.\"\"\"\n    return self._night_mode\n</code></pre>"},{"location":"api/cellects/gui/custom_widgets/#cellects.gui.custom_widgets.MainTabsWidget.set_in_use","title":"<code>set_in_use()</code>","text":"<p>Set the tab to 'in_use' state with black border.</p> Source code in <code>src/cellects/gui/custom_widgets.py</code> <pre><code>def set_in_use(self):\n    \"\"\"Set the tab to 'in_use' state with black border.\"\"\"\n    self.state = \"in_use\"\n    self.setToolTip(\"\")  # Clear any tooltip\n    self.update_style()\n</code></pre>"},{"location":"api/cellects/gui/custom_widgets/#cellects.gui.custom_widgets.MainTabsWidget.set_not_in_use","title":"<code>set_not_in_use()</code>","text":"<p>Set the tab to 'not_in_use' state with grey border.</p> Source code in <code>src/cellects/gui/custom_widgets.py</code> <pre><code>def set_not_in_use(self):\n    \"\"\"Set the tab to 'not_in_use' state with grey border.\"\"\"\n    self.state = \"not_in_use\"\n    self.setToolTip(\"\")  # Clear any tooltip\n    self.update_style()\n</code></pre>"},{"location":"api/cellects/gui/custom_widgets/#cellects.gui.custom_widgets.MainTabsWidget.set_not_usable","title":"<code>set_not_usable(tooltip_text='This tab is not usable')</code>","text":"<p>Set the tab to 'not_usable' state with grey text.</p> <p>Args:     tooltip_text (str): Custom tooltip text to show when hovering</p> Source code in <code>src/cellects/gui/custom_widgets.py</code> <pre><code>def set_not_usable(self, tooltip_text=\"This tab is not usable\"):\n    \"\"\"\n    Set the tab to 'not_usable' state with grey text.\n\n    Args:\n        tooltip_text (str): Custom tooltip text to show when hovering\n    \"\"\"\n    self.state = \"not_usable\"\n    self.setToolTip(tooltip_text)\n    self.update_style()\n</code></pre>"},{"location":"api/cellects/gui/custom_widgets/#cellects.gui.custom_widgets.MainTabsWidget.update_style","title":"<code>update_style()</code>","text":"<p>Update the widget's stylesheet\u00b5</p> Source code in <code>src/cellects/gui/custom_widgets.py</code> <pre><code>def update_style(self):\n    \"\"\"Update the widget's stylesheet\u00b5\"\"\"\n\n    if self.state == \"not_usable\":\n        tab_text_color = \"#888888\"\n        self.setCursor(QtCore.Qt.CursorShape.ForbiddenCursor)\n    else:\n        if self._night_mode:\n            tab_text_color = night_text_Color\n        else:\n            tab_text_color = textColor\n        if self.state == \"not_in_use\":\n            self.setCursor(QtCore.Qt.CursorShape.PointingHandCursor)\n    if self.state == \"in_use\":\n        border_width = 2\n        if self._night_mode:\n            tab_border = f\"{border_width}px solid #adadad\"\n        else:\n            tab_border = f\"{border_width}px solid #323241\"\n    else:\n        border_width = 1\n        if self._night_mode:\n            tab_border = f\"{border_width}px solid #323241\"\n        else:\n            tab_border = f\"{border_width}px solid #adadad\"\n\n    if self._night_mode:\n        style = {\"buttoncolor\": night_button_color, \"buttontextColor\": tab_text_color, \"border\": tab_border,\n                 \"font_family\": tabfont, \"font_size\": \"22pt\", #\"font_weight\": \"bold\",\n                 \"border-top-color\": \"#323241\", \"border-right-color\": \"#323241\", # invisible\n                 \"border-top-left-radius\": \"10\", \"border-bottom-left-radius\": \"1\"}\n    else:\n        style = {\"buttoncolor\": buttoncolor, \"buttontextColor\": tab_text_color, \"border\": tab_border,\n                 \"font_family\": tabfont, \"font_size\": \"22pt\", #\"font_weight\": \"bold\",\n                 \"border-top-color\": \"#ffffff\", \"border-right-color\": \"#ffffff\", # invisible\n                 \"border-top-left-radius\": \"10\", \"border-bottom-left-radius\": \"1\"\n                 }\n    self.setStyleSheet(\n        \"background-color: %s; color: %s; border: %s; font-family: %s; font-size: %s;  border-top-color: %s; border-right-color: %s; border-top-left-radius: %s; border-bottom-left-radius: %s\" % tuple(style.values()))\n</code></pre>"},{"location":"api/cellects/gui/custom_widgets/#cellects.gui.custom_widgets.PButton","title":"<code>PButton</code>","text":"<p>               Bases: <code>QPushButton</code></p> Source code in <code>src/cellects/gui/custom_widgets.py</code> <pre><code>class PButton(QtWidgets.QPushButton):\n    def __init__(self, text, fade=True, tip=None, night_mode=False):\n        \"\"\"\n\n        self.setStyleSheet(\"background-color: rgb(107, 145, 202);\\n\"\n                                \"border-color: rgb(255, 255, 255);\\n\"\n                                \"color: rgb(0, 0, 0);\\n\"\n                                \"font: 17pt \\\"Britannic Bold\\\";\")\n        :param text:\n        \"\"\"\n        super().__init__()\n        self.setText(text)\n        self.setToolTip(tip)\n        self.night_mode_switch(night_mode)\n        self.setFont(buttonfont)\n        self.setSizePolicy(QtWidgets.QSizePolicy.Fixed, QtWidgets.QSizePolicy.Fixed)\n        self.setFixedWidth(len(text)*15 + 25)\n        self.setCursor(QtCore.Qt.CursorShape.PointingHandCursor)\n\n    def night_mode_switch(self, night_mode):\n        if night_mode:\n            self.style = {\"buttoncolor\": night_button_color, \"buttontextColor\": night_text_Color, \"buttonborder\": night_button_border,\n                          \"buttonangles\": buttonangles}\n        else:\n            self.style = {\"buttoncolor\": buttoncolor, \"buttontextColor\": textColor, \"buttonborder\": buttonborder,\n                          \"buttonangles\": buttonangles}\n        self.update_style()\n\n    def event_filter(self, event):\n        if event.type() == QtCore.QEvent.MouseMove:\n            if event.buttons() == QtCore.Qt.NoButton:\n                self.fade()\n            else:\n                self.unfade()\n\n    def update_style(self):\n        self.setStyleSheet(\n            \"background-color: %s; color: %s; border: %s; border-radius: %s\" % tuple(self.style.values()))\n\n    def color(self, color):\n        self.style[\"buttoncolor\"] = color\n        self.update_style()\n\n    def textcolor(self, textcolor):\n        self.style[\"buttontextColor\"] = textcolor\n        self.update_style()\n\n    def border(self, border):\n        self.style[\"buttonborder\"] = border\n        self.update_style()\n\n    def angles(self, angles):\n        self.style[\"buttonangles\"] = angles\n        self.update_style()\n\n    def fade(self):\n        self.setWindowOpacity(0.5)\n        self.setStyleSheet(\"background-color: %s; color: %s; border: %s; border-radius: %s\" % (buttonclickedcolor, textColor, buttonborder, buttonangles))\n        QtCore.QTimer.singleShot(300, self.unfade)\n\n    def unfade(self):\n        self.setWindowOpacity(1)\n        self.setStyleSheet(\"background-color: %s; color: %s; border: %s; border-radius: %s\" % (buttoncolor, textColor, buttonborder, buttonangles))\n</code></pre>"},{"location":"api/cellects/gui/custom_widgets/#cellects.gui.custom_widgets.PButton.__init__","title":"<code>__init__(text, fade=True, tip=None, night_mode=False)</code>","text":"<pre><code>    self.setStyleSheet(\"background-color: rgb(107, 145, 202);\n</code></pre> <p>\"                                 \"border-color: rgb(255, 255, 255); \"                                 \"color: rgb(0, 0, 0); \"                                 \"font: 17pt \"Britannic Bold\";\")         :param text:</p> Source code in <code>src/cellects/gui/custom_widgets.py</code> <pre><code>def __init__(self, text, fade=True, tip=None, night_mode=False):\n    \"\"\"\n\n    self.setStyleSheet(\"background-color: rgb(107, 145, 202);\\n\"\n                            \"border-color: rgb(255, 255, 255);\\n\"\n                            \"color: rgb(0, 0, 0);\\n\"\n                            \"font: 17pt \\\"Britannic Bold\\\";\")\n    :param text:\n    \"\"\"\n    super().__init__()\n    self.setText(text)\n    self.setToolTip(tip)\n    self.night_mode_switch(night_mode)\n    self.setFont(buttonfont)\n    self.setSizePolicy(QtWidgets.QSizePolicy.Fixed, QtWidgets.QSizePolicy.Fixed)\n    self.setFixedWidth(len(text)*15 + 25)\n    self.setCursor(QtCore.Qt.CursorShape.PointingHandCursor)\n</code></pre>"},{"location":"api/cellects/gui/custom_widgets/#cellects.gui.custom_widgets.WindowType","title":"<code>WindowType</code>","text":"<p>               Bases: <code>QWidget</code></p> Source code in <code>src/cellects/gui/custom_widgets.py</code> <pre><code>class WindowType(QtWidgets.QWidget):\n    resized = QtCore.Signal()\n    def __init__(self, parent, night_mode=False):\n        super().__init__()\n\n        self.thread_dict = {}\n        self.setVisible(False)\n        self.setParent(parent)\n        self.frame = QtWidgets.QFrame(self)\n        self.frame.setGeometry(QtCore.QRect(0, 0, self.parent().screen_width, self.parent().screen_height))\n        self.display_image = None\n        self.setFont(QFont(textfont, textsize, QFont.Medium))\n        self.night_mode_switch(night_mode)\n\n    def resizeEvent(self, event):\n        '''\n        # Use this signal to detect a resize event and call center window function\n        :param event:\n        :return:\n        '''\n        self.resized.emit()\n        if self.display_image is not None:\n            win_width, win_height = self.size().width(), self.size().height()\n            self.display_image.max_width = win_width - self.parent().image_window_width_diff\n            self.display_image.max_height = win_height - self.parent().image_window_height_diff\n            if self.display_image.max_width * self.display_image.height_width_ratio &lt; self.display_image.max_height:\n                self.display_image.scaled_shape = [round(self.display_image.max_width * self.display_image.height_width_ratio), self.display_image.max_width]\n            else:\n                self.display_image.scaled_shape = [self.display_image.max_height, np.round(self.display_image.max_height / self.display_image.height_width_ratio)]\n\n            self.display_image.setMaximumHeight(self.display_image.scaled_shape[0])\n            self.display_image.setMaximumWidth(self.display_image.scaled_shape[1])\n        return super(WindowType, self).resizeEvent(event)\n\n    def center_window(self):\n        self.parent().center()\n\n    def night_mode_switch(self, night_mode):\n        if night_mode:\n            self.setStyleSheet(\n                \"background-color: %s; color: %s; font: %s; selection-color: %s; selection-background-color: %s\" % (\n                night_background_color, night_text_Color, f\"{textsize}pt {textfont};\",\n                night_selection_color, night_selection_background_color))\n        else:\n            self.setStyleSheet(\n                \"background-color: %s; color: %s; font: %s; selection-color: %s; selection-background-color: %s\" % (\n                backgroundcolor, textColor, f\"{textsize}pt {textfont};\", selectioncolor,\n                selectionbackgroundcolor))\n</code></pre>"},{"location":"api/cellects/gui/custom_widgets/#cellects.gui.custom_widgets.WindowType.resizeEvent","title":"<code>resizeEvent(event)</code>","text":""},{"location":"api/cellects/gui/custom_widgets/#cellects.gui.custom_widgets.WindowType.resizeEvent--use-this-signal-to-detect-a-resize-event-and-call-center-window-function","title":"Use this signal to detect a resize event and call center window function","text":"<p>:param event: :return:</p> Source code in <code>src/cellects/gui/custom_widgets.py</code> <pre><code>def resizeEvent(self, event):\n    '''\n    # Use this signal to detect a resize event and call center window function\n    :param event:\n    :return:\n    '''\n    self.resized.emit()\n    if self.display_image is not None:\n        win_width, win_height = self.size().width(), self.size().height()\n        self.display_image.max_width = win_width - self.parent().image_window_width_diff\n        self.display_image.max_height = win_height - self.parent().image_window_height_diff\n        if self.display_image.max_width * self.display_image.height_width_ratio &lt; self.display_image.max_height:\n            self.display_image.scaled_shape = [round(self.display_image.max_width * self.display_image.height_width_ratio), self.display_image.max_width]\n        else:\n            self.display_image.scaled_shape = [self.display_image.max_height, np.round(self.display_image.max_height / self.display_image.height_width_ratio)]\n\n        self.display_image.setMaximumHeight(self.display_image.scaled_shape[0])\n        self.display_image.setMaximumWidth(self.display_image.scaled_shape[1])\n    return super(WindowType, self).resizeEvent(event)\n</code></pre>"},{"location":"api/cellects/gui/first_window/","title":"<code>cellects.gui.first_window</code>","text":""},{"location":"api/cellects/gui/first_window/#cellects.gui.first_window","title":"<code>cellects.gui.first_window</code>","text":"<p>First window of the Cellects graphical user interface (GUI).</p> <p>This module implements the initial setup UI for Cellects data processing. It provides widgets for selecting image/video inputs, configuring folder paths, arena numbers, and prefixes/extensions. Threaded operations ensure UI responsiveness during background tasks.</p> <p>Main Components FirstWindow : QWidget subclass implementing the first GUI window with tabs and interactive widgets</p>"},{"location":"api/cellects/gui/first_window/#cellects.gui.first_window.FirstWindow","title":"<code>FirstWindow</code>","text":"<p>               Bases: <code>MainTabsType</code></p> <p>First window of the Cellects GUI.</p> Source code in <code>src/cellects/gui/first_window.py</code> <pre><code>class FirstWindow(MainTabsType):\n    \"\"\"\n    First window of the Cellects GUI.\n    \"\"\"\n    def __init__(self, parent, night_mode):\n        \"\"\"\n        Initialize the First window with a parent widget and night mode setting.\n\n        Parameters\n        ----------\n        parent : QWidget\n            The parent widget to which this window will be attached.\n        night_mode : bool\n            A boolean indicating whether the night mode should be enabled.\n\n\n        Examples\n        --------\n        &gt;&gt;&gt; from PySide6 import QtWidgets\n        &gt;&gt;&gt; from cellects.gui.cellects import CellectsMainWidget\n        &gt;&gt;&gt; from cellects.gui.first_window import FirstWindow\n        &gt;&gt;&gt; import sys\n        &gt;&gt;&gt; app = QtWidgets.QApplication([])\n        &gt;&gt;&gt; parent = CellectsMainWidget()\n        &gt;&gt;&gt; session = FirstWindow(parent, False)\n        &gt;&gt;&gt; session.true_init()\n        &gt;&gt;&gt; parent.insertWidget(0, session)\n        &gt;&gt;&gt; parent.show()\n        &gt;&gt;&gt; sys.exit(app.exec())\n        \"\"\"\n        super().__init__(parent, night_mode)\n        logging.info(\"Initialize first window\")\n        self.setParent(parent)\n\n        self.true_init()\n\n    def true_init(self):\n        \"\"\"\n        Initialize the FirstWindow components and setup its layout.\n\n        Sets up various widgets, layouts, and threading components for the Cellects GUI,\n        including image or video selection, folder path input, arena number management,\n        and display setup.\n\n        Notes\n        -----\n        This method assumes that the parent widget has a 'po' attribute with specific settings and variables.\n        \"\"\"\n        self.data_tab.set_in_use()\n        self.image_tab.set_not_usable()\n        self.video_tab.set_not_usable()\n        self.thread_dict = {}\n        self.thread_dict[\"LookForData\"] = LookForDataThreadInFirstW(self.parent())\n        self.thread_dict[\"RunAll\"] = RunAllThread(self.parent())\n        self.thread_dict[\"LoadDataToRunCellectsQuickly\"] = LoadDataToRunCellectsQuicklyThread(self.parent())\n        self.thread_dict[\"GetFirstIm\"] = GetFirstImThread(self.parent())\n        self.thread_dict[\"GetExifDataThread\"] = GetExifDataThread(self.parent())\n        self.instantiate: bool = True\n        self.title_label = FixedText('Cellects', police=60, night_mode=self.parent().po.all['night_mode'])\n        self.title_label.setAlignment(QtCore.Qt.AlignHCenter)\n        self.subtitle_line = LineWidget(size=[1, 50], night_mode=self.parent().po.all['night_mode'])\n\n        self.Vlayout.addWidget(self.title_label)\n        self.Vlayout.addWidget(self.subtitle_line)\n        self.Vlayout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.Maximum, QtWidgets.QSizePolicy.MinimumExpanding))\n\n        # 1) Set if this a Image list or Videos\n        # Open the layout:\n        self.second_row_widget = QtWidgets.QWidget()\n        self.second_row_layout = QtWidgets.QHBoxLayout()\n        self.im_or_vid_label = FixedText(FW['Image_list_or_videos']['label'], tip=FW['Image_list_or_videos']['tips'],\n                                         night_mode=self.parent().po.all['night_mode'])\n        # self.im_or_vid_label = FixedText('Image list or Videos:', tip=\"What type of data do(es) contain(s) folder(s)?\", night_mode=self.parent().po.all['night_mode'])\n        self.im_or_vid = Combobox([\"Image list\", \"Videos\"], self.parent().po.all['im_or_vid'], night_mode=self.parent().po.all['night_mode'])\n        self.im_or_vid.setFixedWidth(150)\n        self.im_or_vid.currentTextChanged.connect(self.im2vid)\n        # Set their positions on layout\n        self.second_row_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n        self.second_row_layout.addWidget(self.im_or_vid_label)\n        self.second_row_layout.addWidget(self.im_or_vid)\n        self.second_row_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n        self.second_row_widget.setLayout(self.second_row_layout)\n        self.Vlayout.addWidget(self.second_row_widget)\n\n        # 2) Open the third row layout\n        self.third_row_widget = QtWidgets.QWidget()\n        self.third_row_layout = QtWidgets.QHBoxLayout()\n        # Set default images radical and extension widgets\n        if self.parent().po.all['im_or_vid'] == 0:\n            what = 'Images'\n            if self.parent().po.all['extension'] == '.mp4':\n                self.parent().po.all['radical'] = 'IMG_'\n                self.parent().po.all['extension'] = '.JPG'\n            self.arena_number_label = FixedText('Arena number per folder:',\n                                                tip=FW[\"Arena_number_per_folder\"][\"tips\"] , #\"If this number is not always the same (depending on the folder), it can be changed later\",\n                                                night_mode=self.parent().po.all['night_mode'])\n        else:\n            if self.parent().po.all['extension'] == '.JPG':\n                self.parent().po.all['radical'] = ''\n                self.parent().po.all['extension'] = '.mp4'\n            self.arena_number_label = FixedText('Arena number per folder:',\n                                                tip=FW[\"Arena_number_per_folder\"][\"tips\"], #\"If this number is not always the same (depending on the video), it can be changed later\",\n                                                night_mode=self.parent().po.all['night_mode'])\n            what = 'Videos'\n        self.arena_number_label.setAlignment(QtCore.Qt.AlignVCenter)\n        self.arena_number = Spinbox(min=0, max=255, val=self.parent().po.all['first_folder_sample_number'],\n                                     decimals=0, night_mode=self.parent().po.all['night_mode'])\n        self.arena_number.valueChanged.connect(self.re_instantiate_widgets)\n        self.radical_label = FixedText(what + ' prefix:', tip=FW[\"Image_prefix_and_extension\"][\"tips\"], night_mode=self.parent().po.all['night_mode'])\n        self.radical_label.setAlignment(QtCore.Qt.AlignVCenter)\n        self.radical = EditText(self.parent().po.all['radical'],\n                                       night_mode=self.parent().po.all['night_mode'])\n        self.radical.textChanged.connect(self.re_instantiate_widgets)\n\n        self.extension_label = FixedText(what + ' extension:', tip=FW[\"Image_prefix_and_extension\"][\"tips\"], night_mode=self.parent().po.all['night_mode'])\n        self.extension_label.setAlignment(QtCore.Qt.AlignVCenter)\n        self.extension = EditText(self.parent().po.all['extension'],\n                                       night_mode=self.parent().po.all['night_mode'])\n        self.extension.textChanged.connect(self.re_instantiate_widgets)\n\n        # Set their positions on layout\n        self.third_row_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n        self.third_row_layout.addWidget(self.radical_label)\n        self.third_row_layout.addWidget(self.radical)\n        # self.third_row_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n        self.third_row_layout.addWidget(self.extension_label)\n        self.third_row_layout.addWidget(self.extension)\n        self.third_row_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n        self.third_row_widget.setLayout(self.third_row_layout)\n        self.Vlayout.addWidget(self.third_row_widget)\n        # If im_or_vid changes, adjust these 2 widgets\n\n        # 3) Get the path to the right folder:\n        # Open the layout:\n        self.first_row_widget = QtWidgets.QWidget()\n        self.first_row_layout = QtWidgets.QHBoxLayout()\n\n        self.folder_label = FixedText(FW[\"Folder\"][\"label\"] + ':',\n                                      tip=FW[\"Folder\"][\"tips\"],#\"Path to the folder containing images or videos\\nThe selected folder may also contain several folders of data\",\n                                      night_mode=self.parent().po.all['night_mode'])\n        self.folder_label.setAlignment(QtCore.Qt.AlignVCenter)\n        self.global_pathway = EditText(self.parent().po.all['global_pathway'],\n                                       night_mode=self.parent().po.all['night_mode'])\n        self.global_pathway.setSizePolicy(QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum)\n        self.global_pathway.textChanged.connect(self.pathway_changed)\n        self.browse = PButton(FW[\"Browse\"][\"label\"], tip=FW[\"Browse\"][\"tips\"],\n                              night_mode=self.parent().po.all['night_mode'])\n        self.browse.clicked.connect(self.browse_is_clicked)\n\n        # Set their positions on layout\n        self.first_row_layout.addWidget(self.folder_label)\n        self.first_row_layout.addWidget(self.global_pathway)\n        self.first_row_layout.addWidget(self.browse)\n        self.first_row_widget.setLayout(self.first_row_layout)\n        self.Vlayout.addWidget(self.first_row_widget)\n\n        self.fourth_row_widget = QtWidgets.QWidget()\n        self.fourth_row_layout = QtWidgets.QHBoxLayout()\n        self.fourth_row_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n        self.fourth_row_layout.addWidget(self.arena_number_label)\n        self.fourth_row_layout.addWidget(self.arena_number)\n        self.fourth_row_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n        self.fourth_row_widget.setLayout(self.fourth_row_layout)\n        self.Vlayout.addWidget(self.fourth_row_widget)\n        self.Vlayout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.Maximum, QtWidgets.QSizePolicy.MinimumExpanding))\n\n        # Add the central image display widget\n        self.display_image = np.zeros((self.parent().im_max_height, self.parent().im_max_width, 3), np.uint8)\n        self.display_image = InsertImage(self.display_image, self.parent().im_max_height, self.parent().im_max_width)\n        self.Vlayout.addWidget(self.display_image, alignment=QtCore.Qt.AlignCenter)\n        self.display_image.setVisible(False)\n        self.display_image.mousePressEvent = self.full_screen_display\n\n        # 4) Create the shortcuts row\n        self.shortcuts_widget = QtWidgets.QWidget()\n        self.shortcuts_layout = QtWidgets.QHBoxLayout()\n        # Add shortcuts: Video_analysis and Run directly\n        # Shortcut 1 : Advanced Parameters\n        self.advanced_parameters = PButton(FW[\"Advanced_parameters\"][\"label\"], tip=FW[\"Advanced_parameters\"][\"tips\"],\n                                           night_mode=self.parent().po.all['night_mode'])\n        self.advanced_parameters.clicked.connect(self.advanced_parameters_is_clicked)\n        # Shortcut 2 : Required Outputs\n        self.required_outputs = PButton(FW[\"Required_outputs\"][\"label\"], tip=FW[\"Required_outputs\"][\"tips\"],\n                                        night_mode=self.parent().po.all['night_mode'])\n        self.required_outputs.clicked.connect(self.required_outputs_is_clicked)\n        # Shortcut 3 :\n        self.video_tab.clicked.connect(self.video_analysis_window_is_clicked)\n        # Shortcut 4 :\n        self.Run_all_directly = PButton(FW[\"Run_all_directly\"][\"label\"], tip=FW[\"Run_all_directly\"][\"tips\"],\n                                        night_mode=self.parent().po.all['night_mode'])\n        self.Run_all_directly.clicked.connect(self.Run_all_directly_is_clicked)\n        self.Run_all_directly.setVisible(False)\n\n        self.shortcuts_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n        self.shortcuts_layout.addWidget(self.advanced_parameters)\n        self.shortcuts_layout.addWidget(self.required_outputs)\n        self.shortcuts_layout.addWidget(self.Run_all_directly)\n        self.shortcuts_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n        self.shortcuts_widget.setLayout(self.shortcuts_layout)\n        self.Vlayout.addWidget(self.shortcuts_widget)\n\n        # 5) Open the last row layout\n        self.last_row_widget = QtWidgets.QWidget()\n        self.last_row_layout = QtWidgets.QHBoxLayout()\n\n        # Message\n        self.message = FixedText('', halign='r', night_mode=self.parent().po.all['night_mode'])\n        self.message.setStyleSheet(\"color: rgb(230, 145, 18)\")\n        # Next button\n        self.next = PButton(FW['Next']['label'], tip=FW['Next']['tips'],\n                            night_mode=self.parent().po.all['night_mode'])\n        self.image_tab.clicked.connect(self.next_is_clicked)\n        self.next.clicked.connect(self.next_is_clicked)\n        # Add widgets to the last_row_layout\n        self.last_row_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n        self.last_row_layout.addWidget(self.message)\n        self.last_row_layout.addWidget(self.next)\n        # Close the last_row_layout\n        self.last_row_widget.setLayout(self.last_row_layout)\n        self.Vlayout.addWidget(self.last_row_widget)\n        self.setLayout(self.Vlayout)\n\n        # Check if there is data in the saved folder\n        self.pathway_changed()\n\n    def full_screen_display(self, event):\n        \"\"\"\n        Display an image in full screen.\n\n        Displays the current `image_to_display` of the parent window\n        in a separate full-screen window.\n\n        Parameters\n        ----------\n        event : QEvent\n            The event that triggers the full-screen display.\n\n        Other Parameters\n        ----------------\n        popup_img : FullScreenImage\n            The instance of `FullScreenImage` created to display the image.\n\n        Notes\n        -----\n        The method creates a new instance of `FullScreenImage` and displays it.\n        This is intended to provide a full-screen view of the image currently\n        displayed in the parent window.\n        \"\"\"\n        self.popup_img = FullScreenImage(self.parent().image_to_display, self.parent().screen_width, self.parent().screen_height)\n        self.popup_img.show()\n\n    def browse_is_clicked(self):\n        \"\"\"\n        Handles the logic for when a \"Browse\" button is clicked in the interface.\n\n        Opens a file dialog to select a directory and updates the global pathway.\n\n        Notes\n        -----\n        This function assumes that `self.parent().po.all` is a dictionary with a key `'global_pathway'`.\n        \"\"\"\n        dialog = QtWidgets.QFileDialog()\n        dialog.setDirectory(str(self.parent().po.all['global_pathway']))\n        self.parent().po.all['global_pathway'] = dialog.getExistingDirectory(self,\n                                                                             'Select a folder containing images (/videos) or folders of data images (/videos)')\n        self.global_pathway.setText(self.parent().po.all['global_pathway'])\n\n    def im2vid(self):\n        \"\"\"\n        Toggle between processing images or videos based on UI selection.\n        \"\"\"\n        self.parent().po.all['im_or_vid'] = self.im_or_vid.currentIndex()\n        if self.im_or_vid.currentIndex() == 0:\n            what = 'Images'\n            if self.parent().po.all['extension'] == '.mp4':\n                self.parent().po.all['radical'] = 'IMG_'\n                self.parent().po.all['extension'] = '.JPG'\n        else:\n            if self.parent().po.all['extension'] == '.JPG':\n                self.parent().po.all['radical'] = ''\n                self.parent().po.all['extension'] = '.mp4'\n            what = 'Videos'\n        self.radical_label.setText(what + ' prefix:')\n        self.extension_label.setText(what + ' extension:')\n        self.radical.setText(self.parent().po.all['radical'])\n        self.extension.setText(self.parent().po.all['extension'])\n\n    def display_message_from_thread(self, text_from_thread: str):\n        \"\"\"\n        Updates the message displayed in the UI with text from a thread.\n\n        Parameters\n        ----------\n        text_from_thread : str\n            The text to be displayed in the UI message.\n        \"\"\"\n        self.message.setText(text_from_thread)\n\n    def display_image_during_thread(self, dictionary: dict):\n        \"\"\"\n        Display an image and set a message during a thread operation.\n\n        Parameters\n        ----------\n        dictionary : dict\n            A dictionary containing the 'message' and 'current_image'.\n                The message is a string to display.\n                The current_image is the image data that will be displayed.\n        \"\"\"\n        self.message.setText(dictionary['message'])\n        self.parent().image_to_display = dictionary['current_image']\n        self.display_image.update_image(dictionary['current_image'])\n\n    def next_is_clicked(self):\n        \"\"\"\n        Handles the logic for when a \"Next\" button is clicked in the interface.\n\n        Checks if certain threads are running, updates parent object's attributes,\n        and starts a data-looking thread if conditions are met.\n        \"\"\"\n        if not self.thread_dict[\"LookForData\"].isRunning() and not self.thread_dict[\"RunAll\"].isRunning():\n            self.parent().po.all['im_or_vid'] = self.im_or_vid.currentIndex()\n            self.parent().po.all['radical'] = self.radical.text()\n            self.parent().po.all['extension'] = self.extension.text()\n            self.parent().po.sample_number = int(self.arena_number.value())\n            self.parent().po.all['first_folder_sample_number'] = self.parent().po.sample_number\n            self.parent().po.all['sample_number_per_folder'] = [self.parent().po.sample_number]\n            if not self.instantiate:  # not self.parent().imageanalysiswindow.initialized:\n                logging.info(\"No need to look for data, images or videos already found previously.\")\n                self.first_im_read(True)\n            else:\n                self.parent().po.all['global_pathway'] = Path(self.global_pathway.text())\n                if not os.path.isdir(Path(self.parent().po.all['global_pathway'])):\n                    self.message.setText('The folder selected is not valid')\n                else:\n                    self.message.setText('')\n                    self.message.setText(f\"Looking for {self.parent().po.all['radical']}***{self.parent().po.all['extension']} Wait...\")\n                    self.message.setStyleSheet(\"color: rgb(230, 145, 18)\")\n                    self.thread_dict[\"LookForData\"].start()\n                    self.thread_dict[\"LookForData\"].finished.connect(self.when_look_for_data_finished)\n        else:\n            self.message.setText('Analysis has already begun, wait or restart Cellects.')\n\n    def when_look_for_data_finished(self):\n        \"\"\"\n        Check if there are any data items left in the selected folder and its sub-folders.\n        Display appropriate error messages or proceed with further actions based on the data availability.\n\n        Notes\n        -----\n        This function checks if there are any data items (images or videos) left in the selected folder and its sub-folders.\n        If no data is found, it displays an error message. Otherwise, it proceeds with instantiating widgets or starting a thread.\n        \"\"\"\n        if len(self.parent().po.all['folder_list']) == 0 and len(self.parent().po.data_list) == 0:\n            if self.parent().po.all['im_or_vid'] == 1:\n                error_message = f\"There is no videos ({self.parent().po.all['extension']}) in the selected folder and its sub-folders\"\n            else:\n                error_message = f\"There is no images ({self.parent().po.all['extension']}) in the selected folder and its sub-folders\"\n            self.message.setText(error_message)\n        else:\n            self.message.setText('')\n            if self.parent().po.all['folder_number'] &gt; 1:\n                self.parent().instantiate_widgets()\n                self.parent().ifseveralfolderswindow.true_init()\n                self.instantiate = False\n                self.parent().change_widget(1) # IfSeveralFoldersWindow\n            else:\n                self.thread_dict[\"GetFirstIm\"].start()\n                self.thread_dict[\"GetFirstIm\"].message_when_thread_finished.connect(self.first_im_read)\n\n    def first_im_read(self, greyscale):\n        \"\"\"\n        Initialize the image analysis window and prepare for reading images.\n\n        Notes\n        -----\n        This function prepares the image analysis window and sets it to be ready for\n        reading images. It also ensures that certain tabs are set as not in use.\n        \"\"\"\n        self.parent().instantiate_widgets()\n        self.parent().imageanalysiswindow.true_init()\n        self.instantiate = False\n        if self.parent().po.first_exp_ready_to_run and (self.parent().po.all[\"im_or_vid\"] == 1 or len(self.parent().po.data_list) &gt; 1):\n            self.parent().imageanalysiswindow.video_tab.set_not_in_use()\n        self.parent().change_widget(2) # imageanalysiswindow\n        # From now on, image analysis will be available from video analysis:\n        self.parent().videoanalysiswindow.image_tab.set_not_in_use()\n        self.thread_dict[\"GetExifDataThread\"].start()\n\n    def required_outputs_is_clicked(self):\n        \"\"\"\n        Handle the click event for switching to required outputs.\n\n        This function sets the `last_is_first` attribute of the parent to True\n        and changes the widget to the Required Outputs view.\n        \"\"\"\n        self.parent().last_is_first = True\n        self.parent().change_widget(4)  # RequiredOutput\n\n    def advanced_parameters_is_clicked(self):\n        \"\"\"\n        Handle the click event for switching to advanced parameters.\n\n        Checks if an Exif data reading thread is running and acts accordingly.\n        If not, it updates the display for advanced parameters.\n\n        Notes\n        -----\n        This function updates the display for advanced parameters only if no Exif data reading thread is running.\n        If a thread is active, it informs the user to wait or restart Cellects.\n        \"\"\"\n        if self.thread_dict[\"GetExifDataThread\"].isRunning():\n            self.message.setText(\"Reading data, wait or restart Cellects\")\n        else:\n            self.parent().last_is_first = True\n            self.parent().widget(5).update_csc_editing_display()\n            self.parent().change_widget(5) # AdvancedParameters\n\n    def video_analysis_window_is_clicked(self):\n        \"\"\"\n        Handles the logic for when the \"Video tracking\" button is clicked in the interface,\n        leading to the video analysis window.\n\n        Notes\n        -----\n        This function displays an error message when a thread relative to the current window is running.\n        This function also save the id of the following window for later use.\n        \"\"\"\n        if self.video_tab.state != \"not_usable\":\n            if self.thread_dict[\"LookForData\"].isRunning() or self.thread_dict[\"LoadDataToRunCellectsQuickly\"].isRunning() or self.thread_dict[\"GetFirstIm\"].isRunning() or self.thread_dict[\"RunAll\"].isRunning():\n                self.message.setText(\"Wait for the analysis to end, or restart Cellects\")\n            else:\n                self.parent().last_tab = \"data_specifications\"\n                self.parent().change_widget(3) # Should be VideoAnalysisW\n\n    def Run_all_directly_is_clicked(self):\n        \"\"\"\n        Run_all_directly_is_clicked\n\n        This method initiates a complete analysis process by starting the `RunAll` thread\n        after ensuring no other relevant threads are currently running.\n\n        Notes\n        -----\n        - This method ensures that the `LookForData` and `RunAll` threads are not running\n          before initiating a new analysis.\n        - The method updates the UI to indicate that an analysis has started and displays\n          progress messages.\n        \"\"\"\n        if not self.thread_dict[\"LookForData\"].isRunning() and not self.thread_dict[\"RunAll\"].isRunning():\n            self.parent().po.motion = None\n            self.message.setText(\"Complete analysis has started, wait until this message disappear...\")\n            self.thread_dict[\"RunAll\"].start()\n            self.thread_dict[\"RunAll\"].message_from_thread.connect(self.display_message_from_thread)\n            self.thread_dict[\"RunAll\"].image_from_thread.connect(self.display_image_during_thread)\n            self.display_image.setVisible(True)\n\n    def pathway_changed(self):\n        \"\"\"\n        Method for handling pathway changes in the application.\n\n        This method performs several operations when a new global pathway is set:\n        1. Waits for any running thread to complete.\n        2. Updates the global pathway if a valid directory is found.\n        3. Changes the current working directory to the new global pathway.\n        4. Hides various widgets associated with advanced options and outputs.\n        5. Starts a background thread to load data quickly.\n        6. If the provided pathway is invalid, it hides relevant tabs and outputs an error message.\n\n        Notes\n        -----\n        This method performs actions to prepare the application for loading data from a new pathway.\n        It ensures that certain widgets are hidden and starts necessary background processes.\n        \"\"\"\n        if self.thread_dict[\"LoadDataToRunCellectsQuickly\"].isRunning():\n            self.thread_dict[\"LoadDataToRunCellectsQuickly\"].wait()\n        if os.path.isdir(Path(self.global_pathway.text())):\n            self.parent().po.all['global_pathway'] = self.global_pathway.text()\n            logging.info(f\"Dir: {self.parent().po.all['global_pathway']}\")\n            os.chdir(Path(self.parent().po.all['global_pathway']))\n            # 1) Put invisible widgets\n            self.radical.setVisible(False)\n            self.extension.setVisible(False)\n            self.arena_number.setVisible(False)\n            self.im_or_vid.setVisible(False)\n            self.advanced_parameters.setVisible(False)\n            self.required_outputs.setVisible(False)\n            self.Run_all_directly.setVisible(False)\n            self.next.setVisible(False)\n            self.instantiate = True\n            self.video_tab.set_not_usable()\n            # 2) Load the dict\n            self.thread_dict[\"LoadDataToRunCellectsQuickly\"].start()\n            self.thread_dict[\"LoadDataToRunCellectsQuickly\"].message_from_thread.connect(self.load_data_quickly_finished)\n            # 3) go to another func to change, put visible and re_instantiate\n        else:\n            self.Run_all_directly.setVisible(False)\n            self.image_tab.set_not_usable()\n            self.video_tab.set_not_usable()\n            self.message.setText(\"Please, enter a valid path\")\n\n    def load_data_quickly_finished(self, message: str):\n        \"\"\"\n        Set up the UI components for a new one_experiment.\n\n        Parameters\n        ----------\n        message : str\n            The message to be displayed on the UI component.\n\n        Notes\n        -----\n        This function sets several visibility flags and values for UI components\n        in preparation for starting an one_experiment.\n        \"\"\"\n        self.image_tab.set_not_in_use()\n        self.message.setText(message)\n        self.radical.setVisible(True)\n        self.extension.setVisible(True)\n        self.arena_number.setVisible(True)\n        self.im_or_vid.setVisible(True)\n        self.advanced_parameters.setVisible(True)\n        self.required_outputs.setVisible(True)\n        self.next.setVisible(True)\n\n        if self.parent().po.first_exp_ready_to_run:\n            self.parent().po.all['folder_number'] = 1\n            self.parent().instantiate_widgets(True)\n            self.arena_number.setValue(self.parent().po.all['first_folder_sample_number'])\n            self.im_or_vid.setCurrentIndex(self.parent().po.all['im_or_vid'])\n            self.radical.setText(self.parent().po.all['radical'])\n            self.extension.setText(self.parent().po.all['extension'])\n            self.Run_all_directly.setVisible(True)\n            if self.parent().po.all[\"im_or_vid\"] == 1 or len(self.parent().po.data_list) &gt; 1:\n                self.video_tab.set_not_in_use()\n\n\n    def re_instantiate_widgets(self):\n        \"\"\"\n        Reinstantiate the videoanalysis window from the parent of the current window.\n        \"\"\"\n        self.instantiate = True\n        # Since we re-instantiate everything, image analysis will no longer be available from video analysis:\n        self.parent().videoanalysiswindow.image_tab.set_not_usable()\n</code></pre>"},{"location":"api/cellects/gui/first_window/#cellects.gui.first_window.FirstWindow.Run_all_directly_is_clicked","title":"<code>Run_all_directly_is_clicked()</code>","text":"<p>Run_all_directly_is_clicked</p> <p>This method initiates a complete analysis process by starting the <code>RunAll</code> thread after ensuring no other relevant threads are currently running.</p> Notes <ul> <li>This method ensures that the <code>LookForData</code> and <code>RunAll</code> threads are not running   before initiating a new analysis.</li> <li>The method updates the UI to indicate that an analysis has started and displays   progress messages.</li> </ul> Source code in <code>src/cellects/gui/first_window.py</code> <pre><code>def Run_all_directly_is_clicked(self):\n    \"\"\"\n    Run_all_directly_is_clicked\n\n    This method initiates a complete analysis process by starting the `RunAll` thread\n    after ensuring no other relevant threads are currently running.\n\n    Notes\n    -----\n    - This method ensures that the `LookForData` and `RunAll` threads are not running\n      before initiating a new analysis.\n    - The method updates the UI to indicate that an analysis has started and displays\n      progress messages.\n    \"\"\"\n    if not self.thread_dict[\"LookForData\"].isRunning() and not self.thread_dict[\"RunAll\"].isRunning():\n        self.parent().po.motion = None\n        self.message.setText(\"Complete analysis has started, wait until this message disappear...\")\n        self.thread_dict[\"RunAll\"].start()\n        self.thread_dict[\"RunAll\"].message_from_thread.connect(self.display_message_from_thread)\n        self.thread_dict[\"RunAll\"].image_from_thread.connect(self.display_image_during_thread)\n        self.display_image.setVisible(True)\n</code></pre>"},{"location":"api/cellects/gui/first_window/#cellects.gui.first_window.FirstWindow.__init__","title":"<code>__init__(parent, night_mode)</code>","text":"<p>Initialize the First window with a parent widget and night mode setting.</p> <p>Parameters:</p> Name Type Description Default <code>parent</code> <code>QWidget</code> <p>The parent widget to which this window will be attached.</p> required <code>night_mode</code> <code>bool</code> <p>A boolean indicating whether the night mode should be enabled.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from PySide6 import QtWidgets\n&gt;&gt;&gt; from cellects.gui.cellects import CellectsMainWidget\n&gt;&gt;&gt; from cellects.gui.first_window import FirstWindow\n&gt;&gt;&gt; import sys\n&gt;&gt;&gt; app = QtWidgets.QApplication([])\n&gt;&gt;&gt; parent = CellectsMainWidget()\n&gt;&gt;&gt; session = FirstWindow(parent, False)\n&gt;&gt;&gt; session.true_init()\n&gt;&gt;&gt; parent.insertWidget(0, session)\n&gt;&gt;&gt; parent.show()\n&gt;&gt;&gt; sys.exit(app.exec())\n</code></pre> Source code in <code>src/cellects/gui/first_window.py</code> <pre><code>def __init__(self, parent, night_mode):\n    \"\"\"\n    Initialize the First window with a parent widget and night mode setting.\n\n    Parameters\n    ----------\n    parent : QWidget\n        The parent widget to which this window will be attached.\n    night_mode : bool\n        A boolean indicating whether the night mode should be enabled.\n\n\n    Examples\n    --------\n    &gt;&gt;&gt; from PySide6 import QtWidgets\n    &gt;&gt;&gt; from cellects.gui.cellects import CellectsMainWidget\n    &gt;&gt;&gt; from cellects.gui.first_window import FirstWindow\n    &gt;&gt;&gt; import sys\n    &gt;&gt;&gt; app = QtWidgets.QApplication([])\n    &gt;&gt;&gt; parent = CellectsMainWidget()\n    &gt;&gt;&gt; session = FirstWindow(parent, False)\n    &gt;&gt;&gt; session.true_init()\n    &gt;&gt;&gt; parent.insertWidget(0, session)\n    &gt;&gt;&gt; parent.show()\n    &gt;&gt;&gt; sys.exit(app.exec())\n    \"\"\"\n    super().__init__(parent, night_mode)\n    logging.info(\"Initialize first window\")\n    self.setParent(parent)\n\n    self.true_init()\n</code></pre>"},{"location":"api/cellects/gui/first_window/#cellects.gui.first_window.FirstWindow.advanced_parameters_is_clicked","title":"<code>advanced_parameters_is_clicked()</code>","text":"<p>Handle the click event for switching to advanced parameters.</p> <p>Checks if an Exif data reading thread is running and acts accordingly. If not, it updates the display for advanced parameters.</p> Notes <p>This function updates the display for advanced parameters only if no Exif data reading thread is running. If a thread is active, it informs the user to wait or restart Cellects.</p> Source code in <code>src/cellects/gui/first_window.py</code> <pre><code>def advanced_parameters_is_clicked(self):\n    \"\"\"\n    Handle the click event for switching to advanced parameters.\n\n    Checks if an Exif data reading thread is running and acts accordingly.\n    If not, it updates the display for advanced parameters.\n\n    Notes\n    -----\n    This function updates the display for advanced parameters only if no Exif data reading thread is running.\n    If a thread is active, it informs the user to wait or restart Cellects.\n    \"\"\"\n    if self.thread_dict[\"GetExifDataThread\"].isRunning():\n        self.message.setText(\"Reading data, wait or restart Cellects\")\n    else:\n        self.parent().last_is_first = True\n        self.parent().widget(5).update_csc_editing_display()\n        self.parent().change_widget(5) # AdvancedParameters\n</code></pre>"},{"location":"api/cellects/gui/first_window/#cellects.gui.first_window.FirstWindow.browse_is_clicked","title":"<code>browse_is_clicked()</code>","text":"<p>Handles the logic for when a \"Browse\" button is clicked in the interface.</p> <p>Opens a file dialog to select a directory and updates the global pathway.</p> Notes <p>This function assumes that <code>self.parent().po.all</code> is a dictionary with a key <code>'global_pathway'</code>.</p> Source code in <code>src/cellects/gui/first_window.py</code> <pre><code>def browse_is_clicked(self):\n    \"\"\"\n    Handles the logic for when a \"Browse\" button is clicked in the interface.\n\n    Opens a file dialog to select a directory and updates the global pathway.\n\n    Notes\n    -----\n    This function assumes that `self.parent().po.all` is a dictionary with a key `'global_pathway'`.\n    \"\"\"\n    dialog = QtWidgets.QFileDialog()\n    dialog.setDirectory(str(self.parent().po.all['global_pathway']))\n    self.parent().po.all['global_pathway'] = dialog.getExistingDirectory(self,\n                                                                         'Select a folder containing images (/videos) or folders of data images (/videos)')\n    self.global_pathway.setText(self.parent().po.all['global_pathway'])\n</code></pre>"},{"location":"api/cellects/gui/first_window/#cellects.gui.first_window.FirstWindow.display_image_during_thread","title":"<code>display_image_during_thread(dictionary)</code>","text":"<p>Display an image and set a message during a thread operation.</p> <p>Parameters:</p> Name Type Description Default <code>dictionary</code> <code>dict</code> <p>A dictionary containing the 'message' and 'current_image'.     The message is a string to display.     The current_image is the image data that will be displayed.</p> required Source code in <code>src/cellects/gui/first_window.py</code> <pre><code>def display_image_during_thread(self, dictionary: dict):\n    \"\"\"\n    Display an image and set a message during a thread operation.\n\n    Parameters\n    ----------\n    dictionary : dict\n        A dictionary containing the 'message' and 'current_image'.\n            The message is a string to display.\n            The current_image is the image data that will be displayed.\n    \"\"\"\n    self.message.setText(dictionary['message'])\n    self.parent().image_to_display = dictionary['current_image']\n    self.display_image.update_image(dictionary['current_image'])\n</code></pre>"},{"location":"api/cellects/gui/first_window/#cellects.gui.first_window.FirstWindow.display_message_from_thread","title":"<code>display_message_from_thread(text_from_thread)</code>","text":"<p>Updates the message displayed in the UI with text from a thread.</p> <p>Parameters:</p> Name Type Description Default <code>text_from_thread</code> <code>str</code> <p>The text to be displayed in the UI message.</p> required Source code in <code>src/cellects/gui/first_window.py</code> <pre><code>def display_message_from_thread(self, text_from_thread: str):\n    \"\"\"\n    Updates the message displayed in the UI with text from a thread.\n\n    Parameters\n    ----------\n    text_from_thread : str\n        The text to be displayed in the UI message.\n    \"\"\"\n    self.message.setText(text_from_thread)\n</code></pre>"},{"location":"api/cellects/gui/first_window/#cellects.gui.first_window.FirstWindow.first_im_read","title":"<code>first_im_read(greyscale)</code>","text":"<p>Initialize the image analysis window and prepare for reading images.</p> Notes <p>This function prepares the image analysis window and sets it to be ready for reading images. It also ensures that certain tabs are set as not in use.</p> Source code in <code>src/cellects/gui/first_window.py</code> <pre><code>def first_im_read(self, greyscale):\n    \"\"\"\n    Initialize the image analysis window and prepare for reading images.\n\n    Notes\n    -----\n    This function prepares the image analysis window and sets it to be ready for\n    reading images. It also ensures that certain tabs are set as not in use.\n    \"\"\"\n    self.parent().instantiate_widgets()\n    self.parent().imageanalysiswindow.true_init()\n    self.instantiate = False\n    if self.parent().po.first_exp_ready_to_run and (self.parent().po.all[\"im_or_vid\"] == 1 or len(self.parent().po.data_list) &gt; 1):\n        self.parent().imageanalysiswindow.video_tab.set_not_in_use()\n    self.parent().change_widget(2) # imageanalysiswindow\n    # From now on, image analysis will be available from video analysis:\n    self.parent().videoanalysiswindow.image_tab.set_not_in_use()\n    self.thread_dict[\"GetExifDataThread\"].start()\n</code></pre>"},{"location":"api/cellects/gui/first_window/#cellects.gui.first_window.FirstWindow.full_screen_display","title":"<code>full_screen_display(event)</code>","text":"<p>Display an image in full screen.</p> <p>Displays the current <code>image_to_display</code> of the parent window in a separate full-screen window.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>QEvent</code> <p>The event that triggers the full-screen display.</p> required <p>Other Parameters:</p> Name Type Description <code>popup_img</code> <code>FullScreenImage</code> <p>The instance of <code>FullScreenImage</code> created to display the image.</p> Notes <p>The method creates a new instance of <code>FullScreenImage</code> and displays it. This is intended to provide a full-screen view of the image currently displayed in the parent window.</p> Source code in <code>src/cellects/gui/first_window.py</code> <pre><code>def full_screen_display(self, event):\n    \"\"\"\n    Display an image in full screen.\n\n    Displays the current `image_to_display` of the parent window\n    in a separate full-screen window.\n\n    Parameters\n    ----------\n    event : QEvent\n        The event that triggers the full-screen display.\n\n    Other Parameters\n    ----------------\n    popup_img : FullScreenImage\n        The instance of `FullScreenImage` created to display the image.\n\n    Notes\n    -----\n    The method creates a new instance of `FullScreenImage` and displays it.\n    This is intended to provide a full-screen view of the image currently\n    displayed in the parent window.\n    \"\"\"\n    self.popup_img = FullScreenImage(self.parent().image_to_display, self.parent().screen_width, self.parent().screen_height)\n    self.popup_img.show()\n</code></pre>"},{"location":"api/cellects/gui/first_window/#cellects.gui.first_window.FirstWindow.im2vid","title":"<code>im2vid()</code>","text":"<p>Toggle between processing images or videos based on UI selection.</p> Source code in <code>src/cellects/gui/first_window.py</code> <pre><code>def im2vid(self):\n    \"\"\"\n    Toggle between processing images or videos based on UI selection.\n    \"\"\"\n    self.parent().po.all['im_or_vid'] = self.im_or_vid.currentIndex()\n    if self.im_or_vid.currentIndex() == 0:\n        what = 'Images'\n        if self.parent().po.all['extension'] == '.mp4':\n            self.parent().po.all['radical'] = 'IMG_'\n            self.parent().po.all['extension'] = '.JPG'\n    else:\n        if self.parent().po.all['extension'] == '.JPG':\n            self.parent().po.all['radical'] = ''\n            self.parent().po.all['extension'] = '.mp4'\n        what = 'Videos'\n    self.radical_label.setText(what + ' prefix:')\n    self.extension_label.setText(what + ' extension:')\n    self.radical.setText(self.parent().po.all['radical'])\n    self.extension.setText(self.parent().po.all['extension'])\n</code></pre>"},{"location":"api/cellects/gui/first_window/#cellects.gui.first_window.FirstWindow.load_data_quickly_finished","title":"<code>load_data_quickly_finished(message)</code>","text":"<p>Set up the UI components for a new one_experiment.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>The message to be displayed on the UI component.</p> required Notes <p>This function sets several visibility flags and values for UI components in preparation for starting an one_experiment.</p> Source code in <code>src/cellects/gui/first_window.py</code> <pre><code>def load_data_quickly_finished(self, message: str):\n    \"\"\"\n    Set up the UI components for a new one_experiment.\n\n    Parameters\n    ----------\n    message : str\n        The message to be displayed on the UI component.\n\n    Notes\n    -----\n    This function sets several visibility flags and values for UI components\n    in preparation for starting an one_experiment.\n    \"\"\"\n    self.image_tab.set_not_in_use()\n    self.message.setText(message)\n    self.radical.setVisible(True)\n    self.extension.setVisible(True)\n    self.arena_number.setVisible(True)\n    self.im_or_vid.setVisible(True)\n    self.advanced_parameters.setVisible(True)\n    self.required_outputs.setVisible(True)\n    self.next.setVisible(True)\n\n    if self.parent().po.first_exp_ready_to_run:\n        self.parent().po.all['folder_number'] = 1\n        self.parent().instantiate_widgets(True)\n        self.arena_number.setValue(self.parent().po.all['first_folder_sample_number'])\n        self.im_or_vid.setCurrentIndex(self.parent().po.all['im_or_vid'])\n        self.radical.setText(self.parent().po.all['radical'])\n        self.extension.setText(self.parent().po.all['extension'])\n        self.Run_all_directly.setVisible(True)\n        if self.parent().po.all[\"im_or_vid\"] == 1 or len(self.parent().po.data_list) &gt; 1:\n            self.video_tab.set_not_in_use()\n</code></pre>"},{"location":"api/cellects/gui/first_window/#cellects.gui.first_window.FirstWindow.next_is_clicked","title":"<code>next_is_clicked()</code>","text":"<p>Handles the logic for when a \"Next\" button is clicked in the interface.</p> <p>Checks if certain threads are running, updates parent object's attributes, and starts a data-looking thread if conditions are met.</p> Source code in <code>src/cellects/gui/first_window.py</code> <pre><code>def next_is_clicked(self):\n    \"\"\"\n    Handles the logic for when a \"Next\" button is clicked in the interface.\n\n    Checks if certain threads are running, updates parent object's attributes,\n    and starts a data-looking thread if conditions are met.\n    \"\"\"\n    if not self.thread_dict[\"LookForData\"].isRunning() and not self.thread_dict[\"RunAll\"].isRunning():\n        self.parent().po.all['im_or_vid'] = self.im_or_vid.currentIndex()\n        self.parent().po.all['radical'] = self.radical.text()\n        self.parent().po.all['extension'] = self.extension.text()\n        self.parent().po.sample_number = int(self.arena_number.value())\n        self.parent().po.all['first_folder_sample_number'] = self.parent().po.sample_number\n        self.parent().po.all['sample_number_per_folder'] = [self.parent().po.sample_number]\n        if not self.instantiate:  # not self.parent().imageanalysiswindow.initialized:\n            logging.info(\"No need to look for data, images or videos already found previously.\")\n            self.first_im_read(True)\n        else:\n            self.parent().po.all['global_pathway'] = Path(self.global_pathway.text())\n            if not os.path.isdir(Path(self.parent().po.all['global_pathway'])):\n                self.message.setText('The folder selected is not valid')\n            else:\n                self.message.setText('')\n                self.message.setText(f\"Looking for {self.parent().po.all['radical']}***{self.parent().po.all['extension']} Wait...\")\n                self.message.setStyleSheet(\"color: rgb(230, 145, 18)\")\n                self.thread_dict[\"LookForData\"].start()\n                self.thread_dict[\"LookForData\"].finished.connect(self.when_look_for_data_finished)\n    else:\n        self.message.setText('Analysis has already begun, wait or restart Cellects.')\n</code></pre>"},{"location":"api/cellects/gui/first_window/#cellects.gui.first_window.FirstWindow.pathway_changed","title":"<code>pathway_changed()</code>","text":"<p>Method for handling pathway changes in the application.</p> <p>This method performs several operations when a new global pathway is set: 1. Waits for any running thread to complete. 2. Updates the global pathway if a valid directory is found. 3. Changes the current working directory to the new global pathway. 4. Hides various widgets associated with advanced options and outputs. 5. Starts a background thread to load data quickly. 6. If the provided pathway is invalid, it hides relevant tabs and outputs an error message.</p> Notes <p>This method performs actions to prepare the application for loading data from a new pathway. It ensures that certain widgets are hidden and starts necessary background processes.</p> Source code in <code>src/cellects/gui/first_window.py</code> <pre><code>def pathway_changed(self):\n    \"\"\"\n    Method for handling pathway changes in the application.\n\n    This method performs several operations when a new global pathway is set:\n    1. Waits for any running thread to complete.\n    2. Updates the global pathway if a valid directory is found.\n    3. Changes the current working directory to the new global pathway.\n    4. Hides various widgets associated with advanced options and outputs.\n    5. Starts a background thread to load data quickly.\n    6. If the provided pathway is invalid, it hides relevant tabs and outputs an error message.\n\n    Notes\n    -----\n    This method performs actions to prepare the application for loading data from a new pathway.\n    It ensures that certain widgets are hidden and starts necessary background processes.\n    \"\"\"\n    if self.thread_dict[\"LoadDataToRunCellectsQuickly\"].isRunning():\n        self.thread_dict[\"LoadDataToRunCellectsQuickly\"].wait()\n    if os.path.isdir(Path(self.global_pathway.text())):\n        self.parent().po.all['global_pathway'] = self.global_pathway.text()\n        logging.info(f\"Dir: {self.parent().po.all['global_pathway']}\")\n        os.chdir(Path(self.parent().po.all['global_pathway']))\n        # 1) Put invisible widgets\n        self.radical.setVisible(False)\n        self.extension.setVisible(False)\n        self.arena_number.setVisible(False)\n        self.im_or_vid.setVisible(False)\n        self.advanced_parameters.setVisible(False)\n        self.required_outputs.setVisible(False)\n        self.Run_all_directly.setVisible(False)\n        self.next.setVisible(False)\n        self.instantiate = True\n        self.video_tab.set_not_usable()\n        # 2) Load the dict\n        self.thread_dict[\"LoadDataToRunCellectsQuickly\"].start()\n        self.thread_dict[\"LoadDataToRunCellectsQuickly\"].message_from_thread.connect(self.load_data_quickly_finished)\n        # 3) go to another func to change, put visible and re_instantiate\n    else:\n        self.Run_all_directly.setVisible(False)\n        self.image_tab.set_not_usable()\n        self.video_tab.set_not_usable()\n        self.message.setText(\"Please, enter a valid path\")\n</code></pre>"},{"location":"api/cellects/gui/first_window/#cellects.gui.first_window.FirstWindow.re_instantiate_widgets","title":"<code>re_instantiate_widgets()</code>","text":"<p>Reinstantiate the videoanalysis window from the parent of the current window.</p> Source code in <code>src/cellects/gui/first_window.py</code> <pre><code>def re_instantiate_widgets(self):\n    \"\"\"\n    Reinstantiate the videoanalysis window from the parent of the current window.\n    \"\"\"\n    self.instantiate = True\n    # Since we re-instantiate everything, image analysis will no longer be available from video analysis:\n    self.parent().videoanalysiswindow.image_tab.set_not_usable()\n</code></pre>"},{"location":"api/cellects/gui/first_window/#cellects.gui.first_window.FirstWindow.required_outputs_is_clicked","title":"<code>required_outputs_is_clicked()</code>","text":"<p>Handle the click event for switching to required outputs.</p> <p>This function sets the <code>last_is_first</code> attribute of the parent to True and changes the widget to the Required Outputs view.</p> Source code in <code>src/cellects/gui/first_window.py</code> <pre><code>def required_outputs_is_clicked(self):\n    \"\"\"\n    Handle the click event for switching to required outputs.\n\n    This function sets the `last_is_first` attribute of the parent to True\n    and changes the widget to the Required Outputs view.\n    \"\"\"\n    self.parent().last_is_first = True\n    self.parent().change_widget(4)  # RequiredOutput\n</code></pre>"},{"location":"api/cellects/gui/first_window/#cellects.gui.first_window.FirstWindow.true_init","title":"<code>true_init()</code>","text":"<p>Initialize the FirstWindow components and setup its layout.</p> <p>Sets up various widgets, layouts, and threading components for the Cellects GUI, including image or video selection, folder path input, arena number management, and display setup.</p> Notes <p>This method assumes that the parent widget has a 'po' attribute with specific settings and variables.</p> Source code in <code>src/cellects/gui/first_window.py</code> <pre><code>def true_init(self):\n    \"\"\"\n    Initialize the FirstWindow components and setup its layout.\n\n    Sets up various widgets, layouts, and threading components for the Cellects GUI,\n    including image or video selection, folder path input, arena number management,\n    and display setup.\n\n    Notes\n    -----\n    This method assumes that the parent widget has a 'po' attribute with specific settings and variables.\n    \"\"\"\n    self.data_tab.set_in_use()\n    self.image_tab.set_not_usable()\n    self.video_tab.set_not_usable()\n    self.thread_dict = {}\n    self.thread_dict[\"LookForData\"] = LookForDataThreadInFirstW(self.parent())\n    self.thread_dict[\"RunAll\"] = RunAllThread(self.parent())\n    self.thread_dict[\"LoadDataToRunCellectsQuickly\"] = LoadDataToRunCellectsQuicklyThread(self.parent())\n    self.thread_dict[\"GetFirstIm\"] = GetFirstImThread(self.parent())\n    self.thread_dict[\"GetExifDataThread\"] = GetExifDataThread(self.parent())\n    self.instantiate: bool = True\n    self.title_label = FixedText('Cellects', police=60, night_mode=self.parent().po.all['night_mode'])\n    self.title_label.setAlignment(QtCore.Qt.AlignHCenter)\n    self.subtitle_line = LineWidget(size=[1, 50], night_mode=self.parent().po.all['night_mode'])\n\n    self.Vlayout.addWidget(self.title_label)\n    self.Vlayout.addWidget(self.subtitle_line)\n    self.Vlayout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.Maximum, QtWidgets.QSizePolicy.MinimumExpanding))\n\n    # 1) Set if this a Image list or Videos\n    # Open the layout:\n    self.second_row_widget = QtWidgets.QWidget()\n    self.second_row_layout = QtWidgets.QHBoxLayout()\n    self.im_or_vid_label = FixedText(FW['Image_list_or_videos']['label'], tip=FW['Image_list_or_videos']['tips'],\n                                     night_mode=self.parent().po.all['night_mode'])\n    # self.im_or_vid_label = FixedText('Image list or Videos:', tip=\"What type of data do(es) contain(s) folder(s)?\", night_mode=self.parent().po.all['night_mode'])\n    self.im_or_vid = Combobox([\"Image list\", \"Videos\"], self.parent().po.all['im_or_vid'], night_mode=self.parent().po.all['night_mode'])\n    self.im_or_vid.setFixedWidth(150)\n    self.im_or_vid.currentTextChanged.connect(self.im2vid)\n    # Set their positions on layout\n    self.second_row_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n    self.second_row_layout.addWidget(self.im_or_vid_label)\n    self.second_row_layout.addWidget(self.im_or_vid)\n    self.second_row_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n    self.second_row_widget.setLayout(self.second_row_layout)\n    self.Vlayout.addWidget(self.second_row_widget)\n\n    # 2) Open the third row layout\n    self.third_row_widget = QtWidgets.QWidget()\n    self.third_row_layout = QtWidgets.QHBoxLayout()\n    # Set default images radical and extension widgets\n    if self.parent().po.all['im_or_vid'] == 0:\n        what = 'Images'\n        if self.parent().po.all['extension'] == '.mp4':\n            self.parent().po.all['radical'] = 'IMG_'\n            self.parent().po.all['extension'] = '.JPG'\n        self.arena_number_label = FixedText('Arena number per folder:',\n                                            tip=FW[\"Arena_number_per_folder\"][\"tips\"] , #\"If this number is not always the same (depending on the folder), it can be changed later\",\n                                            night_mode=self.parent().po.all['night_mode'])\n    else:\n        if self.parent().po.all['extension'] == '.JPG':\n            self.parent().po.all['radical'] = ''\n            self.parent().po.all['extension'] = '.mp4'\n        self.arena_number_label = FixedText('Arena number per folder:',\n                                            tip=FW[\"Arena_number_per_folder\"][\"tips\"], #\"If this number is not always the same (depending on the video), it can be changed later\",\n                                            night_mode=self.parent().po.all['night_mode'])\n        what = 'Videos'\n    self.arena_number_label.setAlignment(QtCore.Qt.AlignVCenter)\n    self.arena_number = Spinbox(min=0, max=255, val=self.parent().po.all['first_folder_sample_number'],\n                                 decimals=0, night_mode=self.parent().po.all['night_mode'])\n    self.arena_number.valueChanged.connect(self.re_instantiate_widgets)\n    self.radical_label = FixedText(what + ' prefix:', tip=FW[\"Image_prefix_and_extension\"][\"tips\"], night_mode=self.parent().po.all['night_mode'])\n    self.radical_label.setAlignment(QtCore.Qt.AlignVCenter)\n    self.radical = EditText(self.parent().po.all['radical'],\n                                   night_mode=self.parent().po.all['night_mode'])\n    self.radical.textChanged.connect(self.re_instantiate_widgets)\n\n    self.extension_label = FixedText(what + ' extension:', tip=FW[\"Image_prefix_and_extension\"][\"tips\"], night_mode=self.parent().po.all['night_mode'])\n    self.extension_label.setAlignment(QtCore.Qt.AlignVCenter)\n    self.extension = EditText(self.parent().po.all['extension'],\n                                   night_mode=self.parent().po.all['night_mode'])\n    self.extension.textChanged.connect(self.re_instantiate_widgets)\n\n    # Set their positions on layout\n    self.third_row_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n    self.third_row_layout.addWidget(self.radical_label)\n    self.third_row_layout.addWidget(self.radical)\n    # self.third_row_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n    self.third_row_layout.addWidget(self.extension_label)\n    self.third_row_layout.addWidget(self.extension)\n    self.third_row_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n    self.third_row_widget.setLayout(self.third_row_layout)\n    self.Vlayout.addWidget(self.third_row_widget)\n    # If im_or_vid changes, adjust these 2 widgets\n\n    # 3) Get the path to the right folder:\n    # Open the layout:\n    self.first_row_widget = QtWidgets.QWidget()\n    self.first_row_layout = QtWidgets.QHBoxLayout()\n\n    self.folder_label = FixedText(FW[\"Folder\"][\"label\"] + ':',\n                                  tip=FW[\"Folder\"][\"tips\"],#\"Path to the folder containing images or videos\\nThe selected folder may also contain several folders of data\",\n                                  night_mode=self.parent().po.all['night_mode'])\n    self.folder_label.setAlignment(QtCore.Qt.AlignVCenter)\n    self.global_pathway = EditText(self.parent().po.all['global_pathway'],\n                                   night_mode=self.parent().po.all['night_mode'])\n    self.global_pathway.setSizePolicy(QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum)\n    self.global_pathway.textChanged.connect(self.pathway_changed)\n    self.browse = PButton(FW[\"Browse\"][\"label\"], tip=FW[\"Browse\"][\"tips\"],\n                          night_mode=self.parent().po.all['night_mode'])\n    self.browse.clicked.connect(self.browse_is_clicked)\n\n    # Set their positions on layout\n    self.first_row_layout.addWidget(self.folder_label)\n    self.first_row_layout.addWidget(self.global_pathway)\n    self.first_row_layout.addWidget(self.browse)\n    self.first_row_widget.setLayout(self.first_row_layout)\n    self.Vlayout.addWidget(self.first_row_widget)\n\n    self.fourth_row_widget = QtWidgets.QWidget()\n    self.fourth_row_layout = QtWidgets.QHBoxLayout()\n    self.fourth_row_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n    self.fourth_row_layout.addWidget(self.arena_number_label)\n    self.fourth_row_layout.addWidget(self.arena_number)\n    self.fourth_row_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n    self.fourth_row_widget.setLayout(self.fourth_row_layout)\n    self.Vlayout.addWidget(self.fourth_row_widget)\n    self.Vlayout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.Maximum, QtWidgets.QSizePolicy.MinimumExpanding))\n\n    # Add the central image display widget\n    self.display_image = np.zeros((self.parent().im_max_height, self.parent().im_max_width, 3), np.uint8)\n    self.display_image = InsertImage(self.display_image, self.parent().im_max_height, self.parent().im_max_width)\n    self.Vlayout.addWidget(self.display_image, alignment=QtCore.Qt.AlignCenter)\n    self.display_image.setVisible(False)\n    self.display_image.mousePressEvent = self.full_screen_display\n\n    # 4) Create the shortcuts row\n    self.shortcuts_widget = QtWidgets.QWidget()\n    self.shortcuts_layout = QtWidgets.QHBoxLayout()\n    # Add shortcuts: Video_analysis and Run directly\n    # Shortcut 1 : Advanced Parameters\n    self.advanced_parameters = PButton(FW[\"Advanced_parameters\"][\"label\"], tip=FW[\"Advanced_parameters\"][\"tips\"],\n                                       night_mode=self.parent().po.all['night_mode'])\n    self.advanced_parameters.clicked.connect(self.advanced_parameters_is_clicked)\n    # Shortcut 2 : Required Outputs\n    self.required_outputs = PButton(FW[\"Required_outputs\"][\"label\"], tip=FW[\"Required_outputs\"][\"tips\"],\n                                    night_mode=self.parent().po.all['night_mode'])\n    self.required_outputs.clicked.connect(self.required_outputs_is_clicked)\n    # Shortcut 3 :\n    self.video_tab.clicked.connect(self.video_analysis_window_is_clicked)\n    # Shortcut 4 :\n    self.Run_all_directly = PButton(FW[\"Run_all_directly\"][\"label\"], tip=FW[\"Run_all_directly\"][\"tips\"],\n                                    night_mode=self.parent().po.all['night_mode'])\n    self.Run_all_directly.clicked.connect(self.Run_all_directly_is_clicked)\n    self.Run_all_directly.setVisible(False)\n\n    self.shortcuts_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n    self.shortcuts_layout.addWidget(self.advanced_parameters)\n    self.shortcuts_layout.addWidget(self.required_outputs)\n    self.shortcuts_layout.addWidget(self.Run_all_directly)\n    self.shortcuts_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n    self.shortcuts_widget.setLayout(self.shortcuts_layout)\n    self.Vlayout.addWidget(self.shortcuts_widget)\n\n    # 5) Open the last row layout\n    self.last_row_widget = QtWidgets.QWidget()\n    self.last_row_layout = QtWidgets.QHBoxLayout()\n\n    # Message\n    self.message = FixedText('', halign='r', night_mode=self.parent().po.all['night_mode'])\n    self.message.setStyleSheet(\"color: rgb(230, 145, 18)\")\n    # Next button\n    self.next = PButton(FW['Next']['label'], tip=FW['Next']['tips'],\n                        night_mode=self.parent().po.all['night_mode'])\n    self.image_tab.clicked.connect(self.next_is_clicked)\n    self.next.clicked.connect(self.next_is_clicked)\n    # Add widgets to the last_row_layout\n    self.last_row_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n    self.last_row_layout.addWidget(self.message)\n    self.last_row_layout.addWidget(self.next)\n    # Close the last_row_layout\n    self.last_row_widget.setLayout(self.last_row_layout)\n    self.Vlayout.addWidget(self.last_row_widget)\n    self.setLayout(self.Vlayout)\n\n    # Check if there is data in the saved folder\n    self.pathway_changed()\n</code></pre>"},{"location":"api/cellects/gui/first_window/#cellects.gui.first_window.FirstWindow.video_analysis_window_is_clicked","title":"<code>video_analysis_window_is_clicked()</code>","text":"<p>Handles the logic for when the \"Video tracking\" button is clicked in the interface, leading to the video analysis window.</p> Notes <p>This function displays an error message when a thread relative to the current window is running. This function also save the id of the following window for later use.</p> Source code in <code>src/cellects/gui/first_window.py</code> <pre><code>def video_analysis_window_is_clicked(self):\n    \"\"\"\n    Handles the logic for when the \"Video tracking\" button is clicked in the interface,\n    leading to the video analysis window.\n\n    Notes\n    -----\n    This function displays an error message when a thread relative to the current window is running.\n    This function also save the id of the following window for later use.\n    \"\"\"\n    if self.video_tab.state != \"not_usable\":\n        if self.thread_dict[\"LookForData\"].isRunning() or self.thread_dict[\"LoadDataToRunCellectsQuickly\"].isRunning() or self.thread_dict[\"GetFirstIm\"].isRunning() or self.thread_dict[\"RunAll\"].isRunning():\n            self.message.setText(\"Wait for the analysis to end, or restart Cellects\")\n        else:\n            self.parent().last_tab = \"data_specifications\"\n            self.parent().change_widget(3) # Should be VideoAnalysisW\n</code></pre>"},{"location":"api/cellects/gui/first_window/#cellects.gui.first_window.FirstWindow.when_look_for_data_finished","title":"<code>when_look_for_data_finished()</code>","text":"<p>Check if there are any data items left in the selected folder and its sub-folders. Display appropriate error messages or proceed with further actions based on the data availability.</p> Notes <p>This function checks if there are any data items (images or videos) left in the selected folder and its sub-folders. If no data is found, it displays an error message. Otherwise, it proceeds with instantiating widgets or starting a thread.</p> Source code in <code>src/cellects/gui/first_window.py</code> <pre><code>def when_look_for_data_finished(self):\n    \"\"\"\n    Check if there are any data items left in the selected folder and its sub-folders.\n    Display appropriate error messages or proceed with further actions based on the data availability.\n\n    Notes\n    -----\n    This function checks if there are any data items (images or videos) left in the selected folder and its sub-folders.\n    If no data is found, it displays an error message. Otherwise, it proceeds with instantiating widgets or starting a thread.\n    \"\"\"\n    if len(self.parent().po.all['folder_list']) == 0 and len(self.parent().po.data_list) == 0:\n        if self.parent().po.all['im_or_vid'] == 1:\n            error_message = f\"There is no videos ({self.parent().po.all['extension']}) in the selected folder and its sub-folders\"\n        else:\n            error_message = f\"There is no images ({self.parent().po.all['extension']}) in the selected folder and its sub-folders\"\n        self.message.setText(error_message)\n    else:\n        self.message.setText('')\n        if self.parent().po.all['folder_number'] &gt; 1:\n            self.parent().instantiate_widgets()\n            self.parent().ifseveralfolderswindow.true_init()\n            self.instantiate = False\n            self.parent().change_widget(1) # IfSeveralFoldersWindow\n        else:\n            self.thread_dict[\"GetFirstIm\"].start()\n            self.thread_dict[\"GetFirstIm\"].message_when_thread_finished.connect(self.first_im_read)\n</code></pre>"},{"location":"api/cellects/gui/if_several_folders_window/","title":"<code>cellects.gui.if_several_folders_window</code>","text":""},{"location":"api/cellects/gui/if_several_folders_window/#cellects.gui.if_several_folders_window","title":"<code>cellects.gui.if_several_folders_window</code>","text":"<p>GUI window for selecting folders when multiple options exist in Cellects analysis workflow.</p> <p>This module implements a second-stage GUI dialog that appears when multiple experiment folders are available. It provides an interface for folder selection via checkboxes and table visualization, with navigation controls to proceed to image analysis or return to prior steps. Includes thread-safe background operations for loading data without freezing the UI.</p> <p>Main Components IfSeveralFoldersWindow : QWidget subclass managing folder selection and analysis workflow navigation</p>"},{"location":"api/cellects/gui/if_several_folders_window/#cellects.gui.if_several_folders_window.IfSeveralFoldersWindow","title":"<code>IfSeveralFoldersWindow</code>","text":"<p>               Bases: <code>WindowType</code></p> <p>Second window of the Cellects GUI, only appears when there are multiple folders.</p> Source code in <code>src/cellects/gui/if_several_folders_window.py</code> <pre><code>class IfSeveralFoldersWindow(WindowType):\n    \"\"\"\n    Second window of the Cellects GUI, only appears when there are multiple folders.\n    \"\"\"\n    def __init__(self, parent, night_mode):\n        \"\"\"\n        Initialize the IfSeveralFolders window with a parent widget and night mode setting.\n\n        Parameters\n        ----------\n        parent : QWidget\n            The parent widget to which this window will be attached.\n        night_mode : bool\n            A boolean indicating whether the night mode should be enabled.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from PySide6 import QtWidgets\n        &gt;&gt;&gt; from cellects.gui.cellects import CellectsMainWidget\n        &gt;&gt;&gt; from cellects.gui.if_several_folders_window import IfSeveralFoldersWindow\n        &gt;&gt;&gt; import sys\n        &gt;&gt;&gt; app = QtWidgets.QApplication([])\n        &gt;&gt;&gt; parent = CellectsMainWidget()\n        &gt;&gt;&gt; session = IfSeveralFoldersWindow(parent, False)\n        &gt;&gt;&gt; session.true_init()\n        &gt;&gt;&gt; parent.insertWidget(0, session)\n        &gt;&gt;&gt; parent.show()\n        &gt;&gt;&gt; sys.exit(app.exec())\n        \"\"\"\n        super().__init__(parent, night_mode)\n        self.setParent(parent)\n\n    def true_init(self):\n        \"\"\"\n        Initialize the IfSeveralFoldersWindow with UI components and settings.\n\n        Extended Description\n        --------------------\n        This method sets up the user interface for the IfSeveralFoldersWindow,\n        including labels, a table widget for folders and sample sizes, checkboxes,\n        buttons for video analysis and running tasks directly, and navigation\n        buttons for previous and next steps. The window supports multiple folder\n        selection and provides a means to control the analysis process.\n\n        Notes\n        -----\n        This method assumes that the parent widget has a 'po' attribute with specific settings and variables.\n        \"\"\"\n        logging.info(\"Initialize IfSeveralFoldersWindow\")\n        self.thread_dict = {}\n        self.thread_dict[\"LoadFirstFolderIfSeveral\"] = LoadFirstFolderIfSeveralThread(self.parent())\n        self.next_clicked_once:bool = False\n        self.layout = QtWidgets.QVBoxLayout()\n\n        self.title_label = FixedText('Select folders to analyze', police=30, night_mode=self.parent().po.all['night_mode'])\n        self.title_label.setAlignment(QtCore.Qt.AlignHCenter)\n        self.layout.addWidget(self.title_label)\n        self.layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.Maximum, QtWidgets.QSizePolicy.MinimumExpanding))\n\n        # 1) add a check box allowing to select every folders\n        self.cb_layout = QtWidgets.QHBoxLayout()\n        self.cb_widget = QtWidgets.QWidget()\n        self.cb_label = FixedText(MF[\"Check_to_select_all_folders\"][\"label\"] + ':', tip=MF[\"Check_to_select_all_folders\"][\"tips\"], night_mode=self.parent().po.all['night_mode'])\n        self.cb = QtWidgets.QCheckBox()\n        self.cb.setChecked(True)\n        self.cb.clicked.connect(self.checked)\n        self.cb_layout.addWidget(self.cb_label)\n        self.cb_layout.addWidget(self.cb)\n        self.cb_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n        self.cb_widget.setLayout(self.cb_layout)\n        self.layout.addWidget(self.cb_widget)\n\n        # 2) Create a folder list and sample number per folder\n        self.tableau = QtWidgets.QTableWidget()  # Scroll Area which contains the widgets, set as the centralWidget\n        self.tableau.setColumnCount(2)\n        self.tableau.setRowCount(len(self.parent().po.all['folder_list']))\n        self.tableau.setHorizontalHeaderLabels(['Folders', 'Sample size'])\n        self.parent().po.all['sample_number_per_folder'] = np.repeat(int(self.parent().po.all['first_folder_sample_number']), self.parent().po.all['folder_number'])\n\n        for i, folder in enumerate(self.parent().po.all['folder_list']):\n            self.tableau.setItem(i, 0, QtWidgets.QTableWidgetItem(folder))\n            self.tableau.setItem(i, 1, QtWidgets.QTableWidgetItem(str(self.parent().po.all['sample_number_per_folder'][i])))\n        self.tableau.horizontalHeader().setSectionResizeMode(0, QtWidgets.QHeaderView.ResizeToContents)\n        self.tableau.horizontalHeader().setSectionResizeMode(1, QtWidgets.QHeaderView.ResizeToContents)\n        self.tableau.setSelectionBehavior(QtWidgets.QAbstractItemView.SelectRows)\n        self.tableau.selectAll()\n        self.tableau.itemSelectionChanged.connect(self.item_selection_changed)\n\n        self.tableau.setShowGrid(False)\n        self.tableau.setSizePolicy(QtWidgets.QSizePolicy.Expanding, QtWidgets.QSizePolicy.Minimum)\n        self.tableau.setVerticalScrollBarPolicy(QtCore.Qt.ScrollBarAsNeeded)\n        self.tableau.horizontalHeader().hide()\n        self.tableau.verticalHeader().hide()\n        self.layout.addWidget(self.tableau)\n\n        # Create the shortcuts row\n        self.shortcuts_widget = QtWidgets.QWidget()\n        self.shortcuts_layout = QtWidgets.QHBoxLayout()\n        self.Video_analysis_window = PButton(\"Video tracking window\", night_mode=self.parent().po.all['night_mode'])\n        self.Video_analysis_window.clicked.connect(self.Video_analysis_window_is_clicked)\n        self.Run_all_directly = PButton(\"Run all directly\", tip=VAW[\"Run_All\"][\"tips\"],\n                                        night_mode=self.parent().po.all['night_mode'])\n        self.Run_all_directly.clicked.connect(self.Run_all_directly_is_clicked)\n        self.Video_analysis_window.setVisible(False)\n        self.Run_all_directly.setVisible(False)\n        self.shortcuts_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.Maximum, QtWidgets.QSizePolicy.MinimumExpanding))\n        self.shortcuts_layout.addWidget(self.Video_analysis_window)\n        self.shortcuts_layout.addWidget(self.Run_all_directly)\n        self.shortcuts_widget.setLayout(self.shortcuts_layout)\n        self.layout.addWidget(self.shortcuts_widget)\n\n        # 3) Previous button\n        self.last_row_layout = QtWidgets.QHBoxLayout()\n        self.last_row_widget = QtWidgets.QWidget()\n        self.previous = PButton('Previous', night_mode=self.parent().po.all['night_mode'])\n        self.previous.clicked.connect(self.previous_is_clicked)\n\n        # 4) Message\n        self.message = QtWidgets.QLabel(self)\n        self.message.setText('')\n        self.message.setStyleSheet(\"color: rgb(230, 145, 18)\")\n        self.message.setAlignment(QtCore.Qt.AlignRight)\n\n        # 5) Next button\n        self.next = PButton('Next', night_mode=self.parent().po.all['night_mode'])\n        self.next.clicked.connect(self.next_is_clicked)\n        self.last_row_layout.addWidget(self.previous)\n        self.last_row_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n        self.last_row_layout.addWidget(self.message)\n        self.last_row_layout.addWidget(self.next)\n        self.last_row_widget.setLayout(self.last_row_layout)\n        self.layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.Maximum, QtWidgets.QSizePolicy.MinimumExpanding))\n        self.layout.addWidget(self.last_row_widget)\n        self.setLayout(self.layout)\n\n    def checked(self):\n        \"\"\"\n        Check or uncheck all entries in the tableau based on checkbox state.\n\n        If the associated checkbox is checked, select all items in the\n        tableau. Otherwise, clear the selection.\n        \"\"\"\n        if self.cb.isChecked():\n            self.tableau.selectAll()\n        else:\n            self.tableau.clearSelection()\n\n    def item_selection_changed(self):\n        \"\"\"\n        Update the checkbox state based on the number of selected items.\n        \"\"\"\n        if (len(self.tableau.selectedItems()) // 2) == len(self.parent().po.all['folder_list']):\n            self.cb.setChecked(True)\n        else:\n            self.cb.setChecked(False)\n\n    def previous_is_clicked(self):\n        \"\"\"\n        Handles the logic for when a \"Previous\" button is clicked in the interface, leading to the FirstWindow.\n\n        It modifies internal state to force the decision between IfSeveralFoldersWindow and ImageAnalysisWindow to be\n        done once again if the user clicks on the \"Next\" button of the FirstWindow.\n        \"\"\"\n        self.next_clicked_once = False\n        self.parent().firstwindow.instantiate = True\n        self.parent().change_widget(0)\n\n    def next_is_clicked(self):\n        \"\"\"\n        Handles the logic for when a \"Next\" button is clicked in the interface, leading to the ImageAnalysisWindow.\n\n        If `self.next_clicked_once` is True, instanties widgets and performs image analysis.\n        Otherwise, checks for selected folders and samples. Updates internal state and starts\n        a thread for loading the first folder if multiple folders are selected.\n\n        Notes\n        -----\n        This function updates the internal state based on user selection and starts a thread\n        for loading data. The `self.parent().po.update_folder_id` method is called to update\n        folder IDs.\n        \"\"\"\n        if self.next_clicked_once:\n            self.instantiates_widgets_and_do_image_analysis()\n        else:\n            self.message.setText(\"Loading, wait...\")\n            item_number = len(self.tableau.selectedItems())\n            if item_number == 0:\n                self.message.setText(\"Select at least one folder\")\n            else:\n                sample_number_per_folder = []\n                folder_list = []\n                for i in np.arange(item_number):\n                    if i % 2 == 0:\n                        folder_list.append(self.tableau.selectedItems()[i].text())\n                    else:\n                        sample_number_per_folder.append(int(self.tableau.selectedItems()[i].text()))\n                self.parent().po.all['first_folder_sample_number'] = int(self.tableau.selectedItems()[1].text())\n\n                self.parent().po.all['folder_list'] = folder_list\n                self.parent().po.all['sample_number_per_folder'] = sample_number_per_folder\n                self.parent().po.update_folder_id(self.parent().po.all['first_folder_sample_number'],\n                                                  self.parent().po.all['folder_list'][0])\n                self.thread_dict[\"LoadFirstFolderIfSeveral\"].start()\n                self.thread_dict[\"LoadFirstFolderIfSeveral\"].message_when_thread_finished.connect(self.first_folder_loaded)\n\n    def first_folder_loaded(self, first_exp_ready_to_run: bool):\n        \"\"\"\n        Set the visibility of widgets and messages based on whether data is found.\n\n        Parameters\n        ----------\n        first_exp_ready_to_run : bool\n            Indicates if the one_experiment data is ready to be run.\n        \"\"\"\n        if first_exp_ready_to_run:\n            self.cb_widget.setVisible(False)\n            self.tableau.setVisible(False)\n            if len(self.parent().po.vars['analyzed_individuals']) != self.parent().po.all['first_folder_sample_number']:\n                self.parent().po.vars['analyzed_individuals'] = np.arange(\n                    self.parent().po.all['first_folder_sample_number']) + 1\n                self.parent().po.sample_number = self.parent().po.all['first_folder_sample_number']\n            self.message.setText(\"Data found, shortcuts are available. Click Next again to redo/improve the image analysis\")\n            self.next_clicked_once = True\n            self.Video_analysis_window.setVisible(True)\n            self.Run_all_directly.setVisible(True)\n            self.parent().firstwindow.Video_analysis_window.setVisible(True)\n            self.parent().firstwindow.Run_all_directly.setVisible(True)\n        else:\n            self.instantiates_widgets_and_do_image_analysis()\n\n    def instantiates_widgets_and_do_image_analysis(self):\n        \"\"\"\n        Instantiate widgets and initialize the image analysis window.\n        -----\n        This function is responsible for:\n            - Instantiating widgets with a specific condition.\n            - Initializing the true initialization of the image analysis window.\n            - Remember to not re-instantiate the image analysis window if the user goes back to the first window.\n            - Changing the current widget to ImageAnalysisWindow.\n        \"\"\"\n        self.parent().instantiate_widgets(severalfolder_included=False)\n        self.parent().imageanalysiswindow.true_init()\n        self.parent().firstwindow.instantiate = False\n        self.parent().change_widget(2)# ImageAnalysisWindow\n\n    def Video_analysis_window_is_clicked(self):\n        \"\"\"\n        Save the identity of the current widget (for future navigation) and change to the video analysis window.\n        \"\"\"\n        self.parent().last_tab = \"data_specifications\"\n        self.parent().change_widget(3)\n\n    def Run_all_directly_is_clicked(self):\n        \"\"\"\n        Run the \"Run all directly\" operation in the parent window.\n\n        This function triggers the execution of the \"Run all directly\"\n        functionality by calling the corresponding method in the parent\n        window and then updates the state accordingly.\n        \"\"\"\n        self.parent().firstwindow.Run_all_directly_is_clicked()\n        self.previous_is_clicked()\n</code></pre>"},{"location":"api/cellects/gui/if_several_folders_window/#cellects.gui.if_several_folders_window.IfSeveralFoldersWindow.Run_all_directly_is_clicked","title":"<code>Run_all_directly_is_clicked()</code>","text":"<p>Run the \"Run all directly\" operation in the parent window.</p> <p>This function triggers the execution of the \"Run all directly\" functionality by calling the corresponding method in the parent window and then updates the state accordingly.</p> Source code in <code>src/cellects/gui/if_several_folders_window.py</code> <pre><code>def Run_all_directly_is_clicked(self):\n    \"\"\"\n    Run the \"Run all directly\" operation in the parent window.\n\n    This function triggers the execution of the \"Run all directly\"\n    functionality by calling the corresponding method in the parent\n    window and then updates the state accordingly.\n    \"\"\"\n    self.parent().firstwindow.Run_all_directly_is_clicked()\n    self.previous_is_clicked()\n</code></pre>"},{"location":"api/cellects/gui/if_several_folders_window/#cellects.gui.if_several_folders_window.IfSeveralFoldersWindow.Video_analysis_window_is_clicked","title":"<code>Video_analysis_window_is_clicked()</code>","text":"<p>Save the identity of the current widget (for future navigation) and change to the video analysis window.</p> Source code in <code>src/cellects/gui/if_several_folders_window.py</code> <pre><code>def Video_analysis_window_is_clicked(self):\n    \"\"\"\n    Save the identity of the current widget (for future navigation) and change to the video analysis window.\n    \"\"\"\n    self.parent().last_tab = \"data_specifications\"\n    self.parent().change_widget(3)\n</code></pre>"},{"location":"api/cellects/gui/if_several_folders_window/#cellects.gui.if_several_folders_window.IfSeveralFoldersWindow.__init__","title":"<code>__init__(parent, night_mode)</code>","text":"<p>Initialize the IfSeveralFolders window with a parent widget and night mode setting.</p> <p>Parameters:</p> Name Type Description Default <code>parent</code> <code>QWidget</code> <p>The parent widget to which this window will be attached.</p> required <code>night_mode</code> <code>bool</code> <p>A boolean indicating whether the night mode should be enabled.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from PySide6 import QtWidgets\n&gt;&gt;&gt; from cellects.gui.cellects import CellectsMainWidget\n&gt;&gt;&gt; from cellects.gui.if_several_folders_window import IfSeveralFoldersWindow\n&gt;&gt;&gt; import sys\n&gt;&gt;&gt; app = QtWidgets.QApplication([])\n&gt;&gt;&gt; parent = CellectsMainWidget()\n&gt;&gt;&gt; session = IfSeveralFoldersWindow(parent, False)\n&gt;&gt;&gt; session.true_init()\n&gt;&gt;&gt; parent.insertWidget(0, session)\n&gt;&gt;&gt; parent.show()\n&gt;&gt;&gt; sys.exit(app.exec())\n</code></pre> Source code in <code>src/cellects/gui/if_several_folders_window.py</code> <pre><code>def __init__(self, parent, night_mode):\n    \"\"\"\n    Initialize the IfSeveralFolders window with a parent widget and night mode setting.\n\n    Parameters\n    ----------\n    parent : QWidget\n        The parent widget to which this window will be attached.\n    night_mode : bool\n        A boolean indicating whether the night mode should be enabled.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from PySide6 import QtWidgets\n    &gt;&gt;&gt; from cellects.gui.cellects import CellectsMainWidget\n    &gt;&gt;&gt; from cellects.gui.if_several_folders_window import IfSeveralFoldersWindow\n    &gt;&gt;&gt; import sys\n    &gt;&gt;&gt; app = QtWidgets.QApplication([])\n    &gt;&gt;&gt; parent = CellectsMainWidget()\n    &gt;&gt;&gt; session = IfSeveralFoldersWindow(parent, False)\n    &gt;&gt;&gt; session.true_init()\n    &gt;&gt;&gt; parent.insertWidget(0, session)\n    &gt;&gt;&gt; parent.show()\n    &gt;&gt;&gt; sys.exit(app.exec())\n    \"\"\"\n    super().__init__(parent, night_mode)\n    self.setParent(parent)\n</code></pre>"},{"location":"api/cellects/gui/if_several_folders_window/#cellects.gui.if_several_folders_window.IfSeveralFoldersWindow.checked","title":"<code>checked()</code>","text":"<p>Check or uncheck all entries in the tableau based on checkbox state.</p> <p>If the associated checkbox is checked, select all items in the tableau. Otherwise, clear the selection.</p> Source code in <code>src/cellects/gui/if_several_folders_window.py</code> <pre><code>def checked(self):\n    \"\"\"\n    Check or uncheck all entries in the tableau based on checkbox state.\n\n    If the associated checkbox is checked, select all items in the\n    tableau. Otherwise, clear the selection.\n    \"\"\"\n    if self.cb.isChecked():\n        self.tableau.selectAll()\n    else:\n        self.tableau.clearSelection()\n</code></pre>"},{"location":"api/cellects/gui/if_several_folders_window/#cellects.gui.if_several_folders_window.IfSeveralFoldersWindow.first_folder_loaded","title":"<code>first_folder_loaded(first_exp_ready_to_run)</code>","text":"<p>Set the visibility of widgets and messages based on whether data is found.</p> <p>Parameters:</p> Name Type Description Default <code>first_exp_ready_to_run</code> <code>bool</code> <p>Indicates if the one_experiment data is ready to be run.</p> required Source code in <code>src/cellects/gui/if_several_folders_window.py</code> <pre><code>def first_folder_loaded(self, first_exp_ready_to_run: bool):\n    \"\"\"\n    Set the visibility of widgets and messages based on whether data is found.\n\n    Parameters\n    ----------\n    first_exp_ready_to_run : bool\n        Indicates if the one_experiment data is ready to be run.\n    \"\"\"\n    if first_exp_ready_to_run:\n        self.cb_widget.setVisible(False)\n        self.tableau.setVisible(False)\n        if len(self.parent().po.vars['analyzed_individuals']) != self.parent().po.all['first_folder_sample_number']:\n            self.parent().po.vars['analyzed_individuals'] = np.arange(\n                self.parent().po.all['first_folder_sample_number']) + 1\n            self.parent().po.sample_number = self.parent().po.all['first_folder_sample_number']\n        self.message.setText(\"Data found, shortcuts are available. Click Next again to redo/improve the image analysis\")\n        self.next_clicked_once = True\n        self.Video_analysis_window.setVisible(True)\n        self.Run_all_directly.setVisible(True)\n        self.parent().firstwindow.Video_analysis_window.setVisible(True)\n        self.parent().firstwindow.Run_all_directly.setVisible(True)\n    else:\n        self.instantiates_widgets_and_do_image_analysis()\n</code></pre>"},{"location":"api/cellects/gui/if_several_folders_window/#cellects.gui.if_several_folders_window.IfSeveralFoldersWindow.instantiates_widgets_and_do_image_analysis","title":"<code>instantiates_widgets_and_do_image_analysis()</code>","text":"Instantiate widgets and initialize the image analysis window. <p>This function is responsible for:     - Instantiating widgets with a specific condition.     - Initializing the true initialization of the image analysis window.     - Remember to not re-instantiate the image analysis window if the user goes back to the first window.     - Changing the current widget to ImageAnalysisWindow.</p> Source code in <code>src/cellects/gui/if_several_folders_window.py</code> <pre><code>def instantiates_widgets_and_do_image_analysis(self):\n    \"\"\"\n    Instantiate widgets and initialize the image analysis window.\n    -----\n    This function is responsible for:\n        - Instantiating widgets with a specific condition.\n        - Initializing the true initialization of the image analysis window.\n        - Remember to not re-instantiate the image analysis window if the user goes back to the first window.\n        - Changing the current widget to ImageAnalysisWindow.\n    \"\"\"\n    self.parent().instantiate_widgets(severalfolder_included=False)\n    self.parent().imageanalysiswindow.true_init()\n    self.parent().firstwindow.instantiate = False\n    self.parent().change_widget(2)# ImageAnalysisWindow\n</code></pre>"},{"location":"api/cellects/gui/if_several_folders_window/#cellects.gui.if_several_folders_window.IfSeveralFoldersWindow.item_selection_changed","title":"<code>item_selection_changed()</code>","text":"<p>Update the checkbox state based on the number of selected items.</p> Source code in <code>src/cellects/gui/if_several_folders_window.py</code> <pre><code>def item_selection_changed(self):\n    \"\"\"\n    Update the checkbox state based on the number of selected items.\n    \"\"\"\n    if (len(self.tableau.selectedItems()) // 2) == len(self.parent().po.all['folder_list']):\n        self.cb.setChecked(True)\n    else:\n        self.cb.setChecked(False)\n</code></pre>"},{"location":"api/cellects/gui/if_several_folders_window/#cellects.gui.if_several_folders_window.IfSeveralFoldersWindow.next_is_clicked","title":"<code>next_is_clicked()</code>","text":"<p>Handles the logic for when a \"Next\" button is clicked in the interface, leading to the ImageAnalysisWindow.</p> <p>If <code>self.next_clicked_once</code> is True, instanties widgets and performs image analysis. Otherwise, checks for selected folders and samples. Updates internal state and starts a thread for loading the first folder if multiple folders are selected.</p> Notes <p>This function updates the internal state based on user selection and starts a thread for loading data. The <code>self.parent().po.update_folder_id</code> method is called to update folder IDs.</p> Source code in <code>src/cellects/gui/if_several_folders_window.py</code> <pre><code>def next_is_clicked(self):\n    \"\"\"\n    Handles the logic for when a \"Next\" button is clicked in the interface, leading to the ImageAnalysisWindow.\n\n    If `self.next_clicked_once` is True, instanties widgets and performs image analysis.\n    Otherwise, checks for selected folders and samples. Updates internal state and starts\n    a thread for loading the first folder if multiple folders are selected.\n\n    Notes\n    -----\n    This function updates the internal state based on user selection and starts a thread\n    for loading data. The `self.parent().po.update_folder_id` method is called to update\n    folder IDs.\n    \"\"\"\n    if self.next_clicked_once:\n        self.instantiates_widgets_and_do_image_analysis()\n    else:\n        self.message.setText(\"Loading, wait...\")\n        item_number = len(self.tableau.selectedItems())\n        if item_number == 0:\n            self.message.setText(\"Select at least one folder\")\n        else:\n            sample_number_per_folder = []\n            folder_list = []\n            for i in np.arange(item_number):\n                if i % 2 == 0:\n                    folder_list.append(self.tableau.selectedItems()[i].text())\n                else:\n                    sample_number_per_folder.append(int(self.tableau.selectedItems()[i].text()))\n            self.parent().po.all['first_folder_sample_number'] = int(self.tableau.selectedItems()[1].text())\n\n            self.parent().po.all['folder_list'] = folder_list\n            self.parent().po.all['sample_number_per_folder'] = sample_number_per_folder\n            self.parent().po.update_folder_id(self.parent().po.all['first_folder_sample_number'],\n                                              self.parent().po.all['folder_list'][0])\n            self.thread_dict[\"LoadFirstFolderIfSeveral\"].start()\n            self.thread_dict[\"LoadFirstFolderIfSeveral\"].message_when_thread_finished.connect(self.first_folder_loaded)\n</code></pre>"},{"location":"api/cellects/gui/if_several_folders_window/#cellects.gui.if_several_folders_window.IfSeveralFoldersWindow.previous_is_clicked","title":"<code>previous_is_clicked()</code>","text":"<p>Handles the logic for when a \"Previous\" button is clicked in the interface, leading to the FirstWindow.</p> <p>It modifies internal state to force the decision between IfSeveralFoldersWindow and ImageAnalysisWindow to be done once again if the user clicks on the \"Next\" button of the FirstWindow.</p> Source code in <code>src/cellects/gui/if_several_folders_window.py</code> <pre><code>def previous_is_clicked(self):\n    \"\"\"\n    Handles the logic for when a \"Previous\" button is clicked in the interface, leading to the FirstWindow.\n\n    It modifies internal state to force the decision between IfSeveralFoldersWindow and ImageAnalysisWindow to be\n    done once again if the user clicks on the \"Next\" button of the FirstWindow.\n    \"\"\"\n    self.next_clicked_once = False\n    self.parent().firstwindow.instantiate = True\n    self.parent().change_widget(0)\n</code></pre>"},{"location":"api/cellects/gui/if_several_folders_window/#cellects.gui.if_several_folders_window.IfSeveralFoldersWindow.true_init","title":"<code>true_init()</code>","text":"<p>Initialize the IfSeveralFoldersWindow with UI components and settings.</p> Extended Description <p>This method sets up the user interface for the IfSeveralFoldersWindow, including labels, a table widget for folders and sample sizes, checkboxes, buttons for video analysis and running tasks directly, and navigation buttons for previous and next steps. The window supports multiple folder selection and provides a means to control the analysis process.</p> Notes <p>This method assumes that the parent widget has a 'po' attribute with specific settings and variables.</p> Source code in <code>src/cellects/gui/if_several_folders_window.py</code> <pre><code>def true_init(self):\n    \"\"\"\n    Initialize the IfSeveralFoldersWindow with UI components and settings.\n\n    Extended Description\n    --------------------\n    This method sets up the user interface for the IfSeveralFoldersWindow,\n    including labels, a table widget for folders and sample sizes, checkboxes,\n    buttons for video analysis and running tasks directly, and navigation\n    buttons for previous and next steps. The window supports multiple folder\n    selection and provides a means to control the analysis process.\n\n    Notes\n    -----\n    This method assumes that the parent widget has a 'po' attribute with specific settings and variables.\n    \"\"\"\n    logging.info(\"Initialize IfSeveralFoldersWindow\")\n    self.thread_dict = {}\n    self.thread_dict[\"LoadFirstFolderIfSeveral\"] = LoadFirstFolderIfSeveralThread(self.parent())\n    self.next_clicked_once:bool = False\n    self.layout = QtWidgets.QVBoxLayout()\n\n    self.title_label = FixedText('Select folders to analyze', police=30, night_mode=self.parent().po.all['night_mode'])\n    self.title_label.setAlignment(QtCore.Qt.AlignHCenter)\n    self.layout.addWidget(self.title_label)\n    self.layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.Maximum, QtWidgets.QSizePolicy.MinimumExpanding))\n\n    # 1) add a check box allowing to select every folders\n    self.cb_layout = QtWidgets.QHBoxLayout()\n    self.cb_widget = QtWidgets.QWidget()\n    self.cb_label = FixedText(MF[\"Check_to_select_all_folders\"][\"label\"] + ':', tip=MF[\"Check_to_select_all_folders\"][\"tips\"], night_mode=self.parent().po.all['night_mode'])\n    self.cb = QtWidgets.QCheckBox()\n    self.cb.setChecked(True)\n    self.cb.clicked.connect(self.checked)\n    self.cb_layout.addWidget(self.cb_label)\n    self.cb_layout.addWidget(self.cb)\n    self.cb_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n    self.cb_widget.setLayout(self.cb_layout)\n    self.layout.addWidget(self.cb_widget)\n\n    # 2) Create a folder list and sample number per folder\n    self.tableau = QtWidgets.QTableWidget()  # Scroll Area which contains the widgets, set as the centralWidget\n    self.tableau.setColumnCount(2)\n    self.tableau.setRowCount(len(self.parent().po.all['folder_list']))\n    self.tableau.setHorizontalHeaderLabels(['Folders', 'Sample size'])\n    self.parent().po.all['sample_number_per_folder'] = np.repeat(int(self.parent().po.all['first_folder_sample_number']), self.parent().po.all['folder_number'])\n\n    for i, folder in enumerate(self.parent().po.all['folder_list']):\n        self.tableau.setItem(i, 0, QtWidgets.QTableWidgetItem(folder))\n        self.tableau.setItem(i, 1, QtWidgets.QTableWidgetItem(str(self.parent().po.all['sample_number_per_folder'][i])))\n    self.tableau.horizontalHeader().setSectionResizeMode(0, QtWidgets.QHeaderView.ResizeToContents)\n    self.tableau.horizontalHeader().setSectionResizeMode(1, QtWidgets.QHeaderView.ResizeToContents)\n    self.tableau.setSelectionBehavior(QtWidgets.QAbstractItemView.SelectRows)\n    self.tableau.selectAll()\n    self.tableau.itemSelectionChanged.connect(self.item_selection_changed)\n\n    self.tableau.setShowGrid(False)\n    self.tableau.setSizePolicy(QtWidgets.QSizePolicy.Expanding, QtWidgets.QSizePolicy.Minimum)\n    self.tableau.setVerticalScrollBarPolicy(QtCore.Qt.ScrollBarAsNeeded)\n    self.tableau.horizontalHeader().hide()\n    self.tableau.verticalHeader().hide()\n    self.layout.addWidget(self.tableau)\n\n    # Create the shortcuts row\n    self.shortcuts_widget = QtWidgets.QWidget()\n    self.shortcuts_layout = QtWidgets.QHBoxLayout()\n    self.Video_analysis_window = PButton(\"Video tracking window\", night_mode=self.parent().po.all['night_mode'])\n    self.Video_analysis_window.clicked.connect(self.Video_analysis_window_is_clicked)\n    self.Run_all_directly = PButton(\"Run all directly\", tip=VAW[\"Run_All\"][\"tips\"],\n                                    night_mode=self.parent().po.all['night_mode'])\n    self.Run_all_directly.clicked.connect(self.Run_all_directly_is_clicked)\n    self.Video_analysis_window.setVisible(False)\n    self.Run_all_directly.setVisible(False)\n    self.shortcuts_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.Maximum, QtWidgets.QSizePolicy.MinimumExpanding))\n    self.shortcuts_layout.addWidget(self.Video_analysis_window)\n    self.shortcuts_layout.addWidget(self.Run_all_directly)\n    self.shortcuts_widget.setLayout(self.shortcuts_layout)\n    self.layout.addWidget(self.shortcuts_widget)\n\n    # 3) Previous button\n    self.last_row_layout = QtWidgets.QHBoxLayout()\n    self.last_row_widget = QtWidgets.QWidget()\n    self.previous = PButton('Previous', night_mode=self.parent().po.all['night_mode'])\n    self.previous.clicked.connect(self.previous_is_clicked)\n\n    # 4) Message\n    self.message = QtWidgets.QLabel(self)\n    self.message.setText('')\n    self.message.setStyleSheet(\"color: rgb(230, 145, 18)\")\n    self.message.setAlignment(QtCore.Qt.AlignRight)\n\n    # 5) Next button\n    self.next = PButton('Next', night_mode=self.parent().po.all['night_mode'])\n    self.next.clicked.connect(self.next_is_clicked)\n    self.last_row_layout.addWidget(self.previous)\n    self.last_row_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n    self.last_row_layout.addWidget(self.message)\n    self.last_row_layout.addWidget(self.next)\n    self.last_row_widget.setLayout(self.last_row_layout)\n    self.layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.Maximum, QtWidgets.QSizePolicy.MinimumExpanding))\n    self.layout.addWidget(self.last_row_widget)\n    self.setLayout(self.layout)\n</code></pre>"},{"location":"api/cellects/gui/image_analysis_window/","title":"<code>cellects.gui.image_analysis_window</code>","text":""},{"location":"api/cellects/gui/image_analysis_window/#cellects.gui.image_analysis_window","title":"<code>cellects.gui.image_analysis_window</code>","text":"<p>Image analysis GUI module for Cellects application</p> <p>This module provides a user interface for configuring and performing image analysis with the Cellects system. It allows users to adjust scaling parameters, manually label cell/background regions, select segmentation methods (quick/careful), visualize results, and validate analysis outcomes through interactive decision prompts. The UI supports manual arena delineation when automatic detection fails, using threaded operations for background processing.</p> <p>Main Components ImageAnalysisWindow : Main UI window for image analysis configuration and execution.</p> <p>Includes parameter controls (scaling, spot shape/size), segmentation options (quick/careful/visualize) Provides cell/background selection buttons with manual drawing capabilities Features decision prompts via Yes/No buttons to validate intermediate results Displays real-time image updates with user-defined annotations Notes Uses QThread for background operations to maintain UI responsiveness.</p>"},{"location":"api/cellects/gui/image_analysis_window/#cellects.gui.image_analysis_window.ImageAnalysisWindow","title":"<code>ImageAnalysisWindow</code>","text":"<p>               Bases: <code>MainTabsType</code></p> Source code in <code>src/cellects/gui/image_analysis_window.py</code> <pre><code>class ImageAnalysisWindow(MainTabsType):\n    def __init__(self, parent: object, night_mode: bool):\n        \"\"\"\n        Initialize the ImageAnalysis window with a parent widget and night mode setting.\n\n        Parameters\n        ----------\n        parent : QWidget\n            The parent widget to which this window will be attached.\n        night_mode : bool\n            A boolean indicating whether the night mode should be enabled.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from PySide6 import QtWidgets\n        &gt;&gt;&gt; from cellects.gui.cellects import CellectsMainWidget\n        &gt;&gt;&gt; from cellects.gui.image_analysis_window import ImageAnalysisWindow\n        &gt;&gt;&gt; from cellects.core.program_organizer import ProgramOrganizer\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; import sys\n        &gt;&gt;&gt; app = QtWidgets.QApplication([])\n        &gt;&gt;&gt; parent = CellectsMainWidget()\n        &gt;&gt;&gt; parent.po = ProgramOrganizer()\n        &gt;&gt;&gt; parent.po.update_variable_dict()\n        &gt;&gt;&gt; parent.po.get_first_image(np.zeros((10, 10), dtype=np.uint8), 1)\n        &gt;&gt;&gt; session = ImageAnalysisWindow(parent, False)\n        &gt;&gt;&gt; session.true_init()\n        &gt;&gt;&gt; parent.insertWidget(0, session)\n        &gt;&gt;&gt; parent.show()\n        &gt;&gt;&gt; sys.exit(app.exec())\n        \"\"\"\n        super().__init__(parent, night_mode)\n        self.setParent(parent)\n        self.csc_dict = self.parent().po.vars['convert_for_origin'] # To change\n        self.manual_delineation_flag: bool = False\n\n    def true_init(self):\n        \"\"\"\n        Initialize the ImageAnalysisWindow class with default settings and UI components.\n\n        This function sets up the initial state of the ImageAnalysisWindow, including various flags,\n        labels, input fields, and layout configurations. It also initializes the display image\n        and connects UI elements to their respective event handlers.\n\n        Notes\n        -----\n        This method assumes that the parent widget has a 'po' attribute with specific settings and variables.\n        \"\"\"\n        logging.info(\"Initialize ImageAnalysisWindow\")\n        self.data_tab.set_not_in_use()\n        self.image_tab.set_in_use()\n        self.video_tab.set_not_usable()\n        self.hold_click_flag: bool = False\n        self.is_first_image_flag: bool = True\n        self.is_image_analysis_running: bool = False\n        self.is_image_analysis_display_running: bool = False\n        self.asking_first_im_parameters_flag: bool = True\n        self.first_im_parameters_answered: bool = False\n        self.auto_delineation_flag: bool = False\n        self.delineation_done: bool = False\n        self.asking_delineation_flag: bool = False\n        self.asking_slower_or_manual_delineation_flag: bool = False\n        self.slower_delineation_flag: bool = False\n        self.asking_last_image_flag: bool = False\n        self.step = 0\n        self.temporary_mask_coord = []\n        self.saved_coord = []\n        self.back1_bio2 = 0\n        self.bio_masks_number = 0\n        self.back_masks_number = 0\n        self.arena_masks_number = 0\n        self.available_bio_names = np.arange(1, 1000, dtype=np.uint16)\n        self.available_back_names = np.arange(1, 1000, dtype=np.uint16)\n        self.parent().po.current_combination_id = 0\n\n        self.display_image = np.zeros((self.parent().im_max_width, self.parent().im_max_width, 3), np.uint8)\n        self.display_image = InsertImage(self.display_image, self.parent().im_max_height, self.parent().im_max_width)\n        self.display_image.mousePressEvent = self.get_click_coordinates\n        self.display_image.mouseMoveEvent = self.get_mouse_move_coordinates\n        self.display_image.mouseReleaseEvent = self.get_mouse_release_coordinates\n\n        ## Title\n        self.image_number_label = FixedText(IAW[\"Image_number\"][\"label\"],\n                                            tip=IAW[\"Image_number\"][\"tips\"],\n                                            night_mode=self.parent().po.all['night_mode'])\n        self.image_number_label.setAlignment(QtCore.Qt.AlignVCenter)\n        self.image_number = Spinbox(min=0, max=self.parent().po.vars['img_number'] - 1, val=self.parent().po.vars['first_detection_frame'], night_mode=self.parent().po.all['night_mode'])\n        self.read = PButton(\"Read\", night_mode=self.parent().po.all['night_mode'])\n        self.read.clicked.connect(self.read_is_clicked)\n        if self.parent().po.all[\"im_or_vid\"] == 0 and len(self.parent().po.data_list) == 1:\n            # If there is only one image in the folder\n            self.image_number.setVisible(False)\n            self.image_number_label.setVisible(False)\n            self.read.setVisible(False)\n\n        self.one_blob_per_arena = Checkbox(not self.parent().po.vars['several_blob_per_arena'])\n        self.one_blob_per_arena.stateChanged.connect(self.several_blob_per_arena_check)\n        self.one_blob_per_arena_label = FixedText(IAW[\"several_blob_per_arena\"][\"label\"], valign=\"c\",\n                                                  tip=IAW[\"several_blob_per_arena\"][\"tips\"],\n                                                  night_mode=self.parent().po.all['night_mode'])\n\n\n        self.scale_with_label = FixedText(IAW[\"Scale_with\"][\"label\"] + ':', valign=\"c\",\n                                        tip=IAW[\"Scale_with\"][\"tips\"],\n                                        night_mode=self.parent().po.all['night_mode'])\n        self.scale_with = Combobox([\"Image horizontal size\", \"Cell(s) horizontal size\"], night_mode=self.parent().po.all['night_mode'])\n        self.scale_with.setFixedWidth(280)\n        self.scale_with.setCurrentIndex(self.parent().po.all['scale_with_image_or_cells'])\n        self.scale_size_label = FixedText(IAW[\"Scale_size\"][\"label\"] + ':', valign=\"c\",\n                                          tip=IAW[\"Scale_size\"][\"tips\"],\n                                          night_mode=self.parent().po.all['night_mode'])\n        if self.parent().po.all['scale_with_image_or_cells'] == 0:\n            self.horizontal_size = Spinbox(min=0, max=100000,\n                                        val=self.parent().po.all['image_horizontal_size_in_mm'],\n                                        night_mode=self.parent().po.all['night_mode'])\n        else:\n            self.horizontal_size = Spinbox(min=0, max=100000,\n                                        val=self.parent().po.all['starting_blob_hsize_in_mm'],\n                                        night_mode=self.parent().po.all['night_mode'])\n        self.horizontal_size.valueChanged.connect(self.horizontal_size_changed)\n        self.scale_with.currentTextChanged.connect(self.scale_with_changed)\n        self.scale_unit_label = FixedText(' mm', night_mode=self.parent().po.all['night_mode'])\n\n        # 1) Open the first row layout\n        self.row1_widget = QtWidgets.QWidget()\n        self.row1_layout = QtWidgets.QHBoxLayout()\n        self.row1_layout.addWidget(self.image_number_label)\n        self.row1_layout.addWidget(self.image_number)\n        self.row1_layout.addWidget(self.read)\n        self.row1_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n        self.row1_layout.addWidget(self.one_blob_per_arena_label)\n        self.row1_layout.addWidget(self.one_blob_per_arena)\n        self.row1_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n        self.row1_layout.addWidget(self.scale_with_label)\n        self.row1_layout.addWidget(self.scale_with)\n        self.row1_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n        self.row1_layout.addWidget(self.scale_size_label)\n        self.row1_layout.addWidget(self.horizontal_size)\n\n        self.row1_widget.setLayout(self.row1_layout)\n        self.Vlayout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.Maximum, QtWidgets.QSizePolicy.MinimumExpanding))\n        self.Vlayout.addWidget(self.row1_widget)\n        self.Vlayout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.Maximum, QtWidgets.QSizePolicy.MinimumExpanding))\n\n        # 2) Open the central row layout\n        self.central_row_widget = QtWidgets.QWidget()\n        self.central_row_layout = QtWidgets.QGridLayout()\n\n        # it will contain a) the user drawn lines, b) the image, c) the csc\n        # 2)a) the user drawn lines\n        self.user_drawn_lines_widget = QtWidgets.QWidget()\n        self.user_drawn_lines_layout = QtWidgets.QVBoxLayout()\n        self.user_drawn_lines_label = FixedText(IAW[\"Select_and_draw\"][\"label\"] + \":\",\n                                                tip=IAW[\"Select_and_draw\"][\"tips\"],\n                                                night_mode=self.parent().po.all['night_mode'])\n        self.user_drawn_lines_label.setAlignment(QtCore.Qt.AlignHCenter)\n        self.user_drawn_lines_layout.addWidget(self.user_drawn_lines_label)\n        self.pbuttons_widget = QtWidgets.QWidget()\n        self.pbuttons_layout = QtWidgets.QHBoxLayout()\n        self.cell = PButton(\"Cell\", False, tip=IAW[\"Draw_buttons\"][\"tips\"],\n                            night_mode=self.parent().po.all['night_mode'])\n        self.cell.setFixedWidth(150)\n        self.background = PButton(\"Back\", False, tip=IAW[\"Draw_buttons\"][\"tips\"],\n                                  night_mode=self.parent().po.all['night_mode'])\n        self.background.setFixedWidth(150)\n        self.cell.clicked.connect(self.cell_is_clicked)\n        self.background.clicked.connect(self.background_is_clicked)\n        self.pbuttons_layout.addWidget(self.cell)\n        self.pbuttons_layout.addWidget(self.background)\n        self.pbuttons_widget.setLayout(self.pbuttons_layout)\n        self.user_drawn_lines_layout.addWidget(self.pbuttons_widget)\n\n        self.pbuttons_tables_widget = QtWidgets.QWidget()\n        self.pbuttons_tables_layout = QtWidgets.QHBoxLayout()\n        self.pbuttons_tables_layout.setAlignment(QtCore.Qt.AlignHCenter)\n        self.bio_pbuttons_table = QtWidgets.QScrollArea()#QTableWidget()  # Scroll Area which contains the widgets, set as the centralWidget\n        self.bio_pbuttons_table.setSizePolicy(QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.MinimumExpanding)\n        self.bio_pbuttons_table.setMinimumHeight(self.parent().im_max_height // 2)\n        self.bio_pbuttons_table.setFrameShape(QtWidgets.QFrame.NoFrame)\n        self.bio_pbuttons_table.setVerticalScrollBarPolicy(QtCore.Qt.ScrollBarAsNeeded)\n        self.back_pbuttons_table = QtWidgets.QScrollArea()#QTableWidget()  # Scroll Area which contains the widgets, set as the centralWidget\n        self.back_pbuttons_table.setMinimumHeight(self.parent().im_max_height // 2)\n        self.back_pbuttons_table.setFrameShape(QtWidgets.QFrame.NoFrame)\n        self.back_pbuttons_table.setSizePolicy(QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.MinimumExpanding)\n        self.back_pbuttons_table.setVerticalScrollBarPolicy(QtCore.Qt.ScrollBarAsNeeded)\n\n        self.bio_added_lines_widget = QtWidgets.QWidget()\n        self.back_added_lines_widget = QtWidgets.QWidget()\n        self.bio_added_lines_layout = QtWidgets.QVBoxLayout()\n        self.back_added_lines_layout = QtWidgets.QVBoxLayout()\n        self.back_added_lines_widget.setLayout(self.back_added_lines_layout)\n        self.bio_added_lines_widget.setLayout(self.bio_added_lines_layout)\n        self.bio_pbuttons_table.setWidget(self.bio_added_lines_widget)\n        self.back_pbuttons_table.setWidget(self.back_added_lines_widget)\n        self.bio_pbuttons_table.setWidgetResizable(True)\n        self.back_pbuttons_table.setWidgetResizable(True)\n\n        self.pbuttons_tables_layout.addWidget(self.bio_pbuttons_table)\n        self.pbuttons_tables_layout.addWidget(self.back_pbuttons_table)\n        self.pbuttons_tables_widget.setLayout(self.pbuttons_tables_layout)\n        self.user_drawn_lines_layout.addWidget(self.pbuttons_tables_widget)\n\n        # # Dynamically add the lines\n        self.bio_lines = {}\n        self.back_lines = {}\n        self.arena_lines = {}\n\n        self.user_drawn_lines_widget.setLayout(self.user_drawn_lines_layout)\n        self.user_drawn_lines_widget.setSizePolicy(QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.MinimumExpanding)\n        self.central_row_layout.addWidget(self.user_drawn_lines_widget, 0, 0)\n\n        # 2)b) the image\n        self.central_row_layout.addWidget(self.display_image, 0, 1)\n\n        # Need to create this before self.generate_csc_editing()\n        self.message = FixedText(\"\", halign=\"r\", night_mode=self.parent().po.all['night_mode'])\n        self.message.setStyleSheet(\"color: rgb(230, 145, 18)\")\n\n        # 2)c) The csc editing\n        self.generate_csc_editing()\n\n        self.central_row_layout.addWidget(self.central_right_widget, 0, 2)\n        self.central_row_layout.setAlignment(QtCore.Qt.AlignLeft)\n        self.central_row_layout.setAlignment(QtCore.Qt.AlignHCenter)\n        # 2) Close the central row layout\n        self.central_row_widget.setLayout(self.central_row_layout)\n        self.Vlayout.addWidget(self.central_row_widget)\n        self.Vlayout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.Maximum, QtWidgets.QSizePolicy.MinimumExpanding))\n\n        # 3) Add Set supplementary parameters row 1\n        self.sup_param_row1_widget = QtWidgets.QWidget()\n        self.sup_param_row1_layout = QtWidgets.QHBoxLayout()\n\n        # 4) Add Set supplementary parameters row2\n        self.sup_param_row2_widget = QtWidgets.QWidget()\n        self.sup_param_row2_layout = QtWidgets.QHBoxLayout()\n\n        self.arena_shape_label = FixedText(IAW[\"Arena_shape\"][\"label\"], tip=IAW[\"Arena_shape\"][\"tips\"],\n                                           night_mode=self.parent().po.all['night_mode'])\n        self.arena_shape = Combobox(['circle', 'rectangle'], night_mode=self.parent().po.all['night_mode'])\n        self.arena_shape.setFixedWidth(160)\n        self.arena_shape.setCurrentText(self.parent().po.vars['arena_shape'])\n        self.arena_shape.currentTextChanged.connect(self.arena_shape_changed)\n        self.set_spot_shape = Checkbox(self.parent().po.all['set_spot_shape'])\n        self.set_spot_shape.stateChanged.connect(self.set_spot_shape_check)\n        self.spot_shape_label = FixedText(IAW[\"Spot_shape\"][\"label\"], tip=IAW[\"Spot_shape\"][\"tips\"], night_mode=self.parent().po.all['night_mode'])\n        self.spot_shape = Combobox(['circle', 'rectangle'], night_mode=self.parent().po.all['night_mode'])\n        self.spot_shape.setFixedWidth(160)\n        if self.parent().po.all['starting_blob_shape'] is None:\n            self.spot_shape.setCurrentIndex(0)\n        else:\n            self.spot_shape.setCurrentText(self.parent().po.all['starting_blob_shape'])\n        self.spot_shape.currentTextChanged.connect(self.spot_shape_changed)\n        self.set_spot_size = Checkbox(self.parent().po.all['set_spot_size'])\n        self.set_spot_size.stateChanged.connect(self.set_spot_size_check)\n        self.spot_size_label = FixedText(IAW[\"Spot_size\"][\"label\"], tip=IAW[\"Spot_size\"][\"tips\"],\n                                         night_mode=self.parent().po.all['night_mode'])\n        self.spot_size = Spinbox(min=0, max=100000, val=self.parent().po.all['starting_blob_hsize_in_mm'], decimals=2,\n                                 night_mode=self.parent().po.all['night_mode'])\n        self.spot_size.valueChanged.connect(self.spot_size_changed)\n        self.sup_param_row2_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n        self.sup_param_row2_layout.addWidget(self.arena_shape_label)\n        self.sup_param_row2_layout.addWidget(self.arena_shape)\n        self.sup_param_row2_layout.addWidget(self.set_spot_shape)\n        self.sup_param_row2_layout.addWidget(self.spot_shape_label)\n        self.sup_param_row2_layout.addWidget(self.spot_shape)\n        self.sup_param_row2_layout.addWidget(self.set_spot_size)\n        self.sup_param_row2_layout.addWidget(self.spot_size_label)\n        self.sup_param_row2_layout.addWidget(self.spot_size)\n        self.sup_param_row2_widget.setLayout(self.sup_param_row2_layout)\n        self.sup_param_row2_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n        self.Vlayout.addWidget(self.sup_param_row2_widget)\n\n        self.one_blob_per_arena.setVisible(True)\n        self.one_blob_per_arena_label.setVisible(True)\n        self.set_spot_shape.setVisible(False)\n        self.spot_shape_label.setVisible(False)\n        self.spot_shape.setVisible(False)\n        self.arena_shape_label.setVisible(False)\n        self.arena_shape.setVisible(False)\n        self.set_spot_size.setVisible(False)\n        self.spot_size_label.setVisible(False)\n        self.spot_size.setVisible(False)\n\n        # 5) Add the generate option row\n        self.generate_analysis_options = FixedText(IAW[\"Generate_analysis_options\"][\"label\"] + \": \",\n                                                   tip=IAW[\"Generate_analysis_options\"][\"tips\"],\n                                                   night_mode=self.parent().po.all['night_mode'])\n        self.basic = PButton(\"Basic\", night_mode=self.parent().po.all['night_mode'])\n        self.basic.clicked.connect(self.basic_is_clicked)\n        self.network_shaped = PButton(\"Network-shaped\", night_mode=self.parent().po.all['night_mode'])\n        self.network_shaped.clicked.connect(self.network_shaped_is_clicked)\n        self.network_shaped.setVisible(False)\n        self.visualize = PButton('Apply current config', night_mode=self.parent().po.all['night_mode'])\n        self.visualize.clicked.connect(self.visualize_is_clicked)\n        if self.parent().po.vars['already_greyscale']:\n            self.visualize_label = FixedText(\"Directly: \", night_mode=self.parent().po.all['night_mode'])\n        else:\n            self.visualize_label = FixedText(\"Or directly: \", night_mode=self.parent().po.all['night_mode'])\n\n        self.sup_param_row1_layout.addWidget(self.generate_analysis_options)\n        self.sup_param_row1_layout.addWidget(self.basic)\n        self.sup_param_row1_layout.addWidget(self.network_shaped)\n        self.sup_param_row1_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n        self.sup_param_row1_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n        self.sup_param_row1_layout.addWidget(self.visualize_label)\n        self.sup_param_row1_layout.addWidget(self.visualize)\n\n        self.sup_param_row1_widget.setLayout(self.sup_param_row1_layout)\n        self.Vlayout.addWidget(self.sup_param_row1_widget)\n\n        # 6) Open the choose best option row layout\n        self.options_row_widget = QtWidgets.QWidget()\n        self.options_row_layout = QtWidgets.QHBoxLayout()\n        self.select_option_label = FixedText(IAW[\"Select_option_to_read\"][\"label\"],\n                                             tip=IAW[\"Select_option_to_read\"][\"tips\"],\n                                             night_mode=self.parent().po.all['night_mode'])\n        self.select_option = Combobox([], night_mode=self.parent().po.all['night_mode'])\n        if self.parent().po.vars['color_number'] == 2:\n            self.select_option.setCurrentIndex(self.parent().po.all['video_option'])\n        self.select_option.setSizePolicy(QtWidgets.QSizePolicy.Maximum, QtWidgets.QSizePolicy.Maximum)\n        self.select_option.setMinimumWidth(145)\n        self.select_option.currentTextChanged.connect(self.option_changed)\n        self.n_shapes_detected = FixedText(f'', night_mode=self.parent().po.all['night_mode'])\n        self.select_option_label.setVisible(False)\n        self.select_option.setVisible(False)\n        self.n_shapes_detected.setVisible(False)\n        self.options_row_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n        self.options_row_layout.addWidget(self.select_option_label)\n        self.options_row_layout.addWidget(self.select_option)\n        self.options_row_layout.addWidget(self.n_shapes_detected)\n        self.options_row_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n        self.options_row_widget.setLayout(self.options_row_layout)\n        self.Vlayout.addWidget(self.options_row_widget)\n\n        # 7) Open decision row layout\n        self.decision_row_widget = QtWidgets.QWidget()\n        self.decision_row_layout = QtWidgets.QHBoxLayout()\n        self.decision_label = FixedText(\"\", night_mode=self.parent().po.all['night_mode'])\n        self.yes = PButton(\"Yes\", night_mode=self.parent().po.all['night_mode'])\n        self.yes.clicked.connect(self.when_yes_is_clicked)\n        self.no = PButton(\"No\", night_mode=self.parent().po.all['night_mode'])\n        self.no.clicked.connect(self.when_no_is_clicked)\n\n        self.decision_label.setVisible(False)\n        self.yes.setVisible(False)\n        self.no.setVisible(False)\n        self.decision_row_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n        self.decision_row_layout.addWidget(self.decision_label)\n        self.decision_row_layout.addWidget(self.yes)\n        self.decision_row_layout.addWidget(self.no)\n        self.decision_row_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n        self.decision_row_widget.setLayout(self.decision_row_layout)\n        self.Vlayout.addWidget(self.decision_row_widget)\n\n        # 8) Open the special cases layout\n        self.special_cases_widget = QtWidgets.QWidget()\n        self.special_cases_layout = QtWidgets.QHBoxLayout()\n        self.starting_differs_from_growing_cb = Checkbox(self.parent().po.vars['origin_state'] == 'constant')\n        self.starting_differs_from_growing_cb.stateChanged.connect(self.starting_differs_from_growing_check)\n        self.starting_differs_from_growing_label = FixedText(IAW[\"Start_differs_from_arena\"][\"label\"],\n                                                             tip=IAW[\"Start_differs_from_arena\"][\"tips\"],\n                                                             night_mode=self.parent().po.all['night_mode'])\n        self.starting_differs_from_growing_cb.setVisible(False)\n        self.starting_differs_from_growing_label.setVisible(False)\n        self.special_cases_layout.addWidget(self.starting_differs_from_growing_cb)\n        self.special_cases_layout.addWidget(self.starting_differs_from_growing_label)\n        self.special_cases_widget.setLayout(self.special_cases_layout)\n        self.Vlayout.addWidget(self.special_cases_widget)\n\n        # 9) Open the last row layout\n        self.last_row_widget = QtWidgets.QWidget()\n        self.last_row_layout = QtWidgets.QHBoxLayout()\n        self.previous = PButton('Previous', night_mode=self.parent().po.all['night_mode'])\n        self.previous.clicked.connect(self.previous_is_clicked)\n        self.data_tab.clicked.connect(self.data_is_clicked)\n        self.video_tab.clicked.connect(self.video_is_clicked)\n        self.complete_image_analysis = PButton(IAW[\"Save_image_analysis\"][\"label\"],\n                                               tip=IAW[\"Save_image_analysis\"][\"tips\"],\n                                               night_mode=self.parent().po.all['night_mode'])\n        self.complete_image_analysis.setVisible(False)\n        self.complete_image_analysis.clicked.connect(self.complete_image_analysis_is_clicked)\n        self.next = PButton(\"Next\", night_mode=self.parent().po.all['night_mode'])\n        self.next.setVisible(False)\n        self.next.clicked.connect(self.go_to_next_widget)\n        self.last_row_layout.addWidget(self.previous)\n        self.last_row_layout.addWidget(self.message)\n        self.last_row_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n        self.last_row_layout.addWidget(self.complete_image_analysis)\n        self.last_row_layout.addWidget(self.next)\n        self.last_row_widget.setLayout(self.last_row_layout)\n        self.Vlayout.addWidget(self.last_row_widget)\n        self.Vlayout.setSpacing(0)\n        self.setLayout(self.Vlayout)\n\n        self.advanced_mode_check()\n\n        self.thread_dict = {}\n        self.thread_dict[\"GetFirstIm\"] = GetFirstImThread(self.parent())\n        self.reinitialize_image_and_masks(self.parent().po.first_im)\n        self.thread_dict[\"GetLastIm\"] = GetLastImThread(self.parent())\n        if self.parent().po.all['im_or_vid'] == 0:\n            self.thread_dict[\"GetLastIm\"].start()\n        self.parent().po.first_image = OneImageAnalysis(self.parent().po.first_im)\n        self.thread_dict[\"FirstImageAnalysis\"] = FirstImageAnalysisThread(self.parent())\n        self.thread_dict[\"LastImageAnalysis\"] = LastImageAnalysisThread(self.parent())\n        self.thread_dict['UpdateImage'] = UpdateImageThread(self.parent())\n        self.thread_dict['CropScaleSubtractDelineate'] = CropScaleSubtractDelineateThread(self.parent())\n        self.thread_dict['SaveManualDelineation'] = SaveManualDelineationThread(self.parent())\n        self.thread_dict['CompleteImageAnalysisThread'] = CompleteImageAnalysisThread(self.parent())\n        self.thread_dict['PrepareVideoAnalysis'] = PrepareVideoAnalysisThread(self.parent())\n\n    def previous_is_clicked(self):\n        \"\"\"\n        Handles the logic for when a \"Previous\" button is clicked in the interface, leading to the FirstWindow.\n\n        This method resets various flags and variables related to image analysis\n        to their initial state. It is called when the \"Previous\" button is clicked,\n        preparing the application for new input and reinitialization.\n        \"\"\"\n        if self.is_image_analysis_running:\n            self.message.setText(\"Wait for the analysis to end, or restart Cellects\")\n        else:\n            self.parent().firstwindow.instantiate = True\n            self.hold_click_flag: bool = False\n            self.is_first_image_flag: bool = True\n            self.is_image_analysis_running: bool = False\n            self.is_image_analysis_display_running: bool = False\n            self.asking_first_im_parameters_flag: bool = True\n            self.first_im_parameters_answered: bool = False\n            self.auto_delineation_flag: bool = False\n            self.delineation_done: bool = False\n            self.asking_delineation_flag: bool = False\n            self.asking_slower_or_manual_delineation_flag: bool = False\n            self.slower_delineation_flag: bool = False\n            self.asking_last_image_flag: bool = False\n            self.step = 0\n            self.temporary_mask_coord = []\n            self.saved_coord = []\n            self.back1_bio2 = 0\n            self.bio_masks_number = 0\n            self.back_masks_number = 0\n            self.arena_masks_number = 0\n            self.available_bio_names = np.arange(1, 1000, dtype=np.uint16)\n            self.available_back_names = np.arange(1, 1000, dtype=np.uint16)\n            self.parent().po.current_combination_id = 0\n            self.parent().last_tab = \"data_specifications\"\n            self.parent().change_widget(0)  # First\n\n    def data_is_clicked(self):\n        \"\"\"\n        Handles the logic for when the \"Data specifications\" button is clicked in the interface,\n        leading to the FirstWindow.\n\n        Notes\n        -----\n        This function displays an error message when a thread relative to the current window is running.\n        This function also save the id of this tab for later use.\n        \"\"\"\n        if self.is_image_analysis_running:\n            self.message.setText(\"Wait for the analysis to end, or restart Cellects\")\n        else:\n            self.parent().last_tab = \"data_specifications\"\n            self.parent().change_widget(0)  # First\n\n    def video_is_clicked(self):\n        \"\"\"\n        Handles the logic for when the \"Video tracking\" button is clicked in the interface,\n        leading to the video analysis window.\n\n        Notes\n        -----\n        This function displays an error message when a thread relative to the current window is running.\n        This function also save the id of the following window for later use.\n        \"\"\"\n        if self.video_tab.state != \"not_usable\":\n            if self.is_image_analysis_running:\n                self.message.setText(\"Wait for the analysis to end, or restart Cellects\")\n            else:\n                self.parent().last_tab = \"image_analysis\"\n                self.parent().change_widget(3)\n\n    def read_is_clicked(self):\n        \"\"\"\n        Read an image (numbered using natural sorting) from the selected folder\n\n        This method handles the logic for starting image reading when the \"Read\" button is clicked.\n        It ensures that only one thread runs at a time, updates the UI with relevant messages,\n        and resets visual components once processing begins.\n        \"\"\"\n        if not self.thread_dict[\"GetFirstIm\"].isRunning():\n            self.parent().po.vars['first_detection_frame'] = int(self.image_number.value())\n            self.message.setText(f\"Reading image n\u00b0{self.parent().po.vars['first_detection_frame']}\")\n            self.thread_dict[\"GetFirstIm\"].start()\n            self.thread_dict[\"GetFirstIm\"].message_when_thread_finished.connect(self.reinitialize_image_and_masks)\n            self.reinitialize_bio_and_back_legend()\n\n\n    def several_blob_per_arena_check(self):\n        \"\"\"\n        Checks or unchecks the option for having several blobs per arena.\n        \"\"\"\n        is_checked = self.one_blob_per_arena.isChecked()\n        self.parent().po.vars['several_blob_per_arena'] = not is_checked\n        self.set_spot_size.setVisible(is_checked)\n        self.spot_size_label.setVisible(is_checked)\n        self.spot_size.setVisible(is_checked and self.set_spot_size.isChecked())\n\n    def set_spot_size_check(self):\n        \"\"\"\n        Set the visibility of spot size based on checkbox state.\n        \"\"\"\n        is_checked = self.set_spot_size.isChecked()\n        if self.step == 1:\n            self.spot_size.setVisible(is_checked)\n        self.parent().po.all['set_spot_size'] = is_checked\n\n    def spot_size_changed(self):\n        \"\"\"\n        Update the starting blob size and corresponding horizontal size based on user input.\n        \"\"\"\n        self.parent().po.all['starting_blob_hsize_in_mm'] = self.spot_size.value()\n        if self.parent().po.all['scale_with_image_or_cells'] == 1:\n            self.horizontal_size.setValue(self.parent().po.all['starting_blob_hsize_in_mm'])\n        self.set_spot_size_check()\n\n    def set_spot_shape_check(self):\n        \"\"\"\n        Set the spot shape setting visibility.\n        \"\"\"\n        is_checked = self.set_spot_shape.isChecked()\n        self.spot_shape.setVisible(is_checked)\n        self.parent().po.all['set_spot_shape'] = is_checked\n        if not is_checked:\n            self.parent().po.all['starting_blob_shape'] = None\n\n    def spot_shape_changed(self):\n        \"\"\"\n        Save the user selection of shape.\n        \"\"\"\n        self.parent().po.all['starting_blob_shape'] = self.spot_shape.currentText()\n        self.set_spot_shape_check()\n\n    def arena_shape_changed(self):\n        \"\"\"\n        Calculate and update the arena shape in response to user input and manage threading operations.\n\n        Extended Description\n        --------------------\n        This method updates the arena shape variable based on user selection from a dropdown menu.\n        It ensures that certain background threading operations are completed before proceeding with updates\n        and reinitializes necessary components to reflect the new arena shape.\n\n        Notes\n        -----\n        This method handles threading operations to ensure proper synchronization and updates.\n        It reinitializes the biological legend, image, and masks when the arena shape is changed.\n        \"\"\"\n        self.parent().po.vars['arena_shape'] = self.arena_shape.currentText()\n        if self.asking_delineation_flag:\n            if self.thread_dict['CropScaleSubtractDelineate'].isRunning():\n                self.thread_dict['CropScaleSubtractDelineate'].wait()\n            if self.thread_dict['UpdateImage'].isRunning():\n                self.thread_dict['UpdateImage'].wait()\n            self.message.setText(\"Updating display...\")\n            self.decision_label.setVisible(False)\n            self.yes.setVisible(False)\n            self.no.setVisible(False)\n            self.reinitialize_bio_and_back_legend()\n            self.reinitialize_image_and_masks(self.parent().po.first_image.bgr)\n            self.delineation_done = True\n            if self.thread_dict[\"UpdateImage\"].isRunning():\n                self.thread_dict[\"UpdateImage\"].wait()\n            self.thread_dict[\"UpdateImage\"].start()\n            self.thread_dict[\"UpdateImage\"].message_when_thread_finished.connect(self.automatic_delineation_display_done)\n\n    def reinitialize_bio_and_back_legend(self):\n        \"\"\"\n        Reinitialize the bio and back legend.\n\n        Reinitializes the bio and back legends, removing all existing lines\n        and resetting counters for masks. This function ensures that the UI\n        components associated with bio and back lines are correctly cleaned up.\n        \"\"\"\n        lines_names_to_remove = []\n        for line_number, back_line_dict in self.back_lines.items():\n            line_name = u\"\\u00D7\" + \" Back\" + str(line_number)\n            self.back_added_lines_layout.removeWidget(back_line_dict[line_name])\n            back_line_dict[line_name].deleteLater()\n            lines_names_to_remove.append(line_number)\n        for line_number in lines_names_to_remove:\n            self.back_lines.pop(line_number)\n        lines_names_to_remove = []\n        for line_number, bio_line_dict in self.bio_lines.items():\n            line_name = u\"\\u00D7\" + \" Cell\" + str(line_number)\n            self.bio_added_lines_layout.removeWidget(bio_line_dict[line_name])\n            bio_line_dict[line_name].deleteLater()\n            lines_names_to_remove.append(line_number)\n        for line_number in lines_names_to_remove:\n            self.bio_lines.pop(line_number)\n        if len(self.arena_lines) &gt; 0:\n            lines_names_to_remove = []\n            for i, (line_number, arena_line_dict) in enumerate(self.arena_lines.items()):\n                line_name = u\"\\u00D7\" + \" Arena\" + str(line_number)\n                if i % 2 == 0:\n                    self.bio_added_lines_layout.removeWidget(arena_line_dict[line_name])\n                else:\n                    self.back_added_lines_layout.removeWidget(arena_line_dict[line_name])\n                arena_line_dict[line_name].deleteLater()\n                lines_names_to_remove.append(line_number)\n            for line_number in lines_names_to_remove:\n                self.arena_lines.pop(line_number)\n        self.bio_masks_number = 0\n        self.back_masks_number = 0\n\n    def reinitialize_image_and_masks(self, image: np.ndarray):\n        \"\"\"\n        Reinitialize the image and masks for analysis.\n\n        This method reinitializes the current image and its associated masks\n        used in the analysis process. It checks if the input image is grayscale\n        and converts it to a 3-channel RGB image, stacking identical channels.\n        It also updates the visibility of various UI components based on\n        the image type and reinitializes masks to prepare for new analysis.\n        \"\"\"\n        if len(image.shape) == 2:\n            self.parent().po.current_image = np.stack((image, image, image), axis=2)\n\n            self.generate_analysis_options.setVisible(False)\n            self.network_shaped.setVisible(False)\n            self.basic.setVisible(False)\n            self.select_option.setVisible(False)\n            self.select_option_label.setVisible(False)\n            self.visualize.setVisible(True)\n            self.visualize_label.setVisible(True)\n        else:\n            self.parent().po.current_image = deepcopy(image)\n        self.drawn_image = deepcopy(self.parent().po.current_image)\n        self.display_image.update_image(self.parent().po.current_image)\n        self.arena_mask = None\n        self.bio_mask = np.zeros(self.parent().po.current_image.shape[:2], dtype=np.uint16)\n        self.back_mask = np.zeros(self.parent().po.current_image.shape[:2], dtype=np.uint16)\n\n    def scale_with_changed(self):\n        \"\"\"\n        Modifies how the image scale is computed: using the image width or the blob unitary size (horizontal diameter).\n        \"\"\"\n        self.parent().po.all['scale_with_image_or_cells'] = self.scale_with.currentIndex()\n        if self.parent().po.all['scale_with_image_or_cells'] == 0:\n            self.horizontal_size.setValue(self.parent().po.all['image_horizontal_size_in_mm'])\n        else:\n            self.horizontal_size.setValue(self.parent().po.all['starting_blob_hsize_in_mm'])\n\n    def horizontal_size_changed(self):\n        \"\"\"\n        Changes the horizontal size value of the image or of the blobs in the image, depending on user's choice.\n        \"\"\"\n        if self.parent().po.all['scale_with_image_or_cells'] == 0:\n            self.parent().po.all['image_horizontal_size_in_mm'] = self.horizontal_size.value()\n        else:\n            self.parent().po.all['starting_blob_hsize_in_mm'] = self.horizontal_size.value()\n            self.spot_size.setValue(self.parent().po.all['starting_blob_hsize_in_mm'])\n\n    def advanced_mode_check(self):\n        \"\"\"\n        Update widget visibility based on advanced mode check.\n\n        This function updates the visbility of various UI elements depending on\n        the state of the advanced mode check box and other conditions.\n        \"\"\"\n        is_checked = self.advanced_mode_cb.isChecked()\n        color_analysis = is_checked and not self.parent().po.vars['already_greyscale']\n        self.parent().po.all['expert_mode'] = is_checked\n\n        if is_checked and (self.asking_first_im_parameters_flag or self.auto_delineation_flag):\n            self.arena_shape_label.setVisible(True)\n            self.arena_shape.setVisible(True)\n            self.set_spot_shape.setVisible(True)\n            self.spot_shape_label.setVisible(True)\n            self.spot_shape.setVisible(self.set_spot_shape.isChecked())\n            self.set_spot_size.setVisible(self.one_blob_per_arena.isChecked())\n            self.spot_size_label.setVisible(self.one_blob_per_arena.isChecked())\n            self.spot_size.setVisible(\n                self.one_blob_per_arena.isChecked() and self.set_spot_size.isChecked())\n            self.first_im_parameters_answered = True\n\n        self.space_label.setVisible(color_analysis)\n        display_logical = self.logical_operator_between_combination_result.currentText() != 'None'\n        self.logical_operator_between_combination_result.setVisible(color_analysis and display_logical)\n        self.logical_operator_label.setVisible(color_analysis and display_logical)\n\n        at_least_one_line_drawn = self.bio_masks_number &gt; 0\n        self.more_than_two_colors.setVisible(is_checked and at_least_one_line_drawn)\n        self.more_than_two_colors_label.setVisible(is_checked and at_least_one_line_drawn)\n        self.distinct_colors_number.setVisible(is_checked and at_least_one_line_drawn and self.parent().po.all[\"more_than_two_colors\"])\n\n        # Check whether filter 1 and its potential parameters should be visible\n        self.filter1.setVisible(is_checked)\n        self.filter1_label.setVisible(is_checked)\n        has_param1 = is_checked and 'Param1' in filter_dict[self.filter1.currentText()]\n        self.filter1_param1.setVisible(has_param1)\n        self.filter1_param1_label.setVisible(has_param1)\n        has_param2 = is_checked and 'Param2' in filter_dict[self.filter1.currentText()]\n        self.filter1_param2.setVisible(has_param2)\n        self.filter1_param2_label.setVisible(has_param2)\n\n        # Check whether filter 2 and its potential parameters should be visible\n        self.filter2.setVisible(is_checked and display_logical)\n        self.filter2_label.setVisible(is_checked and display_logical)\n        has_param1 = is_checked and display_logical and 'Param1' in filter_dict[self.filter2.currentText()]\n        self.filter2_param1.setVisible(has_param1)\n        self.filter2_param1_label.setVisible(has_param1)\n        has_param2 = is_checked and display_logical and 'Param2' in filter_dict[self.filter2.currentText()]\n        self.filter2_param2.setVisible(has_param2)\n        self.filter2_param2_label.setVisible(has_param2)\n\n        self.rolling_window_segmentation.setVisible(is_checked)\n        self.rolling_window_segmentation_label.setVisible(is_checked)\n\n        for i in range(5):\n            if i == 0:\n                self.row1[i].setVisible(color_analysis)\n            else:\n                self.row1[i].setVisible(color_analysis and not \"PCA\" in self.csc_dict)\n            self.row21[i].setVisible(color_analysis and self.row21[0].currentText() != \"None\")\n            self.row2[i].setVisible(color_analysis and self.row2[0].currentText() != \"None\")\n            self.row22[i].setVisible(color_analysis and self.row22[0].currentText() != \"None\")\n            if i &lt; 4:\n                self.row3[i].setVisible(color_analysis and self.row3[0].currentText() != \"None\")\n                self.row23[i].setVisible(color_analysis and self.row23[0].currentText() != \"None\")\n        if color_analysis:\n            if self.row1[0].currentText() != \"PCA\":\n                if self.row2[0].currentText() == \"None\":\n                    self.row1[4].setVisible(True)\n                else:\n                    self.row2[4].setVisible(True)\n            if self.row21[0].currentText() != \"None\":\n                if self.row22[0].currentText() == \"None\":\n                    self.row21[4].setVisible(True)\n                else:\n                    self.row22[4].setVisible(True)\n        else:\n            self.row1[4].setVisible(False)\n            self.row2[4].setVisible(False)\n            self.row21[4].setVisible(False)\n            self.row22[4].setVisible(False)\n\n    def cell_is_clicked(self):\n        \"\"\"\n        Handles the logic for when a \"cell\" button is clicked in the interface,\n        allowing the user to draw cells on the image.\n        \"\"\"\n        if self.back1_bio2 == 2:\n            self.cell.night_mode_switch(night_mode=self.parent().po.all['night_mode'])\n            self.back1_bio2 = 0\n        else:\n            self.cell.color(\"rgb(230, 145, 18)\")\n            self.background.night_mode_switch(night_mode=self.parent().po.all['night_mode'])\n            self.back1_bio2 = 2\n        self.saved_coord = []\n\n    def background_is_clicked(self):\n        \"\"\"\n        Handles the logic for when a \"back\" button is clicked in the interface,\n        allowing the user to draw where there is background on the image.\n        \"\"\"\n        if self.back1_bio2 == 1:\n            self.background.night_mode_switch(night_mode=self.parent().po.all['night_mode'])\n            self.back1_bio2 = 0\n        else:\n            self.background.color(\"rgb(81, 160, 224)\")\n            self.cell.night_mode_switch(night_mode=self.parent().po.all['night_mode'])\n            self.back1_bio2 = 1\n        self.saved_coord = []\n\n    def get_click_coordinates(self, event):\n        \"\"\"\n        Handle mouse click events to capture coordinate data or display an image.\n\n        This function determines the handling of click events based on various\n        flags and states, including whether image analysis is running or if a\n        manual delineation flag is set.\n\n        Parameters\n        ----------\n        event : QMouseEvent\n            The mouse event that triggered the function.\n        \"\"\"\n        if self.back1_bio2 &gt; 0 or self.manual_delineation_flag:\n            if not self.is_image_analysis_display_running and not self.thread_dict[\"UpdateImage\"].isRunning():\n                self.hold_click_flag = True\n                self.saved_coord.append([event.pos().y(), event.pos().x()])\n        else:\n            self.popup_img = FullScreenImage(self.drawn_image, self.parent().screen_width, self.parent().screen_height)\n            self.popup_img.show()\n\n    def get_mouse_move_coordinates(self, event):\n        \"\"\"\n        Handles mouse movement events to update the temporary mask coordinate.\n\n        Parameters\n        ----------\n        event : QMouseEvent\n            The mouse event object containing position information.\n        \"\"\"\n        if self.hold_click_flag:\n            if not self.thread_dict[\"UpdateImage\"].isRunning():\n                if self.saved_coord[0][0] != event.pos().y() and self.saved_coord[0][1] != event.pos().x():\n                    self.temporary_mask_coord = [self.saved_coord[0], [event.pos().y(), event.pos().x()]]\n                    self.thread_dict[\"UpdateImage\"].start()\n\n    def get_mouse_release_coordinates(self, event):\n        \"\"\"\n        Process mouse release event to save coordinates and manage image update thread.\n\n        This method handles the logic for saving mouse release coordinates during\n        manual delineation, checks conditions to prevent exceeding the number of arenas,\n        and manages an image update thread for display purposes.\n\n        Parameters\n        ----------\n        event : QMouseEvent\n            The mouse event containing the release position.\n\n        Notes\n        -----\n        This method requires an active image update thread and assumes certain attributes\n        like `hold_click_flag`, `manual_delineation_flag`, etc., are part of the class\n        state.\n        \"\"\"\n        if self.hold_click_flag:\n            if self.thread_dict[\"UpdateImage\"].isRunning():\n                self.thread_dict[\"UpdateImage\"].wait()\n            self.temporary_mask_coord = []\n            if self.manual_delineation_flag and len(self.parent().imageanalysiswindow.available_arena_names) == 0:\n                self.message.setText(f\"The total number of arenas are already drawn ({self.parent().po.sample_number})\")\n                self.saved_coord = []\n            else:\n                self.saved_coord.append([event.pos().y(), event.pos().x()])\n                self.thread_dict[\"UpdateImage\"].start()\n                self.thread_dict[\"UpdateImage\"].message_when_thread_finished.connect(self.user_defined_shape_displayed)\n            self.hold_click_flag = False\n\n    def user_defined_shape_displayed(self, when_finished: bool):\n        \"\"\"\n        Display user-defined shapes or elements based on specific conditions and update the UI accordingly.\n\n        Parameters\n        ----------\n        when_finished : bool\n            A flag indicating whether a certain operation has finished.\n\n        Notes\n        -----\n        This method modifies the user interface by adding buttons and updating layouts based on the current state and conditions.\n        \"\"\"\n        if self.back1_bio2 == 1:\n            back_name = self.parent().imageanalysiswindow.available_back_names[0]\n            self.back_lines[back_name] = {}\n            pbutton_name = u\"\\u00D7\" + \" Back\" + str(back_name)\n            self.back_lines[back_name][pbutton_name] = self.new_pbutton_on_the_left(pbutton_name)\n            self.back_added_lines_layout.addWidget(self.back_lines[back_name][pbutton_name])\n            self.background.night_mode_switch(night_mode=self.parent().po.all['night_mode'])\n            self.available_back_names = self.available_back_names[1:]\n        elif self.back1_bio2 == 2:\n            bio_name = self.parent().imageanalysiswindow.available_bio_names[0]\n            self.bio_lines[bio_name] = {}\n            pbutton_name = u\"\\u00D7\" + \" Cell\" + str(bio_name)\n            self.bio_lines[bio_name][pbutton_name] = self.new_pbutton_on_the_left(pbutton_name)\n            self.bio_added_lines_layout.addWidget(self.bio_lines[bio_name][pbutton_name])\n            self.cell.night_mode_switch(night_mode=self.parent().po.all['night_mode'])\n            self.available_bio_names = self.available_bio_names[1:]\n            if self.bio_masks_number == 0:\n                self.display_more_than_two_colors_option()\n\n            self.more_than_two_colors.setVisible(self.advanced_mode_cb.isChecked())\n            self.more_than_two_colors_label.setVisible(self.advanced_mode_cb.isChecked())\n            self.distinct_colors_number.setVisible(self.advanced_mode_cb.isChecked() and self.more_than_two_colors.isChecked())\n        elif self.manual_delineation_flag:\n            arena_name = self.parent().imageanalysiswindow.available_arena_names[0]\n            self.arena_lines[arena_name] = {}\n            pbutton_name = u\"\\u00D7\" + \" Arena\" + str(arena_name)\n            self.arena_lines[arena_name][pbutton_name] = self.new_pbutton_on_the_left(pbutton_name)\n            if self.arena_masks_number % 2 == 1:\n                self.bio_added_lines_layout.addWidget(self.arena_lines[arena_name][pbutton_name])\n            else:\n                self.back_added_lines_layout.addWidget(self.arena_lines[arena_name][pbutton_name])\n            self.available_arena_names = self.available_arena_names[1:]\n        self.saved_coord = []\n        self.back1_bio2 = 0\n\n        self.thread_dict[\"UpdateImage\"].message_when_thread_finished.disconnect()\n\n    def new_pbutton_on_the_left(self, pbutton_name: str):\n        \"\"\"\n        Create a styled PButton instance positioned on the left of the image.\n\n        Notes\n        -----\n        The button's appearance is customized based on the value of\n        `self.back1_bio2`, which affects its color. The button also has a fixed\n        size and specific font settings.\n        \"\"\"\n        pbutton = PButton(pbutton_name, False, night_mode=self.parent().po.all['night_mode'])\n        pbutton.setFixedHeight(20)\n        pbutton.setFixedWidth(100)\n        pbutton.setFont(QtGui.QFont(\"Segoe UI Semibold\", 8, QtGui.QFont.Thin))\n        pbutton.textcolor(\"rgb(0, 0, 0)\")\n        pbutton.border(\"0px\")\n        pbutton.angles(\"10px\")\n        if self.back1_bio2 == 1:\n            pbutton.color(\"rgb(81, 160, 224)\")\n        elif self.back1_bio2 == 2:\n            pbutton.color(\"rgb(230, 145, 18)\")\n        else:\n            pbutton.color(\"rgb(126, 126, 126)\")\n        pbutton.clicked.connect(self.remove_line)\n        return pbutton\n\n    def remove_line(self):\n        \"\"\"\n        Remove the specified line from the image analysis display.\n\n        This method removes a line identified by its button name from the appropriate mask\n        and updates the layout and available names accordingly. It starts the image update thread\n        after removing the line.\n        \"\"\"\n        if not self.is_image_analysis_display_running and not self.thread_dict[\"UpdateImage\"].isRunning() and hasattr(self.sender(), 'text'):\n            pbutton_name = self.sender().text()\n            if pbutton_name[2:6] == \"Back\":\n                line_name = np.uint8(pbutton_name[6:])\n                self.back_mask[self.back_mask == line_name] = 0\n                self.back_added_lines_layout.removeWidget(self.back_lines[line_name][pbutton_name])\n                self.back_lines[line_name][pbutton_name].deleteLater()\n                self.back_lines.pop(line_name)\n                self.back_masks_number -= 1\n                self.available_back_names = np.sort(np.concatenate(([line_name], self.available_back_names)))\n            elif pbutton_name[2:6] == \"Cell\":\n                line_name = np.uint8(pbutton_name[6:])\n                self.bio_mask[self.bio_mask == line_name] = 0\n                self.bio_added_lines_layout.removeWidget(self.bio_lines[line_name][pbutton_name])\n                self.bio_lines[line_name][pbutton_name].deleteLater()\n                self.bio_lines.pop(line_name)\n                self.bio_masks_number -= 1\n                self.available_bio_names = np.sort(np.concatenate(([line_name], self.available_bio_names)))\n                self.display_more_than_two_colors_option()\n            else:\n                line_name = np.uint8(pbutton_name[7:])\n                self.arena_mask[self.arena_mask == line_name] = 0\n                if line_name % 2 == 1:\n                    self.bio_added_lines_layout.removeWidget(self.arena_lines[line_name][pbutton_name])\n                else:\n                    self.back_added_lines_layout.removeWidget(self.arena_lines[line_name][pbutton_name])\n                self.arena_lines[line_name][pbutton_name].deleteLater()\n                self.arena_lines.pop(line_name)\n\n                self.arena_masks_number -= 1\n                self.available_arena_names = np.sort(np.concatenate(([line_name], self.available_arena_names)))\n            self.thread_dict[\"UpdateImage\"].start()\n\n    def network_shaped_is_clicked(self):\n        \"\"\"\n        Sets the GUI state for analyzing a network-shaped image when clicked.\n\n        This method triggers the analysis process for a network-shaped image. It ensures that image analysis is not\n        already running, updates GUI elements accordingly, and starts the appropriate analysis function based on a flag.\n        \"\"\"\n        if not self.is_image_analysis_running:\n            self.is_image_analysis_running = True\n            self.message.setText('Loading, wait...')\n            self.parent().po.visualize = False\n            self.parent().po.basic = False\n            self.parent().po.network_shaped = True\n            self.select_option.clear()\n            if self.is_first_image_flag:\n                self.run_first_image_analysis()\n            else:\n                self.run_last_image_analysis()\n\n    def basic_is_clicked(self):\n        \"\"\"\n        Toggle image analysis mode and trigger appropriate image analysis process.\n\n        This method enables the image analysis mode, sets a loading message,\n        and initiates either the first or last image analysis based on\n        the current state.\n        \"\"\"\n        if not self.is_image_analysis_running:\n            self.is_image_analysis_running = True\n            self.message.setText('Loading, wait...')\n            self.parent().po.visualize = False\n            self.parent().po.basic = True\n            self.parent().po.network_shaped = False\n            if self.is_first_image_flag:\n                self.run_first_image_analysis()\n            else:\n                self.run_last_image_analysis()\n\n    def visualize_is_clicked(self):\n        \"\"\"\n        Instructs the system to perform an image analysis and updates the UI accordingly.\n\n        If image analysis is not currently running, this method triggers the analysis process\n        and updates the UI message to indicate loading.\n        \"\"\"\n        if not self.is_image_analysis_running:\n            self.is_image_analysis_running = True\n            self.message.setText('Loading, wait...')\n            self.parent().po.visualize = True\n            self.parent().po.basic = False\n            self.parent().po.network_shaped = False\n            if self.is_first_image_flag:\n                self.run_first_image_analysis()\n            else:\n                self.run_last_image_analysis()\n\n    def run_first_image_analysis(self):\n        \"\"\"\n        Run the first image analysis.\n\n        This method performs a series of checks and updates based on user-defined parameters\n        before running the first image analysis. If visualization is enabled, it saves user-defined\n        combinations and checks for empty color selection dictionaries. It then starts the thread\n        for image analysis.\n\n        Notes\n        -----\n        This method assumes that the parent object has already been initialized and contains all\n        necessary variables for image analysis.\n        \"\"\"\n        if self.first_im_parameters_answered:\n            self.several_blob_per_arena_check()\n            self.horizontal_size_changed()\n            self.spot_shape_changed()\n            self.arena_shape_changed()\n\n        if self.parent().po.visualize:\n            self.save_user_defined_csc()\n            self.parent().po.vars[\"color_number\"] = int(self.distinct_colors_number.value())\n            if self.csc_dict_is_empty:\n                self.message.setText('Select non null value(s) to combine colors')\n                self.message.setStyleSheet(\"color: rgb(230, 145, 18)\")\n                self.is_image_analysis_running = False\n        if not self.parent().po.visualize or not self.csc_dict_is_empty:\n            self.parent().po.vars['convert_for_origin'] = self.csc_dict.copy()\n            self.thread_dict[\"FirstImageAnalysis\"].start()\n            self.thread_dict[\"FirstImageAnalysis\"].message_from_thread.connect(self.display_message_from_thread)\n            self.thread_dict[\"FirstImageAnalysis\"].message_when_thread_finished.connect(self.when_image_analysis_finishes)\n\n    def run_last_image_analysis(self):\n        \"\"\"\n        Run the last image analysis thread.\n\n        This function updates relevant variables, saves user-defined color-space configurations (CSC),\n        and manages thread operations for image analysis. The function does not handle any direct processing but\n        prepares the environment by setting variables and starting threads.\n        \"\"\"\n        self.save_user_defined_csc()\n        self.parent().po.vars[\"color_number\"] = int(self.distinct_colors_number.value())\n        if not self.csc_dict_is_empty:\n            self.parent().po.vars['convert_for_motion'] = self.csc_dict.copy()\n        if self.parent().po.visualize and self.csc_dict_is_empty:\n            self.message.setText('Select non null value(s) to combine colors')\n            self.message.setStyleSheet(\"color: rgb(230, 145, 18)\")\n        else:\n            self.thread_dict[\"LastImageAnalysis\"].start()\n            self.thread_dict[\"LastImageAnalysis\"].message_from_thread.connect(self.display_message_from_thread)\n            self.thread_dict[\"LastImageAnalysis\"].message_when_thread_finished.connect(self.when_image_analysis_finishes)\n\n    def when_image_analysis_finishes(self):\n        \"\"\"\n        Logs the completion of an image analysis operation, updates the current combination ID,\n        handles visualization settings, manages image combinations, and updates the display.\n\n        Notes\n        -----\n        - This method interacts with the parent object's properties and thread management.\n        - The `is_first_image_flag` determines which set of image combinations to use.\n        \"\"\"\n\n        if self.is_first_image_flag:\n            im_combinations = self.parent().po.first_image.im_combinations\n        else:\n            im_combinations = self.parent().po.last_image.im_combinations\n        self.init_drawn_image(im_combinations)\n        if self.parent().po.visualize:\n            if self.parent().po.current_combination_id != self.select_option.currentIndex():\n                self.select_option.setCurrentIndex(self.parent().po.current_combination_id)\n        else:\n            self.parent().po.current_combination_id = 0\n            if len(im_combinations) &gt; 0:\n                self.csc_dict = im_combinations[self.parent().po.current_combination_id][\"csc\"]\n                if self.is_first_image_flag:\n                    self.parent().po.vars['convert_for_origin'] = self.csc_dict.copy()\n                else:\n                    self.parent().po.vars['convert_for_motion'] = self.csc_dict.copy()\n                option_number = len(im_combinations)\n\n                if option_number &gt; 1:\n                    # Update the available options of the scrolling menu\n                    self.select_option.clear()\n                    for option in range(option_number):\n                        self.select_option.addItem(f\"Option {option + 1}\")\n                self.update_csc_editing_display()\n            else:\n                self.message.setText(\"No options could be generated automatically, use the advanced mode\")\n                self.is_image_analysis_running = False\n\n        if self.parent().po.visualize or len(im_combinations) &gt; 0:\n            self.is_image_analysis_display_running = True\n            # Update image display\n            if self.thread_dict[\"UpdateImage\"].isRunning():\n                self.thread_dict[\"UpdateImage\"].wait()\n            self.thread_dict[\"UpdateImage\"].start()\n            self.thread_dict[\"UpdateImage\"].message_when_thread_finished.connect(self.image_analysis_displayed)\n\n    def image_analysis_displayed(self):\n        \"\"\"\n        Display results of image analysis based on the current step and configuration.\n\n        Update the user interface elements based on the current step of image analysis,\n        the detected number of shapes, and whether color analysis is enabled. Handles\n        visibilities of buttons and labels to guide the user through the process.\n\n        Notes\n        -----\n        This method updates the user interface based on the current state of image analysis.\n        \"\"\"\n        color_analysis = not self.parent().po.vars['already_greyscale']\n        self.message.setText(\"\")\n\n        if self.step &lt; 2:\n            detected_shape_nb = self.parent().po.first_image.im_combinations[self.parent().po.current_combination_id][\n                'shape_number']\n            if detected_shape_nb == self.parent().po.sample_number or self.parent().po.vars['several_blob_per_arena']:\n                self.decision_label.setText(\n                    f\"{detected_shape_nb} distinct specimen(s) detected in {self.parent().po.sample_number} arena(s). Does the color match the cell(s)?\")\n                if self.step == 1:\n                    self.yes.setVisible(True)\n                    self.message.setText(\"If not, draw more Cell and Back ellipses on the image and retry\")\n            else:\n                if self.no.isVisible():\n                    self.decision_label.setText(\n                        f\"{detected_shape_nb} distinct specimen(s) detected in {self.parent().po.sample_number} arena(s). Click Yes when satisfied, Click No to fill in more parameters\")\n                    self.yes.setVisible(True)\n                    self.no.setVisible(True)\n                else:\n                    self.decision_label.setText(\n                        f\"{detected_shape_nb} distinct specimen(s) detected in {self.parent().po.sample_number} arena(s). Click Yes when satisfied\")\n                    self.yes.setVisible(True)\n\n            if self.parent().po.vars['several_blob_per_arena'] and (detected_shape_nb == self.parent().po.sample_number):\n                self.message.setText(\"Beware: Contrary to what has been checked, there is one spot per arena\")\n\n        if not self.parent().po.visualize:\n            self.select_option.setVisible(color_analysis)\n            self.select_option_label.setVisible(color_analysis)\n        if self.step == 0:\n            if self.parent().po.first_image.im_combinations[self.parent().po.current_combination_id]['shape_number'] == 0:\n                self.message.setText(\"Make sure that scaling metric and spot size are correct\")\n            self.decision_label.setVisible(True)\n            self.yes.setVisible(True)\n            self.no.setVisible(True)\n            self.arena_shape.setVisible(True)\n            self.arena_shape_label.setVisible(True)\n            self.n_shapes_detected.setVisible(True)\n\n        elif self.step == 2:\n            self.generate_analysis_options.setVisible(color_analysis)\n            self.network_shaped.setVisible(True)\n            self.basic.setVisible(color_analysis)\n            self.visualize.setVisible(True)\n\n            self.decision_label.setText(\"Adjust parameters until the color delimits the specimen(s) correctly\")\n            self.yes.setVisible(False)\n            self.no.setVisible(False)\n            if self.parent().po.all[\"im_or_vid\"] == 1 or len(self.parent().po.data_list) &gt; 1:\n                self.next.setVisible(True)\n                self.message.setText('When the resulting segmentation of the last image seems good, click next.')\n            else:\n                self.video_tab.set_not_usable()\n                self.message.setText('When the resulting segmentation of the last image seems good, save image analysis.')\n            self.complete_image_analysis.setVisible(True)\n\n        self.thread_dict[\"UpdateImage\"].message_when_thread_finished.disconnect()\n        self.is_image_analysis_running = False\n        self.is_image_analysis_display_running = False\n\n    def init_drawn_image(self, im_combinations: list=None):\n        \"\"\"\n        Initialize the drawn image from a list of image combinations.\n\n        Parameters\n        ----------\n        im_combinations : list or None, optional\n            List of image combinations to initialize the drawn image from.\n            Each combination should be a dictionary containing 'csc' and\n            'converted_image'. If None, the current state is maintained.\n        \"\"\"\n        if im_combinations is not None and len(im_combinations) &gt; 0:\n            if self.parent().po.current_combination_id + 1 &gt; len(im_combinations):\n                self.parent().po.current_combination_id = 0\n            self.csc_dict = im_combinations[self.parent().po.current_combination_id][\"csc\"]\n            self.parent().po.current_image = np.stack((im_combinations[self.parent().po.current_combination_id]['converted_image'],\n                                                    im_combinations[self.parent().po.current_combination_id]['converted_image'],\n                                                    im_combinations[self.parent().po.current_combination_id]['converted_image']), axis=2)\n            self.drawn_image = deepcopy(self.parent().po.current_image)\n\n    def option_changed(self):\n        \"\"\"\n        Update the current image and related display information based on the selected image segmentation option.\n\n        Notes\n        -----\n        This function updates several properties of the parent object, including the current image,\n        combination ID, and display settings. It also handles thread management for updating the\n        image display.\n        \"\"\"\n        # Update the current image\n        self.parent().po.current_combination_id = self.select_option.currentIndex()\n        if self.is_first_image_flag:\n            im_combinations = self.parent().po.first_image.im_combinations\n        else:\n            im_combinations = self.parent().po.last_image.im_combinations\n        self.init_drawn_image(im_combinations)\n        if im_combinations is not None and len(im_combinations) &gt; 0:\n            # Update image display\n            if self.thread_dict[\"UpdateImage\"].isRunning():\n                self.thread_dict[\"UpdateImage\"].wait()\n            self.thread_dict[\"UpdateImage\"].start()\n            # Update csc editing\n            self.update_csc_editing_display()\n\n            # Update the detected shape number\n            if self.is_first_image_flag:\n                self.parent().po.vars['convert_for_origin'] = im_combinations[self.parent().po.current_combination_id][\"csc\"]\n                detected_shape_nb = im_combinations[self.parent().po.current_combination_id]['shape_number']\n                if self.parent().po.vars['several_blob_per_arena']:\n                    if detected_shape_nb == self.parent().po.sample_number:\n                        self.message.setText(\"Beware: Contrary to what has been checked, there is one spot per arena\")\n                else:\n                    if detected_shape_nb == self.parent().po.sample_number:\n                        self.decision_label.setText(\n                            f\"{detected_shape_nb} distinct specimen(s) detected in {self.parent().po.sample_number} arena(s). Does the color match the cell(s)?\")\n                        self.yes.setVisible(True)\n                    else:\n                        self.decision_label.setText(\n                            f\"{detected_shape_nb} distinct specimen(s) detected in {self.parent().po.sample_number} arena(s). Adjust settings, draw more cells and background, and try again\")\n                        self.yes.setVisible(False)\n                if im_combinations[self.parent().po.current_combination_id]['shape_number'] == 0:\n                    self.message.setText(\"Make sure that scaling metric and spot size are correct\")\n            else:\n                self.parent().po.vars['convert_for_motion'] = im_combinations[self.parent().po.current_combination_id][\"csc\"]\n                self.decision_label.setText(\"Do colored contours correctly match cell(s) contours?\")\n            if \"rolling_window\" in im_combinations[self.parent().po.current_combination_id]:\n                self.parent().po.vars['rolling_window_segmentation']['do'] = im_combinations[self.parent().po.current_combination_id][\"rolling_window\"]\n            if \"filter_spec\" in im_combinations[self.parent().po.current_combination_id]:\n                self.parent().po.vars['filter_spec'] = im_combinations[self.parent().po.current_combination_id][\n                    \"filter_spec\"]\n                self.update_filter_display()\n\n    def generate_csc_editing(self):\n        \"\"\"\n        Create and configure a user interface for color space combination editing.\n\n        This method sets up the UI components needed to edit color space combinations,\n        including checkboxes, labels, and drop-down menus. It also configures the layout\n        and connections between components.\n        \"\"\"\n        self.central_right_widget = QtWidgets.QWidget()\n        self.central_right_layout = QtWidgets.QVBoxLayout()\n\n        # 1) Advanced mode option\n        self.advanced_mode_widget = QtWidgets.QWidget()\n        self.advanced_mode_layout = QtWidgets.QHBoxLayout()\n        self.advanced_mode_cb = Checkbox(self.parent().po.all['expert_mode'])\n        self.advanced_mode_cb.setStyleSheet(\"QCheckBox::indicator {width: 12px;height: 12px;background-color: transparent;\"\n                            \"border-radius: 5px;border-style: solid;border-width: 1px;\"\n                            \"border-color: rgb(100,100,100);}\"\n                            \"QCheckBox::indicator:checked {background-color: rgb(70,130,180);}\"\n                            \"QCheckBox:checked, QCheckBox::indicator:checked {border-color: black black white white;}\"\n                            \"QCheckBox:checked {background-color: transparent;}\"\n                            \"QCheckBox:margin-left {0%}\"\n                            \"QCheckBox:margin-right {0%}\")\n        self.advanced_mode_cb.stateChanged.connect(self.advanced_mode_check)\n        self.advanced_mode_label = FixedText(IAW[\"Advanced_mode\"][\"label\"], halign='l',\n                                             tip=IAW[\"Advanced_mode\"][\"tips\"],\n                                             night_mode=self.parent().po.all['night_mode'])\n        self.advanced_mode_label.setAlignment(QtCore.Qt.AlignTop)\n        self.advanced_mode_layout.addWidget(self.advanced_mode_cb)\n        self.advanced_mode_layout.addWidget(self.advanced_mode_label)\n        self.advanced_mode_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n        self.advanced_mode_widget.setLayout(self.advanced_mode_layout)\n        self.central_right_layout.addWidget(self.advanced_mode_widget)\n\n        self.csc_scroll_table = QtWidgets.QScrollArea()  # QTableWidget()  # Scroll Area which contains the widgets, set as the centralWidget\n        self.csc_scroll_table.setSizePolicy(QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.MinimumExpanding)\n        self.csc_scroll_table.setMinimumHeight(self.parent().im_max_height - 100)\n        self.csc_scroll_table.setFrameShape(QtWidgets.QFrame.NoFrame)\n        self.csc_scroll_table.setVerticalScrollBarPolicy(QtCore.Qt.ScrollBarAsNeeded)\n        self.csc_scroll_table.setHorizontalScrollBarPolicy(QtCore.Qt.ScrollBarAsNeeded)\n        self.csc_table_widget = QtWidgets.QWidget()\n        self.csc_table_layout = QtWidgets.QVBoxLayout()\n\n        # 2) Titles\n        self.edit_labels_widget = QtWidgets.QWidget()\n        self.edit_labels_layout = QtWidgets.QHBoxLayout()\n\n        self.space_label = FixedText(IAW[\"Color_combination\"][\"label\"] + ':', halign='l',\n                                    tip=IAW[\"Color_combination\"][\"tips\"],\n                                    night_mode=self.parent().po.all['night_mode'])\n\n        self.edit_labels_layout.addWidget(self.space_label)\n        self.edit_labels_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n        self.space_label.setVisible(False)\n        self.edit_labels_widget.setLayout(self.edit_labels_layout)\n        self.csc_table_layout.addWidget(self.edit_labels_widget)\n\n        # 3) First CSC\n        self.first_csc_widget = QtWidgets.QWidget()\n        self.first_csc_layout = QtWidgets.QGridLayout()\n        self.row1 = self.one_csc_editing(with_PCA=True)\n        self.row1[4].clicked.connect(self.display_row2)\n        self.row2 = self.one_csc_editing()\n        self.row2[4].clicked.connect(self.display_row3)\n        self.row3 = self.one_csc_editing()# Second CSC\n        self.logical_operator_between_combination_result = Combobox([\"None\", \"Or\", \"And\", \"Xor\"],\n                                                                    night_mode=self.parent().po.all['night_mode'])\n        self.logical_operator_between_combination_result.setCurrentText(self.parent().po.vars['convert_for_motion']['logical'])\n        self.logical_operator_between_combination_result.currentTextChanged.connect(self.logical_op_changed)\n        self.logical_operator_between_combination_result.setFixedWidth(100)\n        self.logical_operator_label = FixedText(IAW[\"Logical_operator\"][\"label\"], tip=IAW[\"Logical_operator\"][\"tips\"],\n                                                night_mode=self.parent().po.all['night_mode'])\n\n        self.row21 = self.one_csc_editing()\n        self.row21[4].clicked.connect(self.display_row22)\n        self.row22 = self.one_csc_editing()\n        self.row22[4].clicked.connect(self.display_row23)\n        self.row23 = self.one_csc_editing()\n        if self.csc_dict is not None:\n            self.update_csc_editing_display()\n        else:\n            self.row1[0].setCurrentIndex(4)\n            self.row1[3].setValue(1)\n            self.row21[0].setCurrentIndex(0)\n            self.row21[3].setValue(0)\n\n        for i in range(5):\n            self.first_csc_layout.addWidget(self.row1[i], 0, i, 1, 1)\n            self.first_csc_layout.addWidget(self.row2[i], 1, i, 1, 1)\n            self.first_csc_layout.addWidget(self.row3[i], 2, i, 1, 1)\n            self.row1[i].setVisible(False)\n            self.row2[i].setVisible(False)\n            self.row3[i].setVisible(False)\n        self.first_csc_layout.setHorizontalSpacing(0)\n        self.first_csc_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum), 0, 5, 3, 1)\n        self.first_csc_widget.setLayout(self.first_csc_layout)\n        self.csc_table_layout.addWidget(self.first_csc_widget)\n\n        # First filters\n        self.filter1_label = FixedText(IAW[\"Filter\"][\"label\"] + ': ', halign='l',\n                                    tip=IAW[\"Filter\"][\"tips\"],\n                                    night_mode=self.parent().po.all['night_mode'])\n        self.csc_table_layout.addWidget(self.filter1_label)\n        self.filter1_widget = QtWidgets.QWidget()\n        self.filter1_layout = QtWidgets.QHBoxLayout()\n        self.filter1 = Combobox(list(filter_dict.keys()), night_mode=self.parent().po.all['night_mode'])\n        self.filter1.setCurrentText(self.parent().po.vars['filter_spec']['filter1_type'])\n        self.filter1.currentTextChanged.connect(self.filter1_changed)\n        self.filter1.setFixedWidth(100)\n        if \"Param1\" in filter_dict[self.parent().po.vars['filter_spec']['filter1_type']].keys():\n            param1_name = filter_dict[self.parent().po.vars['filter_spec']['filter1_type']][\"Param1\"][\"Name\"]\n        else:\n            param1_name = \"\"\n        self.filter1_param1_label = FixedText(param1_name, halign='l', tip=\"The parameter to adjust the filter effect\",\n                                    night_mode=self.parent().po.all['night_mode'])\n        filter_param_spinbox_width = 60\n        self.filter1_param1 = Spinbox(min=-1000, max=1000, val=self.parent().po.vars['filter_spec']['filter1_param'][0], decimals=3, night_mode=self.parent().po.all['night_mode'])\n        self.filter1_param1.setFixedWidth(filter_param_spinbox_width)\n        self.filter1_param1.valueChanged.connect(self.filter1_param1_changed)\n        if \"Param2\" in filter_dict[self.parent().po.vars['filter_spec']['filter1_type']].keys():\n            param2_name = filter_dict[self.parent().po.vars['filter_spec']['filter1_type']][\"Param2\"][\"Name\"]\n        else:\n            param2_name = \"\"\n        self.filter1_param2_label = FixedText(param2_name, halign='l', tip=\"The parameter to adjust the filter effect\",\n            night_mode=self.parent().po.all['night_mode'])\n        self.filter1_param2 = Spinbox(min=-1000, max=1000, val=self.parent().po.vars['filter_spec']['filter1_param'][1], decimals=3, night_mode=self.parent().po.all['night_mode'])\n        self.filter1_param2.setFixedWidth(filter_param_spinbox_width)\n        self.filter1_param2.valueChanged.connect(self.filter1_param2_changed)\n        self.filter1_layout.addWidget(self.filter1)\n        # self.filter1_layout.addWidget(self.filter1_label)\n        self.filter1_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n        self.filter1_layout.addWidget(self.filter1_param1_label)\n        self.filter1_layout.addWidget(self.filter1_param1)\n        self.filter1_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n        self.filter1_layout.addWidget(self.filter1_param2_label)\n        self.filter1_layout.addWidget(self.filter1_param2)\n        self.filter1.setVisible(False)\n        self.filter1_label.setVisible(False)\n        self.filter1_param1_label.setVisible(False)\n        self.filter1_param1.setVisible(False)\n        self.filter1_param2_label.setVisible(False)\n        self.filter1_param2.setVisible(False)\n        self.filter1_widget.setLayout(self.filter1_layout)\n        self.csc_table_layout.addWidget(self.filter1_widget)\n\n        # 4) logical_operator\n        self.logical_op_widget = QtWidgets.QWidget()\n        self.logical_op_layout = QtWidgets.QHBoxLayout()\n        self.logical_op_layout.addWidget(self.logical_operator_label)\n        self.logical_op_layout.addWidget(self.logical_operator_between_combination_result)\n        self.logical_op_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n        self.logical_operator_between_combination_result.setVisible(False)\n        self.logical_operator_label.setVisible(False)\n        self.logical_op_widget.setLayout(self.logical_op_layout)\n        self.csc_table_layout.addWidget(self.logical_op_widget)\n\n        # 5) Second CSC\n        self.second_csc_widget = QtWidgets.QWidget()\n        self.second_csc_layout = QtWidgets.QGridLayout()\n        for i in range(5):\n            self.second_csc_layout.addWidget(self.row21[i], 0, i, 1, 1)\n            self.second_csc_layout.addWidget(self.row22[i], 1, i, 1, 1)\n            self.second_csc_layout.addWidget(self.row23[i], 2, i, 1, 1)\n            self.row21[i].setVisible(False)\n            self.row22[i].setVisible(False)\n            self.row23[i].setVisible(False)\n        self.second_csc_layout.setHorizontalSpacing(0)\n        self.second_csc_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum), 0, 5, 3, 1)\n        self.second_csc_widget.setLayout(self.second_csc_layout)\n        self.csc_table_layout.addWidget(self.second_csc_widget)\n\n        self.csc_table_widget.setLayout(self.csc_table_layout)\n        self.csc_scroll_table.setWidget(self.csc_table_widget)\n        self.csc_scroll_table.setWidgetResizable(True)\n        self.central_right_layout.addWidget(self.csc_scroll_table)\n        self.central_right_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.Maximum, QtWidgets.QSizePolicy.MinimumExpanding))\n\n        # Second filters\n        self.filter2_label = FixedText(IAW[\"Filter\"][\"label\"] + ': ', halign='l',\n                                    tip=IAW[\"Filter\"][\"tips\"],\n                                    night_mode=self.parent().po.all['night_mode'])\n        self.csc_table_layout.addWidget(self.filter2_label)\n        self.filter2_widget = QtWidgets.QWidget()\n        self.filter2_layout = QtWidgets.QHBoxLayout()\n        self.filter2 = Combobox(list(filter_dict.keys()), night_mode=self.parent().po.all['night_mode'])\n        self.filter2.setCurrentText(self.parent().po.vars['filter_spec']['filter2_type'])\n        self.filter2.currentTextChanged.connect(self.filter2_changed)\n        self.filter2.setFixedWidth(100)\n        if \"Param1\" in filter_dict[self.parent().po.vars['filter_spec']['filter2_type']].keys():\n            param1_name = filter_dict[self.parent().po.vars['filter_spec']['filter2_type']][\"Param1\"][\"Name\"]\n        else:\n            param1_name = \"\"\n        self.filter2_param1_label = FixedText(param1_name, halign='l',\n                                    tip=\"The parameter to adjust the filter effect\",\n                                    night_mode=self.parent().po.all['night_mode'])\n        self.filter2_param1 = Spinbox(min=-1000, max=1000, val=self.parent().po.vars['filter_spec']['filter2_param'][0], decimals=3, night_mode=self.parent().po.all['night_mode'])\n        self.filter2_param1.setFixedWidth(filter_param_spinbox_width)\n        self.filter2_param1.valueChanged.connect(self.filter2_param1_changed)\n        if \"Param2\" in filter_dict[self.parent().po.vars['filter_spec']['filter2_type']].keys():\n            param2_name = filter_dict[self.parent().po.vars['filter_spec']['filter2_type']][\"Param2\"][\"Name\"]\n        else:\n            param2_name = \"\"\n        self.filter2_param2_label = FixedText(param2_name, halign='l', tip=\"The parameter to adjust the filter effect\",\n            night_mode=self.parent().po.all['night_mode'])\n        self.filter2_param2 = Spinbox(min=-1000, max=1000, val=self.parent().po.vars['filter_spec']['filter2_param'][1], decimals=3, night_mode=self.parent().po.all['night_mode'])\n        self.filter2_param2.setFixedWidth(filter_param_spinbox_width)\n\n        self.filter1_param2.valueChanged.connect(self.filter2_param2_changed)\n        self.filter2_layout.addWidget(self.filter2)\n        self.filter2_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n        self.filter2_layout.addWidget(self.filter2_param1_label)\n        self.filter2_layout.addWidget(self.filter2_param1)\n        self.filter2_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n        self.filter2_layout.addWidget(self.filter2_param2_label)\n        self.filter2_layout.addWidget(self.filter2_param2)\n        self.filter2.setVisible(False)\n        self.filter2_label.setVisible(False)\n        self.filter2_widget.setLayout(self.filter2_layout)\n        self.csc_table_layout.addWidget(self.filter2_widget)\n\n        # 6) Open the rolling_window_segmentation row layout\n        self.rolling_window_segmentation_widget = QtWidgets.QWidget()\n        self.rolling_window_segmentation_layout = QtWidgets.QHBoxLayout()\n        try:\n            self.parent().po.vars[\"rolling_window_segmentation\"]\n        except KeyError:\n            self.parent().po.vars[\"rolling_window_segmentation\"] = False\n        self.rolling_window_segmentation = Checkbox(self.parent().po.vars[\"rolling_window_segmentation\"]['do'])\n        self.rolling_window_segmentation.setStyleSheet(\"QCheckBox::indicator {width: 12px;height: 12px;background-color: transparent;\"\n                            \"border-radius: 5px;border-style: solid;border-width: 1px;\"\n                            \"border-color: rgb(100,100,100);}\"\n                            \"QCheckBox::indicator:checked {background-color: rgb(70,130,180);}\"\n                            \"QCheckBox:checked, QCheckBox::indicator:checked {border-color: black black white white;}\"\n                            \"QCheckBox:checked {background-color: transparent;}\"\n                            \"QCheckBox:margin-left {0%}\"\n                            \"QCheckBox:margin-right {-10%}\")\n        self.rolling_window_segmentation.stateChanged.connect(self.rolling_window_segmentation_option)\n\n        self.rolling_window_segmentation_label = FixedText(IAW[\"Rolling_window_segmentation\"][\"label\"],\n                                                    tip=IAW[\"Rolling_window_segmentation\"][\"tips\"], night_mode=self.parent().po.all['night_mode'])\n        self.rolling_window_segmentation_label.setSizePolicy(QtWidgets.QSizePolicy.Maximum, QtWidgets.QSizePolicy.Maximum)\n        self.rolling_window_segmentation_label.setAlignment(QtCore.Qt.AlignLeft)\n\n        self.rolling_window_segmentation_layout.addWidget(self.rolling_window_segmentation)\n        self.rolling_window_segmentation_layout.addWidget(self.rolling_window_segmentation_label)\n        self.rolling_window_segmentation_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n        self.rolling_window_segmentation_widget.setLayout(self.rolling_window_segmentation_layout)\n        self.central_right_layout.addWidget(self.rolling_window_segmentation_widget)\n\n        # 6) Open the more_than_2_colors row layout\n        self.more_than_2_colors_widget = QtWidgets.QWidget()\n        self.more_than_2_colors_layout = QtWidgets.QHBoxLayout()\n        self.more_than_two_colors = Checkbox(self.parent().po.all[\"more_than_two_colors\"])\n        self.more_than_two_colors.setStyleSheet(\"QCheckBox::indicator {width: 12px;height: 12px;background-color: transparent;\"\n                            \"border-radius: 5px;border-style: solid;border-width: 1px;\"\n                            \"border-color: rgb(100,100,100);}\"\n                            \"QCheckBox::indicator:checked {background-color: rgb(70,130,180);}\"\n                            \"QCheckBox:checked, QCheckBox::indicator:checked {border-color: black black white white;}\"\n                            \"QCheckBox:checked {background-color: transparent;}\"\n                            \"QCheckBox:margin-left {0%}\"\n                            \"QCheckBox:margin-right {-10%}\")\n        self.more_than_two_colors.stateChanged.connect(self.display_more_than_two_colors_option)\n\n        self.more_than_two_colors_label = FixedText(IAW[\"Kmeans\"][\"label\"],\n                                                    tip=IAW[\"Kmeans\"][\"tips\"], night_mode=self.parent().po.all['night_mode'])\n        self.more_than_two_colors_label.setSizePolicy(QtWidgets.QSizePolicy.Maximum, QtWidgets.QSizePolicy.Maximum)\n        self.more_than_two_colors_label.setAlignment(QtCore.Qt.AlignLeft)\n        self.distinct_colors_number = Spinbox(min=2, max=5, val=self.parent().po.vars[\"color_number\"], night_mode=self.parent().po.all['night_mode'])\n\n        self.distinct_colors_number.valueChanged.connect(self.distinct_colors_number_changed)\n        self.display_more_than_two_colors_option()\n        self.more_than_two_colors.setVisible(False)\n        self.more_than_two_colors_label.setVisible(False)\n        self.distinct_colors_number.setVisible(False)\n        self.rolling_window_segmentation.setVisible(False)\n        self.rolling_window_segmentation_label.setVisible(False)\n\n        self.more_than_2_colors_layout.addWidget(self.more_than_two_colors)\n        self.more_than_2_colors_layout.addWidget(self.more_than_two_colors_label)\n        self.more_than_2_colors_layout.addWidget(self.distinct_colors_number)\n        self.more_than_2_colors_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n        self.more_than_2_colors_widget.setLayout(self.more_than_2_colors_layout)\n        self.central_right_layout.addWidget(self.more_than_2_colors_widget)\n\n        self.central_right_widget.setLayout(self.central_right_layout)\n\n    def update_filter_display(self):\n        self.filter1.setCurrentText(self.parent().po.vars['filter_spec']['filter1_type'])\n        self.filter1_param1.setValue(self.parent().po.vars['filter_spec']['filter1_param'][0])\n        if len(self.parent().po.vars['filter_spec']['filter1_param']) &gt; 1:\n            self.filter1_param2.setValue(self.parent().po.vars['filter_spec']['filter1_param'][1])\n        if 'filter2_type' in self.parent().po.vars['filter_spec']:\n            self.filter2.setCurrentText(self.parent().po.vars['filter_spec']['filter2_type'])\n            self.filter2_param1.setValue(self.parent().po.vars['filter_spec']['filter2_param'][0])\n            if len(self.parent().po.vars['filter_spec']['filter2_param']) &gt; 1:\n                self.filter2_param2.setValue(self.parent().po.vars['filter_spec']['filter2_param'][1])\n\n    def filter1_changed(self):\n        \"\"\"\n        Update the UI elements and internal state when the `filter1` selection changes.\n\n        This method updates labels, visibility, and values of filter parameters\n        based on the currently selected filter type.\n\n        Parameters\n        ----------\n        self : object\n            The instance of the class containing this method.\n        \"\"\"\n        current_filter = self.filter1.currentText()\n        self.parent().po.vars['filter_spec']['filter1_type'] = current_filter\n        show_param1 = \"Param1\" in filter_dict[current_filter].keys()\n        if self.advanced_mode_cb.isChecked():\n            self.filter1_param1_label.setVisible(show_param1)\n            self.filter1_param1.setVisible(show_param1)\n        if show_param1:\n            self.filter1_param1_label.setText(filter_dict[current_filter]['Param1']['Name'])\n            self.filter1_param1.setMinimum(filter_dict[current_filter]['Param1']['Minimum'])\n            self.filter1_param1.setMaximum(filter_dict[current_filter]['Param1']['Maximum'])\n            if self.filter1_param1.value() &lt; filter_dict[current_filter]['Param1']['Minimum'] or self.filter1_param1.value() &gt; filter_dict[current_filter]['Param1']['Maximum']:\n                self.filter1_param1.setValue(filter_dict[current_filter]['Param1']['Default'])\n        if 'Param2' in list(filter_dict[current_filter].keys()):\n            self.filter1_param2_label.setText(filter_dict[current_filter]['Param2']['Name'])\n            self.filter1_param2.setMinimum(filter_dict[current_filter]['Param2']['Minimum'])\n            self.filter1_param2.setMaximum(filter_dict[current_filter]['Param2']['Maximum'])\n            if self.filter1_param2.value() &lt; filter_dict[current_filter]['Param2']['Minimum'] or self.filter1_param2.value() &gt; filter_dict[current_filter]['Param2']['Maximum']:\n                self.filter1_param2.setValue(filter_dict[current_filter]['Param2']['Default'])\n            if self.advanced_mode_cb.isChecked():\n                self.filter1_param2_label.setVisible(True)\n                self.filter1_param2.setVisible(True)\n        else:\n            self.filter1_param2_label.setVisible(False)\n            self.filter1_param2.setVisible(False)\n\n    def filter1_param1_changed(self):\n        \"\"\"\n        Save the first parameter (most often the lower bound) of the first filter.\n        \"\"\"\n        self.parent().po.vars['filter_spec']['filter1_param'][0] = float(self.filter1_param1.value())\n\n    def filter1_param2_changed(self):\n        \"\"\"\n        Save the second parameter (most often the higher bound) of the first filter.\n        \"\"\"\n        self.parent().po.vars['filter_spec']['filter1_param'][1] = float(self.filter1_param2.value())\n\n    def filter2_changed(self):\n        \"\"\"\n        Update the UI elements and internal state when the `filter2` selection changes.\n\n        This method updates labels, visibility, and values of filter parameters\n        based on the currently selected filter type.\n\n        Parameters\n        ----------\n        self : object\n            The instance of the class containing this method.\n        \"\"\"\n        current_filter = self.filter2.currentText()\n        self.parent().po.vars['filter_spec']['filter2_type'] = current_filter\n        show_param1 = \"Param1\" in filter_dict[current_filter].keys()\n        if self.advanced_mode_cb.isChecked():\n            self.filter2_param1_label.setVisible(show_param1)\n            self.filter2_param1.setVisible(show_param1)\n        if show_param1:\n            self.filter2_param1_label.setText(filter_dict[current_filter]['Param1']['Name'])\n            self.filter2_param1.setMinimum(filter_dict[current_filter]['Param1']['Minimum'])\n            self.filter2_param1.setMaximum(filter_dict[current_filter]['Param1']['Maximum'])\n            if self.filter2_param1.value() &lt; filter_dict[current_filter]['Param1']['Minimum'] or self.filter2_param1.value() &gt; filter_dict[current_filter]['Param1']['Maximum']:\n                self.filter2_param1.setValue(filter_dict[current_filter]['Param1']['Default'])\n        if 'Param2' in list(filter_dict[current_filter].keys()):\n            self.filter2_param2_label.setText(filter_dict[current_filter]['Param2']['Name'])\n            self.filter2_param2.setMinimum(filter_dict[current_filter]['Param2']['Minimum'])\n            self.filter2_param2.setMaximum(filter_dict[current_filter]['Param2']['Maximum'])\n            if self.filter2_param2.value() &lt; filter_dict[current_filter]['Param2']['Minimum'] or self.filter2_param2.value() &gt; filter_dict[current_filter]['Param2']['Maximum']:\n                self.filter2_param2.setValue(filter_dict[current_filter]['Param2']['Default'])\n            if self.advanced_mode_cb.isChecked():\n                self.filter2_param2_label.setVisible(True)\n                self.filter2_param2.setVisible(True)\n        else:\n            self.filter2_param2_label.setVisible(False)\n            self.filter2_param2.setVisible(False)\n\n    def filter2_param1_changed(self):\n        \"\"\"\n        Save the first parameter (most often the lower bound) of the second filter.\n        \"\"\"\n        self.parent().po.vars['filter_spec']['filter2_param'][0] = float(self.filter2_param1.value())\n\n    def filter2_param2_changed(self):\n        \"\"\"\n        Save the second parameter (most often the higher bound) of the second filter.\n        \"\"\"\n        self.parent().po.vars['filter_spec']['filter2_param'][1] = float(self.filter2_param2.value())\n\n    def one_csc_editing(self, with_PCA: bool=False):\n        \"\"\"\n        Summary\n        --------\n        Edit the color space configuration and add widgets for PCA or other options.\n\n        Parameters\n        ----------\n        with_PCA : bool, optional\n            Flag indicating whether to include PCA options.\n            Default is False.\n\n        Returns\n        -------\n        list\n            List of widgets for color space configuration.\n        \"\"\"\n        widget_list = []\n        if with_PCA:\n            widget_list.insert(0, Combobox([\"PCA\", \"bgr\", \"hsv\", \"hls\", \"lab\", \"luv\", \"yuv\"],\n                                           night_mode=self.parent().po.all['night_mode']))\n            widget_list[0].currentTextChanged.connect(self.pca_changed)\n        else:\n            widget_list.insert(0, Combobox([\"None\", \"bgr\", \"hsv\", \"hls\", \"lab\", \"luv\", \"yuv\"],\n                                           night_mode=self.parent().po.all['night_mode']))\n        widget_list[0].setSizePolicy(QtWidgets.QSizePolicy.Maximum, QtWidgets.QSizePolicy.Maximum)\n        widget_list[0].setFixedWidth(100)\n        for i in [1, 2, 3]:\n            widget_list.insert(i, Spinbox(min=-126, max=126, val=0, night_mode=self.parent().po.all['night_mode']))\n            widget_list[i].setFixedWidth(45)\n        widget_list.insert(i + 1, PButton(\"+\", night_mode=self.parent().po.all['night_mode']))\n        return widget_list\n\n    def pca_changed(self):\n        \"\"\"\n        Handles the UI changes when 'PCA' is selected in dropdown menu.\n\n        Notes\n        -----\n        This function modifies the visibility of UI elements based on the selection in a dropdown menu.\n        It is triggered when 'PCA' is selected, and hides elements related to logical operators.\n        \"\"\"\n        if self.row1[0].currentText() == 'PCA':\n            self.logical_operator_between_combination_result.setCurrentText('None')\n            for i in range(1, 5):\n                self.row1[i].setVisible(False)\n                self.row2[i].setVisible(False)\n                self.row3[i].setVisible(False)\n            self.logical_operator_label.setVisible(False)\n            self.logical_operator_between_combination_result.setVisible(False)\n        else:\n            for i in range(1, 5):\n                self.row1[i].setVisible(True)\n\n\n    def logical_op_changed(self):\n        \"\"\"\n        Handles the visibility and values of UI elements based on the current\n        logical operator selection in a combination result dropdown.\n        \"\"\"\n        if self.logical_operator_between_combination_result.currentText() == 'None':\n            self.row21[0].setVisible(False)\n            self.row21[0].setCurrentIndex(0)\n            for i1 in [1, 2, 3]:\n                self.row21[i1].setVisible(False)\n                self.row21[i1].setValue(0)\n            self.row21[i1 + 1].setVisible(False)\n\n            self.row22[0].setVisible(False)\n            self.row22[0].setCurrentIndex(0)\n            for i1 in [1, 2, 3]:\n                self.row22[i1].setVisible(False)\n                self.row22[i1].setValue(0)\n            self.row22[i1 + 1].setVisible(False)\n\n            self.row23[0].setVisible(False)\n            self.row23[0].setCurrentIndex(0)\n            for i1 in [1, 2, 3]:\n                self.row23[i1].setVisible(False)\n                self.row23[i1].setValue(0)\n            self.row23[i1 + 1].setVisible(False)\n        else:\n            self.filter2_label.setVisible(self.parent().po.all['expert_mode'])\n            self.filter2.setVisible(self.parent().po.all['expert_mode'])\n            self.filter2_changed()\n            self.row21[0].setVisible(self.parent().po.all['expert_mode'])\n            for i1 in [1, 2, 3]:\n                self.row21[i1].setVisible(self.parent().po.all['expert_mode'])\n            self.row21[i1 + 1].setVisible(self.parent().po.all['expert_mode'])\n\n    def display_logical_operator(self):\n        \"\"\"\n        Displays the logical operator UI elements based on expert mode setting.\n        \"\"\"\n        self.logical_operator_between_combination_result.setVisible(self.parent().po.all['expert_mode'])\n        self.logical_operator_label.setVisible(self.parent().po.all['expert_mode'])\n\n    def display_row2(self):\n        \"\"\"\n        Display or hide the second row of the csc editing widgets based on expert mode.\n        \"\"\"\n        self.row1[4].setVisible(False)\n        for i in range(5):\n            self.row2[i].setVisible(self.parent().po.all['expert_mode'])\n        self.display_logical_operator()\n\n    def display_row3(self):\n        \"\"\"\n        Display or hide the third row of the csc editing widgets based on expert mode.\n        \"\"\"\n        self.row2[4].setVisible(False)\n        for i in range(4):\n            self.row3[i].setVisible(self.parent().po.all['expert_mode'])\n        self.display_logical_operator()\n\n    def display_row22(self):\n        \"\"\"\n        Display or hide the second row (for the second image segmentation pipeline) of the csc editing widgets based on expert mode.\n        \"\"\"\n        self.row21[4].setVisible(False)\n        for i in range(5):\n            self.row22[i].setVisible(self.parent().po.all['expert_mode'])\n        self.display_logical_operator()\n\n    def display_row23(self):\n        \"\"\"\n        Display or hide the third row (for the second image segmentation pipeline) of the csc editing widgets based on expert mode.\n        \"\"\"\n        self.row22[4].setVisible(False)\n        for i in range(4):\n            self.row23[i].setVisible(self.parent().po.all['expert_mode'])\n        self.display_logical_operator()\n\n    def update_csc_editing_display(self):\n        \"\"\"\n        Update the color space conversion (CSC) editing display.\n\n        This method updates the visibility and values of UI elements related to color\n        space conversions based on the current state of `self.csc_dict`. It handles\n        the display logic for different color spaces and their combinations, ensuring\n        that the UI reflects the current configuration accurately.\n        \"\"\"\n        remaining_c_spaces = []\n        row_number1 = 0\n        row_number2 = 0\n        if \"PCA\" in self.csc_dict.keys():\n            self.row1[0].setCurrentIndex(0)\n            for i in range(1, 4):\n                self.row1[i].setVisible(False)\n        else:\n            c_space_order = [\"PCA\", \"bgr\", \"hsv\", \"hls\", \"lab\", \"luv\", \"yuv\"]\n            for i, (k, v) in enumerate(self.csc_dict.items()):\n                if k != \"logical\":\n                    if k[-1] != \"2\":\n                        if row_number1 == 0:\n                            row_to_change = self.row1\n                        elif row_number1 == 1:\n                            row_to_change = self.row2\n                        elif row_number1 == 2:\n                            row_to_change = self.row3\n                        else:\n                            remaining_c_spaces.append(k + \" \" + str(v))\n                        row_number1 += 1\n                        current_row_number = row_number1\n                    else:\n                        if row_number2 == 0:\n                            row_to_change = self.row21\n                        elif row_number2 == 1:\n                            row_to_change = self.row22\n                        elif row_number2 == 2:\n                            row_to_change = self.row23\n                        else:\n                            remaining_c_spaces.append(k + \" \" + str(v))\n                        row_number2 += 1\n                        current_row_number = row_number2\n                        k = k[:-1]\n                    if current_row_number &lt;= 3:\n                        row_to_change[0].setCurrentIndex(np.nonzero(np.isin(c_space_order, k))[0][0])\n                        row_to_change[0].setVisible(self.parent().po.all['expert_mode'])\n                        for i1, i2 in zip([1, 2, 3], [0, 1, 2]):\n                            row_to_change[i1].setValue(v[i2])\n                            row_to_change[i1].setVisible(self.parent().po.all['expert_mode'])\n                        if current_row_number &lt; 3:\n                            row_to_change[i1 + 1].setVisible(self.parent().po.all['expert_mode'])\n\n        # If not all color space combinations are filled, put None and 0 in boxes\n        if row_number1 &lt; 3:\n            self.row3[0].setVisible(False)\n            self.row3[0].setCurrentIndex(0)\n            for i1 in [1, 2, 3]:\n                self.row3[i1].setVisible(False)\n                self.row3[i1].setValue(0)\n            if row_number1 &lt; 2:\n                self.row2[0].setVisible(False)\n                self.row2[0].setCurrentIndex(0)\n                for i1 in [1, 2, 3]:\n                    self.row2[i1].setVisible(False)\n                    self.row2[i1].setValue(0)\n                self.row2[i1 + 1].setVisible(False)\n\n        self.row1[4].setVisible(self.parent().po.all['expert_mode'] and row_number1 == 1)\n        self.row2[4].setVisible(self.parent().po.all['expert_mode'] and row_number1 == 2)\n        self.row21[4].setVisible(self.parent().po.all['expert_mode'] and row_number2 == 1)\n        self.row22[4].setVisible(self.parent().po.all['expert_mode'] and row_number2 == 2)\n        if row_number2 &gt; 0:\n            self.logical_operator_between_combination_result.setCurrentText(self.csc_dict['logical'])\n        if row_number2 == 0:\n            self.logical_operator_between_combination_result.setCurrentText('None')\n            self.logical_operator_between_combination_result.setVisible(False)\n            self.logical_operator_label.setVisible(False)\n            self.row21[0].setVisible(False)\n            self.row21[0].setCurrentIndex(0)\n            for i1 in [1, 2, 3]:\n                self.row21[i1].setVisible(False)\n                self.row21[i1].setValue(0)\n            self.row21[i1 + 1].setVisible(False)\n\n        self.logical_operator_between_combination_result.setVisible((row_number2 &gt; 0) and self.parent().po.all['expert_mode'])\n        self.logical_operator_label.setVisible((row_number2 &gt; 0) and self.parent().po.all['expert_mode'])\n\n        if row_number2 &lt; 3:\n            self.row23[0].setVisible(False)\n            self.row23[0].setCurrentIndex(0)\n            for i1 in [1, 2, 3]:\n                self.row23[i1].setVisible(False)\n                self.row23[i1].setValue(0)\n            self.row23[i1 + 1].setVisible(False)\n            self.row22[4].setVisible(False)\n            if row_number2 &lt; 2:\n                self.row22[0].setVisible(False)\n                self.row22[0].setCurrentIndex(0)\n                for i1 in [1, 2, 3]:\n                    self.row22[i1].setVisible(False)\n                    self.row22[i1].setValue(0)\n                self.row22[i1 + 1].setVisible(False)\n\n        if self.advanced_mode_cb.isChecked():\n            if len(remaining_c_spaces) &gt; 0:\n                self.message.setText(f'Combination also includes {remaining_c_spaces}')\n                self.message.setStyleSheet(\"color: rgb(230, 145, 18)\")\n            else:\n                self.message.setText(f'')\n\n    def save_user_defined_csc(self):\n        \"\"\"\n        Save user-defined combination of color spaces and channels.\n        \"\"\"\n        self.csc_dict = {}\n        spaces = np.array((self.row1[0].currentText(), self.row2[0].currentText(), self.row3[0].currentText()))\n        channels = np.array(\n            ((self.row1[1].value(), self.row1[2].value(), self.row1[3].value()),\n             (self.row2[1].value(), self.row2[2].value(), self.row2[3].value()),\n             (self.row3[1].value(), self.row3[2].value(), self.row3[3].value()),\n             (self.row21[1].value(), self.row21[2].value(), self.row21[3].value()),\n             (self.row22[1].value(), self.row22[2].value(), self.row22[3].value()),\n             (self.row23[1].value(), self.row23[2].value(), self.row23[3].value())),\n            dtype=np.float64)\n        if self.logical_operator_between_combination_result.currentText() != 'None':\n            spaces = np.concatenate((spaces, np.array((\n                        self.row21[0].currentText() + \"2\", self.row22[0].currentText() + \"2\",\n                        self.row23[0].currentText() + \"2\"))))\n            channels = np.concatenate((channels, np.array(((self.row21[1].value(), self.row21[2].value(), self.row21[3].value()),\n             (self.row22[1].value(), self.row22[2].value(), self.row22[3].value()),\n             (self.row23[1].value(), self.row23[2].value(), self.row23[3].value())),\n             dtype=np.float64)))\n            self.csc_dict['logical'] = self.logical_operator_between_combination_result.currentText()\n        else:\n            self.csc_dict['logical'] = 'None'\n        if not np.all(spaces == \"None\"):\n            for i, space in enumerate(spaces):\n                if space != \"None\" and space != \"None2\":\n                    self.csc_dict[space] = channels[i, :]\n        if not 'PCA' in self.csc_dict and (len(self.csc_dict) == 1 or np.absolute(channels).sum() == 0):\n            self.csc_dict_is_empty = True\n        else:\n            self.csc_dict_is_empty = False\n\n    def rolling_window_segmentation_option(self):\n        \"\"\"\n        Set True the grid segmentation option for future image analysis.\n        \"\"\"\n        self.parent().po.vars[\"rolling_window_segmentation\"]['do'] = self.rolling_window_segmentation.isChecked()\n\n    def display_more_than_two_colors_option(self):\n        \"\"\"\n        Display the More Than Two Colors Options\n\n        This method manages the visibility and state of UI elements related to selecting\n        more than two colors for displaying biological masks in advanced mode.\n        \"\"\"\n        if self.bio_masks_number &gt; 0 and self.advanced_mode_cb.isChecked():\n            self.more_than_two_colors.setVisible(True)\n            self.more_than_two_colors_label.setVisible(True)\n            if self.more_than_two_colors.isChecked():\n                self.distinct_colors_number.setVisible(True)\n                self.more_than_two_colors_label.setText(\"How many distinct colors?\")\n                self.distinct_colors_number.setValue(3)\n            else:\n                self.more_than_two_colors_label.setText(\"Heterogeneous background\")\n                self.distinct_colors_number.setVisible(False)\n                self.distinct_colors_number.setValue(2)\n            self.parent().po.all[\"more_than_two_colors\"] = self.more_than_two_colors.isChecked()\n        else:\n            self.more_than_two_colors.setChecked(False)\n            self.more_than_two_colors.setVisible(False)\n            self.more_than_two_colors_label.setVisible(False)\n            self.distinct_colors_number.setVisible(False)\n            self.distinct_colors_number.setValue(2)\n            # self.parent().po.vars[\"color_number\"] = 2\n\n    def distinct_colors_number_changed(self):\n        \"\"\"\n        Update the parent object's color number variable based on the current value of a distinct colors control.\n\n        Notes\n        -----\n        This function expects that the parent object has an attribute `po` with a dictionary-like 'vars' that can be updated.\n        \"\"\"\n        self.parent().po.vars[\"color_number\"] = int(self.distinct_colors_number.value())\n\n    def start_crop_scale_subtract_delineate(self):\n        \"\"\"\n        Start the crop, scale, subtract, and delineate process.\n\n        Extended Description\n        --------------------\n        This function initiates a background thread to perform the crop, scale,\n        subtract, and delineate operations on the image. It also updates the\n        UI elements to reflect the ongoing process.\n        \"\"\"\n        if not self.thread_dict['CropScaleSubtractDelineate'].isRunning():\n            self.message.setText(\"Looking for each arena contour, wait...\")\n            self.thread_dict['CropScaleSubtractDelineate'].start()\n            self.thread_dict['CropScaleSubtractDelineate'].message_from_thread.connect(self.display_message_from_thread)\n            self.thread_dict['CropScaleSubtractDelineate'].message_when_thread_finished.connect(self.delineate_is_done)\n\n            self.yes.setVisible(False)\n            self.no.setVisible(False)\n            self.reinitialize_bio_and_back_legend()\n            self.user_drawn_lines_label.setVisible(False)\n            self.cell.setVisible(False)\n            self.background.setVisible(False)\n            self.one_blob_per_arena.setVisible(False)\n            self.one_blob_per_arena_label.setVisible(False)\n            self.set_spot_shape.setVisible(False)\n            self.spot_shape.setVisible(False)\n            self.spot_shape_label.setVisible(False)\n            self.set_spot_size.setVisible(False)\n            self.spot_size.setVisible(False)\n            self.spot_size_label.setVisible(False)\n            self.advanced_mode_cb.setChecked(False)\n            self.advanced_mode_cb.setVisible(False)\n            self.advanced_mode_label.setVisible(False)\n            self.generate_analysis_options.setVisible(False)\n            self.network_shaped.setVisible(False)\n            self.basic.setVisible(False)\n            self.visualize.setVisible(False)\n            self.visualize_label.setVisible(False)\n            self.select_option.setVisible(False)\n            self.select_option_label.setVisible(False)\n\n    def delineate_is_done(self, analysis_status: dict):\n        \"\"\"\n        Update GUI after delineation is complete.\n        \"\"\"\n        if analysis_status['continue']:\n            logging.info(\"Delineation is done, update GUI\")\n            self.message.setText(analysis_status[\"message\"])\n            self.arena_shape_label.setVisible(False)\n            self.arena_shape.setVisible(False)\n            self.reinitialize_bio_and_back_legend()\n            self.reinitialize_image_and_masks(self.parent().po.first_image.bgr)\n            self.delineation_done = True\n            if self.thread_dict[\"UpdateImage\"].isRunning():\n                self.thread_dict[\"UpdateImage\"].wait()\n            self.thread_dict[\"UpdateImage\"].start()\n            self.thread_dict[\"UpdateImage\"].message_when_thread_finished.connect(self.automatic_delineation_display_done)\n\n            try:\n                self.thread_dict['CropScaleSubtractDelineate'].message_from_thread.disconnect()\n                self.thread_dict['CropScaleSubtractDelineate'].message_when_thread_finished.disconnect()\n            except RuntimeError:\n                pass\n            if not self.slower_delineation_flag:\n                self.asking_delineation_flag = True\n        else:\n            self.delineation_done = False\n            self.asking_delineation_flag = False\n            self.auto_delineation_flag = False\n            self.asking_slower_or_manual_delineation_flag = False\n            self.slower_delineation_flag = False\n            self.manual_delineation()\n\n    def automatic_delineation_display_done(self, boole):\n        \"\"\"\n        Automatically handles the delineation display status for the user interface.\n\n        This function updates the visibility of various UI elements and resets\n        certain flags to ensure that delineation is not redrawn unnecessarily.\n        \"\"\"\n        # Remove this flag to not draw it again next time UpdateImage runs for another reason\n        self.delineation_done = False\n        self.auto_delineation_flag = False\n        self.select_option_label.setVisible(False)\n        self.select_option.setVisible(False)\n        self.arena_shape_label.setVisible(True)\n        self.arena_shape.setVisible(True)\n\n        self.decision_label.setText('Is arena delineation correct?')\n        self.decision_label.setToolTip(IAW[\"Video_delimitation\"][\"tips\"])\n        self.decision_label.setVisible(True)\n        self.user_drawn_lines_label.setText('Draw each arena on the image')\n        self.yes.setVisible(True)\n        self.no.setVisible(True)\n\n        self.thread_dict[\"UpdateImage\"].message_when_thread_finished.disconnect()\n\n    def display_message_from_thread(self, text_from_thread: str):\n        \"\"\"\n        Display a message from a thread.\n\n        Parameters\n        ----------\n        text_from_thread : str\n            The message to display.\n        \"\"\"\n        self.message.setText(text_from_thread)\n\n    def starting_differs_from_growing_check(self):\n        \"\"\"\n        Set the `origin_state` variable based on checkbox state and frame detection.\n        \"\"\"\n        if self.parent().po.vars['first_detection_frame'] &gt; 1:\n            self.parent().po.vars['origin_state'] = 'invisible'\n        else:\n            if self.starting_differs_from_growing_cb.isChecked():\n                self.parent().po.vars['origin_state'] = 'constant'\n            else:\n                self.parent().po.vars['origin_state'] = 'fluctuating'\n\n    def when_yes_is_clicked(self):\n        \"\"\"\n        Handles the event when the 'Yes' button is clicked.\n\n        If image analysis is not running, trigger the decision tree process.\n        \"\"\"\n        if not self.is_image_analysis_running:\n            # self.message.setText('Loading, wait...')\n            self.decision_tree(True)\n\n    def when_no_is_clicked(self):\n        \"\"\"\n        Handles the event when the 'No' button is clicked.\n\n        If image analysis is not running, trigger the decision tree process.\n        \"\"\"\n        if not self.is_image_analysis_running:\n            self.decision_tree(False)\n\n    def decision_tree(self, is_yes: bool):\n        \"\"\"\n        Determine the next step in image processing based on user interaction.\n\n        Parameters\n        ----------\n        is_yes : bool\n            Boolean indicating the user's choice (Yes or No).\n\n        Notes\n        -----\n        This function handles various flags and states to determine the next step in\n        image processing workflow. It updates internal state variables and triggers\n        appropriate methods based on the user's input.\n        \"\"\"\n        color_analysis = not self.parent().po.vars['already_greyscale']\n        if self.is_first_image_flag:\n            if self.asking_first_im_parameters_flag:\n                # Ask for the right number of distinct arenas, if not add parameters\n                if not is_yes:\n                    self.first_im_parameters()\n                else:\n                    self.auto_delineation()\n                self.asking_first_im_parameters_flag = False\n\n            elif self.auto_delineation_flag:\n                self.auto_delineation()\n\n            # Is automatic Video delineation correct?\n            elif self.asking_delineation_flag:\n                self.decision_label.setToolTip(\"\")\n                if not is_yes:\n                    self.asking_slower_or_manual_delineation()\n                else:\n                    self.last_image_question()\n                self.asking_delineation_flag = False\n\n            # Slower or manual delineation?\n            elif self.asking_slower_or_manual_delineation_flag:\n                self.back1_bio2 = 0\n                if not is_yes:\n                    self.manual_delineation()\n                else:\n                    self.slower_delineation_flag = True\n                    self.slower_delineation()\n                self.asking_slower_or_manual_delineation_flag = False\n\n            # Is slower delineation correct?\n            elif self.slower_delineation_flag:\n                self.yes.setText(\"Yes\")\n                self.no.setText(\"No\")\n                if not is_yes:\n                    self.manual_delineation()\n                else:\n                    self.last_image_question()\n                self.slower_delineation_flag = False\n\n            elif self.manual_delineation_flag:\n                if is_yes:\n                    if self.parent().po.sample_number == self.arena_masks_number:\n                        self.thread_dict['SaveManualDelineation'].start()\n                        self.last_image_question()\n                        self.manual_delineation_flag = False\n                    else:\n                        self.message.setText(\n                            f\"{self.arena_masks_number} arenas are drawn over the {self.parent().po.sample_number} expected\")\n\n            elif self.asking_last_image_flag:\n                self.decision_label.setToolTip(\"\")\n                self.parent().po.first_image.im_combinations = None\n                self.select_option.clear()\n                self.arena_shape.setVisible(False)\n                self.arena_shape_label.setVisible(False)\n                if is_yes:\n                    self.start_last_image()\n                else:\n                    if \"PCA\" in self.csc_dict:\n                        if self.parent().po.last_image.first_pc_vector is None:\n                            self.csc_dict = {\"bgr\": bracket_to_uint8_image_contrast(self.parent().po.first_image.first_pc_vector), \"logical\": None}\n                        else:\n                            self.csc_dict = {\"bgr\": bracket_to_uint8_image_contrast(self.parent().po.last_image.first_pc_vector), \"logical\": None}\n                    self.parent().po.vars['convert_for_origin'] = deepcopy(self.csc_dict)\n                    self.parent().po.vars['convert_for_motion'] = deepcopy(self.csc_dict)\n                    self.go_to_next_widget()\n                self.asking_last_image_flag = False\n        else:\n            if is_yes:\n                self.parent().po.vars['convert_for_motion'] = deepcopy(self.csc_dict)\n                self.go_to_next_widget()\n\n    def first_im_parameters(self):\n        \"\"\"\n        Reset UI components and prepare for first image parameters adjustment.\n\n        This method resets various UI elements to their initial states, hides\n        confirmation buttons, and shows controls for adjusting spot shapes and sizes.\n        It also sets flags to indicate that the user has not yet answered the first\n        image parameters prompt.\n        \"\"\"\n        self.step = 1\n        self.decision_label.setText(\"Adjust settings, draw more cells and background, and try again\")\n        self.yes.setVisible(False)\n        self.no.setVisible(False)\n        self.set_spot_shape.setVisible(True)\n        self.spot_shape_label.setVisible(True)\n        self.spot_shape.setVisible(self.parent().po.all['set_spot_shape'])\n        self.set_spot_size.setVisible(self.one_blob_per_arena.isChecked())\n        self.spot_size_label.setVisible(self.one_blob_per_arena.isChecked())\n        self.spot_size.setVisible(\n            self.one_blob_per_arena.isChecked() and self.set_spot_size.isChecked())\n        self.auto_delineation_flag = True\n        self.first_im_parameters_answered = True\n\n    def auto_delineation(self):\n        \"\"\"\n        Auto delineation process for image analysis.\n\n        Automatically delineate or start manual delineation based on the number of arenas containing distinct specimen(s).\n\n        Notes\n        -----\n        - The automatic delineation algorithm cannot handle situations where there are more than one arena containing distinct specimen(s). In such cases, manual delineation is initiated.\n        - This function updates the current mask and its stats, removes unnecessary memory, initiates image processing steps including cropping, scaling, subtracting, and delineating.\n        - The visualization labels are hidden during this process.\n        \"\"\"\n        # Do not proceed automatic delineation if there are more than one arena containing distinct specimen(s)\n        # The automatic delineation algorithm cannot handle this situation\n        if self.parent().po.vars['several_blob_per_arena'] and self.parent().po.sample_number &gt; 1:\n            self.manual_delineation()\n        else:\n            self.decision_label.setText(f\"\")\n            # Save the current mask, its stats, remove useless memory and start delineation\n            self.parent().po.first_image.update_current_images(self.parent().po.current_combination_id)\n            self.parent().po.get_average_pixel_size()\n            self.parent().po.all['are_gravity_centers_moving'] = 0\n            self.start_crop_scale_subtract_delineate()\n            self.visualize_label.setVisible(False)\n            self.visualize.setVisible(False)\n\n    def asking_slower_or_manual_delineation(self):\n        \"\"\"\n        Sets the asking_slower_or_manual_delineation_flag to True, updates decision_label and message.\n\n        Extended Description\n        --------------------\n        This method is used to prompt the user to choose between a slower but more efficient delineation algorithm and manual delineation.\n\n        Notes\n        -----\n        This function directly modifies instance attributes `asking_slower_or_manual_delineation_flag`, `decision_label`, and `message`.\n\n        \"\"\"\n        self.asking_slower_or_manual_delineation_flag = True\n        self.decision_label.setText(f\"Click 'yes' to try a slower but more efficient delineation algorithm. Click 'no' to do it manually\")\n        self.message.setText(f\"Clicking no will allow you to draw each arena manually\")\n\n    def slower_delineation(self):\n        \"\"\"\n        Perform slower delineation process and clear the decision label.\n\n        Execute a sequence of operations that prepare for a slower\n        delineation process.\n        \"\"\"\n        self.decision_label.setText(f\"\")\n        self.arena_shape.setVisible(False)\n        self.arena_shape_label.setVisible(False)\n        # Save the current mask, its stats, remove useless memory and start delineation\n        self.parent().po.first_image.update_current_images(self.parent().po.current_combination_id)\n        self.parent().po.all['are_gravity_centers_moving'] = 1\n        self.start_crop_scale_subtract_delineate()\n\n    def manual_delineation(self):\n        \"\"\"\n        Manually delineates the analysis arena on the image by enabling user interaction and\n        preparing the necessary attributes for manual drawing of arenas on the image.\n        \"\"\"\n        self.manual_delineation_flag = True\n        self.parent().po.cropping(is_first_image=True)\n        self.parent().po.get_average_pixel_size()\n        self.reinitialize_image_and_masks(self.parent().po.first_image.bgr)\n        self.reinitialize_bio_and_back_legend()\n        self.available_arena_names = np.arange(1, self.parent().po.sample_number + 1)\n        self.saved_coord = []\n        self.arena_mask = np.zeros(self.parent().po.current_image.shape[:2], dtype=np.uint16)\n        # self.next.setVisible(True)\n        self.decision_label.setVisible(True)\n        self.yes.setVisible(True)\n        self.cell.setVisible(False)\n        self.background.setVisible(False)\n        self.no.setVisible(False)\n        self.one_blob_per_arena.setVisible(False)\n        self.one_blob_per_arena_label.setVisible(False)\n        self.generate_analysis_options.setVisible(False)\n        self.network_shaped.setVisible(False)\n        self.basic.setVisible(False)\n        self.visualize.setVisible(False)\n        self.visualize_label.setVisible(False)\n        self.select_option.setVisible(False)\n        self.select_option_label.setVisible(False)\n        self.user_drawn_lines_label.setText(\"Draw each arena\")\n        self.user_drawn_lines_label.setVisible(True)\n        self.decision_label.setText(\n            f\"Hold click to draw {self.parent().po.sample_number} arena(s) on the image. Once done, click yes.\")\n        self.message.setText('An error? Hit one button on the left to remove any drawn arena.')\n\n    def last_image_question(self):\n        \"\"\"\n        Last image question.\n\n        Queries the user if they want to check parameters for the last image,\n        informing them that the best segmentation pipeline may change during analysis.\n        \"\"\"\n\n        self.image_number.setVisible(False)\n        self.image_number_label.setVisible(False)\n        self.read.setVisible(False)\n        self.step = 2\n        if self.parent().po.all[\"im_or_vid\"] == 0 and len(self.parent().po.data_list) == 1:\n            self.starting_differs_from_growing_cb.setChecked(False)\n            self.start_last_image()\n        else:\n            self.asking_last_image_flag = True\n            self.decision_label.setText(\"Click 'yes' to improve the segmentation using the last image\")\n            self.decision_label.setToolTip(IAW[\"Last_image_question\"][\"tips\"])\n            self.message.setText('This is useful when the specimen(s) is more visible.')\n            self.starting_differs_from_growing_cb.setVisible(True)\n            self.starting_differs_from_growing_label.setVisible(True)\n            self.yes.setVisible(True)\n            self.no.setVisible(True)\n\n    def start_last_image(self):\n        \"\"\"\n        Start the process of analyzing the last image in the time-lapse or the video.\n\n        This method initializes various UI elements, retrieves the last image,\n        waits for any running threads to complete, processes the image without\n        considering it as the first image, and updates the visualization.\n        \"\"\"\n        self.is_first_image_flag = False\n        self.decision_label.setText('')\n        self.yes.setVisible(False)\n        self.no.setVisible(False)\n        self.spot_size.setVisible(False)\n        self.starting_differs_from_growing_cb.setVisible(False)\n        self.starting_differs_from_growing_label.setVisible(False)\n        self.message.setText('Gathering data and visualizing last image analysis result')\n        self.parent().po.get_last_image()\n        if self.thread_dict['SaveManualDelineation'].isRunning():\n            self.thread_dict['SaveManualDelineation'].wait()\n        self.parent().po.cropping(is_first_image=False)\n        self.reinitialize_image_and_masks(self.parent().po.last_image.bgr)\n        self.reinitialize_bio_and_back_legend()\n        self.parent().po.current_combination_id = 0\n        self.visualize_is_clicked()\n        self.user_drawn_lines_label.setText('Select and draw')\n        self.user_drawn_lines_label.setVisible(True)\n        self.cell.setVisible(True)\n        self.background.setVisible(True)\n        self.advanced_mode_cb.setVisible(True)\n        self.advanced_mode_label.setVisible(True)\n        self.visualize_label.setVisible(True)\n        self.visualize.setVisible(True)\n        self.row1_widget.setVisible(False)\n\n    def complete_image_analysis_is_clicked(self):\n        \"\"\"\n        Completes the image analysis process if no listed threads are running.\n        \"\"\"\n        if (not self.thread_dict['SaveManualDelineation'].isRunning() or not self.thread_dict[\n            'PrepareVideoAnalysis'].isRunning() or not self.thread_dict['SaveData'].isRunning() or not\n        self.thread_dict['CompleteImageAnalysisThread'].isRunning()):\n            self.message.setText(f\"Analyzing and saving the segmentation result, wait... \")\n            self.thread_dict['CompleteImageAnalysisThread'].start()\n            self.thread_dict['CompleteImageAnalysisThread'].message_when_thread_finished.connect(self.complete_image_analysis_done)\n\n    def complete_image_analysis_done(self, res):\n        self.message.setText(f\"Complete image analysis done.\")\n\n    def go_to_next_widget(self):\n        \"\"\"\n        Advances the user interface to the next widget after performing final checks.\n\n        Notes\n        -----\n        This function performs several actions in sequence:\n            - Displays a message box to inform the user about final checks.\n            - Waits for some background threads to complete their execution.\n            - Advances the UI to the video analysis window if certain conditions are met.\n        \"\"\"\n        if not self.thread_dict['SaveManualDelineation'].isRunning() or not self.thread_dict['PrepareVideoAnalysis'].isRunning() or not self.thread_dict['SaveData'].isRunning():\n\n            self.popup = QtWidgets.QMessageBox()\n            self.popup.setWindowTitle(\"Info\")\n            self.popup.setText(\"Final checks...\")\n            self.popup.setInformativeText(\"Close and wait until the video tracking window appears.\")\n            self.popup.setStandardButtons(QtWidgets.QMessageBox.Close)\n            x = self.popup.exec_()\n            self.decision_label.setVisible(False)\n            self.yes.setVisible(False)\n            self.no.setVisible(False)\n            self.next.setVisible(True)\n\n\n            self.message.setText(f\"Final checks, wait... \")\n            self.parent().last_tab = \"image_analysis\"\n            self.thread_dict['PrepareVideoAnalysis'].start()\n            if self.parent().po.vars[\"color_number\"] &gt; 2:\n                self.parent().videoanalysiswindow.select_option.clear()\n                self.parent().videoanalysiswindow.select_option.addItem(f\"1) Kmeans\")\n                self.parent().videoanalysiswindow.select_option.setCurrentIndex(0)\n                self.parent().po.all['video_option'] = 0\n            time.sleep(1 / 10)\n            self.thread_dict['PrepareVideoAnalysis'].wait()\n            self.message.setText(f\"\")\n\n            self.video_tab.set_not_in_use()\n            self.parent().last_tab = \"image_analysis\"\n            self.parent().change_widget(3)  # VideoAnalysisWindow\n\n            self.popup.close()\n</code></pre>"},{"location":"api/cellects/gui/image_analysis_window/#cellects.gui.image_analysis_window.ImageAnalysisWindow.__init__","title":"<code>__init__(parent, night_mode)</code>","text":"<p>Initialize the ImageAnalysis window with a parent widget and night mode setting.</p> <p>Parameters:</p> Name Type Description Default <code>parent</code> <code>QWidget</code> <p>The parent widget to which this window will be attached.</p> required <code>night_mode</code> <code>bool</code> <p>A boolean indicating whether the night mode should be enabled.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from PySide6 import QtWidgets\n&gt;&gt;&gt; from cellects.gui.cellects import CellectsMainWidget\n&gt;&gt;&gt; from cellects.gui.image_analysis_window import ImageAnalysisWindow\n&gt;&gt;&gt; from cellects.core.program_organizer import ProgramOrganizer\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import sys\n&gt;&gt;&gt; app = QtWidgets.QApplication([])\n&gt;&gt;&gt; parent = CellectsMainWidget()\n&gt;&gt;&gt; parent.po = ProgramOrganizer()\n&gt;&gt;&gt; parent.po.update_variable_dict()\n&gt;&gt;&gt; parent.po.get_first_image(np.zeros((10, 10), dtype=np.uint8), 1)\n&gt;&gt;&gt; session = ImageAnalysisWindow(parent, False)\n&gt;&gt;&gt; session.true_init()\n&gt;&gt;&gt; parent.insertWidget(0, session)\n&gt;&gt;&gt; parent.show()\n&gt;&gt;&gt; sys.exit(app.exec())\n</code></pre> Source code in <code>src/cellects/gui/image_analysis_window.py</code> <pre><code>def __init__(self, parent: object, night_mode: bool):\n    \"\"\"\n    Initialize the ImageAnalysis window with a parent widget and night mode setting.\n\n    Parameters\n    ----------\n    parent : QWidget\n        The parent widget to which this window will be attached.\n    night_mode : bool\n        A boolean indicating whether the night mode should be enabled.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from PySide6 import QtWidgets\n    &gt;&gt;&gt; from cellects.gui.cellects import CellectsMainWidget\n    &gt;&gt;&gt; from cellects.gui.image_analysis_window import ImageAnalysisWindow\n    &gt;&gt;&gt; from cellects.core.program_organizer import ProgramOrganizer\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; import sys\n    &gt;&gt;&gt; app = QtWidgets.QApplication([])\n    &gt;&gt;&gt; parent = CellectsMainWidget()\n    &gt;&gt;&gt; parent.po = ProgramOrganizer()\n    &gt;&gt;&gt; parent.po.update_variable_dict()\n    &gt;&gt;&gt; parent.po.get_first_image(np.zeros((10, 10), dtype=np.uint8), 1)\n    &gt;&gt;&gt; session = ImageAnalysisWindow(parent, False)\n    &gt;&gt;&gt; session.true_init()\n    &gt;&gt;&gt; parent.insertWidget(0, session)\n    &gt;&gt;&gt; parent.show()\n    &gt;&gt;&gt; sys.exit(app.exec())\n    \"\"\"\n    super().__init__(parent, night_mode)\n    self.setParent(parent)\n    self.csc_dict = self.parent().po.vars['convert_for_origin'] # To change\n    self.manual_delineation_flag: bool = False\n</code></pre>"},{"location":"api/cellects/gui/image_analysis_window/#cellects.gui.image_analysis_window.ImageAnalysisWindow.advanced_mode_check","title":"<code>advanced_mode_check()</code>","text":"<p>Update widget visibility based on advanced mode check.</p> <p>This function updates the visbility of various UI elements depending on the state of the advanced mode check box and other conditions.</p> Source code in <code>src/cellects/gui/image_analysis_window.py</code> <pre><code>def advanced_mode_check(self):\n    \"\"\"\n    Update widget visibility based on advanced mode check.\n\n    This function updates the visbility of various UI elements depending on\n    the state of the advanced mode check box and other conditions.\n    \"\"\"\n    is_checked = self.advanced_mode_cb.isChecked()\n    color_analysis = is_checked and not self.parent().po.vars['already_greyscale']\n    self.parent().po.all['expert_mode'] = is_checked\n\n    if is_checked and (self.asking_first_im_parameters_flag or self.auto_delineation_flag):\n        self.arena_shape_label.setVisible(True)\n        self.arena_shape.setVisible(True)\n        self.set_spot_shape.setVisible(True)\n        self.spot_shape_label.setVisible(True)\n        self.spot_shape.setVisible(self.set_spot_shape.isChecked())\n        self.set_spot_size.setVisible(self.one_blob_per_arena.isChecked())\n        self.spot_size_label.setVisible(self.one_blob_per_arena.isChecked())\n        self.spot_size.setVisible(\n            self.one_blob_per_arena.isChecked() and self.set_spot_size.isChecked())\n        self.first_im_parameters_answered = True\n\n    self.space_label.setVisible(color_analysis)\n    display_logical = self.logical_operator_between_combination_result.currentText() != 'None'\n    self.logical_operator_between_combination_result.setVisible(color_analysis and display_logical)\n    self.logical_operator_label.setVisible(color_analysis and display_logical)\n\n    at_least_one_line_drawn = self.bio_masks_number &gt; 0\n    self.more_than_two_colors.setVisible(is_checked and at_least_one_line_drawn)\n    self.more_than_two_colors_label.setVisible(is_checked and at_least_one_line_drawn)\n    self.distinct_colors_number.setVisible(is_checked and at_least_one_line_drawn and self.parent().po.all[\"more_than_two_colors\"])\n\n    # Check whether filter 1 and its potential parameters should be visible\n    self.filter1.setVisible(is_checked)\n    self.filter1_label.setVisible(is_checked)\n    has_param1 = is_checked and 'Param1' in filter_dict[self.filter1.currentText()]\n    self.filter1_param1.setVisible(has_param1)\n    self.filter1_param1_label.setVisible(has_param1)\n    has_param2 = is_checked and 'Param2' in filter_dict[self.filter1.currentText()]\n    self.filter1_param2.setVisible(has_param2)\n    self.filter1_param2_label.setVisible(has_param2)\n\n    # Check whether filter 2 and its potential parameters should be visible\n    self.filter2.setVisible(is_checked and display_logical)\n    self.filter2_label.setVisible(is_checked and display_logical)\n    has_param1 = is_checked and display_logical and 'Param1' in filter_dict[self.filter2.currentText()]\n    self.filter2_param1.setVisible(has_param1)\n    self.filter2_param1_label.setVisible(has_param1)\n    has_param2 = is_checked and display_logical and 'Param2' in filter_dict[self.filter2.currentText()]\n    self.filter2_param2.setVisible(has_param2)\n    self.filter2_param2_label.setVisible(has_param2)\n\n    self.rolling_window_segmentation.setVisible(is_checked)\n    self.rolling_window_segmentation_label.setVisible(is_checked)\n\n    for i in range(5):\n        if i == 0:\n            self.row1[i].setVisible(color_analysis)\n        else:\n            self.row1[i].setVisible(color_analysis and not \"PCA\" in self.csc_dict)\n        self.row21[i].setVisible(color_analysis and self.row21[0].currentText() != \"None\")\n        self.row2[i].setVisible(color_analysis and self.row2[0].currentText() != \"None\")\n        self.row22[i].setVisible(color_analysis and self.row22[0].currentText() != \"None\")\n        if i &lt; 4:\n            self.row3[i].setVisible(color_analysis and self.row3[0].currentText() != \"None\")\n            self.row23[i].setVisible(color_analysis and self.row23[0].currentText() != \"None\")\n    if color_analysis:\n        if self.row1[0].currentText() != \"PCA\":\n            if self.row2[0].currentText() == \"None\":\n                self.row1[4].setVisible(True)\n            else:\n                self.row2[4].setVisible(True)\n        if self.row21[0].currentText() != \"None\":\n            if self.row22[0].currentText() == \"None\":\n                self.row21[4].setVisible(True)\n            else:\n                self.row22[4].setVisible(True)\n    else:\n        self.row1[4].setVisible(False)\n        self.row2[4].setVisible(False)\n        self.row21[4].setVisible(False)\n        self.row22[4].setVisible(False)\n</code></pre>"},{"location":"api/cellects/gui/image_analysis_window/#cellects.gui.image_analysis_window.ImageAnalysisWindow.arena_shape_changed","title":"<code>arena_shape_changed()</code>","text":"<p>Calculate and update the arena shape in response to user input and manage threading operations.</p> Extended Description <p>This method updates the arena shape variable based on user selection from a dropdown menu. It ensures that certain background threading operations are completed before proceeding with updates and reinitializes necessary components to reflect the new arena shape.</p> Notes <p>This method handles threading operations to ensure proper synchronization and updates. It reinitializes the biological legend, image, and masks when the arena shape is changed.</p> Source code in <code>src/cellects/gui/image_analysis_window.py</code> <pre><code>def arena_shape_changed(self):\n    \"\"\"\n    Calculate and update the arena shape in response to user input and manage threading operations.\n\n    Extended Description\n    --------------------\n    This method updates the arena shape variable based on user selection from a dropdown menu.\n    It ensures that certain background threading operations are completed before proceeding with updates\n    and reinitializes necessary components to reflect the new arena shape.\n\n    Notes\n    -----\n    This method handles threading operations to ensure proper synchronization and updates.\n    It reinitializes the biological legend, image, and masks when the arena shape is changed.\n    \"\"\"\n    self.parent().po.vars['arena_shape'] = self.arena_shape.currentText()\n    if self.asking_delineation_flag:\n        if self.thread_dict['CropScaleSubtractDelineate'].isRunning():\n            self.thread_dict['CropScaleSubtractDelineate'].wait()\n        if self.thread_dict['UpdateImage'].isRunning():\n            self.thread_dict['UpdateImage'].wait()\n        self.message.setText(\"Updating display...\")\n        self.decision_label.setVisible(False)\n        self.yes.setVisible(False)\n        self.no.setVisible(False)\n        self.reinitialize_bio_and_back_legend()\n        self.reinitialize_image_and_masks(self.parent().po.first_image.bgr)\n        self.delineation_done = True\n        if self.thread_dict[\"UpdateImage\"].isRunning():\n            self.thread_dict[\"UpdateImage\"].wait()\n        self.thread_dict[\"UpdateImage\"].start()\n        self.thread_dict[\"UpdateImage\"].message_when_thread_finished.connect(self.automatic_delineation_display_done)\n</code></pre>"},{"location":"api/cellects/gui/image_analysis_window/#cellects.gui.image_analysis_window.ImageAnalysisWindow.asking_slower_or_manual_delineation","title":"<code>asking_slower_or_manual_delineation()</code>","text":"<p>Sets the asking_slower_or_manual_delineation_flag to True, updates decision_label and message.</p> Extended Description <p>This method is used to prompt the user to choose between a slower but more efficient delineation algorithm and manual delineation.</p> Notes <p>This function directly modifies instance attributes <code>asking_slower_or_manual_delineation_flag</code>, <code>decision_label</code>, and <code>message</code>.</p> Source code in <code>src/cellects/gui/image_analysis_window.py</code> <pre><code>def asking_slower_or_manual_delineation(self):\n    \"\"\"\n    Sets the asking_slower_or_manual_delineation_flag to True, updates decision_label and message.\n\n    Extended Description\n    --------------------\n    This method is used to prompt the user to choose between a slower but more efficient delineation algorithm and manual delineation.\n\n    Notes\n    -----\n    This function directly modifies instance attributes `asking_slower_or_manual_delineation_flag`, `decision_label`, and `message`.\n\n    \"\"\"\n    self.asking_slower_or_manual_delineation_flag = True\n    self.decision_label.setText(f\"Click 'yes' to try a slower but more efficient delineation algorithm. Click 'no' to do it manually\")\n    self.message.setText(f\"Clicking no will allow you to draw each arena manually\")\n</code></pre>"},{"location":"api/cellects/gui/image_analysis_window/#cellects.gui.image_analysis_window.ImageAnalysisWindow.auto_delineation","title":"<code>auto_delineation()</code>","text":"<p>Auto delineation process for image analysis.</p> <p>Automatically delineate or start manual delineation based on the number of arenas containing distinct specimen(s).</p> Notes <ul> <li>The automatic delineation algorithm cannot handle situations where there are more than one arena containing distinct specimen(s). In such cases, manual delineation is initiated.</li> <li>This function updates the current mask and its stats, removes unnecessary memory, initiates image processing steps including cropping, scaling, subtracting, and delineating.</li> <li>The visualization labels are hidden during this process.</li> </ul> Source code in <code>src/cellects/gui/image_analysis_window.py</code> <pre><code>def auto_delineation(self):\n    \"\"\"\n    Auto delineation process for image analysis.\n\n    Automatically delineate or start manual delineation based on the number of arenas containing distinct specimen(s).\n\n    Notes\n    -----\n    - The automatic delineation algorithm cannot handle situations where there are more than one arena containing distinct specimen(s). In such cases, manual delineation is initiated.\n    - This function updates the current mask and its stats, removes unnecessary memory, initiates image processing steps including cropping, scaling, subtracting, and delineating.\n    - The visualization labels are hidden during this process.\n    \"\"\"\n    # Do not proceed automatic delineation if there are more than one arena containing distinct specimen(s)\n    # The automatic delineation algorithm cannot handle this situation\n    if self.parent().po.vars['several_blob_per_arena'] and self.parent().po.sample_number &gt; 1:\n        self.manual_delineation()\n    else:\n        self.decision_label.setText(f\"\")\n        # Save the current mask, its stats, remove useless memory and start delineation\n        self.parent().po.first_image.update_current_images(self.parent().po.current_combination_id)\n        self.parent().po.get_average_pixel_size()\n        self.parent().po.all['are_gravity_centers_moving'] = 0\n        self.start_crop_scale_subtract_delineate()\n        self.visualize_label.setVisible(False)\n        self.visualize.setVisible(False)\n</code></pre>"},{"location":"api/cellects/gui/image_analysis_window/#cellects.gui.image_analysis_window.ImageAnalysisWindow.automatic_delineation_display_done","title":"<code>automatic_delineation_display_done(boole)</code>","text":"<p>Automatically handles the delineation display status for the user interface.</p> <p>This function updates the visibility of various UI elements and resets certain flags to ensure that delineation is not redrawn unnecessarily.</p> Source code in <code>src/cellects/gui/image_analysis_window.py</code> <pre><code>def automatic_delineation_display_done(self, boole):\n    \"\"\"\n    Automatically handles the delineation display status for the user interface.\n\n    This function updates the visibility of various UI elements and resets\n    certain flags to ensure that delineation is not redrawn unnecessarily.\n    \"\"\"\n    # Remove this flag to not draw it again next time UpdateImage runs for another reason\n    self.delineation_done = False\n    self.auto_delineation_flag = False\n    self.select_option_label.setVisible(False)\n    self.select_option.setVisible(False)\n    self.arena_shape_label.setVisible(True)\n    self.arena_shape.setVisible(True)\n\n    self.decision_label.setText('Is arena delineation correct?')\n    self.decision_label.setToolTip(IAW[\"Video_delimitation\"][\"tips\"])\n    self.decision_label.setVisible(True)\n    self.user_drawn_lines_label.setText('Draw each arena on the image')\n    self.yes.setVisible(True)\n    self.no.setVisible(True)\n\n    self.thread_dict[\"UpdateImage\"].message_when_thread_finished.disconnect()\n</code></pre>"},{"location":"api/cellects/gui/image_analysis_window/#cellects.gui.image_analysis_window.ImageAnalysisWindow.background_is_clicked","title":"<code>background_is_clicked()</code>","text":"<p>Handles the logic for when a \"back\" button is clicked in the interface, allowing the user to draw where there is background on the image.</p> Source code in <code>src/cellects/gui/image_analysis_window.py</code> <pre><code>def background_is_clicked(self):\n    \"\"\"\n    Handles the logic for when a \"back\" button is clicked in the interface,\n    allowing the user to draw where there is background on the image.\n    \"\"\"\n    if self.back1_bio2 == 1:\n        self.background.night_mode_switch(night_mode=self.parent().po.all['night_mode'])\n        self.back1_bio2 = 0\n    else:\n        self.background.color(\"rgb(81, 160, 224)\")\n        self.cell.night_mode_switch(night_mode=self.parent().po.all['night_mode'])\n        self.back1_bio2 = 1\n    self.saved_coord = []\n</code></pre>"},{"location":"api/cellects/gui/image_analysis_window/#cellects.gui.image_analysis_window.ImageAnalysisWindow.basic_is_clicked","title":"<code>basic_is_clicked()</code>","text":"<p>Toggle image analysis mode and trigger appropriate image analysis process.</p> <p>This method enables the image analysis mode, sets a loading message, and initiates either the first or last image analysis based on the current state.</p> Source code in <code>src/cellects/gui/image_analysis_window.py</code> <pre><code>def basic_is_clicked(self):\n    \"\"\"\n    Toggle image analysis mode and trigger appropriate image analysis process.\n\n    This method enables the image analysis mode, sets a loading message,\n    and initiates either the first or last image analysis based on\n    the current state.\n    \"\"\"\n    if not self.is_image_analysis_running:\n        self.is_image_analysis_running = True\n        self.message.setText('Loading, wait...')\n        self.parent().po.visualize = False\n        self.parent().po.basic = True\n        self.parent().po.network_shaped = False\n        if self.is_first_image_flag:\n            self.run_first_image_analysis()\n        else:\n            self.run_last_image_analysis()\n</code></pre>"},{"location":"api/cellects/gui/image_analysis_window/#cellects.gui.image_analysis_window.ImageAnalysisWindow.cell_is_clicked","title":"<code>cell_is_clicked()</code>","text":"<p>Handles the logic for when a \"cell\" button is clicked in the interface, allowing the user to draw cells on the image.</p> Source code in <code>src/cellects/gui/image_analysis_window.py</code> <pre><code>def cell_is_clicked(self):\n    \"\"\"\n    Handles the logic for when a \"cell\" button is clicked in the interface,\n    allowing the user to draw cells on the image.\n    \"\"\"\n    if self.back1_bio2 == 2:\n        self.cell.night_mode_switch(night_mode=self.parent().po.all['night_mode'])\n        self.back1_bio2 = 0\n    else:\n        self.cell.color(\"rgb(230, 145, 18)\")\n        self.background.night_mode_switch(night_mode=self.parent().po.all['night_mode'])\n        self.back1_bio2 = 2\n    self.saved_coord = []\n</code></pre>"},{"location":"api/cellects/gui/image_analysis_window/#cellects.gui.image_analysis_window.ImageAnalysisWindow.complete_image_analysis_is_clicked","title":"<code>complete_image_analysis_is_clicked()</code>","text":"<p>Completes the image analysis process if no listed threads are running.</p> Source code in <code>src/cellects/gui/image_analysis_window.py</code> <pre><code>def complete_image_analysis_is_clicked(self):\n    \"\"\"\n    Completes the image analysis process if no listed threads are running.\n    \"\"\"\n    if (not self.thread_dict['SaveManualDelineation'].isRunning() or not self.thread_dict[\n        'PrepareVideoAnalysis'].isRunning() or not self.thread_dict['SaveData'].isRunning() or not\n    self.thread_dict['CompleteImageAnalysisThread'].isRunning()):\n        self.message.setText(f\"Analyzing and saving the segmentation result, wait... \")\n        self.thread_dict['CompleteImageAnalysisThread'].start()\n        self.thread_dict['CompleteImageAnalysisThread'].message_when_thread_finished.connect(self.complete_image_analysis_done)\n</code></pre>"},{"location":"api/cellects/gui/image_analysis_window/#cellects.gui.image_analysis_window.ImageAnalysisWindow.data_is_clicked","title":"<code>data_is_clicked()</code>","text":"<p>Handles the logic for when the \"Data specifications\" button is clicked in the interface, leading to the FirstWindow.</p> Notes <p>This function displays an error message when a thread relative to the current window is running. This function also save the id of this tab for later use.</p> Source code in <code>src/cellects/gui/image_analysis_window.py</code> <pre><code>def data_is_clicked(self):\n    \"\"\"\n    Handles the logic for when the \"Data specifications\" button is clicked in the interface,\n    leading to the FirstWindow.\n\n    Notes\n    -----\n    This function displays an error message when a thread relative to the current window is running.\n    This function also save the id of this tab for later use.\n    \"\"\"\n    if self.is_image_analysis_running:\n        self.message.setText(\"Wait for the analysis to end, or restart Cellects\")\n    else:\n        self.parent().last_tab = \"data_specifications\"\n        self.parent().change_widget(0)  # First\n</code></pre>"},{"location":"api/cellects/gui/image_analysis_window/#cellects.gui.image_analysis_window.ImageAnalysisWindow.decision_tree","title":"<code>decision_tree(is_yes)</code>","text":"<p>Determine the next step in image processing based on user interaction.</p> <p>Parameters:</p> Name Type Description Default <code>is_yes</code> <code>bool</code> <p>Boolean indicating the user's choice (Yes or No).</p> required Notes <p>This function handles various flags and states to determine the next step in image processing workflow. It updates internal state variables and triggers appropriate methods based on the user's input.</p> Source code in <code>src/cellects/gui/image_analysis_window.py</code> <pre><code>def decision_tree(self, is_yes: bool):\n    \"\"\"\n    Determine the next step in image processing based on user interaction.\n\n    Parameters\n    ----------\n    is_yes : bool\n        Boolean indicating the user's choice (Yes or No).\n\n    Notes\n    -----\n    This function handles various flags and states to determine the next step in\n    image processing workflow. It updates internal state variables and triggers\n    appropriate methods based on the user's input.\n    \"\"\"\n    color_analysis = not self.parent().po.vars['already_greyscale']\n    if self.is_first_image_flag:\n        if self.asking_first_im_parameters_flag:\n            # Ask for the right number of distinct arenas, if not add parameters\n            if not is_yes:\n                self.first_im_parameters()\n            else:\n                self.auto_delineation()\n            self.asking_first_im_parameters_flag = False\n\n        elif self.auto_delineation_flag:\n            self.auto_delineation()\n\n        # Is automatic Video delineation correct?\n        elif self.asking_delineation_flag:\n            self.decision_label.setToolTip(\"\")\n            if not is_yes:\n                self.asking_slower_or_manual_delineation()\n            else:\n                self.last_image_question()\n            self.asking_delineation_flag = False\n\n        # Slower or manual delineation?\n        elif self.asking_slower_or_manual_delineation_flag:\n            self.back1_bio2 = 0\n            if not is_yes:\n                self.manual_delineation()\n            else:\n                self.slower_delineation_flag = True\n                self.slower_delineation()\n            self.asking_slower_or_manual_delineation_flag = False\n\n        # Is slower delineation correct?\n        elif self.slower_delineation_flag:\n            self.yes.setText(\"Yes\")\n            self.no.setText(\"No\")\n            if not is_yes:\n                self.manual_delineation()\n            else:\n                self.last_image_question()\n            self.slower_delineation_flag = False\n\n        elif self.manual_delineation_flag:\n            if is_yes:\n                if self.parent().po.sample_number == self.arena_masks_number:\n                    self.thread_dict['SaveManualDelineation'].start()\n                    self.last_image_question()\n                    self.manual_delineation_flag = False\n                else:\n                    self.message.setText(\n                        f\"{self.arena_masks_number} arenas are drawn over the {self.parent().po.sample_number} expected\")\n\n        elif self.asking_last_image_flag:\n            self.decision_label.setToolTip(\"\")\n            self.parent().po.first_image.im_combinations = None\n            self.select_option.clear()\n            self.arena_shape.setVisible(False)\n            self.arena_shape_label.setVisible(False)\n            if is_yes:\n                self.start_last_image()\n            else:\n                if \"PCA\" in self.csc_dict:\n                    if self.parent().po.last_image.first_pc_vector is None:\n                        self.csc_dict = {\"bgr\": bracket_to_uint8_image_contrast(self.parent().po.first_image.first_pc_vector), \"logical\": None}\n                    else:\n                        self.csc_dict = {\"bgr\": bracket_to_uint8_image_contrast(self.parent().po.last_image.first_pc_vector), \"logical\": None}\n                self.parent().po.vars['convert_for_origin'] = deepcopy(self.csc_dict)\n                self.parent().po.vars['convert_for_motion'] = deepcopy(self.csc_dict)\n                self.go_to_next_widget()\n            self.asking_last_image_flag = False\n    else:\n        if is_yes:\n            self.parent().po.vars['convert_for_motion'] = deepcopy(self.csc_dict)\n            self.go_to_next_widget()\n</code></pre>"},{"location":"api/cellects/gui/image_analysis_window/#cellects.gui.image_analysis_window.ImageAnalysisWindow.delineate_is_done","title":"<code>delineate_is_done(analysis_status)</code>","text":"<p>Update GUI after delineation is complete.</p> Source code in <code>src/cellects/gui/image_analysis_window.py</code> <pre><code>def delineate_is_done(self, analysis_status: dict):\n    \"\"\"\n    Update GUI after delineation is complete.\n    \"\"\"\n    if analysis_status['continue']:\n        logging.info(\"Delineation is done, update GUI\")\n        self.message.setText(analysis_status[\"message\"])\n        self.arena_shape_label.setVisible(False)\n        self.arena_shape.setVisible(False)\n        self.reinitialize_bio_and_back_legend()\n        self.reinitialize_image_and_masks(self.parent().po.first_image.bgr)\n        self.delineation_done = True\n        if self.thread_dict[\"UpdateImage\"].isRunning():\n            self.thread_dict[\"UpdateImage\"].wait()\n        self.thread_dict[\"UpdateImage\"].start()\n        self.thread_dict[\"UpdateImage\"].message_when_thread_finished.connect(self.automatic_delineation_display_done)\n\n        try:\n            self.thread_dict['CropScaleSubtractDelineate'].message_from_thread.disconnect()\n            self.thread_dict['CropScaleSubtractDelineate'].message_when_thread_finished.disconnect()\n        except RuntimeError:\n            pass\n        if not self.slower_delineation_flag:\n            self.asking_delineation_flag = True\n    else:\n        self.delineation_done = False\n        self.asking_delineation_flag = False\n        self.auto_delineation_flag = False\n        self.asking_slower_or_manual_delineation_flag = False\n        self.slower_delineation_flag = False\n        self.manual_delineation()\n</code></pre>"},{"location":"api/cellects/gui/image_analysis_window/#cellects.gui.image_analysis_window.ImageAnalysisWindow.display_logical_operator","title":"<code>display_logical_operator()</code>","text":"<p>Displays the logical operator UI elements based on expert mode setting.</p> Source code in <code>src/cellects/gui/image_analysis_window.py</code> <pre><code>def display_logical_operator(self):\n    \"\"\"\n    Displays the logical operator UI elements based on expert mode setting.\n    \"\"\"\n    self.logical_operator_between_combination_result.setVisible(self.parent().po.all['expert_mode'])\n    self.logical_operator_label.setVisible(self.parent().po.all['expert_mode'])\n</code></pre>"},{"location":"api/cellects/gui/image_analysis_window/#cellects.gui.image_analysis_window.ImageAnalysisWindow.display_message_from_thread","title":"<code>display_message_from_thread(text_from_thread)</code>","text":"<p>Display a message from a thread.</p> <p>Parameters:</p> Name Type Description Default <code>text_from_thread</code> <code>str</code> <p>The message to display.</p> required Source code in <code>src/cellects/gui/image_analysis_window.py</code> <pre><code>def display_message_from_thread(self, text_from_thread: str):\n    \"\"\"\n    Display a message from a thread.\n\n    Parameters\n    ----------\n    text_from_thread : str\n        The message to display.\n    \"\"\"\n    self.message.setText(text_from_thread)\n</code></pre>"},{"location":"api/cellects/gui/image_analysis_window/#cellects.gui.image_analysis_window.ImageAnalysisWindow.display_more_than_two_colors_option","title":"<code>display_more_than_two_colors_option()</code>","text":"<p>Display the More Than Two Colors Options</p> <p>This method manages the visibility and state of UI elements related to selecting more than two colors for displaying biological masks in advanced mode.</p> Source code in <code>src/cellects/gui/image_analysis_window.py</code> <pre><code>def display_more_than_two_colors_option(self):\n    \"\"\"\n    Display the More Than Two Colors Options\n\n    This method manages the visibility and state of UI elements related to selecting\n    more than two colors for displaying biological masks in advanced mode.\n    \"\"\"\n    if self.bio_masks_number &gt; 0 and self.advanced_mode_cb.isChecked():\n        self.more_than_two_colors.setVisible(True)\n        self.more_than_two_colors_label.setVisible(True)\n        if self.more_than_two_colors.isChecked():\n            self.distinct_colors_number.setVisible(True)\n            self.more_than_two_colors_label.setText(\"How many distinct colors?\")\n            self.distinct_colors_number.setValue(3)\n        else:\n            self.more_than_two_colors_label.setText(\"Heterogeneous background\")\n            self.distinct_colors_number.setVisible(False)\n            self.distinct_colors_number.setValue(2)\n        self.parent().po.all[\"more_than_two_colors\"] = self.more_than_two_colors.isChecked()\n    else:\n        self.more_than_two_colors.setChecked(False)\n        self.more_than_two_colors.setVisible(False)\n        self.more_than_two_colors_label.setVisible(False)\n        self.distinct_colors_number.setVisible(False)\n        self.distinct_colors_number.setValue(2)\n</code></pre>"},{"location":"api/cellects/gui/image_analysis_window/#cellects.gui.image_analysis_window.ImageAnalysisWindow.display_row2","title":"<code>display_row2()</code>","text":"<p>Display or hide the second row of the csc editing widgets based on expert mode.</p> Source code in <code>src/cellects/gui/image_analysis_window.py</code> <pre><code>def display_row2(self):\n    \"\"\"\n    Display or hide the second row of the csc editing widgets based on expert mode.\n    \"\"\"\n    self.row1[4].setVisible(False)\n    for i in range(5):\n        self.row2[i].setVisible(self.parent().po.all['expert_mode'])\n    self.display_logical_operator()\n</code></pre>"},{"location":"api/cellects/gui/image_analysis_window/#cellects.gui.image_analysis_window.ImageAnalysisWindow.display_row22","title":"<code>display_row22()</code>","text":"<p>Display or hide the second row (for the second image segmentation pipeline) of the csc editing widgets based on expert mode.</p> Source code in <code>src/cellects/gui/image_analysis_window.py</code> <pre><code>def display_row22(self):\n    \"\"\"\n    Display or hide the second row (for the second image segmentation pipeline) of the csc editing widgets based on expert mode.\n    \"\"\"\n    self.row21[4].setVisible(False)\n    for i in range(5):\n        self.row22[i].setVisible(self.parent().po.all['expert_mode'])\n    self.display_logical_operator()\n</code></pre>"},{"location":"api/cellects/gui/image_analysis_window/#cellects.gui.image_analysis_window.ImageAnalysisWindow.display_row23","title":"<code>display_row23()</code>","text":"<p>Display or hide the third row (for the second image segmentation pipeline) of the csc editing widgets based on expert mode.</p> Source code in <code>src/cellects/gui/image_analysis_window.py</code> <pre><code>def display_row23(self):\n    \"\"\"\n    Display or hide the third row (for the second image segmentation pipeline) of the csc editing widgets based on expert mode.\n    \"\"\"\n    self.row22[4].setVisible(False)\n    for i in range(4):\n        self.row23[i].setVisible(self.parent().po.all['expert_mode'])\n    self.display_logical_operator()\n</code></pre>"},{"location":"api/cellects/gui/image_analysis_window/#cellects.gui.image_analysis_window.ImageAnalysisWindow.display_row3","title":"<code>display_row3()</code>","text":"<p>Display or hide the third row of the csc editing widgets based on expert mode.</p> Source code in <code>src/cellects/gui/image_analysis_window.py</code> <pre><code>def display_row3(self):\n    \"\"\"\n    Display or hide the third row of the csc editing widgets based on expert mode.\n    \"\"\"\n    self.row2[4].setVisible(False)\n    for i in range(4):\n        self.row3[i].setVisible(self.parent().po.all['expert_mode'])\n    self.display_logical_operator()\n</code></pre>"},{"location":"api/cellects/gui/image_analysis_window/#cellects.gui.image_analysis_window.ImageAnalysisWindow.distinct_colors_number_changed","title":"<code>distinct_colors_number_changed()</code>","text":"<p>Update the parent object's color number variable based on the current value of a distinct colors control.</p> Notes <p>This function expects that the parent object has an attribute <code>po</code> with a dictionary-like 'vars' that can be updated.</p> Source code in <code>src/cellects/gui/image_analysis_window.py</code> <pre><code>def distinct_colors_number_changed(self):\n    \"\"\"\n    Update the parent object's color number variable based on the current value of a distinct colors control.\n\n    Notes\n    -----\n    This function expects that the parent object has an attribute `po` with a dictionary-like 'vars' that can be updated.\n    \"\"\"\n    self.parent().po.vars[\"color_number\"] = int(self.distinct_colors_number.value())\n</code></pre>"},{"location":"api/cellects/gui/image_analysis_window/#cellects.gui.image_analysis_window.ImageAnalysisWindow.filter1_changed","title":"<code>filter1_changed()</code>","text":"<p>Update the UI elements and internal state when the <code>filter1</code> selection changes.</p> <p>This method updates labels, visibility, and values of filter parameters based on the currently selected filter type.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>object</code> <p>The instance of the class containing this method.</p> required Source code in <code>src/cellects/gui/image_analysis_window.py</code> <pre><code>def filter1_changed(self):\n    \"\"\"\n    Update the UI elements and internal state when the `filter1` selection changes.\n\n    This method updates labels, visibility, and values of filter parameters\n    based on the currently selected filter type.\n\n    Parameters\n    ----------\n    self : object\n        The instance of the class containing this method.\n    \"\"\"\n    current_filter = self.filter1.currentText()\n    self.parent().po.vars['filter_spec']['filter1_type'] = current_filter\n    show_param1 = \"Param1\" in filter_dict[current_filter].keys()\n    if self.advanced_mode_cb.isChecked():\n        self.filter1_param1_label.setVisible(show_param1)\n        self.filter1_param1.setVisible(show_param1)\n    if show_param1:\n        self.filter1_param1_label.setText(filter_dict[current_filter]['Param1']['Name'])\n        self.filter1_param1.setMinimum(filter_dict[current_filter]['Param1']['Minimum'])\n        self.filter1_param1.setMaximum(filter_dict[current_filter]['Param1']['Maximum'])\n        if self.filter1_param1.value() &lt; filter_dict[current_filter]['Param1']['Minimum'] or self.filter1_param1.value() &gt; filter_dict[current_filter]['Param1']['Maximum']:\n            self.filter1_param1.setValue(filter_dict[current_filter]['Param1']['Default'])\n    if 'Param2' in list(filter_dict[current_filter].keys()):\n        self.filter1_param2_label.setText(filter_dict[current_filter]['Param2']['Name'])\n        self.filter1_param2.setMinimum(filter_dict[current_filter]['Param2']['Minimum'])\n        self.filter1_param2.setMaximum(filter_dict[current_filter]['Param2']['Maximum'])\n        if self.filter1_param2.value() &lt; filter_dict[current_filter]['Param2']['Minimum'] or self.filter1_param2.value() &gt; filter_dict[current_filter]['Param2']['Maximum']:\n            self.filter1_param2.setValue(filter_dict[current_filter]['Param2']['Default'])\n        if self.advanced_mode_cb.isChecked():\n            self.filter1_param2_label.setVisible(True)\n            self.filter1_param2.setVisible(True)\n    else:\n        self.filter1_param2_label.setVisible(False)\n        self.filter1_param2.setVisible(False)\n</code></pre>"},{"location":"api/cellects/gui/image_analysis_window/#cellects.gui.image_analysis_window.ImageAnalysisWindow.filter1_param1_changed","title":"<code>filter1_param1_changed()</code>","text":"<p>Save the first parameter (most often the lower bound) of the first filter.</p> Source code in <code>src/cellects/gui/image_analysis_window.py</code> <pre><code>def filter1_param1_changed(self):\n    \"\"\"\n    Save the first parameter (most often the lower bound) of the first filter.\n    \"\"\"\n    self.parent().po.vars['filter_spec']['filter1_param'][0] = float(self.filter1_param1.value())\n</code></pre>"},{"location":"api/cellects/gui/image_analysis_window/#cellects.gui.image_analysis_window.ImageAnalysisWindow.filter1_param2_changed","title":"<code>filter1_param2_changed()</code>","text":"<p>Save the second parameter (most often the higher bound) of the first filter.</p> Source code in <code>src/cellects/gui/image_analysis_window.py</code> <pre><code>def filter1_param2_changed(self):\n    \"\"\"\n    Save the second parameter (most often the higher bound) of the first filter.\n    \"\"\"\n    self.parent().po.vars['filter_spec']['filter1_param'][1] = float(self.filter1_param2.value())\n</code></pre>"},{"location":"api/cellects/gui/image_analysis_window/#cellects.gui.image_analysis_window.ImageAnalysisWindow.filter2_changed","title":"<code>filter2_changed()</code>","text":"<p>Update the UI elements and internal state when the <code>filter2</code> selection changes.</p> <p>This method updates labels, visibility, and values of filter parameters based on the currently selected filter type.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>object</code> <p>The instance of the class containing this method.</p> required Source code in <code>src/cellects/gui/image_analysis_window.py</code> <pre><code>def filter2_changed(self):\n    \"\"\"\n    Update the UI elements and internal state when the `filter2` selection changes.\n\n    This method updates labels, visibility, and values of filter parameters\n    based on the currently selected filter type.\n\n    Parameters\n    ----------\n    self : object\n        The instance of the class containing this method.\n    \"\"\"\n    current_filter = self.filter2.currentText()\n    self.parent().po.vars['filter_spec']['filter2_type'] = current_filter\n    show_param1 = \"Param1\" in filter_dict[current_filter].keys()\n    if self.advanced_mode_cb.isChecked():\n        self.filter2_param1_label.setVisible(show_param1)\n        self.filter2_param1.setVisible(show_param1)\n    if show_param1:\n        self.filter2_param1_label.setText(filter_dict[current_filter]['Param1']['Name'])\n        self.filter2_param1.setMinimum(filter_dict[current_filter]['Param1']['Minimum'])\n        self.filter2_param1.setMaximum(filter_dict[current_filter]['Param1']['Maximum'])\n        if self.filter2_param1.value() &lt; filter_dict[current_filter]['Param1']['Minimum'] or self.filter2_param1.value() &gt; filter_dict[current_filter]['Param1']['Maximum']:\n            self.filter2_param1.setValue(filter_dict[current_filter]['Param1']['Default'])\n    if 'Param2' in list(filter_dict[current_filter].keys()):\n        self.filter2_param2_label.setText(filter_dict[current_filter]['Param2']['Name'])\n        self.filter2_param2.setMinimum(filter_dict[current_filter]['Param2']['Minimum'])\n        self.filter2_param2.setMaximum(filter_dict[current_filter]['Param2']['Maximum'])\n        if self.filter2_param2.value() &lt; filter_dict[current_filter]['Param2']['Minimum'] or self.filter2_param2.value() &gt; filter_dict[current_filter]['Param2']['Maximum']:\n            self.filter2_param2.setValue(filter_dict[current_filter]['Param2']['Default'])\n        if self.advanced_mode_cb.isChecked():\n            self.filter2_param2_label.setVisible(True)\n            self.filter2_param2.setVisible(True)\n    else:\n        self.filter2_param2_label.setVisible(False)\n        self.filter2_param2.setVisible(False)\n</code></pre>"},{"location":"api/cellects/gui/image_analysis_window/#cellects.gui.image_analysis_window.ImageAnalysisWindow.filter2_param1_changed","title":"<code>filter2_param1_changed()</code>","text":"<p>Save the first parameter (most often the lower bound) of the second filter.</p> Source code in <code>src/cellects/gui/image_analysis_window.py</code> <pre><code>def filter2_param1_changed(self):\n    \"\"\"\n    Save the first parameter (most often the lower bound) of the second filter.\n    \"\"\"\n    self.parent().po.vars['filter_spec']['filter2_param'][0] = float(self.filter2_param1.value())\n</code></pre>"},{"location":"api/cellects/gui/image_analysis_window/#cellects.gui.image_analysis_window.ImageAnalysisWindow.filter2_param2_changed","title":"<code>filter2_param2_changed()</code>","text":"<p>Save the second parameter (most often the higher bound) of the second filter.</p> Source code in <code>src/cellects/gui/image_analysis_window.py</code> <pre><code>def filter2_param2_changed(self):\n    \"\"\"\n    Save the second parameter (most often the higher bound) of the second filter.\n    \"\"\"\n    self.parent().po.vars['filter_spec']['filter2_param'][1] = float(self.filter2_param2.value())\n</code></pre>"},{"location":"api/cellects/gui/image_analysis_window/#cellects.gui.image_analysis_window.ImageAnalysisWindow.first_im_parameters","title":"<code>first_im_parameters()</code>","text":"<p>Reset UI components and prepare for first image parameters adjustment.</p> <p>This method resets various UI elements to their initial states, hides confirmation buttons, and shows controls for adjusting spot shapes and sizes. It also sets flags to indicate that the user has not yet answered the first image parameters prompt.</p> Source code in <code>src/cellects/gui/image_analysis_window.py</code> <pre><code>def first_im_parameters(self):\n    \"\"\"\n    Reset UI components and prepare for first image parameters adjustment.\n\n    This method resets various UI elements to their initial states, hides\n    confirmation buttons, and shows controls for adjusting spot shapes and sizes.\n    It also sets flags to indicate that the user has not yet answered the first\n    image parameters prompt.\n    \"\"\"\n    self.step = 1\n    self.decision_label.setText(\"Adjust settings, draw more cells and background, and try again\")\n    self.yes.setVisible(False)\n    self.no.setVisible(False)\n    self.set_spot_shape.setVisible(True)\n    self.spot_shape_label.setVisible(True)\n    self.spot_shape.setVisible(self.parent().po.all['set_spot_shape'])\n    self.set_spot_size.setVisible(self.one_blob_per_arena.isChecked())\n    self.spot_size_label.setVisible(self.one_blob_per_arena.isChecked())\n    self.spot_size.setVisible(\n        self.one_blob_per_arena.isChecked() and self.set_spot_size.isChecked())\n    self.auto_delineation_flag = True\n    self.first_im_parameters_answered = True\n</code></pre>"},{"location":"api/cellects/gui/image_analysis_window/#cellects.gui.image_analysis_window.ImageAnalysisWindow.generate_csc_editing","title":"<code>generate_csc_editing()</code>","text":"<p>Create and configure a user interface for color space combination editing.</p> <p>This method sets up the UI components needed to edit color space combinations, including checkboxes, labels, and drop-down menus. It also configures the layout and connections between components.</p> Source code in <code>src/cellects/gui/image_analysis_window.py</code> <pre><code>def generate_csc_editing(self):\n    \"\"\"\n    Create and configure a user interface for color space combination editing.\n\n    This method sets up the UI components needed to edit color space combinations,\n    including checkboxes, labels, and drop-down menus. It also configures the layout\n    and connections between components.\n    \"\"\"\n    self.central_right_widget = QtWidgets.QWidget()\n    self.central_right_layout = QtWidgets.QVBoxLayout()\n\n    # 1) Advanced mode option\n    self.advanced_mode_widget = QtWidgets.QWidget()\n    self.advanced_mode_layout = QtWidgets.QHBoxLayout()\n    self.advanced_mode_cb = Checkbox(self.parent().po.all['expert_mode'])\n    self.advanced_mode_cb.setStyleSheet(\"QCheckBox::indicator {width: 12px;height: 12px;background-color: transparent;\"\n                        \"border-radius: 5px;border-style: solid;border-width: 1px;\"\n                        \"border-color: rgb(100,100,100);}\"\n                        \"QCheckBox::indicator:checked {background-color: rgb(70,130,180);}\"\n                        \"QCheckBox:checked, QCheckBox::indicator:checked {border-color: black black white white;}\"\n                        \"QCheckBox:checked {background-color: transparent;}\"\n                        \"QCheckBox:margin-left {0%}\"\n                        \"QCheckBox:margin-right {0%}\")\n    self.advanced_mode_cb.stateChanged.connect(self.advanced_mode_check)\n    self.advanced_mode_label = FixedText(IAW[\"Advanced_mode\"][\"label\"], halign='l',\n                                         tip=IAW[\"Advanced_mode\"][\"tips\"],\n                                         night_mode=self.parent().po.all['night_mode'])\n    self.advanced_mode_label.setAlignment(QtCore.Qt.AlignTop)\n    self.advanced_mode_layout.addWidget(self.advanced_mode_cb)\n    self.advanced_mode_layout.addWidget(self.advanced_mode_label)\n    self.advanced_mode_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n    self.advanced_mode_widget.setLayout(self.advanced_mode_layout)\n    self.central_right_layout.addWidget(self.advanced_mode_widget)\n\n    self.csc_scroll_table = QtWidgets.QScrollArea()  # QTableWidget()  # Scroll Area which contains the widgets, set as the centralWidget\n    self.csc_scroll_table.setSizePolicy(QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.MinimumExpanding)\n    self.csc_scroll_table.setMinimumHeight(self.parent().im_max_height - 100)\n    self.csc_scroll_table.setFrameShape(QtWidgets.QFrame.NoFrame)\n    self.csc_scroll_table.setVerticalScrollBarPolicy(QtCore.Qt.ScrollBarAsNeeded)\n    self.csc_scroll_table.setHorizontalScrollBarPolicy(QtCore.Qt.ScrollBarAsNeeded)\n    self.csc_table_widget = QtWidgets.QWidget()\n    self.csc_table_layout = QtWidgets.QVBoxLayout()\n\n    # 2) Titles\n    self.edit_labels_widget = QtWidgets.QWidget()\n    self.edit_labels_layout = QtWidgets.QHBoxLayout()\n\n    self.space_label = FixedText(IAW[\"Color_combination\"][\"label\"] + ':', halign='l',\n                                tip=IAW[\"Color_combination\"][\"tips\"],\n                                night_mode=self.parent().po.all['night_mode'])\n\n    self.edit_labels_layout.addWidget(self.space_label)\n    self.edit_labels_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n    self.space_label.setVisible(False)\n    self.edit_labels_widget.setLayout(self.edit_labels_layout)\n    self.csc_table_layout.addWidget(self.edit_labels_widget)\n\n    # 3) First CSC\n    self.first_csc_widget = QtWidgets.QWidget()\n    self.first_csc_layout = QtWidgets.QGridLayout()\n    self.row1 = self.one_csc_editing(with_PCA=True)\n    self.row1[4].clicked.connect(self.display_row2)\n    self.row2 = self.one_csc_editing()\n    self.row2[4].clicked.connect(self.display_row3)\n    self.row3 = self.one_csc_editing()# Second CSC\n    self.logical_operator_between_combination_result = Combobox([\"None\", \"Or\", \"And\", \"Xor\"],\n                                                                night_mode=self.parent().po.all['night_mode'])\n    self.logical_operator_between_combination_result.setCurrentText(self.parent().po.vars['convert_for_motion']['logical'])\n    self.logical_operator_between_combination_result.currentTextChanged.connect(self.logical_op_changed)\n    self.logical_operator_between_combination_result.setFixedWidth(100)\n    self.logical_operator_label = FixedText(IAW[\"Logical_operator\"][\"label\"], tip=IAW[\"Logical_operator\"][\"tips\"],\n                                            night_mode=self.parent().po.all['night_mode'])\n\n    self.row21 = self.one_csc_editing()\n    self.row21[4].clicked.connect(self.display_row22)\n    self.row22 = self.one_csc_editing()\n    self.row22[4].clicked.connect(self.display_row23)\n    self.row23 = self.one_csc_editing()\n    if self.csc_dict is not None:\n        self.update_csc_editing_display()\n    else:\n        self.row1[0].setCurrentIndex(4)\n        self.row1[3].setValue(1)\n        self.row21[0].setCurrentIndex(0)\n        self.row21[3].setValue(0)\n\n    for i in range(5):\n        self.first_csc_layout.addWidget(self.row1[i], 0, i, 1, 1)\n        self.first_csc_layout.addWidget(self.row2[i], 1, i, 1, 1)\n        self.first_csc_layout.addWidget(self.row3[i], 2, i, 1, 1)\n        self.row1[i].setVisible(False)\n        self.row2[i].setVisible(False)\n        self.row3[i].setVisible(False)\n    self.first_csc_layout.setHorizontalSpacing(0)\n    self.first_csc_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum), 0, 5, 3, 1)\n    self.first_csc_widget.setLayout(self.first_csc_layout)\n    self.csc_table_layout.addWidget(self.first_csc_widget)\n\n    # First filters\n    self.filter1_label = FixedText(IAW[\"Filter\"][\"label\"] + ': ', halign='l',\n                                tip=IAW[\"Filter\"][\"tips\"],\n                                night_mode=self.parent().po.all['night_mode'])\n    self.csc_table_layout.addWidget(self.filter1_label)\n    self.filter1_widget = QtWidgets.QWidget()\n    self.filter1_layout = QtWidgets.QHBoxLayout()\n    self.filter1 = Combobox(list(filter_dict.keys()), night_mode=self.parent().po.all['night_mode'])\n    self.filter1.setCurrentText(self.parent().po.vars['filter_spec']['filter1_type'])\n    self.filter1.currentTextChanged.connect(self.filter1_changed)\n    self.filter1.setFixedWidth(100)\n    if \"Param1\" in filter_dict[self.parent().po.vars['filter_spec']['filter1_type']].keys():\n        param1_name = filter_dict[self.parent().po.vars['filter_spec']['filter1_type']][\"Param1\"][\"Name\"]\n    else:\n        param1_name = \"\"\n    self.filter1_param1_label = FixedText(param1_name, halign='l', tip=\"The parameter to adjust the filter effect\",\n                                night_mode=self.parent().po.all['night_mode'])\n    filter_param_spinbox_width = 60\n    self.filter1_param1 = Spinbox(min=-1000, max=1000, val=self.parent().po.vars['filter_spec']['filter1_param'][0], decimals=3, night_mode=self.parent().po.all['night_mode'])\n    self.filter1_param1.setFixedWidth(filter_param_spinbox_width)\n    self.filter1_param1.valueChanged.connect(self.filter1_param1_changed)\n    if \"Param2\" in filter_dict[self.parent().po.vars['filter_spec']['filter1_type']].keys():\n        param2_name = filter_dict[self.parent().po.vars['filter_spec']['filter1_type']][\"Param2\"][\"Name\"]\n    else:\n        param2_name = \"\"\n    self.filter1_param2_label = FixedText(param2_name, halign='l', tip=\"The parameter to adjust the filter effect\",\n        night_mode=self.parent().po.all['night_mode'])\n    self.filter1_param2 = Spinbox(min=-1000, max=1000, val=self.parent().po.vars['filter_spec']['filter1_param'][1], decimals=3, night_mode=self.parent().po.all['night_mode'])\n    self.filter1_param2.setFixedWidth(filter_param_spinbox_width)\n    self.filter1_param2.valueChanged.connect(self.filter1_param2_changed)\n    self.filter1_layout.addWidget(self.filter1)\n    # self.filter1_layout.addWidget(self.filter1_label)\n    self.filter1_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n    self.filter1_layout.addWidget(self.filter1_param1_label)\n    self.filter1_layout.addWidget(self.filter1_param1)\n    self.filter1_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n    self.filter1_layout.addWidget(self.filter1_param2_label)\n    self.filter1_layout.addWidget(self.filter1_param2)\n    self.filter1.setVisible(False)\n    self.filter1_label.setVisible(False)\n    self.filter1_param1_label.setVisible(False)\n    self.filter1_param1.setVisible(False)\n    self.filter1_param2_label.setVisible(False)\n    self.filter1_param2.setVisible(False)\n    self.filter1_widget.setLayout(self.filter1_layout)\n    self.csc_table_layout.addWidget(self.filter1_widget)\n\n    # 4) logical_operator\n    self.logical_op_widget = QtWidgets.QWidget()\n    self.logical_op_layout = QtWidgets.QHBoxLayout()\n    self.logical_op_layout.addWidget(self.logical_operator_label)\n    self.logical_op_layout.addWidget(self.logical_operator_between_combination_result)\n    self.logical_op_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n    self.logical_operator_between_combination_result.setVisible(False)\n    self.logical_operator_label.setVisible(False)\n    self.logical_op_widget.setLayout(self.logical_op_layout)\n    self.csc_table_layout.addWidget(self.logical_op_widget)\n\n    # 5) Second CSC\n    self.second_csc_widget = QtWidgets.QWidget()\n    self.second_csc_layout = QtWidgets.QGridLayout()\n    for i in range(5):\n        self.second_csc_layout.addWidget(self.row21[i], 0, i, 1, 1)\n        self.second_csc_layout.addWidget(self.row22[i], 1, i, 1, 1)\n        self.second_csc_layout.addWidget(self.row23[i], 2, i, 1, 1)\n        self.row21[i].setVisible(False)\n        self.row22[i].setVisible(False)\n        self.row23[i].setVisible(False)\n    self.second_csc_layout.setHorizontalSpacing(0)\n    self.second_csc_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum), 0, 5, 3, 1)\n    self.second_csc_widget.setLayout(self.second_csc_layout)\n    self.csc_table_layout.addWidget(self.second_csc_widget)\n\n    self.csc_table_widget.setLayout(self.csc_table_layout)\n    self.csc_scroll_table.setWidget(self.csc_table_widget)\n    self.csc_scroll_table.setWidgetResizable(True)\n    self.central_right_layout.addWidget(self.csc_scroll_table)\n    self.central_right_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.Maximum, QtWidgets.QSizePolicy.MinimumExpanding))\n\n    # Second filters\n    self.filter2_label = FixedText(IAW[\"Filter\"][\"label\"] + ': ', halign='l',\n                                tip=IAW[\"Filter\"][\"tips\"],\n                                night_mode=self.parent().po.all['night_mode'])\n    self.csc_table_layout.addWidget(self.filter2_label)\n    self.filter2_widget = QtWidgets.QWidget()\n    self.filter2_layout = QtWidgets.QHBoxLayout()\n    self.filter2 = Combobox(list(filter_dict.keys()), night_mode=self.parent().po.all['night_mode'])\n    self.filter2.setCurrentText(self.parent().po.vars['filter_spec']['filter2_type'])\n    self.filter2.currentTextChanged.connect(self.filter2_changed)\n    self.filter2.setFixedWidth(100)\n    if \"Param1\" in filter_dict[self.parent().po.vars['filter_spec']['filter2_type']].keys():\n        param1_name = filter_dict[self.parent().po.vars['filter_spec']['filter2_type']][\"Param1\"][\"Name\"]\n    else:\n        param1_name = \"\"\n    self.filter2_param1_label = FixedText(param1_name, halign='l',\n                                tip=\"The parameter to adjust the filter effect\",\n                                night_mode=self.parent().po.all['night_mode'])\n    self.filter2_param1 = Spinbox(min=-1000, max=1000, val=self.parent().po.vars['filter_spec']['filter2_param'][0], decimals=3, night_mode=self.parent().po.all['night_mode'])\n    self.filter2_param1.setFixedWidth(filter_param_spinbox_width)\n    self.filter2_param1.valueChanged.connect(self.filter2_param1_changed)\n    if \"Param2\" in filter_dict[self.parent().po.vars['filter_spec']['filter2_type']].keys():\n        param2_name = filter_dict[self.parent().po.vars['filter_spec']['filter2_type']][\"Param2\"][\"Name\"]\n    else:\n        param2_name = \"\"\n    self.filter2_param2_label = FixedText(param2_name, halign='l', tip=\"The parameter to adjust the filter effect\",\n        night_mode=self.parent().po.all['night_mode'])\n    self.filter2_param2 = Spinbox(min=-1000, max=1000, val=self.parent().po.vars['filter_spec']['filter2_param'][1], decimals=3, night_mode=self.parent().po.all['night_mode'])\n    self.filter2_param2.setFixedWidth(filter_param_spinbox_width)\n\n    self.filter1_param2.valueChanged.connect(self.filter2_param2_changed)\n    self.filter2_layout.addWidget(self.filter2)\n    self.filter2_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n    self.filter2_layout.addWidget(self.filter2_param1_label)\n    self.filter2_layout.addWidget(self.filter2_param1)\n    self.filter2_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n    self.filter2_layout.addWidget(self.filter2_param2_label)\n    self.filter2_layout.addWidget(self.filter2_param2)\n    self.filter2.setVisible(False)\n    self.filter2_label.setVisible(False)\n    self.filter2_widget.setLayout(self.filter2_layout)\n    self.csc_table_layout.addWidget(self.filter2_widget)\n\n    # 6) Open the rolling_window_segmentation row layout\n    self.rolling_window_segmentation_widget = QtWidgets.QWidget()\n    self.rolling_window_segmentation_layout = QtWidgets.QHBoxLayout()\n    try:\n        self.parent().po.vars[\"rolling_window_segmentation\"]\n    except KeyError:\n        self.parent().po.vars[\"rolling_window_segmentation\"] = False\n    self.rolling_window_segmentation = Checkbox(self.parent().po.vars[\"rolling_window_segmentation\"]['do'])\n    self.rolling_window_segmentation.setStyleSheet(\"QCheckBox::indicator {width: 12px;height: 12px;background-color: transparent;\"\n                        \"border-radius: 5px;border-style: solid;border-width: 1px;\"\n                        \"border-color: rgb(100,100,100);}\"\n                        \"QCheckBox::indicator:checked {background-color: rgb(70,130,180);}\"\n                        \"QCheckBox:checked, QCheckBox::indicator:checked {border-color: black black white white;}\"\n                        \"QCheckBox:checked {background-color: transparent;}\"\n                        \"QCheckBox:margin-left {0%}\"\n                        \"QCheckBox:margin-right {-10%}\")\n    self.rolling_window_segmentation.stateChanged.connect(self.rolling_window_segmentation_option)\n\n    self.rolling_window_segmentation_label = FixedText(IAW[\"Rolling_window_segmentation\"][\"label\"],\n                                                tip=IAW[\"Rolling_window_segmentation\"][\"tips\"], night_mode=self.parent().po.all['night_mode'])\n    self.rolling_window_segmentation_label.setSizePolicy(QtWidgets.QSizePolicy.Maximum, QtWidgets.QSizePolicy.Maximum)\n    self.rolling_window_segmentation_label.setAlignment(QtCore.Qt.AlignLeft)\n\n    self.rolling_window_segmentation_layout.addWidget(self.rolling_window_segmentation)\n    self.rolling_window_segmentation_layout.addWidget(self.rolling_window_segmentation_label)\n    self.rolling_window_segmentation_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n    self.rolling_window_segmentation_widget.setLayout(self.rolling_window_segmentation_layout)\n    self.central_right_layout.addWidget(self.rolling_window_segmentation_widget)\n\n    # 6) Open the more_than_2_colors row layout\n    self.more_than_2_colors_widget = QtWidgets.QWidget()\n    self.more_than_2_colors_layout = QtWidgets.QHBoxLayout()\n    self.more_than_two_colors = Checkbox(self.parent().po.all[\"more_than_two_colors\"])\n    self.more_than_two_colors.setStyleSheet(\"QCheckBox::indicator {width: 12px;height: 12px;background-color: transparent;\"\n                        \"border-radius: 5px;border-style: solid;border-width: 1px;\"\n                        \"border-color: rgb(100,100,100);}\"\n                        \"QCheckBox::indicator:checked {background-color: rgb(70,130,180);}\"\n                        \"QCheckBox:checked, QCheckBox::indicator:checked {border-color: black black white white;}\"\n                        \"QCheckBox:checked {background-color: transparent;}\"\n                        \"QCheckBox:margin-left {0%}\"\n                        \"QCheckBox:margin-right {-10%}\")\n    self.more_than_two_colors.stateChanged.connect(self.display_more_than_two_colors_option)\n\n    self.more_than_two_colors_label = FixedText(IAW[\"Kmeans\"][\"label\"],\n                                                tip=IAW[\"Kmeans\"][\"tips\"], night_mode=self.parent().po.all['night_mode'])\n    self.more_than_two_colors_label.setSizePolicy(QtWidgets.QSizePolicy.Maximum, QtWidgets.QSizePolicy.Maximum)\n    self.more_than_two_colors_label.setAlignment(QtCore.Qt.AlignLeft)\n    self.distinct_colors_number = Spinbox(min=2, max=5, val=self.parent().po.vars[\"color_number\"], night_mode=self.parent().po.all['night_mode'])\n\n    self.distinct_colors_number.valueChanged.connect(self.distinct_colors_number_changed)\n    self.display_more_than_two_colors_option()\n    self.more_than_two_colors.setVisible(False)\n    self.more_than_two_colors_label.setVisible(False)\n    self.distinct_colors_number.setVisible(False)\n    self.rolling_window_segmentation.setVisible(False)\n    self.rolling_window_segmentation_label.setVisible(False)\n\n    self.more_than_2_colors_layout.addWidget(self.more_than_two_colors)\n    self.more_than_2_colors_layout.addWidget(self.more_than_two_colors_label)\n    self.more_than_2_colors_layout.addWidget(self.distinct_colors_number)\n    self.more_than_2_colors_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n    self.more_than_2_colors_widget.setLayout(self.more_than_2_colors_layout)\n    self.central_right_layout.addWidget(self.more_than_2_colors_widget)\n\n    self.central_right_widget.setLayout(self.central_right_layout)\n</code></pre>"},{"location":"api/cellects/gui/image_analysis_window/#cellects.gui.image_analysis_window.ImageAnalysisWindow.get_click_coordinates","title":"<code>get_click_coordinates(event)</code>","text":"<p>Handle mouse click events to capture coordinate data or display an image.</p> <p>This function determines the handling of click events based on various flags and states, including whether image analysis is running or if a manual delineation flag is set.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>QMouseEvent</code> <p>The mouse event that triggered the function.</p> required Source code in <code>src/cellects/gui/image_analysis_window.py</code> <pre><code>def get_click_coordinates(self, event):\n    \"\"\"\n    Handle mouse click events to capture coordinate data or display an image.\n\n    This function determines the handling of click events based on various\n    flags and states, including whether image analysis is running or if a\n    manual delineation flag is set.\n\n    Parameters\n    ----------\n    event : QMouseEvent\n        The mouse event that triggered the function.\n    \"\"\"\n    if self.back1_bio2 &gt; 0 or self.manual_delineation_flag:\n        if not self.is_image_analysis_display_running and not self.thread_dict[\"UpdateImage\"].isRunning():\n            self.hold_click_flag = True\n            self.saved_coord.append([event.pos().y(), event.pos().x()])\n    else:\n        self.popup_img = FullScreenImage(self.drawn_image, self.parent().screen_width, self.parent().screen_height)\n        self.popup_img.show()\n</code></pre>"},{"location":"api/cellects/gui/image_analysis_window/#cellects.gui.image_analysis_window.ImageAnalysisWindow.get_mouse_move_coordinates","title":"<code>get_mouse_move_coordinates(event)</code>","text":"<p>Handles mouse movement events to update the temporary mask coordinate.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>QMouseEvent</code> <p>The mouse event object containing position information.</p> required Source code in <code>src/cellects/gui/image_analysis_window.py</code> <pre><code>def get_mouse_move_coordinates(self, event):\n    \"\"\"\n    Handles mouse movement events to update the temporary mask coordinate.\n\n    Parameters\n    ----------\n    event : QMouseEvent\n        The mouse event object containing position information.\n    \"\"\"\n    if self.hold_click_flag:\n        if not self.thread_dict[\"UpdateImage\"].isRunning():\n            if self.saved_coord[0][0] != event.pos().y() and self.saved_coord[0][1] != event.pos().x():\n                self.temporary_mask_coord = [self.saved_coord[0], [event.pos().y(), event.pos().x()]]\n                self.thread_dict[\"UpdateImage\"].start()\n</code></pre>"},{"location":"api/cellects/gui/image_analysis_window/#cellects.gui.image_analysis_window.ImageAnalysisWindow.get_mouse_release_coordinates","title":"<code>get_mouse_release_coordinates(event)</code>","text":"<p>Process mouse release event to save coordinates and manage image update thread.</p> <p>This method handles the logic for saving mouse release coordinates during manual delineation, checks conditions to prevent exceeding the number of arenas, and manages an image update thread for display purposes.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>QMouseEvent</code> <p>The mouse event containing the release position.</p> required Notes <p>This method requires an active image update thread and assumes certain attributes like <code>hold_click_flag</code>, <code>manual_delineation_flag</code>, etc., are part of the class state.</p> Source code in <code>src/cellects/gui/image_analysis_window.py</code> <pre><code>def get_mouse_release_coordinates(self, event):\n    \"\"\"\n    Process mouse release event to save coordinates and manage image update thread.\n\n    This method handles the logic for saving mouse release coordinates during\n    manual delineation, checks conditions to prevent exceeding the number of arenas,\n    and manages an image update thread for display purposes.\n\n    Parameters\n    ----------\n    event : QMouseEvent\n        The mouse event containing the release position.\n\n    Notes\n    -----\n    This method requires an active image update thread and assumes certain attributes\n    like `hold_click_flag`, `manual_delineation_flag`, etc., are part of the class\n    state.\n    \"\"\"\n    if self.hold_click_flag:\n        if self.thread_dict[\"UpdateImage\"].isRunning():\n            self.thread_dict[\"UpdateImage\"].wait()\n        self.temporary_mask_coord = []\n        if self.manual_delineation_flag and len(self.parent().imageanalysiswindow.available_arena_names) == 0:\n            self.message.setText(f\"The total number of arenas are already drawn ({self.parent().po.sample_number})\")\n            self.saved_coord = []\n        else:\n            self.saved_coord.append([event.pos().y(), event.pos().x()])\n            self.thread_dict[\"UpdateImage\"].start()\n            self.thread_dict[\"UpdateImage\"].message_when_thread_finished.connect(self.user_defined_shape_displayed)\n        self.hold_click_flag = False\n</code></pre>"},{"location":"api/cellects/gui/image_analysis_window/#cellects.gui.image_analysis_window.ImageAnalysisWindow.go_to_next_widget","title":"<code>go_to_next_widget()</code>","text":"<p>Advances the user interface to the next widget after performing final checks.</p> Notes <p>This function performs several actions in sequence:     - Displays a message box to inform the user about final checks.     - Waits for some background threads to complete their execution.     - Advances the UI to the video analysis window if certain conditions are met.</p> Source code in <code>src/cellects/gui/image_analysis_window.py</code> <pre><code>def go_to_next_widget(self):\n    \"\"\"\n    Advances the user interface to the next widget after performing final checks.\n\n    Notes\n    -----\n    This function performs several actions in sequence:\n        - Displays a message box to inform the user about final checks.\n        - Waits for some background threads to complete their execution.\n        - Advances the UI to the video analysis window if certain conditions are met.\n    \"\"\"\n    if not self.thread_dict['SaveManualDelineation'].isRunning() or not self.thread_dict['PrepareVideoAnalysis'].isRunning() or not self.thread_dict['SaveData'].isRunning():\n\n        self.popup = QtWidgets.QMessageBox()\n        self.popup.setWindowTitle(\"Info\")\n        self.popup.setText(\"Final checks...\")\n        self.popup.setInformativeText(\"Close and wait until the video tracking window appears.\")\n        self.popup.setStandardButtons(QtWidgets.QMessageBox.Close)\n        x = self.popup.exec_()\n        self.decision_label.setVisible(False)\n        self.yes.setVisible(False)\n        self.no.setVisible(False)\n        self.next.setVisible(True)\n\n\n        self.message.setText(f\"Final checks, wait... \")\n        self.parent().last_tab = \"image_analysis\"\n        self.thread_dict['PrepareVideoAnalysis'].start()\n        if self.parent().po.vars[\"color_number\"] &gt; 2:\n            self.parent().videoanalysiswindow.select_option.clear()\n            self.parent().videoanalysiswindow.select_option.addItem(f\"1) Kmeans\")\n            self.parent().videoanalysiswindow.select_option.setCurrentIndex(0)\n            self.parent().po.all['video_option'] = 0\n        time.sleep(1 / 10)\n        self.thread_dict['PrepareVideoAnalysis'].wait()\n        self.message.setText(f\"\")\n\n        self.video_tab.set_not_in_use()\n        self.parent().last_tab = \"image_analysis\"\n        self.parent().change_widget(3)  # VideoAnalysisWindow\n\n        self.popup.close()\n</code></pre>"},{"location":"api/cellects/gui/image_analysis_window/#cellects.gui.image_analysis_window.ImageAnalysisWindow.horizontal_size_changed","title":"<code>horizontal_size_changed()</code>","text":"<p>Changes the horizontal size value of the image or of the blobs in the image, depending on user's choice.</p> Source code in <code>src/cellects/gui/image_analysis_window.py</code> <pre><code>def horizontal_size_changed(self):\n    \"\"\"\n    Changes the horizontal size value of the image or of the blobs in the image, depending on user's choice.\n    \"\"\"\n    if self.parent().po.all['scale_with_image_or_cells'] == 0:\n        self.parent().po.all['image_horizontal_size_in_mm'] = self.horizontal_size.value()\n    else:\n        self.parent().po.all['starting_blob_hsize_in_mm'] = self.horizontal_size.value()\n        self.spot_size.setValue(self.parent().po.all['starting_blob_hsize_in_mm'])\n</code></pre>"},{"location":"api/cellects/gui/image_analysis_window/#cellects.gui.image_analysis_window.ImageAnalysisWindow.image_analysis_displayed","title":"<code>image_analysis_displayed()</code>","text":"<p>Display results of image analysis based on the current step and configuration.</p> <p>Update the user interface elements based on the current step of image analysis, the detected number of shapes, and whether color analysis is enabled. Handles visibilities of buttons and labels to guide the user through the process.</p> Notes <p>This method updates the user interface based on the current state of image analysis.</p> Source code in <code>src/cellects/gui/image_analysis_window.py</code> <pre><code>def image_analysis_displayed(self):\n    \"\"\"\n    Display results of image analysis based on the current step and configuration.\n\n    Update the user interface elements based on the current step of image analysis,\n    the detected number of shapes, and whether color analysis is enabled. Handles\n    visibilities of buttons and labels to guide the user through the process.\n\n    Notes\n    -----\n    This method updates the user interface based on the current state of image analysis.\n    \"\"\"\n    color_analysis = not self.parent().po.vars['already_greyscale']\n    self.message.setText(\"\")\n\n    if self.step &lt; 2:\n        detected_shape_nb = self.parent().po.first_image.im_combinations[self.parent().po.current_combination_id][\n            'shape_number']\n        if detected_shape_nb == self.parent().po.sample_number or self.parent().po.vars['several_blob_per_arena']:\n            self.decision_label.setText(\n                f\"{detected_shape_nb} distinct specimen(s) detected in {self.parent().po.sample_number} arena(s). Does the color match the cell(s)?\")\n            if self.step == 1:\n                self.yes.setVisible(True)\n                self.message.setText(\"If not, draw more Cell and Back ellipses on the image and retry\")\n        else:\n            if self.no.isVisible():\n                self.decision_label.setText(\n                    f\"{detected_shape_nb} distinct specimen(s) detected in {self.parent().po.sample_number} arena(s). Click Yes when satisfied, Click No to fill in more parameters\")\n                self.yes.setVisible(True)\n                self.no.setVisible(True)\n            else:\n                self.decision_label.setText(\n                    f\"{detected_shape_nb} distinct specimen(s) detected in {self.parent().po.sample_number} arena(s). Click Yes when satisfied\")\n                self.yes.setVisible(True)\n\n        if self.parent().po.vars['several_blob_per_arena'] and (detected_shape_nb == self.parent().po.sample_number):\n            self.message.setText(\"Beware: Contrary to what has been checked, there is one spot per arena\")\n\n    if not self.parent().po.visualize:\n        self.select_option.setVisible(color_analysis)\n        self.select_option_label.setVisible(color_analysis)\n    if self.step == 0:\n        if self.parent().po.first_image.im_combinations[self.parent().po.current_combination_id]['shape_number'] == 0:\n            self.message.setText(\"Make sure that scaling metric and spot size are correct\")\n        self.decision_label.setVisible(True)\n        self.yes.setVisible(True)\n        self.no.setVisible(True)\n        self.arena_shape.setVisible(True)\n        self.arena_shape_label.setVisible(True)\n        self.n_shapes_detected.setVisible(True)\n\n    elif self.step == 2:\n        self.generate_analysis_options.setVisible(color_analysis)\n        self.network_shaped.setVisible(True)\n        self.basic.setVisible(color_analysis)\n        self.visualize.setVisible(True)\n\n        self.decision_label.setText(\"Adjust parameters until the color delimits the specimen(s) correctly\")\n        self.yes.setVisible(False)\n        self.no.setVisible(False)\n        if self.parent().po.all[\"im_or_vid\"] == 1 or len(self.parent().po.data_list) &gt; 1:\n            self.next.setVisible(True)\n            self.message.setText('When the resulting segmentation of the last image seems good, click next.')\n        else:\n            self.video_tab.set_not_usable()\n            self.message.setText('When the resulting segmentation of the last image seems good, save image analysis.')\n        self.complete_image_analysis.setVisible(True)\n\n    self.thread_dict[\"UpdateImage\"].message_when_thread_finished.disconnect()\n    self.is_image_analysis_running = False\n    self.is_image_analysis_display_running = False\n</code></pre>"},{"location":"api/cellects/gui/image_analysis_window/#cellects.gui.image_analysis_window.ImageAnalysisWindow.init_drawn_image","title":"<code>init_drawn_image(im_combinations=None)</code>","text":"<p>Initialize the drawn image from a list of image combinations.</p> <p>Parameters:</p> Name Type Description Default <code>im_combinations</code> <code>list or None</code> <p>List of image combinations to initialize the drawn image from. Each combination should be a dictionary containing 'csc' and 'converted_image'. If None, the current state is maintained.</p> <code>None</code> Source code in <code>src/cellects/gui/image_analysis_window.py</code> <pre><code>def init_drawn_image(self, im_combinations: list=None):\n    \"\"\"\n    Initialize the drawn image from a list of image combinations.\n\n    Parameters\n    ----------\n    im_combinations : list or None, optional\n        List of image combinations to initialize the drawn image from.\n        Each combination should be a dictionary containing 'csc' and\n        'converted_image'. If None, the current state is maintained.\n    \"\"\"\n    if im_combinations is not None and len(im_combinations) &gt; 0:\n        if self.parent().po.current_combination_id + 1 &gt; len(im_combinations):\n            self.parent().po.current_combination_id = 0\n        self.csc_dict = im_combinations[self.parent().po.current_combination_id][\"csc\"]\n        self.parent().po.current_image = np.stack((im_combinations[self.parent().po.current_combination_id]['converted_image'],\n                                                im_combinations[self.parent().po.current_combination_id]['converted_image'],\n                                                im_combinations[self.parent().po.current_combination_id]['converted_image']), axis=2)\n        self.drawn_image = deepcopy(self.parent().po.current_image)\n</code></pre>"},{"location":"api/cellects/gui/image_analysis_window/#cellects.gui.image_analysis_window.ImageAnalysisWindow.last_image_question","title":"<code>last_image_question()</code>","text":"<p>Last image question.</p> <p>Queries the user if they want to check parameters for the last image, informing them that the best segmentation pipeline may change during analysis.</p> Source code in <code>src/cellects/gui/image_analysis_window.py</code> <pre><code>def last_image_question(self):\n    \"\"\"\n    Last image question.\n\n    Queries the user if they want to check parameters for the last image,\n    informing them that the best segmentation pipeline may change during analysis.\n    \"\"\"\n\n    self.image_number.setVisible(False)\n    self.image_number_label.setVisible(False)\n    self.read.setVisible(False)\n    self.step = 2\n    if self.parent().po.all[\"im_or_vid\"] == 0 and len(self.parent().po.data_list) == 1:\n        self.starting_differs_from_growing_cb.setChecked(False)\n        self.start_last_image()\n    else:\n        self.asking_last_image_flag = True\n        self.decision_label.setText(\"Click 'yes' to improve the segmentation using the last image\")\n        self.decision_label.setToolTip(IAW[\"Last_image_question\"][\"tips\"])\n        self.message.setText('This is useful when the specimen(s) is more visible.')\n        self.starting_differs_from_growing_cb.setVisible(True)\n        self.starting_differs_from_growing_label.setVisible(True)\n        self.yes.setVisible(True)\n        self.no.setVisible(True)\n</code></pre>"},{"location":"api/cellects/gui/image_analysis_window/#cellects.gui.image_analysis_window.ImageAnalysisWindow.logical_op_changed","title":"<code>logical_op_changed()</code>","text":"<p>Handles the visibility and values of UI elements based on the current logical operator selection in a combination result dropdown.</p> Source code in <code>src/cellects/gui/image_analysis_window.py</code> <pre><code>def logical_op_changed(self):\n    \"\"\"\n    Handles the visibility and values of UI elements based on the current\n    logical operator selection in a combination result dropdown.\n    \"\"\"\n    if self.logical_operator_between_combination_result.currentText() == 'None':\n        self.row21[0].setVisible(False)\n        self.row21[0].setCurrentIndex(0)\n        for i1 in [1, 2, 3]:\n            self.row21[i1].setVisible(False)\n            self.row21[i1].setValue(0)\n        self.row21[i1 + 1].setVisible(False)\n\n        self.row22[0].setVisible(False)\n        self.row22[0].setCurrentIndex(0)\n        for i1 in [1, 2, 3]:\n            self.row22[i1].setVisible(False)\n            self.row22[i1].setValue(0)\n        self.row22[i1 + 1].setVisible(False)\n\n        self.row23[0].setVisible(False)\n        self.row23[0].setCurrentIndex(0)\n        for i1 in [1, 2, 3]:\n            self.row23[i1].setVisible(False)\n            self.row23[i1].setValue(0)\n        self.row23[i1 + 1].setVisible(False)\n    else:\n        self.filter2_label.setVisible(self.parent().po.all['expert_mode'])\n        self.filter2.setVisible(self.parent().po.all['expert_mode'])\n        self.filter2_changed()\n        self.row21[0].setVisible(self.parent().po.all['expert_mode'])\n        for i1 in [1, 2, 3]:\n            self.row21[i1].setVisible(self.parent().po.all['expert_mode'])\n        self.row21[i1 + 1].setVisible(self.parent().po.all['expert_mode'])\n</code></pre>"},{"location":"api/cellects/gui/image_analysis_window/#cellects.gui.image_analysis_window.ImageAnalysisWindow.manual_delineation","title":"<code>manual_delineation()</code>","text":"<p>Manually delineates the analysis arena on the image by enabling user interaction and preparing the necessary attributes for manual drawing of arenas on the image.</p> Source code in <code>src/cellects/gui/image_analysis_window.py</code> <pre><code>def manual_delineation(self):\n    \"\"\"\n    Manually delineates the analysis arena on the image by enabling user interaction and\n    preparing the necessary attributes for manual drawing of arenas on the image.\n    \"\"\"\n    self.manual_delineation_flag = True\n    self.parent().po.cropping(is_first_image=True)\n    self.parent().po.get_average_pixel_size()\n    self.reinitialize_image_and_masks(self.parent().po.first_image.bgr)\n    self.reinitialize_bio_and_back_legend()\n    self.available_arena_names = np.arange(1, self.parent().po.sample_number + 1)\n    self.saved_coord = []\n    self.arena_mask = np.zeros(self.parent().po.current_image.shape[:2], dtype=np.uint16)\n    # self.next.setVisible(True)\n    self.decision_label.setVisible(True)\n    self.yes.setVisible(True)\n    self.cell.setVisible(False)\n    self.background.setVisible(False)\n    self.no.setVisible(False)\n    self.one_blob_per_arena.setVisible(False)\n    self.one_blob_per_arena_label.setVisible(False)\n    self.generate_analysis_options.setVisible(False)\n    self.network_shaped.setVisible(False)\n    self.basic.setVisible(False)\n    self.visualize.setVisible(False)\n    self.visualize_label.setVisible(False)\n    self.select_option.setVisible(False)\n    self.select_option_label.setVisible(False)\n    self.user_drawn_lines_label.setText(\"Draw each arena\")\n    self.user_drawn_lines_label.setVisible(True)\n    self.decision_label.setText(\n        f\"Hold click to draw {self.parent().po.sample_number} arena(s) on the image. Once done, click yes.\")\n    self.message.setText('An error? Hit one button on the left to remove any drawn arena.')\n</code></pre>"},{"location":"api/cellects/gui/image_analysis_window/#cellects.gui.image_analysis_window.ImageAnalysisWindow.network_shaped_is_clicked","title":"<code>network_shaped_is_clicked()</code>","text":"<p>Sets the GUI state for analyzing a network-shaped image when clicked.</p> <p>This method triggers the analysis process for a network-shaped image. It ensures that image analysis is not already running, updates GUI elements accordingly, and starts the appropriate analysis function based on a flag.</p> Source code in <code>src/cellects/gui/image_analysis_window.py</code> <pre><code>def network_shaped_is_clicked(self):\n    \"\"\"\n    Sets the GUI state for analyzing a network-shaped image when clicked.\n\n    This method triggers the analysis process for a network-shaped image. It ensures that image analysis is not\n    already running, updates GUI elements accordingly, and starts the appropriate analysis function based on a flag.\n    \"\"\"\n    if not self.is_image_analysis_running:\n        self.is_image_analysis_running = True\n        self.message.setText('Loading, wait...')\n        self.parent().po.visualize = False\n        self.parent().po.basic = False\n        self.parent().po.network_shaped = True\n        self.select_option.clear()\n        if self.is_first_image_flag:\n            self.run_first_image_analysis()\n        else:\n            self.run_last_image_analysis()\n</code></pre>"},{"location":"api/cellects/gui/image_analysis_window/#cellects.gui.image_analysis_window.ImageAnalysisWindow.new_pbutton_on_the_left","title":"<code>new_pbutton_on_the_left(pbutton_name)</code>","text":"<p>Create a styled PButton instance positioned on the left of the image.</p> Notes <p>The button's appearance is customized based on the value of <code>self.back1_bio2</code>, which affects its color. The button also has a fixed size and specific font settings.</p> Source code in <code>src/cellects/gui/image_analysis_window.py</code> <pre><code>def new_pbutton_on_the_left(self, pbutton_name: str):\n    \"\"\"\n    Create a styled PButton instance positioned on the left of the image.\n\n    Notes\n    -----\n    The button's appearance is customized based on the value of\n    `self.back1_bio2`, which affects its color. The button also has a fixed\n    size and specific font settings.\n    \"\"\"\n    pbutton = PButton(pbutton_name, False, night_mode=self.parent().po.all['night_mode'])\n    pbutton.setFixedHeight(20)\n    pbutton.setFixedWidth(100)\n    pbutton.setFont(QtGui.QFont(\"Segoe UI Semibold\", 8, QtGui.QFont.Thin))\n    pbutton.textcolor(\"rgb(0, 0, 0)\")\n    pbutton.border(\"0px\")\n    pbutton.angles(\"10px\")\n    if self.back1_bio2 == 1:\n        pbutton.color(\"rgb(81, 160, 224)\")\n    elif self.back1_bio2 == 2:\n        pbutton.color(\"rgb(230, 145, 18)\")\n    else:\n        pbutton.color(\"rgb(126, 126, 126)\")\n    pbutton.clicked.connect(self.remove_line)\n    return pbutton\n</code></pre>"},{"location":"api/cellects/gui/image_analysis_window/#cellects.gui.image_analysis_window.ImageAnalysisWindow.one_csc_editing","title":"<code>one_csc_editing(with_PCA=False)</code>","text":"Summary <p>Edit the color space configuration and add widgets for PCA or other options.</p> <p>Parameters:</p> Name Type Description Default <code>with_PCA</code> <code>bool</code> <p>Flag indicating whether to include PCA options. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>list</code> <p>List of widgets for color space configuration.</p> Source code in <code>src/cellects/gui/image_analysis_window.py</code> <pre><code>def one_csc_editing(self, with_PCA: bool=False):\n    \"\"\"\n    Summary\n    --------\n    Edit the color space configuration and add widgets for PCA or other options.\n\n    Parameters\n    ----------\n    with_PCA : bool, optional\n        Flag indicating whether to include PCA options.\n        Default is False.\n\n    Returns\n    -------\n    list\n        List of widgets for color space configuration.\n    \"\"\"\n    widget_list = []\n    if with_PCA:\n        widget_list.insert(0, Combobox([\"PCA\", \"bgr\", \"hsv\", \"hls\", \"lab\", \"luv\", \"yuv\"],\n                                       night_mode=self.parent().po.all['night_mode']))\n        widget_list[0].currentTextChanged.connect(self.pca_changed)\n    else:\n        widget_list.insert(0, Combobox([\"None\", \"bgr\", \"hsv\", \"hls\", \"lab\", \"luv\", \"yuv\"],\n                                       night_mode=self.parent().po.all['night_mode']))\n    widget_list[0].setSizePolicy(QtWidgets.QSizePolicy.Maximum, QtWidgets.QSizePolicy.Maximum)\n    widget_list[0].setFixedWidth(100)\n    for i in [1, 2, 3]:\n        widget_list.insert(i, Spinbox(min=-126, max=126, val=0, night_mode=self.parent().po.all['night_mode']))\n        widget_list[i].setFixedWidth(45)\n    widget_list.insert(i + 1, PButton(\"+\", night_mode=self.parent().po.all['night_mode']))\n    return widget_list\n</code></pre>"},{"location":"api/cellects/gui/image_analysis_window/#cellects.gui.image_analysis_window.ImageAnalysisWindow.option_changed","title":"<code>option_changed()</code>","text":"<p>Update the current image and related display information based on the selected image segmentation option.</p> Notes <p>This function updates several properties of the parent object, including the current image, combination ID, and display settings. It also handles thread management for updating the image display.</p> Source code in <code>src/cellects/gui/image_analysis_window.py</code> <pre><code>def option_changed(self):\n    \"\"\"\n    Update the current image and related display information based on the selected image segmentation option.\n\n    Notes\n    -----\n    This function updates several properties of the parent object, including the current image,\n    combination ID, and display settings. It also handles thread management for updating the\n    image display.\n    \"\"\"\n    # Update the current image\n    self.parent().po.current_combination_id = self.select_option.currentIndex()\n    if self.is_first_image_flag:\n        im_combinations = self.parent().po.first_image.im_combinations\n    else:\n        im_combinations = self.parent().po.last_image.im_combinations\n    self.init_drawn_image(im_combinations)\n    if im_combinations is not None and len(im_combinations) &gt; 0:\n        # Update image display\n        if self.thread_dict[\"UpdateImage\"].isRunning():\n            self.thread_dict[\"UpdateImage\"].wait()\n        self.thread_dict[\"UpdateImage\"].start()\n        # Update csc editing\n        self.update_csc_editing_display()\n\n        # Update the detected shape number\n        if self.is_first_image_flag:\n            self.parent().po.vars['convert_for_origin'] = im_combinations[self.parent().po.current_combination_id][\"csc\"]\n            detected_shape_nb = im_combinations[self.parent().po.current_combination_id]['shape_number']\n            if self.parent().po.vars['several_blob_per_arena']:\n                if detected_shape_nb == self.parent().po.sample_number:\n                    self.message.setText(\"Beware: Contrary to what has been checked, there is one spot per arena\")\n            else:\n                if detected_shape_nb == self.parent().po.sample_number:\n                    self.decision_label.setText(\n                        f\"{detected_shape_nb} distinct specimen(s) detected in {self.parent().po.sample_number} arena(s). Does the color match the cell(s)?\")\n                    self.yes.setVisible(True)\n                else:\n                    self.decision_label.setText(\n                        f\"{detected_shape_nb} distinct specimen(s) detected in {self.parent().po.sample_number} arena(s). Adjust settings, draw more cells and background, and try again\")\n                    self.yes.setVisible(False)\n            if im_combinations[self.parent().po.current_combination_id]['shape_number'] == 0:\n                self.message.setText(\"Make sure that scaling metric and spot size are correct\")\n        else:\n            self.parent().po.vars['convert_for_motion'] = im_combinations[self.parent().po.current_combination_id][\"csc\"]\n            self.decision_label.setText(\"Do colored contours correctly match cell(s) contours?\")\n        if \"rolling_window\" in im_combinations[self.parent().po.current_combination_id]:\n            self.parent().po.vars['rolling_window_segmentation']['do'] = im_combinations[self.parent().po.current_combination_id][\"rolling_window\"]\n        if \"filter_spec\" in im_combinations[self.parent().po.current_combination_id]:\n            self.parent().po.vars['filter_spec'] = im_combinations[self.parent().po.current_combination_id][\n                \"filter_spec\"]\n            self.update_filter_display()\n</code></pre>"},{"location":"api/cellects/gui/image_analysis_window/#cellects.gui.image_analysis_window.ImageAnalysisWindow.pca_changed","title":"<code>pca_changed()</code>","text":"<p>Handles the UI changes when 'PCA' is selected in dropdown menu.</p> Notes <p>This function modifies the visibility of UI elements based on the selection in a dropdown menu. It is triggered when 'PCA' is selected, and hides elements related to logical operators.</p> Source code in <code>src/cellects/gui/image_analysis_window.py</code> <pre><code>def pca_changed(self):\n    \"\"\"\n    Handles the UI changes when 'PCA' is selected in dropdown menu.\n\n    Notes\n    -----\n    This function modifies the visibility of UI elements based on the selection in a dropdown menu.\n    It is triggered when 'PCA' is selected, and hides elements related to logical operators.\n    \"\"\"\n    if self.row1[0].currentText() == 'PCA':\n        self.logical_operator_between_combination_result.setCurrentText('None')\n        for i in range(1, 5):\n            self.row1[i].setVisible(False)\n            self.row2[i].setVisible(False)\n            self.row3[i].setVisible(False)\n        self.logical_operator_label.setVisible(False)\n        self.logical_operator_between_combination_result.setVisible(False)\n    else:\n        for i in range(1, 5):\n            self.row1[i].setVisible(True)\n</code></pre>"},{"location":"api/cellects/gui/image_analysis_window/#cellects.gui.image_analysis_window.ImageAnalysisWindow.previous_is_clicked","title":"<code>previous_is_clicked()</code>","text":"<p>Handles the logic for when a \"Previous\" button is clicked in the interface, leading to the FirstWindow.</p> <p>This method resets various flags and variables related to image analysis to their initial state. It is called when the \"Previous\" button is clicked, preparing the application for new input and reinitialization.</p> Source code in <code>src/cellects/gui/image_analysis_window.py</code> <pre><code>def previous_is_clicked(self):\n    \"\"\"\n    Handles the logic for when a \"Previous\" button is clicked in the interface, leading to the FirstWindow.\n\n    This method resets various flags and variables related to image analysis\n    to their initial state. It is called when the \"Previous\" button is clicked,\n    preparing the application for new input and reinitialization.\n    \"\"\"\n    if self.is_image_analysis_running:\n        self.message.setText(\"Wait for the analysis to end, or restart Cellects\")\n    else:\n        self.parent().firstwindow.instantiate = True\n        self.hold_click_flag: bool = False\n        self.is_first_image_flag: bool = True\n        self.is_image_analysis_running: bool = False\n        self.is_image_analysis_display_running: bool = False\n        self.asking_first_im_parameters_flag: bool = True\n        self.first_im_parameters_answered: bool = False\n        self.auto_delineation_flag: bool = False\n        self.delineation_done: bool = False\n        self.asking_delineation_flag: bool = False\n        self.asking_slower_or_manual_delineation_flag: bool = False\n        self.slower_delineation_flag: bool = False\n        self.asking_last_image_flag: bool = False\n        self.step = 0\n        self.temporary_mask_coord = []\n        self.saved_coord = []\n        self.back1_bio2 = 0\n        self.bio_masks_number = 0\n        self.back_masks_number = 0\n        self.arena_masks_number = 0\n        self.available_bio_names = np.arange(1, 1000, dtype=np.uint16)\n        self.available_back_names = np.arange(1, 1000, dtype=np.uint16)\n        self.parent().po.current_combination_id = 0\n        self.parent().last_tab = \"data_specifications\"\n        self.parent().change_widget(0)  # First\n</code></pre>"},{"location":"api/cellects/gui/image_analysis_window/#cellects.gui.image_analysis_window.ImageAnalysisWindow.read_is_clicked","title":"<code>read_is_clicked()</code>","text":"<p>Read an image (numbered using natural sorting) from the selected folder</p> <p>This method handles the logic for starting image reading when the \"Read\" button is clicked. It ensures that only one thread runs at a time, updates the UI with relevant messages, and resets visual components once processing begins.</p> Source code in <code>src/cellects/gui/image_analysis_window.py</code> <pre><code>def read_is_clicked(self):\n    \"\"\"\n    Read an image (numbered using natural sorting) from the selected folder\n\n    This method handles the logic for starting image reading when the \"Read\" button is clicked.\n    It ensures that only one thread runs at a time, updates the UI with relevant messages,\n    and resets visual components once processing begins.\n    \"\"\"\n    if not self.thread_dict[\"GetFirstIm\"].isRunning():\n        self.parent().po.vars['first_detection_frame'] = int(self.image_number.value())\n        self.message.setText(f\"Reading image n\u00b0{self.parent().po.vars['first_detection_frame']}\")\n        self.thread_dict[\"GetFirstIm\"].start()\n        self.thread_dict[\"GetFirstIm\"].message_when_thread_finished.connect(self.reinitialize_image_and_masks)\n        self.reinitialize_bio_and_back_legend()\n</code></pre>"},{"location":"api/cellects/gui/image_analysis_window/#cellects.gui.image_analysis_window.ImageAnalysisWindow.reinitialize_bio_and_back_legend","title":"<code>reinitialize_bio_and_back_legend()</code>","text":"<p>Reinitialize the bio and back legend.</p> <p>Reinitializes the bio and back legends, removing all existing lines and resetting counters for masks. This function ensures that the UI components associated with bio and back lines are correctly cleaned up.</p> Source code in <code>src/cellects/gui/image_analysis_window.py</code> <pre><code>def reinitialize_bio_and_back_legend(self):\n    \"\"\"\n    Reinitialize the bio and back legend.\n\n    Reinitializes the bio and back legends, removing all existing lines\n    and resetting counters for masks. This function ensures that the UI\n    components associated with bio and back lines are correctly cleaned up.\n    \"\"\"\n    lines_names_to_remove = []\n    for line_number, back_line_dict in self.back_lines.items():\n        line_name = u\"\\u00D7\" + \" Back\" + str(line_number)\n        self.back_added_lines_layout.removeWidget(back_line_dict[line_name])\n        back_line_dict[line_name].deleteLater()\n        lines_names_to_remove.append(line_number)\n    for line_number in lines_names_to_remove:\n        self.back_lines.pop(line_number)\n    lines_names_to_remove = []\n    for line_number, bio_line_dict in self.bio_lines.items():\n        line_name = u\"\\u00D7\" + \" Cell\" + str(line_number)\n        self.bio_added_lines_layout.removeWidget(bio_line_dict[line_name])\n        bio_line_dict[line_name].deleteLater()\n        lines_names_to_remove.append(line_number)\n    for line_number in lines_names_to_remove:\n        self.bio_lines.pop(line_number)\n    if len(self.arena_lines) &gt; 0:\n        lines_names_to_remove = []\n        for i, (line_number, arena_line_dict) in enumerate(self.arena_lines.items()):\n            line_name = u\"\\u00D7\" + \" Arena\" + str(line_number)\n            if i % 2 == 0:\n                self.bio_added_lines_layout.removeWidget(arena_line_dict[line_name])\n            else:\n                self.back_added_lines_layout.removeWidget(arena_line_dict[line_name])\n            arena_line_dict[line_name].deleteLater()\n            lines_names_to_remove.append(line_number)\n        for line_number in lines_names_to_remove:\n            self.arena_lines.pop(line_number)\n    self.bio_masks_number = 0\n    self.back_masks_number = 0\n</code></pre>"},{"location":"api/cellects/gui/image_analysis_window/#cellects.gui.image_analysis_window.ImageAnalysisWindow.reinitialize_image_and_masks","title":"<code>reinitialize_image_and_masks(image)</code>","text":"<p>Reinitialize the image and masks for analysis.</p> <p>This method reinitializes the current image and its associated masks used in the analysis process. It checks if the input image is grayscale and converts it to a 3-channel RGB image, stacking identical channels. It also updates the visibility of various UI components based on the image type and reinitializes masks to prepare for new analysis.</p> Source code in <code>src/cellects/gui/image_analysis_window.py</code> <pre><code>def reinitialize_image_and_masks(self, image: np.ndarray):\n    \"\"\"\n    Reinitialize the image and masks for analysis.\n\n    This method reinitializes the current image and its associated masks\n    used in the analysis process. It checks if the input image is grayscale\n    and converts it to a 3-channel RGB image, stacking identical channels.\n    It also updates the visibility of various UI components based on\n    the image type and reinitializes masks to prepare for new analysis.\n    \"\"\"\n    if len(image.shape) == 2:\n        self.parent().po.current_image = np.stack((image, image, image), axis=2)\n\n        self.generate_analysis_options.setVisible(False)\n        self.network_shaped.setVisible(False)\n        self.basic.setVisible(False)\n        self.select_option.setVisible(False)\n        self.select_option_label.setVisible(False)\n        self.visualize.setVisible(True)\n        self.visualize_label.setVisible(True)\n    else:\n        self.parent().po.current_image = deepcopy(image)\n    self.drawn_image = deepcopy(self.parent().po.current_image)\n    self.display_image.update_image(self.parent().po.current_image)\n    self.arena_mask = None\n    self.bio_mask = np.zeros(self.parent().po.current_image.shape[:2], dtype=np.uint16)\n    self.back_mask = np.zeros(self.parent().po.current_image.shape[:2], dtype=np.uint16)\n</code></pre>"},{"location":"api/cellects/gui/image_analysis_window/#cellects.gui.image_analysis_window.ImageAnalysisWindow.remove_line","title":"<code>remove_line()</code>","text":"<p>Remove the specified line from the image analysis display.</p> <p>This method removes a line identified by its button name from the appropriate mask and updates the layout and available names accordingly. It starts the image update thread after removing the line.</p> Source code in <code>src/cellects/gui/image_analysis_window.py</code> <pre><code>def remove_line(self):\n    \"\"\"\n    Remove the specified line from the image analysis display.\n\n    This method removes a line identified by its button name from the appropriate mask\n    and updates the layout and available names accordingly. It starts the image update thread\n    after removing the line.\n    \"\"\"\n    if not self.is_image_analysis_display_running and not self.thread_dict[\"UpdateImage\"].isRunning() and hasattr(self.sender(), 'text'):\n        pbutton_name = self.sender().text()\n        if pbutton_name[2:6] == \"Back\":\n            line_name = np.uint8(pbutton_name[6:])\n            self.back_mask[self.back_mask == line_name] = 0\n            self.back_added_lines_layout.removeWidget(self.back_lines[line_name][pbutton_name])\n            self.back_lines[line_name][pbutton_name].deleteLater()\n            self.back_lines.pop(line_name)\n            self.back_masks_number -= 1\n            self.available_back_names = np.sort(np.concatenate(([line_name], self.available_back_names)))\n        elif pbutton_name[2:6] == \"Cell\":\n            line_name = np.uint8(pbutton_name[6:])\n            self.bio_mask[self.bio_mask == line_name] = 0\n            self.bio_added_lines_layout.removeWidget(self.bio_lines[line_name][pbutton_name])\n            self.bio_lines[line_name][pbutton_name].deleteLater()\n            self.bio_lines.pop(line_name)\n            self.bio_masks_number -= 1\n            self.available_bio_names = np.sort(np.concatenate(([line_name], self.available_bio_names)))\n            self.display_more_than_two_colors_option()\n        else:\n            line_name = np.uint8(pbutton_name[7:])\n            self.arena_mask[self.arena_mask == line_name] = 0\n            if line_name % 2 == 1:\n                self.bio_added_lines_layout.removeWidget(self.arena_lines[line_name][pbutton_name])\n            else:\n                self.back_added_lines_layout.removeWidget(self.arena_lines[line_name][pbutton_name])\n            self.arena_lines[line_name][pbutton_name].deleteLater()\n            self.arena_lines.pop(line_name)\n\n            self.arena_masks_number -= 1\n            self.available_arena_names = np.sort(np.concatenate(([line_name], self.available_arena_names)))\n        self.thread_dict[\"UpdateImage\"].start()\n</code></pre>"},{"location":"api/cellects/gui/image_analysis_window/#cellects.gui.image_analysis_window.ImageAnalysisWindow.rolling_window_segmentation_option","title":"<code>rolling_window_segmentation_option()</code>","text":"<p>Set True the grid segmentation option for future image analysis.</p> Source code in <code>src/cellects/gui/image_analysis_window.py</code> <pre><code>def rolling_window_segmentation_option(self):\n    \"\"\"\n    Set True the grid segmentation option for future image analysis.\n    \"\"\"\n    self.parent().po.vars[\"rolling_window_segmentation\"]['do'] = self.rolling_window_segmentation.isChecked()\n</code></pre>"},{"location":"api/cellects/gui/image_analysis_window/#cellects.gui.image_analysis_window.ImageAnalysisWindow.run_first_image_analysis","title":"<code>run_first_image_analysis()</code>","text":"<p>Run the first image analysis.</p> <p>This method performs a series of checks and updates based on user-defined parameters before running the first image analysis. If visualization is enabled, it saves user-defined combinations and checks for empty color selection dictionaries. It then starts the thread for image analysis.</p> Notes <p>This method assumes that the parent object has already been initialized and contains all necessary variables for image analysis.</p> Source code in <code>src/cellects/gui/image_analysis_window.py</code> <pre><code>def run_first_image_analysis(self):\n    \"\"\"\n    Run the first image analysis.\n\n    This method performs a series of checks and updates based on user-defined parameters\n    before running the first image analysis. If visualization is enabled, it saves user-defined\n    combinations and checks for empty color selection dictionaries. It then starts the thread\n    for image analysis.\n\n    Notes\n    -----\n    This method assumes that the parent object has already been initialized and contains all\n    necessary variables for image analysis.\n    \"\"\"\n    if self.first_im_parameters_answered:\n        self.several_blob_per_arena_check()\n        self.horizontal_size_changed()\n        self.spot_shape_changed()\n        self.arena_shape_changed()\n\n    if self.parent().po.visualize:\n        self.save_user_defined_csc()\n        self.parent().po.vars[\"color_number\"] = int(self.distinct_colors_number.value())\n        if self.csc_dict_is_empty:\n            self.message.setText('Select non null value(s) to combine colors')\n            self.message.setStyleSheet(\"color: rgb(230, 145, 18)\")\n            self.is_image_analysis_running = False\n    if not self.parent().po.visualize or not self.csc_dict_is_empty:\n        self.parent().po.vars['convert_for_origin'] = self.csc_dict.copy()\n        self.thread_dict[\"FirstImageAnalysis\"].start()\n        self.thread_dict[\"FirstImageAnalysis\"].message_from_thread.connect(self.display_message_from_thread)\n        self.thread_dict[\"FirstImageAnalysis\"].message_when_thread_finished.connect(self.when_image_analysis_finishes)\n</code></pre>"},{"location":"api/cellects/gui/image_analysis_window/#cellects.gui.image_analysis_window.ImageAnalysisWindow.run_last_image_analysis","title":"<code>run_last_image_analysis()</code>","text":"<p>Run the last image analysis thread.</p> <p>This function updates relevant variables, saves user-defined color-space configurations (CSC), and manages thread operations for image analysis. The function does not handle any direct processing but prepares the environment by setting variables and starting threads.</p> Source code in <code>src/cellects/gui/image_analysis_window.py</code> <pre><code>def run_last_image_analysis(self):\n    \"\"\"\n    Run the last image analysis thread.\n\n    This function updates relevant variables, saves user-defined color-space configurations (CSC),\n    and manages thread operations for image analysis. The function does not handle any direct processing but\n    prepares the environment by setting variables and starting threads.\n    \"\"\"\n    self.save_user_defined_csc()\n    self.parent().po.vars[\"color_number\"] = int(self.distinct_colors_number.value())\n    if not self.csc_dict_is_empty:\n        self.parent().po.vars['convert_for_motion'] = self.csc_dict.copy()\n    if self.parent().po.visualize and self.csc_dict_is_empty:\n        self.message.setText('Select non null value(s) to combine colors')\n        self.message.setStyleSheet(\"color: rgb(230, 145, 18)\")\n    else:\n        self.thread_dict[\"LastImageAnalysis\"].start()\n        self.thread_dict[\"LastImageAnalysis\"].message_from_thread.connect(self.display_message_from_thread)\n        self.thread_dict[\"LastImageAnalysis\"].message_when_thread_finished.connect(self.when_image_analysis_finishes)\n</code></pre>"},{"location":"api/cellects/gui/image_analysis_window/#cellects.gui.image_analysis_window.ImageAnalysisWindow.save_user_defined_csc","title":"<code>save_user_defined_csc()</code>","text":"<p>Save user-defined combination of color spaces and channels.</p> Source code in <code>src/cellects/gui/image_analysis_window.py</code> <pre><code>def save_user_defined_csc(self):\n    \"\"\"\n    Save user-defined combination of color spaces and channels.\n    \"\"\"\n    self.csc_dict = {}\n    spaces = np.array((self.row1[0].currentText(), self.row2[0].currentText(), self.row3[0].currentText()))\n    channels = np.array(\n        ((self.row1[1].value(), self.row1[2].value(), self.row1[3].value()),\n         (self.row2[1].value(), self.row2[2].value(), self.row2[3].value()),\n         (self.row3[1].value(), self.row3[2].value(), self.row3[3].value()),\n         (self.row21[1].value(), self.row21[2].value(), self.row21[3].value()),\n         (self.row22[1].value(), self.row22[2].value(), self.row22[3].value()),\n         (self.row23[1].value(), self.row23[2].value(), self.row23[3].value())),\n        dtype=np.float64)\n    if self.logical_operator_between_combination_result.currentText() != 'None':\n        spaces = np.concatenate((spaces, np.array((\n                    self.row21[0].currentText() + \"2\", self.row22[0].currentText() + \"2\",\n                    self.row23[0].currentText() + \"2\"))))\n        channels = np.concatenate((channels, np.array(((self.row21[1].value(), self.row21[2].value(), self.row21[3].value()),\n         (self.row22[1].value(), self.row22[2].value(), self.row22[3].value()),\n         (self.row23[1].value(), self.row23[2].value(), self.row23[3].value())),\n         dtype=np.float64)))\n        self.csc_dict['logical'] = self.logical_operator_between_combination_result.currentText()\n    else:\n        self.csc_dict['logical'] = 'None'\n    if not np.all(spaces == \"None\"):\n        for i, space in enumerate(spaces):\n            if space != \"None\" and space != \"None2\":\n                self.csc_dict[space] = channels[i, :]\n    if not 'PCA' in self.csc_dict and (len(self.csc_dict) == 1 or np.absolute(channels).sum() == 0):\n        self.csc_dict_is_empty = True\n    else:\n        self.csc_dict_is_empty = False\n</code></pre>"},{"location":"api/cellects/gui/image_analysis_window/#cellects.gui.image_analysis_window.ImageAnalysisWindow.scale_with_changed","title":"<code>scale_with_changed()</code>","text":"<p>Modifies how the image scale is computed: using the image width or the blob unitary size (horizontal diameter).</p> Source code in <code>src/cellects/gui/image_analysis_window.py</code> <pre><code>def scale_with_changed(self):\n    \"\"\"\n    Modifies how the image scale is computed: using the image width or the blob unitary size (horizontal diameter).\n    \"\"\"\n    self.parent().po.all['scale_with_image_or_cells'] = self.scale_with.currentIndex()\n    if self.parent().po.all['scale_with_image_or_cells'] == 0:\n        self.horizontal_size.setValue(self.parent().po.all['image_horizontal_size_in_mm'])\n    else:\n        self.horizontal_size.setValue(self.parent().po.all['starting_blob_hsize_in_mm'])\n</code></pre>"},{"location":"api/cellects/gui/image_analysis_window/#cellects.gui.image_analysis_window.ImageAnalysisWindow.set_spot_shape_check","title":"<code>set_spot_shape_check()</code>","text":"<p>Set the spot shape setting visibility.</p> Source code in <code>src/cellects/gui/image_analysis_window.py</code> <pre><code>def set_spot_shape_check(self):\n    \"\"\"\n    Set the spot shape setting visibility.\n    \"\"\"\n    is_checked = self.set_spot_shape.isChecked()\n    self.spot_shape.setVisible(is_checked)\n    self.parent().po.all['set_spot_shape'] = is_checked\n    if not is_checked:\n        self.parent().po.all['starting_blob_shape'] = None\n</code></pre>"},{"location":"api/cellects/gui/image_analysis_window/#cellects.gui.image_analysis_window.ImageAnalysisWindow.set_spot_size_check","title":"<code>set_spot_size_check()</code>","text":"<p>Set the visibility of spot size based on checkbox state.</p> Source code in <code>src/cellects/gui/image_analysis_window.py</code> <pre><code>def set_spot_size_check(self):\n    \"\"\"\n    Set the visibility of spot size based on checkbox state.\n    \"\"\"\n    is_checked = self.set_spot_size.isChecked()\n    if self.step == 1:\n        self.spot_size.setVisible(is_checked)\n    self.parent().po.all['set_spot_size'] = is_checked\n</code></pre>"},{"location":"api/cellects/gui/image_analysis_window/#cellects.gui.image_analysis_window.ImageAnalysisWindow.several_blob_per_arena_check","title":"<code>several_blob_per_arena_check()</code>","text":"<p>Checks or unchecks the option for having several blobs per arena.</p> Source code in <code>src/cellects/gui/image_analysis_window.py</code> <pre><code>def several_blob_per_arena_check(self):\n    \"\"\"\n    Checks or unchecks the option for having several blobs per arena.\n    \"\"\"\n    is_checked = self.one_blob_per_arena.isChecked()\n    self.parent().po.vars['several_blob_per_arena'] = not is_checked\n    self.set_spot_size.setVisible(is_checked)\n    self.spot_size_label.setVisible(is_checked)\n    self.spot_size.setVisible(is_checked and self.set_spot_size.isChecked())\n</code></pre>"},{"location":"api/cellects/gui/image_analysis_window/#cellects.gui.image_analysis_window.ImageAnalysisWindow.slower_delineation","title":"<code>slower_delineation()</code>","text":"<p>Perform slower delineation process and clear the decision label.</p> <p>Execute a sequence of operations that prepare for a slower delineation process.</p> Source code in <code>src/cellects/gui/image_analysis_window.py</code> <pre><code>def slower_delineation(self):\n    \"\"\"\n    Perform slower delineation process and clear the decision label.\n\n    Execute a sequence of operations that prepare for a slower\n    delineation process.\n    \"\"\"\n    self.decision_label.setText(f\"\")\n    self.arena_shape.setVisible(False)\n    self.arena_shape_label.setVisible(False)\n    # Save the current mask, its stats, remove useless memory and start delineation\n    self.parent().po.first_image.update_current_images(self.parent().po.current_combination_id)\n    self.parent().po.all['are_gravity_centers_moving'] = 1\n    self.start_crop_scale_subtract_delineate()\n</code></pre>"},{"location":"api/cellects/gui/image_analysis_window/#cellects.gui.image_analysis_window.ImageAnalysisWindow.spot_shape_changed","title":"<code>spot_shape_changed()</code>","text":"<p>Save the user selection of shape.</p> Source code in <code>src/cellects/gui/image_analysis_window.py</code> <pre><code>def spot_shape_changed(self):\n    \"\"\"\n    Save the user selection of shape.\n    \"\"\"\n    self.parent().po.all['starting_blob_shape'] = self.spot_shape.currentText()\n    self.set_spot_shape_check()\n</code></pre>"},{"location":"api/cellects/gui/image_analysis_window/#cellects.gui.image_analysis_window.ImageAnalysisWindow.spot_size_changed","title":"<code>spot_size_changed()</code>","text":"<p>Update the starting blob size and corresponding horizontal size based on user input.</p> Source code in <code>src/cellects/gui/image_analysis_window.py</code> <pre><code>def spot_size_changed(self):\n    \"\"\"\n    Update the starting blob size and corresponding horizontal size based on user input.\n    \"\"\"\n    self.parent().po.all['starting_blob_hsize_in_mm'] = self.spot_size.value()\n    if self.parent().po.all['scale_with_image_or_cells'] == 1:\n        self.horizontal_size.setValue(self.parent().po.all['starting_blob_hsize_in_mm'])\n    self.set_spot_size_check()\n</code></pre>"},{"location":"api/cellects/gui/image_analysis_window/#cellects.gui.image_analysis_window.ImageAnalysisWindow.start_crop_scale_subtract_delineate","title":"<code>start_crop_scale_subtract_delineate()</code>","text":"<p>Start the crop, scale, subtract, and delineate process.</p> Extended Description <p>This function initiates a background thread to perform the crop, scale, subtract, and delineate operations on the image. It also updates the UI elements to reflect the ongoing process.</p> Source code in <code>src/cellects/gui/image_analysis_window.py</code> <pre><code>def start_crop_scale_subtract_delineate(self):\n    \"\"\"\n    Start the crop, scale, subtract, and delineate process.\n\n    Extended Description\n    --------------------\n    This function initiates a background thread to perform the crop, scale,\n    subtract, and delineate operations on the image. It also updates the\n    UI elements to reflect the ongoing process.\n    \"\"\"\n    if not self.thread_dict['CropScaleSubtractDelineate'].isRunning():\n        self.message.setText(\"Looking for each arena contour, wait...\")\n        self.thread_dict['CropScaleSubtractDelineate'].start()\n        self.thread_dict['CropScaleSubtractDelineate'].message_from_thread.connect(self.display_message_from_thread)\n        self.thread_dict['CropScaleSubtractDelineate'].message_when_thread_finished.connect(self.delineate_is_done)\n\n        self.yes.setVisible(False)\n        self.no.setVisible(False)\n        self.reinitialize_bio_and_back_legend()\n        self.user_drawn_lines_label.setVisible(False)\n        self.cell.setVisible(False)\n        self.background.setVisible(False)\n        self.one_blob_per_arena.setVisible(False)\n        self.one_blob_per_arena_label.setVisible(False)\n        self.set_spot_shape.setVisible(False)\n        self.spot_shape.setVisible(False)\n        self.spot_shape_label.setVisible(False)\n        self.set_spot_size.setVisible(False)\n        self.spot_size.setVisible(False)\n        self.spot_size_label.setVisible(False)\n        self.advanced_mode_cb.setChecked(False)\n        self.advanced_mode_cb.setVisible(False)\n        self.advanced_mode_label.setVisible(False)\n        self.generate_analysis_options.setVisible(False)\n        self.network_shaped.setVisible(False)\n        self.basic.setVisible(False)\n        self.visualize.setVisible(False)\n        self.visualize_label.setVisible(False)\n        self.select_option.setVisible(False)\n        self.select_option_label.setVisible(False)\n</code></pre>"},{"location":"api/cellects/gui/image_analysis_window/#cellects.gui.image_analysis_window.ImageAnalysisWindow.start_last_image","title":"<code>start_last_image()</code>","text":"<p>Start the process of analyzing the last image in the time-lapse or the video.</p> <p>This method initializes various UI elements, retrieves the last image, waits for any running threads to complete, processes the image without considering it as the first image, and updates the visualization.</p> Source code in <code>src/cellects/gui/image_analysis_window.py</code> <pre><code>def start_last_image(self):\n    \"\"\"\n    Start the process of analyzing the last image in the time-lapse or the video.\n\n    This method initializes various UI elements, retrieves the last image,\n    waits for any running threads to complete, processes the image without\n    considering it as the first image, and updates the visualization.\n    \"\"\"\n    self.is_first_image_flag = False\n    self.decision_label.setText('')\n    self.yes.setVisible(False)\n    self.no.setVisible(False)\n    self.spot_size.setVisible(False)\n    self.starting_differs_from_growing_cb.setVisible(False)\n    self.starting_differs_from_growing_label.setVisible(False)\n    self.message.setText('Gathering data and visualizing last image analysis result')\n    self.parent().po.get_last_image()\n    if self.thread_dict['SaveManualDelineation'].isRunning():\n        self.thread_dict['SaveManualDelineation'].wait()\n    self.parent().po.cropping(is_first_image=False)\n    self.reinitialize_image_and_masks(self.parent().po.last_image.bgr)\n    self.reinitialize_bio_and_back_legend()\n    self.parent().po.current_combination_id = 0\n    self.visualize_is_clicked()\n    self.user_drawn_lines_label.setText('Select and draw')\n    self.user_drawn_lines_label.setVisible(True)\n    self.cell.setVisible(True)\n    self.background.setVisible(True)\n    self.advanced_mode_cb.setVisible(True)\n    self.advanced_mode_label.setVisible(True)\n    self.visualize_label.setVisible(True)\n    self.visualize.setVisible(True)\n    self.row1_widget.setVisible(False)\n</code></pre>"},{"location":"api/cellects/gui/image_analysis_window/#cellects.gui.image_analysis_window.ImageAnalysisWindow.starting_differs_from_growing_check","title":"<code>starting_differs_from_growing_check()</code>","text":"<p>Set the <code>origin_state</code> variable based on checkbox state and frame detection.</p> Source code in <code>src/cellects/gui/image_analysis_window.py</code> <pre><code>def starting_differs_from_growing_check(self):\n    \"\"\"\n    Set the `origin_state` variable based on checkbox state and frame detection.\n    \"\"\"\n    if self.parent().po.vars['first_detection_frame'] &gt; 1:\n        self.parent().po.vars['origin_state'] = 'invisible'\n    else:\n        if self.starting_differs_from_growing_cb.isChecked():\n            self.parent().po.vars['origin_state'] = 'constant'\n        else:\n            self.parent().po.vars['origin_state'] = 'fluctuating'\n</code></pre>"},{"location":"api/cellects/gui/image_analysis_window/#cellects.gui.image_analysis_window.ImageAnalysisWindow.true_init","title":"<code>true_init()</code>","text":"<p>Initialize the ImageAnalysisWindow class with default settings and UI components.</p> <p>This function sets up the initial state of the ImageAnalysisWindow, including various flags, labels, input fields, and layout configurations. It also initializes the display image and connects UI elements to their respective event handlers.</p> Notes <p>This method assumes that the parent widget has a 'po' attribute with specific settings and variables.</p> Source code in <code>src/cellects/gui/image_analysis_window.py</code> <pre><code>def true_init(self):\n    \"\"\"\n    Initialize the ImageAnalysisWindow class with default settings and UI components.\n\n    This function sets up the initial state of the ImageAnalysisWindow, including various flags,\n    labels, input fields, and layout configurations. It also initializes the display image\n    and connects UI elements to their respective event handlers.\n\n    Notes\n    -----\n    This method assumes that the parent widget has a 'po' attribute with specific settings and variables.\n    \"\"\"\n    logging.info(\"Initialize ImageAnalysisWindow\")\n    self.data_tab.set_not_in_use()\n    self.image_tab.set_in_use()\n    self.video_tab.set_not_usable()\n    self.hold_click_flag: bool = False\n    self.is_first_image_flag: bool = True\n    self.is_image_analysis_running: bool = False\n    self.is_image_analysis_display_running: bool = False\n    self.asking_first_im_parameters_flag: bool = True\n    self.first_im_parameters_answered: bool = False\n    self.auto_delineation_flag: bool = False\n    self.delineation_done: bool = False\n    self.asking_delineation_flag: bool = False\n    self.asking_slower_or_manual_delineation_flag: bool = False\n    self.slower_delineation_flag: bool = False\n    self.asking_last_image_flag: bool = False\n    self.step = 0\n    self.temporary_mask_coord = []\n    self.saved_coord = []\n    self.back1_bio2 = 0\n    self.bio_masks_number = 0\n    self.back_masks_number = 0\n    self.arena_masks_number = 0\n    self.available_bio_names = np.arange(1, 1000, dtype=np.uint16)\n    self.available_back_names = np.arange(1, 1000, dtype=np.uint16)\n    self.parent().po.current_combination_id = 0\n\n    self.display_image = np.zeros((self.parent().im_max_width, self.parent().im_max_width, 3), np.uint8)\n    self.display_image = InsertImage(self.display_image, self.parent().im_max_height, self.parent().im_max_width)\n    self.display_image.mousePressEvent = self.get_click_coordinates\n    self.display_image.mouseMoveEvent = self.get_mouse_move_coordinates\n    self.display_image.mouseReleaseEvent = self.get_mouse_release_coordinates\n\n    ## Title\n    self.image_number_label = FixedText(IAW[\"Image_number\"][\"label\"],\n                                        tip=IAW[\"Image_number\"][\"tips\"],\n                                        night_mode=self.parent().po.all['night_mode'])\n    self.image_number_label.setAlignment(QtCore.Qt.AlignVCenter)\n    self.image_number = Spinbox(min=0, max=self.parent().po.vars['img_number'] - 1, val=self.parent().po.vars['first_detection_frame'], night_mode=self.parent().po.all['night_mode'])\n    self.read = PButton(\"Read\", night_mode=self.parent().po.all['night_mode'])\n    self.read.clicked.connect(self.read_is_clicked)\n    if self.parent().po.all[\"im_or_vid\"] == 0 and len(self.parent().po.data_list) == 1:\n        # If there is only one image in the folder\n        self.image_number.setVisible(False)\n        self.image_number_label.setVisible(False)\n        self.read.setVisible(False)\n\n    self.one_blob_per_arena = Checkbox(not self.parent().po.vars['several_blob_per_arena'])\n    self.one_blob_per_arena.stateChanged.connect(self.several_blob_per_arena_check)\n    self.one_blob_per_arena_label = FixedText(IAW[\"several_blob_per_arena\"][\"label\"], valign=\"c\",\n                                              tip=IAW[\"several_blob_per_arena\"][\"tips\"],\n                                              night_mode=self.parent().po.all['night_mode'])\n\n\n    self.scale_with_label = FixedText(IAW[\"Scale_with\"][\"label\"] + ':', valign=\"c\",\n                                    tip=IAW[\"Scale_with\"][\"tips\"],\n                                    night_mode=self.parent().po.all['night_mode'])\n    self.scale_with = Combobox([\"Image horizontal size\", \"Cell(s) horizontal size\"], night_mode=self.parent().po.all['night_mode'])\n    self.scale_with.setFixedWidth(280)\n    self.scale_with.setCurrentIndex(self.parent().po.all['scale_with_image_or_cells'])\n    self.scale_size_label = FixedText(IAW[\"Scale_size\"][\"label\"] + ':', valign=\"c\",\n                                      tip=IAW[\"Scale_size\"][\"tips\"],\n                                      night_mode=self.parent().po.all['night_mode'])\n    if self.parent().po.all['scale_with_image_or_cells'] == 0:\n        self.horizontal_size = Spinbox(min=0, max=100000,\n                                    val=self.parent().po.all['image_horizontal_size_in_mm'],\n                                    night_mode=self.parent().po.all['night_mode'])\n    else:\n        self.horizontal_size = Spinbox(min=0, max=100000,\n                                    val=self.parent().po.all['starting_blob_hsize_in_mm'],\n                                    night_mode=self.parent().po.all['night_mode'])\n    self.horizontal_size.valueChanged.connect(self.horizontal_size_changed)\n    self.scale_with.currentTextChanged.connect(self.scale_with_changed)\n    self.scale_unit_label = FixedText(' mm', night_mode=self.parent().po.all['night_mode'])\n\n    # 1) Open the first row layout\n    self.row1_widget = QtWidgets.QWidget()\n    self.row1_layout = QtWidgets.QHBoxLayout()\n    self.row1_layout.addWidget(self.image_number_label)\n    self.row1_layout.addWidget(self.image_number)\n    self.row1_layout.addWidget(self.read)\n    self.row1_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n    self.row1_layout.addWidget(self.one_blob_per_arena_label)\n    self.row1_layout.addWidget(self.one_blob_per_arena)\n    self.row1_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n    self.row1_layout.addWidget(self.scale_with_label)\n    self.row1_layout.addWidget(self.scale_with)\n    self.row1_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n    self.row1_layout.addWidget(self.scale_size_label)\n    self.row1_layout.addWidget(self.horizontal_size)\n\n    self.row1_widget.setLayout(self.row1_layout)\n    self.Vlayout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.Maximum, QtWidgets.QSizePolicy.MinimumExpanding))\n    self.Vlayout.addWidget(self.row1_widget)\n    self.Vlayout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.Maximum, QtWidgets.QSizePolicy.MinimumExpanding))\n\n    # 2) Open the central row layout\n    self.central_row_widget = QtWidgets.QWidget()\n    self.central_row_layout = QtWidgets.QGridLayout()\n\n    # it will contain a) the user drawn lines, b) the image, c) the csc\n    # 2)a) the user drawn lines\n    self.user_drawn_lines_widget = QtWidgets.QWidget()\n    self.user_drawn_lines_layout = QtWidgets.QVBoxLayout()\n    self.user_drawn_lines_label = FixedText(IAW[\"Select_and_draw\"][\"label\"] + \":\",\n                                            tip=IAW[\"Select_and_draw\"][\"tips\"],\n                                            night_mode=self.parent().po.all['night_mode'])\n    self.user_drawn_lines_label.setAlignment(QtCore.Qt.AlignHCenter)\n    self.user_drawn_lines_layout.addWidget(self.user_drawn_lines_label)\n    self.pbuttons_widget = QtWidgets.QWidget()\n    self.pbuttons_layout = QtWidgets.QHBoxLayout()\n    self.cell = PButton(\"Cell\", False, tip=IAW[\"Draw_buttons\"][\"tips\"],\n                        night_mode=self.parent().po.all['night_mode'])\n    self.cell.setFixedWidth(150)\n    self.background = PButton(\"Back\", False, tip=IAW[\"Draw_buttons\"][\"tips\"],\n                              night_mode=self.parent().po.all['night_mode'])\n    self.background.setFixedWidth(150)\n    self.cell.clicked.connect(self.cell_is_clicked)\n    self.background.clicked.connect(self.background_is_clicked)\n    self.pbuttons_layout.addWidget(self.cell)\n    self.pbuttons_layout.addWidget(self.background)\n    self.pbuttons_widget.setLayout(self.pbuttons_layout)\n    self.user_drawn_lines_layout.addWidget(self.pbuttons_widget)\n\n    self.pbuttons_tables_widget = QtWidgets.QWidget()\n    self.pbuttons_tables_layout = QtWidgets.QHBoxLayout()\n    self.pbuttons_tables_layout.setAlignment(QtCore.Qt.AlignHCenter)\n    self.bio_pbuttons_table = QtWidgets.QScrollArea()#QTableWidget()  # Scroll Area which contains the widgets, set as the centralWidget\n    self.bio_pbuttons_table.setSizePolicy(QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.MinimumExpanding)\n    self.bio_pbuttons_table.setMinimumHeight(self.parent().im_max_height // 2)\n    self.bio_pbuttons_table.setFrameShape(QtWidgets.QFrame.NoFrame)\n    self.bio_pbuttons_table.setVerticalScrollBarPolicy(QtCore.Qt.ScrollBarAsNeeded)\n    self.back_pbuttons_table = QtWidgets.QScrollArea()#QTableWidget()  # Scroll Area which contains the widgets, set as the centralWidget\n    self.back_pbuttons_table.setMinimumHeight(self.parent().im_max_height // 2)\n    self.back_pbuttons_table.setFrameShape(QtWidgets.QFrame.NoFrame)\n    self.back_pbuttons_table.setSizePolicy(QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.MinimumExpanding)\n    self.back_pbuttons_table.setVerticalScrollBarPolicy(QtCore.Qt.ScrollBarAsNeeded)\n\n    self.bio_added_lines_widget = QtWidgets.QWidget()\n    self.back_added_lines_widget = QtWidgets.QWidget()\n    self.bio_added_lines_layout = QtWidgets.QVBoxLayout()\n    self.back_added_lines_layout = QtWidgets.QVBoxLayout()\n    self.back_added_lines_widget.setLayout(self.back_added_lines_layout)\n    self.bio_added_lines_widget.setLayout(self.bio_added_lines_layout)\n    self.bio_pbuttons_table.setWidget(self.bio_added_lines_widget)\n    self.back_pbuttons_table.setWidget(self.back_added_lines_widget)\n    self.bio_pbuttons_table.setWidgetResizable(True)\n    self.back_pbuttons_table.setWidgetResizable(True)\n\n    self.pbuttons_tables_layout.addWidget(self.bio_pbuttons_table)\n    self.pbuttons_tables_layout.addWidget(self.back_pbuttons_table)\n    self.pbuttons_tables_widget.setLayout(self.pbuttons_tables_layout)\n    self.user_drawn_lines_layout.addWidget(self.pbuttons_tables_widget)\n\n    # # Dynamically add the lines\n    self.bio_lines = {}\n    self.back_lines = {}\n    self.arena_lines = {}\n\n    self.user_drawn_lines_widget.setLayout(self.user_drawn_lines_layout)\n    self.user_drawn_lines_widget.setSizePolicy(QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.MinimumExpanding)\n    self.central_row_layout.addWidget(self.user_drawn_lines_widget, 0, 0)\n\n    # 2)b) the image\n    self.central_row_layout.addWidget(self.display_image, 0, 1)\n\n    # Need to create this before self.generate_csc_editing()\n    self.message = FixedText(\"\", halign=\"r\", night_mode=self.parent().po.all['night_mode'])\n    self.message.setStyleSheet(\"color: rgb(230, 145, 18)\")\n\n    # 2)c) The csc editing\n    self.generate_csc_editing()\n\n    self.central_row_layout.addWidget(self.central_right_widget, 0, 2)\n    self.central_row_layout.setAlignment(QtCore.Qt.AlignLeft)\n    self.central_row_layout.setAlignment(QtCore.Qt.AlignHCenter)\n    # 2) Close the central row layout\n    self.central_row_widget.setLayout(self.central_row_layout)\n    self.Vlayout.addWidget(self.central_row_widget)\n    self.Vlayout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.Maximum, QtWidgets.QSizePolicy.MinimumExpanding))\n\n    # 3) Add Set supplementary parameters row 1\n    self.sup_param_row1_widget = QtWidgets.QWidget()\n    self.sup_param_row1_layout = QtWidgets.QHBoxLayout()\n\n    # 4) Add Set supplementary parameters row2\n    self.sup_param_row2_widget = QtWidgets.QWidget()\n    self.sup_param_row2_layout = QtWidgets.QHBoxLayout()\n\n    self.arena_shape_label = FixedText(IAW[\"Arena_shape\"][\"label\"], tip=IAW[\"Arena_shape\"][\"tips\"],\n                                       night_mode=self.parent().po.all['night_mode'])\n    self.arena_shape = Combobox(['circle', 'rectangle'], night_mode=self.parent().po.all['night_mode'])\n    self.arena_shape.setFixedWidth(160)\n    self.arena_shape.setCurrentText(self.parent().po.vars['arena_shape'])\n    self.arena_shape.currentTextChanged.connect(self.arena_shape_changed)\n    self.set_spot_shape = Checkbox(self.parent().po.all['set_spot_shape'])\n    self.set_spot_shape.stateChanged.connect(self.set_spot_shape_check)\n    self.spot_shape_label = FixedText(IAW[\"Spot_shape\"][\"label\"], tip=IAW[\"Spot_shape\"][\"tips\"], night_mode=self.parent().po.all['night_mode'])\n    self.spot_shape = Combobox(['circle', 'rectangle'], night_mode=self.parent().po.all['night_mode'])\n    self.spot_shape.setFixedWidth(160)\n    if self.parent().po.all['starting_blob_shape'] is None:\n        self.spot_shape.setCurrentIndex(0)\n    else:\n        self.spot_shape.setCurrentText(self.parent().po.all['starting_blob_shape'])\n    self.spot_shape.currentTextChanged.connect(self.spot_shape_changed)\n    self.set_spot_size = Checkbox(self.parent().po.all['set_spot_size'])\n    self.set_spot_size.stateChanged.connect(self.set_spot_size_check)\n    self.spot_size_label = FixedText(IAW[\"Spot_size\"][\"label\"], tip=IAW[\"Spot_size\"][\"tips\"],\n                                     night_mode=self.parent().po.all['night_mode'])\n    self.spot_size = Spinbox(min=0, max=100000, val=self.parent().po.all['starting_blob_hsize_in_mm'], decimals=2,\n                             night_mode=self.parent().po.all['night_mode'])\n    self.spot_size.valueChanged.connect(self.spot_size_changed)\n    self.sup_param_row2_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n    self.sup_param_row2_layout.addWidget(self.arena_shape_label)\n    self.sup_param_row2_layout.addWidget(self.arena_shape)\n    self.sup_param_row2_layout.addWidget(self.set_spot_shape)\n    self.sup_param_row2_layout.addWidget(self.spot_shape_label)\n    self.sup_param_row2_layout.addWidget(self.spot_shape)\n    self.sup_param_row2_layout.addWidget(self.set_spot_size)\n    self.sup_param_row2_layout.addWidget(self.spot_size_label)\n    self.sup_param_row2_layout.addWidget(self.spot_size)\n    self.sup_param_row2_widget.setLayout(self.sup_param_row2_layout)\n    self.sup_param_row2_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n    self.Vlayout.addWidget(self.sup_param_row2_widget)\n\n    self.one_blob_per_arena.setVisible(True)\n    self.one_blob_per_arena_label.setVisible(True)\n    self.set_spot_shape.setVisible(False)\n    self.spot_shape_label.setVisible(False)\n    self.spot_shape.setVisible(False)\n    self.arena_shape_label.setVisible(False)\n    self.arena_shape.setVisible(False)\n    self.set_spot_size.setVisible(False)\n    self.spot_size_label.setVisible(False)\n    self.spot_size.setVisible(False)\n\n    # 5) Add the generate option row\n    self.generate_analysis_options = FixedText(IAW[\"Generate_analysis_options\"][\"label\"] + \": \",\n                                               tip=IAW[\"Generate_analysis_options\"][\"tips\"],\n                                               night_mode=self.parent().po.all['night_mode'])\n    self.basic = PButton(\"Basic\", night_mode=self.parent().po.all['night_mode'])\n    self.basic.clicked.connect(self.basic_is_clicked)\n    self.network_shaped = PButton(\"Network-shaped\", night_mode=self.parent().po.all['night_mode'])\n    self.network_shaped.clicked.connect(self.network_shaped_is_clicked)\n    self.network_shaped.setVisible(False)\n    self.visualize = PButton('Apply current config', night_mode=self.parent().po.all['night_mode'])\n    self.visualize.clicked.connect(self.visualize_is_clicked)\n    if self.parent().po.vars['already_greyscale']:\n        self.visualize_label = FixedText(\"Directly: \", night_mode=self.parent().po.all['night_mode'])\n    else:\n        self.visualize_label = FixedText(\"Or directly: \", night_mode=self.parent().po.all['night_mode'])\n\n    self.sup_param_row1_layout.addWidget(self.generate_analysis_options)\n    self.sup_param_row1_layout.addWidget(self.basic)\n    self.sup_param_row1_layout.addWidget(self.network_shaped)\n    self.sup_param_row1_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n    self.sup_param_row1_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n    self.sup_param_row1_layout.addWidget(self.visualize_label)\n    self.sup_param_row1_layout.addWidget(self.visualize)\n\n    self.sup_param_row1_widget.setLayout(self.sup_param_row1_layout)\n    self.Vlayout.addWidget(self.sup_param_row1_widget)\n\n    # 6) Open the choose best option row layout\n    self.options_row_widget = QtWidgets.QWidget()\n    self.options_row_layout = QtWidgets.QHBoxLayout()\n    self.select_option_label = FixedText(IAW[\"Select_option_to_read\"][\"label\"],\n                                         tip=IAW[\"Select_option_to_read\"][\"tips\"],\n                                         night_mode=self.parent().po.all['night_mode'])\n    self.select_option = Combobox([], night_mode=self.parent().po.all['night_mode'])\n    if self.parent().po.vars['color_number'] == 2:\n        self.select_option.setCurrentIndex(self.parent().po.all['video_option'])\n    self.select_option.setSizePolicy(QtWidgets.QSizePolicy.Maximum, QtWidgets.QSizePolicy.Maximum)\n    self.select_option.setMinimumWidth(145)\n    self.select_option.currentTextChanged.connect(self.option_changed)\n    self.n_shapes_detected = FixedText(f'', night_mode=self.parent().po.all['night_mode'])\n    self.select_option_label.setVisible(False)\n    self.select_option.setVisible(False)\n    self.n_shapes_detected.setVisible(False)\n    self.options_row_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n    self.options_row_layout.addWidget(self.select_option_label)\n    self.options_row_layout.addWidget(self.select_option)\n    self.options_row_layout.addWidget(self.n_shapes_detected)\n    self.options_row_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n    self.options_row_widget.setLayout(self.options_row_layout)\n    self.Vlayout.addWidget(self.options_row_widget)\n\n    # 7) Open decision row layout\n    self.decision_row_widget = QtWidgets.QWidget()\n    self.decision_row_layout = QtWidgets.QHBoxLayout()\n    self.decision_label = FixedText(\"\", night_mode=self.parent().po.all['night_mode'])\n    self.yes = PButton(\"Yes\", night_mode=self.parent().po.all['night_mode'])\n    self.yes.clicked.connect(self.when_yes_is_clicked)\n    self.no = PButton(\"No\", night_mode=self.parent().po.all['night_mode'])\n    self.no.clicked.connect(self.when_no_is_clicked)\n\n    self.decision_label.setVisible(False)\n    self.yes.setVisible(False)\n    self.no.setVisible(False)\n    self.decision_row_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n    self.decision_row_layout.addWidget(self.decision_label)\n    self.decision_row_layout.addWidget(self.yes)\n    self.decision_row_layout.addWidget(self.no)\n    self.decision_row_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n    self.decision_row_widget.setLayout(self.decision_row_layout)\n    self.Vlayout.addWidget(self.decision_row_widget)\n\n    # 8) Open the special cases layout\n    self.special_cases_widget = QtWidgets.QWidget()\n    self.special_cases_layout = QtWidgets.QHBoxLayout()\n    self.starting_differs_from_growing_cb = Checkbox(self.parent().po.vars['origin_state'] == 'constant')\n    self.starting_differs_from_growing_cb.stateChanged.connect(self.starting_differs_from_growing_check)\n    self.starting_differs_from_growing_label = FixedText(IAW[\"Start_differs_from_arena\"][\"label\"],\n                                                         tip=IAW[\"Start_differs_from_arena\"][\"tips\"],\n                                                         night_mode=self.parent().po.all['night_mode'])\n    self.starting_differs_from_growing_cb.setVisible(False)\n    self.starting_differs_from_growing_label.setVisible(False)\n    self.special_cases_layout.addWidget(self.starting_differs_from_growing_cb)\n    self.special_cases_layout.addWidget(self.starting_differs_from_growing_label)\n    self.special_cases_widget.setLayout(self.special_cases_layout)\n    self.Vlayout.addWidget(self.special_cases_widget)\n\n    # 9) Open the last row layout\n    self.last_row_widget = QtWidgets.QWidget()\n    self.last_row_layout = QtWidgets.QHBoxLayout()\n    self.previous = PButton('Previous', night_mode=self.parent().po.all['night_mode'])\n    self.previous.clicked.connect(self.previous_is_clicked)\n    self.data_tab.clicked.connect(self.data_is_clicked)\n    self.video_tab.clicked.connect(self.video_is_clicked)\n    self.complete_image_analysis = PButton(IAW[\"Save_image_analysis\"][\"label\"],\n                                           tip=IAW[\"Save_image_analysis\"][\"tips\"],\n                                           night_mode=self.parent().po.all['night_mode'])\n    self.complete_image_analysis.setVisible(False)\n    self.complete_image_analysis.clicked.connect(self.complete_image_analysis_is_clicked)\n    self.next = PButton(\"Next\", night_mode=self.parent().po.all['night_mode'])\n    self.next.setVisible(False)\n    self.next.clicked.connect(self.go_to_next_widget)\n    self.last_row_layout.addWidget(self.previous)\n    self.last_row_layout.addWidget(self.message)\n    self.last_row_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n    self.last_row_layout.addWidget(self.complete_image_analysis)\n    self.last_row_layout.addWidget(self.next)\n    self.last_row_widget.setLayout(self.last_row_layout)\n    self.Vlayout.addWidget(self.last_row_widget)\n    self.Vlayout.setSpacing(0)\n    self.setLayout(self.Vlayout)\n\n    self.advanced_mode_check()\n\n    self.thread_dict = {}\n    self.thread_dict[\"GetFirstIm\"] = GetFirstImThread(self.parent())\n    self.reinitialize_image_and_masks(self.parent().po.first_im)\n    self.thread_dict[\"GetLastIm\"] = GetLastImThread(self.parent())\n    if self.parent().po.all['im_or_vid'] == 0:\n        self.thread_dict[\"GetLastIm\"].start()\n    self.parent().po.first_image = OneImageAnalysis(self.parent().po.first_im)\n    self.thread_dict[\"FirstImageAnalysis\"] = FirstImageAnalysisThread(self.parent())\n    self.thread_dict[\"LastImageAnalysis\"] = LastImageAnalysisThread(self.parent())\n    self.thread_dict['UpdateImage'] = UpdateImageThread(self.parent())\n    self.thread_dict['CropScaleSubtractDelineate'] = CropScaleSubtractDelineateThread(self.parent())\n    self.thread_dict['SaveManualDelineation'] = SaveManualDelineationThread(self.parent())\n    self.thread_dict['CompleteImageAnalysisThread'] = CompleteImageAnalysisThread(self.parent())\n    self.thread_dict['PrepareVideoAnalysis'] = PrepareVideoAnalysisThread(self.parent())\n</code></pre>"},{"location":"api/cellects/gui/image_analysis_window/#cellects.gui.image_analysis_window.ImageAnalysisWindow.update_csc_editing_display","title":"<code>update_csc_editing_display()</code>","text":"<p>Update the color space conversion (CSC) editing display.</p> <p>This method updates the visibility and values of UI elements related to color space conversions based on the current state of <code>self.csc_dict</code>. It handles the display logic for different color spaces and their combinations, ensuring that the UI reflects the current configuration accurately.</p> Source code in <code>src/cellects/gui/image_analysis_window.py</code> <pre><code>def update_csc_editing_display(self):\n    \"\"\"\n    Update the color space conversion (CSC) editing display.\n\n    This method updates the visibility and values of UI elements related to color\n    space conversions based on the current state of `self.csc_dict`. It handles\n    the display logic for different color spaces and their combinations, ensuring\n    that the UI reflects the current configuration accurately.\n    \"\"\"\n    remaining_c_spaces = []\n    row_number1 = 0\n    row_number2 = 0\n    if \"PCA\" in self.csc_dict.keys():\n        self.row1[0].setCurrentIndex(0)\n        for i in range(1, 4):\n            self.row1[i].setVisible(False)\n    else:\n        c_space_order = [\"PCA\", \"bgr\", \"hsv\", \"hls\", \"lab\", \"luv\", \"yuv\"]\n        for i, (k, v) in enumerate(self.csc_dict.items()):\n            if k != \"logical\":\n                if k[-1] != \"2\":\n                    if row_number1 == 0:\n                        row_to_change = self.row1\n                    elif row_number1 == 1:\n                        row_to_change = self.row2\n                    elif row_number1 == 2:\n                        row_to_change = self.row3\n                    else:\n                        remaining_c_spaces.append(k + \" \" + str(v))\n                    row_number1 += 1\n                    current_row_number = row_number1\n                else:\n                    if row_number2 == 0:\n                        row_to_change = self.row21\n                    elif row_number2 == 1:\n                        row_to_change = self.row22\n                    elif row_number2 == 2:\n                        row_to_change = self.row23\n                    else:\n                        remaining_c_spaces.append(k + \" \" + str(v))\n                    row_number2 += 1\n                    current_row_number = row_number2\n                    k = k[:-1]\n                if current_row_number &lt;= 3:\n                    row_to_change[0].setCurrentIndex(np.nonzero(np.isin(c_space_order, k))[0][0])\n                    row_to_change[0].setVisible(self.parent().po.all['expert_mode'])\n                    for i1, i2 in zip([1, 2, 3], [0, 1, 2]):\n                        row_to_change[i1].setValue(v[i2])\n                        row_to_change[i1].setVisible(self.parent().po.all['expert_mode'])\n                    if current_row_number &lt; 3:\n                        row_to_change[i1 + 1].setVisible(self.parent().po.all['expert_mode'])\n\n    # If not all color space combinations are filled, put None and 0 in boxes\n    if row_number1 &lt; 3:\n        self.row3[0].setVisible(False)\n        self.row3[0].setCurrentIndex(0)\n        for i1 in [1, 2, 3]:\n            self.row3[i1].setVisible(False)\n            self.row3[i1].setValue(0)\n        if row_number1 &lt; 2:\n            self.row2[0].setVisible(False)\n            self.row2[0].setCurrentIndex(0)\n            for i1 in [1, 2, 3]:\n                self.row2[i1].setVisible(False)\n                self.row2[i1].setValue(0)\n            self.row2[i1 + 1].setVisible(False)\n\n    self.row1[4].setVisible(self.parent().po.all['expert_mode'] and row_number1 == 1)\n    self.row2[4].setVisible(self.parent().po.all['expert_mode'] and row_number1 == 2)\n    self.row21[4].setVisible(self.parent().po.all['expert_mode'] and row_number2 == 1)\n    self.row22[4].setVisible(self.parent().po.all['expert_mode'] and row_number2 == 2)\n    if row_number2 &gt; 0:\n        self.logical_operator_between_combination_result.setCurrentText(self.csc_dict['logical'])\n    if row_number2 == 0:\n        self.logical_operator_between_combination_result.setCurrentText('None')\n        self.logical_operator_between_combination_result.setVisible(False)\n        self.logical_operator_label.setVisible(False)\n        self.row21[0].setVisible(False)\n        self.row21[0].setCurrentIndex(0)\n        for i1 in [1, 2, 3]:\n            self.row21[i1].setVisible(False)\n            self.row21[i1].setValue(0)\n        self.row21[i1 + 1].setVisible(False)\n\n    self.logical_operator_between_combination_result.setVisible((row_number2 &gt; 0) and self.parent().po.all['expert_mode'])\n    self.logical_operator_label.setVisible((row_number2 &gt; 0) and self.parent().po.all['expert_mode'])\n\n    if row_number2 &lt; 3:\n        self.row23[0].setVisible(False)\n        self.row23[0].setCurrentIndex(0)\n        for i1 in [1, 2, 3]:\n            self.row23[i1].setVisible(False)\n            self.row23[i1].setValue(0)\n        self.row23[i1 + 1].setVisible(False)\n        self.row22[4].setVisible(False)\n        if row_number2 &lt; 2:\n            self.row22[0].setVisible(False)\n            self.row22[0].setCurrentIndex(0)\n            for i1 in [1, 2, 3]:\n                self.row22[i1].setVisible(False)\n                self.row22[i1].setValue(0)\n            self.row22[i1 + 1].setVisible(False)\n\n    if self.advanced_mode_cb.isChecked():\n        if len(remaining_c_spaces) &gt; 0:\n            self.message.setText(f'Combination also includes {remaining_c_spaces}')\n            self.message.setStyleSheet(\"color: rgb(230, 145, 18)\")\n        else:\n            self.message.setText(f'')\n</code></pre>"},{"location":"api/cellects/gui/image_analysis_window/#cellects.gui.image_analysis_window.ImageAnalysisWindow.user_defined_shape_displayed","title":"<code>user_defined_shape_displayed(when_finished)</code>","text":"<p>Display user-defined shapes or elements based on specific conditions and update the UI accordingly.</p> <p>Parameters:</p> Name Type Description Default <code>when_finished</code> <code>bool</code> <p>A flag indicating whether a certain operation has finished.</p> required Notes <p>This method modifies the user interface by adding buttons and updating layouts based on the current state and conditions.</p> Source code in <code>src/cellects/gui/image_analysis_window.py</code> <pre><code>def user_defined_shape_displayed(self, when_finished: bool):\n    \"\"\"\n    Display user-defined shapes or elements based on specific conditions and update the UI accordingly.\n\n    Parameters\n    ----------\n    when_finished : bool\n        A flag indicating whether a certain operation has finished.\n\n    Notes\n    -----\n    This method modifies the user interface by adding buttons and updating layouts based on the current state and conditions.\n    \"\"\"\n    if self.back1_bio2 == 1:\n        back_name = self.parent().imageanalysiswindow.available_back_names[0]\n        self.back_lines[back_name] = {}\n        pbutton_name = u\"\\u00D7\" + \" Back\" + str(back_name)\n        self.back_lines[back_name][pbutton_name] = self.new_pbutton_on_the_left(pbutton_name)\n        self.back_added_lines_layout.addWidget(self.back_lines[back_name][pbutton_name])\n        self.background.night_mode_switch(night_mode=self.parent().po.all['night_mode'])\n        self.available_back_names = self.available_back_names[1:]\n    elif self.back1_bio2 == 2:\n        bio_name = self.parent().imageanalysiswindow.available_bio_names[0]\n        self.bio_lines[bio_name] = {}\n        pbutton_name = u\"\\u00D7\" + \" Cell\" + str(bio_name)\n        self.bio_lines[bio_name][pbutton_name] = self.new_pbutton_on_the_left(pbutton_name)\n        self.bio_added_lines_layout.addWidget(self.bio_lines[bio_name][pbutton_name])\n        self.cell.night_mode_switch(night_mode=self.parent().po.all['night_mode'])\n        self.available_bio_names = self.available_bio_names[1:]\n        if self.bio_masks_number == 0:\n            self.display_more_than_two_colors_option()\n\n        self.more_than_two_colors.setVisible(self.advanced_mode_cb.isChecked())\n        self.more_than_two_colors_label.setVisible(self.advanced_mode_cb.isChecked())\n        self.distinct_colors_number.setVisible(self.advanced_mode_cb.isChecked() and self.more_than_two_colors.isChecked())\n    elif self.manual_delineation_flag:\n        arena_name = self.parent().imageanalysiswindow.available_arena_names[0]\n        self.arena_lines[arena_name] = {}\n        pbutton_name = u\"\\u00D7\" + \" Arena\" + str(arena_name)\n        self.arena_lines[arena_name][pbutton_name] = self.new_pbutton_on_the_left(pbutton_name)\n        if self.arena_masks_number % 2 == 1:\n            self.bio_added_lines_layout.addWidget(self.arena_lines[arena_name][pbutton_name])\n        else:\n            self.back_added_lines_layout.addWidget(self.arena_lines[arena_name][pbutton_name])\n        self.available_arena_names = self.available_arena_names[1:]\n    self.saved_coord = []\n    self.back1_bio2 = 0\n\n    self.thread_dict[\"UpdateImage\"].message_when_thread_finished.disconnect()\n</code></pre>"},{"location":"api/cellects/gui/image_analysis_window/#cellects.gui.image_analysis_window.ImageAnalysisWindow.video_is_clicked","title":"<code>video_is_clicked()</code>","text":"<p>Handles the logic for when the \"Video tracking\" button is clicked in the interface, leading to the video analysis window.</p> Notes <p>This function displays an error message when a thread relative to the current window is running. This function also save the id of the following window for later use.</p> Source code in <code>src/cellects/gui/image_analysis_window.py</code> <pre><code>def video_is_clicked(self):\n    \"\"\"\n    Handles the logic for when the \"Video tracking\" button is clicked in the interface,\n    leading to the video analysis window.\n\n    Notes\n    -----\n    This function displays an error message when a thread relative to the current window is running.\n    This function also save the id of the following window for later use.\n    \"\"\"\n    if self.video_tab.state != \"not_usable\":\n        if self.is_image_analysis_running:\n            self.message.setText(\"Wait for the analysis to end, or restart Cellects\")\n        else:\n            self.parent().last_tab = \"image_analysis\"\n            self.parent().change_widget(3)\n</code></pre>"},{"location":"api/cellects/gui/image_analysis_window/#cellects.gui.image_analysis_window.ImageAnalysisWindow.visualize_is_clicked","title":"<code>visualize_is_clicked()</code>","text":"<p>Instructs the system to perform an image analysis and updates the UI accordingly.</p> <p>If image analysis is not currently running, this method triggers the analysis process and updates the UI message to indicate loading.</p> Source code in <code>src/cellects/gui/image_analysis_window.py</code> <pre><code>def visualize_is_clicked(self):\n    \"\"\"\n    Instructs the system to perform an image analysis and updates the UI accordingly.\n\n    If image analysis is not currently running, this method triggers the analysis process\n    and updates the UI message to indicate loading.\n    \"\"\"\n    if not self.is_image_analysis_running:\n        self.is_image_analysis_running = True\n        self.message.setText('Loading, wait...')\n        self.parent().po.visualize = True\n        self.parent().po.basic = False\n        self.parent().po.network_shaped = False\n        if self.is_first_image_flag:\n            self.run_first_image_analysis()\n        else:\n            self.run_last_image_analysis()\n</code></pre>"},{"location":"api/cellects/gui/image_analysis_window/#cellects.gui.image_analysis_window.ImageAnalysisWindow.when_image_analysis_finishes","title":"<code>when_image_analysis_finishes()</code>","text":"<p>Logs the completion of an image analysis operation, updates the current combination ID, handles visualization settings, manages image combinations, and updates the display.</p> Notes <ul> <li>This method interacts with the parent object's properties and thread management.</li> <li>The <code>is_first_image_flag</code> determines which set of image combinations to use.</li> </ul> Source code in <code>src/cellects/gui/image_analysis_window.py</code> <pre><code>def when_image_analysis_finishes(self):\n    \"\"\"\n    Logs the completion of an image analysis operation, updates the current combination ID,\n    handles visualization settings, manages image combinations, and updates the display.\n\n    Notes\n    -----\n    - This method interacts with the parent object's properties and thread management.\n    - The `is_first_image_flag` determines which set of image combinations to use.\n    \"\"\"\n\n    if self.is_first_image_flag:\n        im_combinations = self.parent().po.first_image.im_combinations\n    else:\n        im_combinations = self.parent().po.last_image.im_combinations\n    self.init_drawn_image(im_combinations)\n    if self.parent().po.visualize:\n        if self.parent().po.current_combination_id != self.select_option.currentIndex():\n            self.select_option.setCurrentIndex(self.parent().po.current_combination_id)\n    else:\n        self.parent().po.current_combination_id = 0\n        if len(im_combinations) &gt; 0:\n            self.csc_dict = im_combinations[self.parent().po.current_combination_id][\"csc\"]\n            if self.is_first_image_flag:\n                self.parent().po.vars['convert_for_origin'] = self.csc_dict.copy()\n            else:\n                self.parent().po.vars['convert_for_motion'] = self.csc_dict.copy()\n            option_number = len(im_combinations)\n\n            if option_number &gt; 1:\n                # Update the available options of the scrolling menu\n                self.select_option.clear()\n                for option in range(option_number):\n                    self.select_option.addItem(f\"Option {option + 1}\")\n            self.update_csc_editing_display()\n        else:\n            self.message.setText(\"No options could be generated automatically, use the advanced mode\")\n            self.is_image_analysis_running = False\n\n    if self.parent().po.visualize or len(im_combinations) &gt; 0:\n        self.is_image_analysis_display_running = True\n        # Update image display\n        if self.thread_dict[\"UpdateImage\"].isRunning():\n            self.thread_dict[\"UpdateImage\"].wait()\n        self.thread_dict[\"UpdateImage\"].start()\n        self.thread_dict[\"UpdateImage\"].message_when_thread_finished.connect(self.image_analysis_displayed)\n</code></pre>"},{"location":"api/cellects/gui/image_analysis_window/#cellects.gui.image_analysis_window.ImageAnalysisWindow.when_no_is_clicked","title":"<code>when_no_is_clicked()</code>","text":"<p>Handles the event when the 'No' button is clicked.</p> <p>If image analysis is not running, trigger the decision tree process.</p> Source code in <code>src/cellects/gui/image_analysis_window.py</code> <pre><code>def when_no_is_clicked(self):\n    \"\"\"\n    Handles the event when the 'No' button is clicked.\n\n    If image analysis is not running, trigger the decision tree process.\n    \"\"\"\n    if not self.is_image_analysis_running:\n        self.decision_tree(False)\n</code></pre>"},{"location":"api/cellects/gui/image_analysis_window/#cellects.gui.image_analysis_window.ImageAnalysisWindow.when_yes_is_clicked","title":"<code>when_yes_is_clicked()</code>","text":"<p>Handles the event when the 'Yes' button is clicked.</p> <p>If image analysis is not running, trigger the decision tree process.</p> Source code in <code>src/cellects/gui/image_analysis_window.py</code> <pre><code>def when_yes_is_clicked(self):\n    \"\"\"\n    Handles the event when the 'Yes' button is clicked.\n\n    If image analysis is not running, trigger the decision tree process.\n    \"\"\"\n    if not self.is_image_analysis_running:\n        # self.message.setText('Loading, wait...')\n        self.decision_tree(True)\n</code></pre>"},{"location":"api/cellects/gui/required_output/","title":"<code>cellects.gui.required_output</code>","text":""},{"location":"api/cellects/gui/required_output/#cellects.gui.required_output","title":"<code>cellects.gui.required_output</code>","text":"<p>Widget for configuring Cellects analysis variables and descriptors.</p> <p>This GUI module provides checkboxes to select raw data outputs (presence/absence coordinates, contours, tubular networks) and dynamic descriptors calculated per time frame. User selections are saved via parent object's background thread when 'Ok' is clicked. Includes categorized sections with grouped options for clarity of specimen tracking parameters.</p> Main Components <p>RequiredOutput : QWidget for configuring required output variables</p> Notes <p>Saves user selections using parent object's SaveAllVars QThread.</p>"},{"location":"api/cellects/gui/required_output/#cellects.gui.required_output.RequiredOutput","title":"<code>RequiredOutput</code>","text":"<p>               Bases: <code>WindowType</code></p> Source code in <code>src/cellects/gui/required_output.py</code> <pre><code>class RequiredOutput(WindowType):\n    def __init__(self, parent, night_mode):\n        \"\"\"\n        Initialize the RequiredOutput window with a parent widget and night mode setting.\n\n        Parameters\n        ----------\n        parent : QWidget\n            The parent widget to which this window will be attached.\n        night_mode : bool\n            A boolean indicating whether the night mode should be enabled.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from PySide6 import QtWidgets\n        &gt;&gt;&gt; from cellects.gui.cellects import CellectsMainWidget\n        &gt;&gt;&gt; from cellects.gui.required_output import RequiredOutput\n        &gt;&gt;&gt; import sys\n        &gt;&gt;&gt; app = QtWidgets.QApplication([])\n        &gt;&gt;&gt; parent = CellectsMainWidget()\n        &gt;&gt;&gt; session = RequiredOutput(parent, False)\n        &gt;&gt;&gt; session.true_init()\n        &gt;&gt;&gt; parent.insertWidget(0, session)\n        &gt;&gt;&gt; parent.show()\n        &gt;&gt;&gt; sys.exit(app.exec())\n        \"\"\"\n        super().__init__(parent, night_mode)\n        self.setParent(parent)\n        # Create the main Title\n        self.true_init()\n\n    def true_init(self):\n        \"\"\"\n        Initialize the RequiredOutput window with various checkboxes and buttons.\n\n        This method sets up the entire UI layout for the RequiredOutput window,\n        including a title, checkboxes for saving different types of coordinates and\n        descriptors, and 'Cancel' and 'Ok' buttons.\n\n        Notes\n        -----\n        This method assumes that the parent widget has a 'po' attribute with specific settings and variables.\n        \"\"\"\n        logging.info(\"Initialize RequiredOutput window\")\n        self.title = FixedText('Required Output', police=30, night_mode=self.parent().po.all['night_mode'])\n        self.title.setAlignment(QtCore.Qt.AlignHCenter)\n        # Create the main layout\n        self.vlayout = QtWidgets.QVBoxLayout()\n        self.vlayout.addWidget(self.title) #\n        self.vlayout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.Maximum, QtWidgets.QSizePolicy.MinimumExpanding)) #\n\n        # Create the stylesheet for the boxes allowing to categorize required outputs.\n        boxstylesheet = \\\n            \".QWidget {\\n\" \\\n            + \"border: 1px solid black;\\n\" \\\n            + \"border-radius: 20px;\\n\" \\\n            + \"}\"\n\n        # I/ First box: Save presence coordinates\n        # I/A/ Title\n        self.save_presence_coordinates_label = FixedText('Save presence coordinates:', tip=\"Saved in the python numpy format: .npy\",\n                                                 night_mode=self.parent().po.all['night_mode'])\n        self.vlayout.addWidget(self.save_presence_coordinates_label) #\n\n        # I/B/ Create the box\n        self.save_presence_coordinates_layout = QtWidgets.QGridLayout()\n        self.save_presence_coordinates_widget = QtWidgets.QWidget()\n        self.save_presence_coordinates_widget.setStyleSheet(boxstylesheet)\n\n        # I/C/ Create widgets\n        self.save_coord_specimen = Checkbox(self.parent().po.vars['save_coord_specimen'])\n        self.save_coord_specimen_label = FixedText(RO[\"coord_specimen\"][\"label\"], tip=RO[\"coord_specimen\"][\"tips\"],\n                                           night_mode=self.parent().po.all['night_mode'])\n        self.save_graph = Checkbox(self.parent().po.vars['save_graph'])\n        self.save_graph_label = FixedText(RO[\"Graph\"][\"label\"], tip=RO[\"Graph\"][\"tips\"],\n                                           night_mode=self.parent().po.all['night_mode'])\n        self.save_coord_thickening_slimming = Checkbox(self.parent().po.vars['save_coord_thickening_slimming'])\n        self.save_coord_thickening_slimming_label = FixedText(RO[\"coord_oscillating\"][\"label\"],\n                                                              tip=RO[\"coord_oscillating\"][\"tips\"],\n                                           night_mode=self.parent().po.all['night_mode'])\n        self.save_coord_network = Checkbox(self.parent().po.vars['save_coord_network'])\n        self.save_coord_network_label = FixedText(RO[\"coord_network\"][\"label\"], tip=RO[\"coord_network\"][\"tips\"],\n                                           night_mode=self.parent().po.all['night_mode'])\n\n        # I/D/ Arrange widgets in the box\n        self.save_presence_coordinates_layout.addWidget(self.save_coord_specimen_label, 0, 0)\n        self.save_presence_coordinates_layout.addWidget(self.save_coord_specimen, 0, 1)\n        self.save_presence_coordinates_layout.addWidget(self.save_coord_thickening_slimming_label, 1, 0)\n        self.save_presence_coordinates_layout.addWidget(self.save_coord_thickening_slimming, 1, 1)\n        self.save_presence_coordinates_layout.addWidget(self.save_coord_network_label, 0, 2)\n        self.save_presence_coordinates_layout.addWidget(self.save_coord_network, 0, 3)\n        self.save_presence_coordinates_layout.addWidget(self.save_graph_label, 1, 2)\n        self.save_presence_coordinates_layout.addWidget(self.save_graph, 1, 3)\n\n        self.save_presence_coordinates_widget.setLayout(self.save_presence_coordinates_layout)\n        self.vlayout.addWidget(self.save_presence_coordinates_widget)\n\n        # II/ Second box: Save descriptors\n        # II/A/ Title\n        self.save_descriptors_label = FixedText('Save descriptors:',\n                                                         tip=\"Saved in .csv\",\n                                                         night_mode=self.parent().po.all['night_mode'])\n        self.vlayout.addWidget(self.save_descriptors_label)  #\n\n        # II/B/ Create the box\n        self.save_descriptors_layout = QtWidgets.QGridLayout()\n        self.save_descriptors_widget = QtWidgets.QWidget()\n        self.save_descriptors_widget.setStyleSheet(boxstylesheet)\n\n        # II/C/ Create widgets\n\n        self.descriptor_widgets_list = []\n\n        # Create the table of the main output the user can select\n        self.create_check_boxes_table()\n        # II/D/ Set the layout\n        self.save_descriptors_widget.setLayout(self.save_descriptors_layout)\n        self.vlayout.addWidget(self.save_descriptors_widget)\n\n\n        # Create the last row layout that will contain a few more output and the ok button.\n        self.last_row_layout = QtWidgets.QHBoxLayout()\n        self.last_row_widget = QtWidgets.QWidget()\n\n        self.cancel = PButton('Cancel', night_mode=self.parent().po.all['night_mode'])\n        self.cancel.clicked.connect(self.cancel_is_clicked)\n        self.ok = PButton('Ok', night_mode=self.parent().po.all['night_mode'])\n        self.ok.clicked.connect(self.ok_is_clicked)\n        self.last_row_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n        self.last_row_layout.addWidget(self.cancel)\n        self.last_row_layout.addWidget(self.ok)\n\n        self.vlayout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.Maximum, QtWidgets.QSizePolicy.MinimumExpanding))\n        self.last_row_widget.setLayout(self.last_row_layout)\n        self.vlayout.addWidget(self.last_row_widget)\n\n        self.hlayout = QtWidgets.QHBoxLayout()\n        self.vwidget = QtWidgets.QWidget()\n        self.vwidget.setLayout(self.vlayout)\n        self.hlayout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n        self.hlayout.addWidget(self.vwidget)\n        self.hlayout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n        self.setLayout(self.hlayout)\n\n    def create_check_boxes_table(self):\n        \"\"\"\n        Create and populate a table of checkboxes for descriptors in the parent object.\n\n        This function initializes or updates the descriptors and iterates through\n        them to create a table of checkboxes, arranging them in a grid layout. The\n        arrangement depends on the total number of descriptors and their visibility.\n\n        Notes\n        -----\n        The layout of checkboxes changes based on the number of descriptors.\n        \"\"\"\n        if not np.array_equal(self.parent().po.all['descriptors'], list(descriptors_categories.keys())):\n            self.parent().po.all['descriptors'] = descriptors_categories\n\n        descriptor_names = self.parent().po.all['descriptors']\n\n        for i, name in enumerate(descriptor_names):\n            label_index = i * 2\n            if i &gt; 9:\n                row = i - 10 + 1 + 3\n                col = 4\n            else:\n                row = i + 1 + 3\n                col = 1\n            self.descriptor_widgets_list.append(FixedText(descriptors_names_to_display[i], 14, night_mode=self.parent().po.all['night_mode']))\n            self.save_descriptors_layout.addWidget(self.descriptor_widgets_list[label_index], row, col)\n            self.descriptor_widgets_list.append(Checkbox(self.parent().po.all['descriptors'][name]))\n            cb_index = label_index + 1\n\n            # if name == 'fractal_analysis' or name == 'oscilacyto_analysis':\n            #     self.descriptor_widgets_list[label_index].setVisible(False)\n            #     self.descriptor_widgets_list[cb_index].setVisible(False)\n\n            self.save_descriptors_layout.addWidget(self.descriptor_widgets_list[cb_index], row, col + 1)\n\n    def cancel_is_clicked(self):\n        \"\"\"\n        Instead of saving the widgets values to the saved states, use the saved states to fill in the widgets.\n\n        This function updates the state of several checkboxes based on saved variables\n        and descriptors. It also changes the active widget to either the first or third\n        widget depending on a condition.\n        \"\"\"\n        self.save_coord_specimen.setChecked(self.parent().po.vars['save_coord_specimen'])\n        self.save_graph.setChecked(self.parent().po.vars['save_graph'])\n        self.save_coord_thickening_slimming.setChecked(self.parent().po.vars['save_coord_thickening_slimming'])\n        self.save_coord_network.setChecked(self.parent().po.vars['save_coord_network'])\n\n        descriptor_names = self.parent().po.all['descriptors']\n        for i, name in enumerate(descriptor_names):\n            k = i * 2 + 1\n            if name == 'iso_digi_analysis':\n                self.descriptor_widgets_list[k].setChecked(self.parent().po.vars['iso_digi_analysis'])\n            elif name == 'oscilacyto_analysis':\n                self.descriptor_widgets_list[k].setChecked(self.parent().po.vars['oscilacyto_analysis'])\n            elif name == 'fractal_analysis':\n                self.descriptor_widgets_list[k].setChecked(self.parent().po.vars['fractal_analysis'])\n            else:\n                self.descriptor_widgets_list[k].setChecked(self.parent().po.all['descriptors'][name])\n\n        if self.parent().last_is_first:\n            self.parent().change_widget(0) # FirstWidget\n        else:\n            self.parent().change_widget(3) # ThirdWidget\n\n    def ok_is_clicked(self):\n        \"\"\"\n        Updates the parent object's variables and descriptor states based on UI checkboxes.\n\n        This method updates various variables and descriptor states in the parent\n        object based on the current state of checkboxes in the UI. It also starts a\n        thread to save all variables and updates the output list accordingly.\n\n        Notes\n        -----\n        This method does not return any value. It updates the internal state of the\n        parent object, which saves all user defined parameters.\n        \"\"\"\n        self.parent().po.vars['save_coord_specimen'] = self.save_coord_specimen.isChecked()\n        self.parent().po.vars['save_graph'] = self.save_graph.isChecked()\n        self.parent().po.vars['save_coord_thickening_slimming'] = self.save_coord_thickening_slimming.isChecked()\n        self.parent().po.vars['save_coord_network'] = self.save_coord_network.isChecked()\n        descriptor_names = self.parent().po.all['descriptors'].keys()\n        for i, name in enumerate(descriptor_names):\n            k = i * 2 + 1\n            checked_status = self.descriptor_widgets_list[k].isChecked()\n            self.parent().po.all['descriptors'][name] = checked_status\n            if name == 'iso_digi_analysis':\n                self.parent().po.vars['iso_digi_analysis'] = checked_status\n            if name == 'oscilacyto_analysis':\n                self.parent().po.vars['oscilacyto_analysis'] = checked_status\n            if name == 'fractal_analysis':\n                self.parent().po.vars['fractal_analysis'] = checked_status\n        if not self.parent().thread_dict['SaveAllVars'].isRunning():\n            self.parent().thread_dict['SaveAllVars'].start()\n        self.parent().po.update_output_list()\n        if self.parent().last_is_first:\n            self.parent().change_widget(0) # FirstWidget\n        else:\n            self.parent().change_widget(3) # ThirdWidget\n</code></pre>"},{"location":"api/cellects/gui/required_output/#cellects.gui.required_output.RequiredOutput.__init__","title":"<code>__init__(parent, night_mode)</code>","text":"<p>Initialize the RequiredOutput window with a parent widget and night mode setting.</p> <p>Parameters:</p> Name Type Description Default <code>parent</code> <code>QWidget</code> <p>The parent widget to which this window will be attached.</p> required <code>night_mode</code> <code>bool</code> <p>A boolean indicating whether the night mode should be enabled.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from PySide6 import QtWidgets\n&gt;&gt;&gt; from cellects.gui.cellects import CellectsMainWidget\n&gt;&gt;&gt; from cellects.gui.required_output import RequiredOutput\n&gt;&gt;&gt; import sys\n&gt;&gt;&gt; app = QtWidgets.QApplication([])\n&gt;&gt;&gt; parent = CellectsMainWidget()\n&gt;&gt;&gt; session = RequiredOutput(parent, False)\n&gt;&gt;&gt; session.true_init()\n&gt;&gt;&gt; parent.insertWidget(0, session)\n&gt;&gt;&gt; parent.show()\n&gt;&gt;&gt; sys.exit(app.exec())\n</code></pre> Source code in <code>src/cellects/gui/required_output.py</code> <pre><code>def __init__(self, parent, night_mode):\n    \"\"\"\n    Initialize the RequiredOutput window with a parent widget and night mode setting.\n\n    Parameters\n    ----------\n    parent : QWidget\n        The parent widget to which this window will be attached.\n    night_mode : bool\n        A boolean indicating whether the night mode should be enabled.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from PySide6 import QtWidgets\n    &gt;&gt;&gt; from cellects.gui.cellects import CellectsMainWidget\n    &gt;&gt;&gt; from cellects.gui.required_output import RequiredOutput\n    &gt;&gt;&gt; import sys\n    &gt;&gt;&gt; app = QtWidgets.QApplication([])\n    &gt;&gt;&gt; parent = CellectsMainWidget()\n    &gt;&gt;&gt; session = RequiredOutput(parent, False)\n    &gt;&gt;&gt; session.true_init()\n    &gt;&gt;&gt; parent.insertWidget(0, session)\n    &gt;&gt;&gt; parent.show()\n    &gt;&gt;&gt; sys.exit(app.exec())\n    \"\"\"\n    super().__init__(parent, night_mode)\n    self.setParent(parent)\n    # Create the main Title\n    self.true_init()\n</code></pre>"},{"location":"api/cellects/gui/required_output/#cellects.gui.required_output.RequiredOutput.cancel_is_clicked","title":"<code>cancel_is_clicked()</code>","text":"<p>Instead of saving the widgets values to the saved states, use the saved states to fill in the widgets.</p> <p>This function updates the state of several checkboxes based on saved variables and descriptors. It also changes the active widget to either the first or third widget depending on a condition.</p> Source code in <code>src/cellects/gui/required_output.py</code> <pre><code>def cancel_is_clicked(self):\n    \"\"\"\n    Instead of saving the widgets values to the saved states, use the saved states to fill in the widgets.\n\n    This function updates the state of several checkboxes based on saved variables\n    and descriptors. It also changes the active widget to either the first or third\n    widget depending on a condition.\n    \"\"\"\n    self.save_coord_specimen.setChecked(self.parent().po.vars['save_coord_specimen'])\n    self.save_graph.setChecked(self.parent().po.vars['save_graph'])\n    self.save_coord_thickening_slimming.setChecked(self.parent().po.vars['save_coord_thickening_slimming'])\n    self.save_coord_network.setChecked(self.parent().po.vars['save_coord_network'])\n\n    descriptor_names = self.parent().po.all['descriptors']\n    for i, name in enumerate(descriptor_names):\n        k = i * 2 + 1\n        if name == 'iso_digi_analysis':\n            self.descriptor_widgets_list[k].setChecked(self.parent().po.vars['iso_digi_analysis'])\n        elif name == 'oscilacyto_analysis':\n            self.descriptor_widgets_list[k].setChecked(self.parent().po.vars['oscilacyto_analysis'])\n        elif name == 'fractal_analysis':\n            self.descriptor_widgets_list[k].setChecked(self.parent().po.vars['fractal_analysis'])\n        else:\n            self.descriptor_widgets_list[k].setChecked(self.parent().po.all['descriptors'][name])\n\n    if self.parent().last_is_first:\n        self.parent().change_widget(0) # FirstWidget\n    else:\n        self.parent().change_widget(3) # ThirdWidget\n</code></pre>"},{"location":"api/cellects/gui/required_output/#cellects.gui.required_output.RequiredOutput.create_check_boxes_table","title":"<code>create_check_boxes_table()</code>","text":"<p>Create and populate a table of checkboxes for descriptors in the parent object.</p> <p>This function initializes or updates the descriptors and iterates through them to create a table of checkboxes, arranging them in a grid layout. The arrangement depends on the total number of descriptors and their visibility.</p> Notes <p>The layout of checkboxes changes based on the number of descriptors.</p> Source code in <code>src/cellects/gui/required_output.py</code> <pre><code>def create_check_boxes_table(self):\n    \"\"\"\n    Create and populate a table of checkboxes for descriptors in the parent object.\n\n    This function initializes or updates the descriptors and iterates through\n    them to create a table of checkboxes, arranging them in a grid layout. The\n    arrangement depends on the total number of descriptors and their visibility.\n\n    Notes\n    -----\n    The layout of checkboxes changes based on the number of descriptors.\n    \"\"\"\n    if not np.array_equal(self.parent().po.all['descriptors'], list(descriptors_categories.keys())):\n        self.parent().po.all['descriptors'] = descriptors_categories\n\n    descriptor_names = self.parent().po.all['descriptors']\n\n    for i, name in enumerate(descriptor_names):\n        label_index = i * 2\n        if i &gt; 9:\n            row = i - 10 + 1 + 3\n            col = 4\n        else:\n            row = i + 1 + 3\n            col = 1\n        self.descriptor_widgets_list.append(FixedText(descriptors_names_to_display[i], 14, night_mode=self.parent().po.all['night_mode']))\n        self.save_descriptors_layout.addWidget(self.descriptor_widgets_list[label_index], row, col)\n        self.descriptor_widgets_list.append(Checkbox(self.parent().po.all['descriptors'][name]))\n        cb_index = label_index + 1\n\n        # if name == 'fractal_analysis' or name == 'oscilacyto_analysis':\n        #     self.descriptor_widgets_list[label_index].setVisible(False)\n        #     self.descriptor_widgets_list[cb_index].setVisible(False)\n\n        self.save_descriptors_layout.addWidget(self.descriptor_widgets_list[cb_index], row, col + 1)\n</code></pre>"},{"location":"api/cellects/gui/required_output/#cellects.gui.required_output.RequiredOutput.ok_is_clicked","title":"<code>ok_is_clicked()</code>","text":"<p>Updates the parent object's variables and descriptor states based on UI checkboxes.</p> <p>This method updates various variables and descriptor states in the parent object based on the current state of checkboxes in the UI. It also starts a thread to save all variables and updates the output list accordingly.</p> Notes <p>This method does not return any value. It updates the internal state of the parent object, which saves all user defined parameters.</p> Source code in <code>src/cellects/gui/required_output.py</code> <pre><code>def ok_is_clicked(self):\n    \"\"\"\n    Updates the parent object's variables and descriptor states based on UI checkboxes.\n\n    This method updates various variables and descriptor states in the parent\n    object based on the current state of checkboxes in the UI. It also starts a\n    thread to save all variables and updates the output list accordingly.\n\n    Notes\n    -----\n    This method does not return any value. It updates the internal state of the\n    parent object, which saves all user defined parameters.\n    \"\"\"\n    self.parent().po.vars['save_coord_specimen'] = self.save_coord_specimen.isChecked()\n    self.parent().po.vars['save_graph'] = self.save_graph.isChecked()\n    self.parent().po.vars['save_coord_thickening_slimming'] = self.save_coord_thickening_slimming.isChecked()\n    self.parent().po.vars['save_coord_network'] = self.save_coord_network.isChecked()\n    descriptor_names = self.parent().po.all['descriptors'].keys()\n    for i, name in enumerate(descriptor_names):\n        k = i * 2 + 1\n        checked_status = self.descriptor_widgets_list[k].isChecked()\n        self.parent().po.all['descriptors'][name] = checked_status\n        if name == 'iso_digi_analysis':\n            self.parent().po.vars['iso_digi_analysis'] = checked_status\n        if name == 'oscilacyto_analysis':\n            self.parent().po.vars['oscilacyto_analysis'] = checked_status\n        if name == 'fractal_analysis':\n            self.parent().po.vars['fractal_analysis'] = checked_status\n    if not self.parent().thread_dict['SaveAllVars'].isRunning():\n        self.parent().thread_dict['SaveAllVars'].start()\n    self.parent().po.update_output_list()\n    if self.parent().last_is_first:\n        self.parent().change_widget(0) # FirstWidget\n    else:\n        self.parent().change_widget(3) # ThirdWidget\n</code></pre>"},{"location":"api/cellects/gui/required_output/#cellects.gui.required_output.RequiredOutput.true_init","title":"<code>true_init()</code>","text":"<p>Initialize the RequiredOutput window with various checkboxes and buttons.</p> <p>This method sets up the entire UI layout for the RequiredOutput window, including a title, checkboxes for saving different types of coordinates and descriptors, and 'Cancel' and 'Ok' buttons.</p> Notes <p>This method assumes that the parent widget has a 'po' attribute with specific settings and variables.</p> Source code in <code>src/cellects/gui/required_output.py</code> <pre><code>def true_init(self):\n    \"\"\"\n    Initialize the RequiredOutput window with various checkboxes and buttons.\n\n    This method sets up the entire UI layout for the RequiredOutput window,\n    including a title, checkboxes for saving different types of coordinates and\n    descriptors, and 'Cancel' and 'Ok' buttons.\n\n    Notes\n    -----\n    This method assumes that the parent widget has a 'po' attribute with specific settings and variables.\n    \"\"\"\n    logging.info(\"Initialize RequiredOutput window\")\n    self.title = FixedText('Required Output', police=30, night_mode=self.parent().po.all['night_mode'])\n    self.title.setAlignment(QtCore.Qt.AlignHCenter)\n    # Create the main layout\n    self.vlayout = QtWidgets.QVBoxLayout()\n    self.vlayout.addWidget(self.title) #\n    self.vlayout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.Maximum, QtWidgets.QSizePolicy.MinimumExpanding)) #\n\n    # Create the stylesheet for the boxes allowing to categorize required outputs.\n    boxstylesheet = \\\n        \".QWidget {\\n\" \\\n        + \"border: 1px solid black;\\n\" \\\n        + \"border-radius: 20px;\\n\" \\\n        + \"}\"\n\n    # I/ First box: Save presence coordinates\n    # I/A/ Title\n    self.save_presence_coordinates_label = FixedText('Save presence coordinates:', tip=\"Saved in the python numpy format: .npy\",\n                                             night_mode=self.parent().po.all['night_mode'])\n    self.vlayout.addWidget(self.save_presence_coordinates_label) #\n\n    # I/B/ Create the box\n    self.save_presence_coordinates_layout = QtWidgets.QGridLayout()\n    self.save_presence_coordinates_widget = QtWidgets.QWidget()\n    self.save_presence_coordinates_widget.setStyleSheet(boxstylesheet)\n\n    # I/C/ Create widgets\n    self.save_coord_specimen = Checkbox(self.parent().po.vars['save_coord_specimen'])\n    self.save_coord_specimen_label = FixedText(RO[\"coord_specimen\"][\"label\"], tip=RO[\"coord_specimen\"][\"tips\"],\n                                       night_mode=self.parent().po.all['night_mode'])\n    self.save_graph = Checkbox(self.parent().po.vars['save_graph'])\n    self.save_graph_label = FixedText(RO[\"Graph\"][\"label\"], tip=RO[\"Graph\"][\"tips\"],\n                                       night_mode=self.parent().po.all['night_mode'])\n    self.save_coord_thickening_slimming = Checkbox(self.parent().po.vars['save_coord_thickening_slimming'])\n    self.save_coord_thickening_slimming_label = FixedText(RO[\"coord_oscillating\"][\"label\"],\n                                                          tip=RO[\"coord_oscillating\"][\"tips\"],\n                                       night_mode=self.parent().po.all['night_mode'])\n    self.save_coord_network = Checkbox(self.parent().po.vars['save_coord_network'])\n    self.save_coord_network_label = FixedText(RO[\"coord_network\"][\"label\"], tip=RO[\"coord_network\"][\"tips\"],\n                                       night_mode=self.parent().po.all['night_mode'])\n\n    # I/D/ Arrange widgets in the box\n    self.save_presence_coordinates_layout.addWidget(self.save_coord_specimen_label, 0, 0)\n    self.save_presence_coordinates_layout.addWidget(self.save_coord_specimen, 0, 1)\n    self.save_presence_coordinates_layout.addWidget(self.save_coord_thickening_slimming_label, 1, 0)\n    self.save_presence_coordinates_layout.addWidget(self.save_coord_thickening_slimming, 1, 1)\n    self.save_presence_coordinates_layout.addWidget(self.save_coord_network_label, 0, 2)\n    self.save_presence_coordinates_layout.addWidget(self.save_coord_network, 0, 3)\n    self.save_presence_coordinates_layout.addWidget(self.save_graph_label, 1, 2)\n    self.save_presence_coordinates_layout.addWidget(self.save_graph, 1, 3)\n\n    self.save_presence_coordinates_widget.setLayout(self.save_presence_coordinates_layout)\n    self.vlayout.addWidget(self.save_presence_coordinates_widget)\n\n    # II/ Second box: Save descriptors\n    # II/A/ Title\n    self.save_descriptors_label = FixedText('Save descriptors:',\n                                                     tip=\"Saved in .csv\",\n                                                     night_mode=self.parent().po.all['night_mode'])\n    self.vlayout.addWidget(self.save_descriptors_label)  #\n\n    # II/B/ Create the box\n    self.save_descriptors_layout = QtWidgets.QGridLayout()\n    self.save_descriptors_widget = QtWidgets.QWidget()\n    self.save_descriptors_widget.setStyleSheet(boxstylesheet)\n\n    # II/C/ Create widgets\n\n    self.descriptor_widgets_list = []\n\n    # Create the table of the main output the user can select\n    self.create_check_boxes_table()\n    # II/D/ Set the layout\n    self.save_descriptors_widget.setLayout(self.save_descriptors_layout)\n    self.vlayout.addWidget(self.save_descriptors_widget)\n\n\n    # Create the last row layout that will contain a few more output and the ok button.\n    self.last_row_layout = QtWidgets.QHBoxLayout()\n    self.last_row_widget = QtWidgets.QWidget()\n\n    self.cancel = PButton('Cancel', night_mode=self.parent().po.all['night_mode'])\n    self.cancel.clicked.connect(self.cancel_is_clicked)\n    self.ok = PButton('Ok', night_mode=self.parent().po.all['night_mode'])\n    self.ok.clicked.connect(self.ok_is_clicked)\n    self.last_row_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n    self.last_row_layout.addWidget(self.cancel)\n    self.last_row_layout.addWidget(self.ok)\n\n    self.vlayout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.Maximum, QtWidgets.QSizePolicy.MinimumExpanding))\n    self.last_row_widget.setLayout(self.last_row_layout)\n    self.vlayout.addWidget(self.last_row_widget)\n\n    self.hlayout = QtWidgets.QHBoxLayout()\n    self.vwidget = QtWidgets.QWidget()\n    self.vwidget.setLayout(self.vlayout)\n    self.hlayout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n    self.hlayout.addWidget(self.vwidget)\n    self.hlayout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n    self.setLayout(self.hlayout)\n</code></pre>"},{"location":"api/cellects/gui/ui_strings/","title":"<code>cellects.gui.ui_strings</code>","text":""},{"location":"api/cellects/gui/ui_strings/#cellects.gui.ui_strings","title":"<code>cellects.gui.ui_strings</code>","text":""},{"location":"api/cellects/gui/video_analysis_window/","title":"<code>cellects.gui.video_analysis_window</code>","text":""},{"location":"api/cellects/gui/video_analysis_window/#cellects.gui.video_analysis_window","title":"<code>cellects.gui.video_analysis_window</code>","text":"<p>Third main widget for Cellects GUI enabling video analysis configuration and execution.</p> <p>This module implements a video tracking interface for analyzing cell movement through configurable parameters like  arena selection, segmentation methods, and smoothing thresholds. It provides interactive controls including spinboxes,   comboboxes, buttons for detection/post-processing, and an image display area with full-screen support. Threaded   operations (VideoReaderThread, OneArenaThread) handle background processing to maintain UI responsiveness.</p> <p>Main Components VideoAnalysisWindow : QWidget subclass implementing the video analysis interface with tab navigation, parameter controls, and thread coordination</p> <p>Notes Uses QThread for background operations to maintain UI responsiveness.</p>"},{"location":"api/cellects/gui/video_analysis_window/#cellects.gui.video_analysis_window.VideoAnalysisWindow","title":"<code>VideoAnalysisWindow</code>","text":"<p>               Bases: <code>MainTabsType</code></p> Source code in <code>src/cellects/gui/video_analysis_window.py</code> <pre><code>class VideoAnalysisWindow(MainTabsType):\n    def __init__(self, parent, night_mode):\n        \"\"\"\n        Initialize the VideoAnalysis window with a parent widget and night mode setting.\n\n        Parameters\n        ----------\n        parent : QWidget\n            The parent widget to which this window will be attached.\n        night_mode : bool\n            A boolean indicating whether the night mode should be enabled.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from PySide6 import QtWidgets\n        &gt;&gt;&gt; from cellects.gui.cellects import CellectsMainWidget\n        &gt;&gt;&gt; from cellects.gui.video_analysis_window import VideoAnalysisWindow\n        &gt;&gt;&gt; import sys\n        &gt;&gt;&gt; app = QtWidgets.QApplication([])\n        &gt;&gt;&gt; parent = CellectsMainWidget()\n        &gt;&gt;&gt; session = VideoAnalysisWindow(parent, False)\n        &gt;&gt;&gt; session.true_init()\n        &gt;&gt;&gt; parent.insertWidget(0, session)\n        &gt;&gt;&gt; parent.show()\n        &gt;&gt;&gt; sys.exit(app.exec())\n        \"\"\"\n        super().__init__(parent, night_mode)\n        logging.info(\"Initialize VideoAnalysisWindow\")\n        self.setParent(parent)\n        self.true_init()\n\n    def true_init(self):\n        \"\"\"\n        Initialize the video tracking interface and set up its UI components.\n\n        Extended Description\n        --------------------\n        This method initializes various tabs, threads, and UI elements for the video tracking interface. It sets up\n        event handlers for tab clicks and configures layout components such as labels, buttons, spinboxes, and\n        comboboxes.\n\n        Notes\n        -----\n        This method assumes that the parent widget has a 'po' attribute with specific settings and variables.\n        \"\"\"\n        self.data_tab.set_not_in_use()\n        self.image_tab.set_not_usable()\n        self.video_tab.set_in_use()\n        self.data_tab.clicked.connect(self.data_tab_is_clicked)\n        self.image_tab.clicked.connect(self.image_tab_is_clicked)\n        self.thread_dict = {}\n        self.thread_dict['VideoReader'] = VideoReaderThread(self.parent())\n        self.thread_dict['OneArena'] = OneArenaThread(self.parent())\n        self.thread_dict['ChangeOneRepResult'] = ChangeOneRepResultThread(self.parent())\n        self.thread_dict['RunAll'] = RunAllThread(self.parent())\n        self.previous_arena = 0\n        curr_row_main_layout = 0\n        ncol = 1\n        self.Vlayout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.Maximum, QtWidgets.QSizePolicy.MinimumExpanding))#, curr_row_main_layout, 0, 1, ncol)\n        curr_row_main_layout += 1\n\n        # Open subtitle\n        self.general_step_widget = QtWidgets.QWidget()\n        self.general_step_layout = QtWidgets.QHBoxLayout()\n        self.current_step = 0\n        self.general_step_label = FixedText('Step 1: Tune parameters to improve Detection', night_mode=self.parent().po.all['night_mode'])\n        self.general_step_button = PButton('Done', night_mode=self.parent().po.all['night_mode'])\n        self.general_step_button.clicked.connect(self.step_done_is_clicked)\n        self.general_step_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n        self.general_step_layout.addWidget(self.general_step_label)\n        self.general_step_layout.addWidget(self.general_step_button)\n        self.general_step_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n\n        self.general_step_widget.setLayout(self.general_step_layout)\n        self.Vlayout.addWidget(self.general_step_widget)#, curr_row_main_layout, 0, 1, ncol)\n        curr_row_main_layout += 1\n\n        # Open central widget\n        self.video_display_widget = QtWidgets.QWidget()\n        self.video_display_layout = QtWidgets.QHBoxLayout()\n        self.video_display_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n        #   Open left widget\n        self.left_options_widget = QtWidgets.QWidget()\n        self.left_options_layout = QtWidgets.QVBoxLayout()\n        self.left_options_widget.setSizePolicy(QtWidgets.QSizePolicy.Maximum, QtWidgets.QSizePolicy.Maximum)\n\n        self.arena_widget = QtWidgets.QWidget()\n        self.arena_layout = QtWidgets.QHBoxLayout()\n        self.arena_label = FixedText(VAW[\"Arena_to_analyze\"][\"label\"] + ':',\n                                       tip=VAW[\"Arena_to_analyze\"][\"tips\"],\n                                       night_mode=self.parent().po.all['night_mode'])\n        sample_size = self.parent().po.all['sample_number_per_folder'][0]\n        if self.parent().po.all['arena'] &gt; sample_size:\n            self.parent().po.all['arena'] = 1\n\n        self.arena = Spinbox(min=1, max=1000000, val=self.parent().po.all['arena'],\n                               night_mode=self.parent().po.all['night_mode'])\n        self.arena.valueChanged.connect(self.arena_changed)\n\n        self.arena_layout.addWidget(self.arena_label)\n        self.arena_layout.addWidget(self.arena)\n        self.arena_widget.setLayout(self.arena_layout)\n        self.left_options_layout.addWidget(self.arena_widget)\n\n\n        self.growth_per_frame_widget = QtWidgets.QWidget()\n        self.growth_per_frame_layout = QtWidgets.QHBoxLayout()\n        try:\n            self.parent().po.vars['maximal_growth_factor']\n        except KeyError:\n            self.parent().po.vars['maximal_growth_factor'] = 0.02\n            self.parent().po.vars['repeat_video_smoothing'] = self.parent().po.vars['iterate_smoothing']\n        self.maximal_growth_factor = Spinbox(min=0, max=0.5, val=self.parent().po.vars['maximal_growth_factor'],\n                                            decimals=3, night_mode=self.parent().po.all['night_mode'])\n        self.maximal_growth_factor_label = FixedText(VAW[\"Maximal_growth_factor\"][\"label\"] + ':',\n                                                    tip=VAW[\"Maximal_growth_factor\"][\"tips\"],\n                                                    night_mode=self.parent().po.all['night_mode'])\n        self.maximal_growth_factor.valueChanged.connect(self.maximal_growth_factor_changed)\n        self.growth_per_frame_layout.addWidget(self.maximal_growth_factor_label)\n        self.growth_per_frame_layout.addWidget(self.maximal_growth_factor)\n        self.growth_per_frame_widget.setLayout(self.growth_per_frame_layout)\n        self.left_options_layout.addWidget(self.growth_per_frame_widget)\n\n        self.iterate_widget = QtWidgets.QWidget()\n        self.iterate_layout = QtWidgets.QHBoxLayout()\n        self.repeat_video_smoothing = Spinbox(min=0, max=10, val=self.parent().po.vars['repeat_video_smoothing'],\n                                         night_mode=self.parent().po.all['night_mode'])\n        self.repeat_video_smoothing_label = FixedText(VAW[\"Temporal_smoothing\"][\"label\"] + ':',\n                                                 tip=VAW[\"Temporal_smoothing\"][\"tips\"],\n                                                 night_mode=self.parent().po.all['night_mode'])\n        self.repeat_video_smoothing.valueChanged.connect(self.repeat_video_smoothing_changed)\n        self.iterate_layout.addWidget(self.repeat_video_smoothing_label)\n        self.iterate_layout.addWidget(self.repeat_video_smoothing)\n        self.iterate_widget.setLayout(self.iterate_layout)\n        self.left_options_layout.addWidget(self.iterate_widget)\n\n\n        self.select_option_label = FixedText(VAW[\"Segmentation_method\"][\"label\"] + ':',\n                                             tip=VAW[\"Segmentation_method\"][\"tips\"],\n                                             night_mode=self.parent().po.all['night_mode'])\n        self.select_option = Combobox([], night_mode=self.parent().po.all['night_mode'])\n        self.select_option_label.setSizePolicy(QtWidgets.QSizePolicy.Maximum, QtWidgets.QSizePolicy.Maximum)\n        self.select_option.setSizePolicy(QtWidgets.QSizePolicy.Maximum, QtWidgets.QSizePolicy.Maximum)\n        self.select_option.setFixedWidth(175)\n        self.select_option.addItem(\"1. Frame by frame\")\n        self.select_option.addItem(\"2. Dynamical threshold\")\n        self.select_option.addItem(\"3. Dynamical slope\")\n        self.select_option.addItem(\"4. Threshold and Slope\")\n        self.select_option.addItem(\"5. Threshold or Slope\")\n        self.select_option.setCurrentIndex(self.parent().po.all['video_option'])\n        self.select_option.currentTextChanged.connect(self.option_changed)\n\n        # Open the choose best option row layout\n        self.options_row_widget = QtWidgets.QWidget()\n        self.options_row_layout = QtWidgets.QHBoxLayout()\n        self.options_row_layout.addWidget(self.select_option_label)\n        self.options_row_layout.addWidget(self.select_option)\n        self.options_row_layout.setAlignment(QtCore.Qt.AlignHCenter)\n        self.options_row_layout.setAlignment(QtCore.Qt.AlignVCenter)\n        self.options_row_widget.setLayout(self.options_row_layout)\n        self.left_options_layout.addWidget(self.options_row_widget)\n\n        #   Close left widget\n        self.left_options_widget.setLayout(self.left_options_layout)\n        self.video_display_layout.addWidget(self.left_options_widget)\n\n\n        # Add the central video display widget\n        self.display_image = np.zeros((self.parent().im_max_height, self.parent().im_max_width, 3), np.uint8)\n        self.display_image = InsertImage(self.display_image, self.parent().im_max_height, self.parent().im_max_width)\n        self.display_image.mousePressEvent = self.full_screen_display\n        self.video_display_layout.addWidget(self.display_image)\n\n\n        #   Open right widget\n        self.right_options_widget = QtWidgets.QWidget()\n        self.right_options_layout = QtWidgets.QVBoxLayout()\n        self.right_options_widget.setSizePolicy(QtWidgets.QSizePolicy.Maximum, QtWidgets.QSizePolicy.Maximum)\n\n        self.compute_all_options_label = FixedText('Compute all options',\n                                                   tip=VAW[\"Segmentation_method\"][\"tips\"],\n                                                   night_mode=self.parent().po.all['night_mode'])\n        self.compute_all_options_cb = Checkbox(self.parent().po.all['compute_all_options'])\n        self.compute_all_options_cb.setStyleSheet(\"QCheckBox::indicator {width: 12px;height: 12px;background-color: transparent;\"\n                            \"border-radius: 5px;border-style: solid;border-width: 1px;\"\n                            \"border-color: rgb(100,100,100);}\"\n                            \"QCheckBox::indicator:checked {background-color: rgb(70,130,180);}\"\n                            \"QCheckBox:checked, QCheckBox::indicator:checked {border-color: black black white white;}\"\n                            \"QCheckBox:checked {background-color: transparent;}\"\n                            \"QCheckBox:margin-left {0%}\"\n                            \"QCheckBox:margin-right {0%}\")\n        self.compute_all_options_cb.stateChanged.connect(self.compute_all_options_check)\n        self.all_options_row_widget = QtWidgets.QWidget()\n        self.all_options_row_layout = QtWidgets.QHBoxLayout()\n        self.all_options_row_layout.addWidget(self.compute_all_options_cb)\n        self.all_options_row_layout.addWidget(self.compute_all_options_label)\n        self.all_options_row_layout.setAlignment(QtCore.Qt.AlignHCenter)\n        self.all_options_row_layout.setAlignment(QtCore.Qt.AlignVCenter)\n        self.all_options_row_widget.setLayout(self.all_options_row_layout)\n        self.right_options_layout.addWidget(self.all_options_row_widget)\n\n        self.load_one_arena = PButton(VAW[\"Load_one_arena\"][\"label\"], tip=VAW[\"Load_one_arena\"][\"tips\"],\n                                      night_mode=self.parent().po.all['night_mode'])\n        self.load_one_arena.clicked.connect(self.load_one_arena_is_clicked)\n        self.detection = PButton(VAW[\"Detection\"][\"label\"], tip=VAW[\"Detection\"][\"tips\"],\n                                 night_mode=self.parent().po.all['night_mode'])\n        self.detection.clicked.connect(self.detection_is_clicked)\n        self.read = PButton(VAW[\"Read\"][\"label\"], tip=VAW[\"Read\"][\"tips\"], night_mode=self.parent().po.all['night_mode'])\n        self.read.clicked.connect(self.read_is_clicked)\n        self.read.setVisible(False)\n        self.right_options_layout.addWidget(self.load_one_arena, alignment=QtCore.Qt.AlignCenter)\n        self.right_options_layout.addWidget(self.detection, alignment=QtCore.Qt.AlignCenter)\n        self.right_options_layout.addWidget(self.read, alignment=QtCore.Qt.AlignCenter)\n\n\n        self.right_options_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n        self.right_options_widget.setLayout(self.right_options_layout)\n        self.video_display_layout.addWidget(self.right_options_widget)\n        self.video_display_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n        # Close central widget\n        self.video_display_widget.setLayout(self.video_display_layout)\n        self.Vlayout.addWidget(self.video_display_widget)#, curr_row_main_layout, 0)\n        curr_row_main_layout += 1\n\n        # Open Second step row\n        self.second_step_widget = QtWidgets.QWidget()\n        self.second_step_layout = QtWidgets.QHBoxLayout()\n        self.second_step_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n        self.second_step_widget.setVisible(False)\n\n        self.fading_widget = QtWidgets.QWidget()\n        self.fading_layout = QtWidgets.QHBoxLayout()\n        self.do_fading = Checkbox(self.parent().po.vars['do_fading'])\n        self.do_fading.setStyleSheet(\"QCheckBox::indicator {width: 12px;height: 12px;background-color: transparent;\"\n                            \"border-radius: 5px;border-style: solid;border-width: 1px;\"\n                            \"border-color: rgb(100,100,100);}\"\n                            \"QCheckBox::indicator:checked {background-color: rgb(70,130,180);}\"\n                            \"QCheckBox:checked, QCheckBox::indicator:checked {border-color: black black white white;}\"\n                            \"QCheckBox:checked {background-color: transparent;}\"\n                            \"QCheckBox:margin-left {0%}\"\n                            \"QCheckBox:margin-right {0%}\")\n        self.do_fading.stateChanged.connect(self.do_fading_check)\n        self.fading = Spinbox(min=- 1, max=1, val=self.parent().po.vars['fading'], decimals=2,\n                               night_mode=self.parent().po.all['night_mode'])\n        self.fading_label = FixedText(VAW[\"Fading_detection\"][\"label\"],\n                                       tip=VAW[\"Fading_detection\"][\"tips\"],\n                                       night_mode=self.parent().po.all['night_mode'])\n        self.fading.valueChanged.connect(self.fading_changed)\n        self.fading_layout.addWidget(self.do_fading)\n        self.fading_layout.addWidget(self.fading_label)\n        self.fading_layout.addWidget(self.fading)\n        self.fading_layout.setAlignment(QtCore.Qt.AlignHCenter)\n        self.fading_widget.setLayout(self.fading_layout)\n        self.second_step_layout.addWidget(self.fading_widget)\n\n        self.post_processing = PButton(VAW[\"Post_processing\"][\"label\"], tip=VAW[\"Post_processing\"][\"tips\"],\n                                       night_mode=self.parent().po.all['night_mode'])\n        self.post_processing.clicked.connect(self.post_processing_is_clicked)\n        self.second_step_layout.addWidget(self.post_processing)\n\n        self.save_one_result = PButton(VAW[\"Save_one_result\"][\"label\"], tip=VAW[\"Save_one_result\"][\"tips\"],\n                                       night_mode=self.parent().po.all['night_mode'])\n        self.save_one_result.clicked.connect(self.save_one_result_is_clicked)\n        self.second_step_layout.addWidget(self.save_one_result)\n\n        # Close Second step row\n        self.second_step_layout.setAlignment(QtCore.Qt.AlignHCenter)\n        self.second_step_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n        self.second_step_widget.setLayout(self.second_step_layout)\n        self.Vlayout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.Maximum, QtWidgets.QSizePolicy.MinimumExpanding))#, curr_row_main_layout, 0, 1, ncol)\n        curr_row_main_layout += 1\n        self.Vlayout.addWidget(self.second_step_widget)#, curr_row_main_layout, 0)\n        curr_row_main_layout += 1\n\n        # Open last options row widget\n        self.last_options_widget = QtWidgets.QWidget()\n        self.last_options_layout = QtWidgets.QHBoxLayout()\n        self.last_options_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n\n        self.advanced_parameters = PButton(FW[\"Advanced_parameters\"][\"label\"], tip=FW[\"Advanced_parameters\"][\"tips\"],\n                                           night_mode=self.parent().po.all['night_mode'])\n        self.advanced_parameters.clicked.connect(self.advanced_parameters_is_clicked)\n        self.last_options_layout.addWidget(self.advanced_parameters)\n\n        #  Required Outputs widget\n        self.required_outputs = PButton(FW[\"Required_outputs\"][\"label\"], tip=FW[\"Required_outputs\"][\"tips\"],\n                                        night_mode=self.parent().po.all['night_mode'])\n        self.required_outputs.clicked.connect(self.required_outputs_is_clicked)\n        self.last_options_layout.addWidget(self.required_outputs)\n\n        #  Save all choices widget\n        self.save_all_vars = PButton(VAW[\"Save_all_choices\"][\"label\"], tip=VAW[\"Save_all_choices\"][\"tips\"],\n                                     night_mode=self.parent().po.all['night_mode'])\n        self.save_all_vars.clicked.connect(self.save_current_settings)\n        self.last_options_layout.addWidget(self.save_all_vars)\n\n        # Close last options widget\n        self.last_options_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n        self.last_options_widget.setLayout(self.last_options_layout)\n        self.Vlayout.addWidget(self.last_options_widget)#, curr_row_main_layout, 0)\n        curr_row_main_layout += 1\n\n        self.message = QtWidgets.QLabel(self)\n        self.message.setText('')\n        self.message.setStyleSheet(\"color: rgb(230, 145, 18)\")\n        self.message.setAlignment(QtCore.Qt.AlignLeft)\n\n        self.previous = PButton('Previous', night_mode=self.parent().po.all['night_mode'])\n        self.previous.clicked.connect(self.previous_is_clicked)\n\n        self.run_all = PButton(VAW[\"Run_All\"][\"label\"], tip=VAW[\"Run_All\"][\"tips\"],\n                               night_mode=self.parent().po.all['night_mode'])\n        self.run_all.clicked.connect(self.run_all_is_clicked)\n\n        # Open last row widget\n        self.last_row_widget = QtWidgets.QWidget()\n        self.last_row_layout = QtWidgets.QHBoxLayout()\n        self.last_row_layout.addWidget(self.previous)\n        self.last_row_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n        self.last_row_layout.addWidget(self.message)\n        self.last_row_layout.addWidget(self.run_all)\n        # Close last row widget\n        self.last_row_widget.setLayout(self.last_row_layout)\n        self.Vlayout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.Maximum, QtWidgets.QSizePolicy.MinimumExpanding))#, curr_row_main_layout, 0, 1, ncol)\n        self.Vlayout.addWidget(self.last_row_widget)#, curr_row_main_layout + 1, 0)\n\n        self.setLayout(self.Vlayout)\n\n    def display_conditionally_visible_widgets(self):\n        \"\"\"\n        Display Conditionally Visible Widgets\n        \"\"\"\n        self.select_option_label.setVisible(self.parent().po.vars[\"color_number\"] == 2)\n        self.select_option.setVisible(self.parent().po.vars[\"color_number\"] == 2)\n        self.fading.setVisible(self.parent().po.vars['do_fading'])\n\n    def step_done_is_clicked(self):\n        \"\"\"\n        Step the analysis progress when 'Done' button is clicked.\n\n        Increments the current step and updates the UI accordingly based on the\n        new step value. Updates labels, tooltips, and visibility of widgets.\n\n        Notes\n        -----\n        This method is automatically called when the 'Done' button is clicked.\n        It updates the GUI elements to reflect progress in a multi-step\n        analysis process.\n        \"\"\"\n        self.current_step += 1\n        if self.current_step == 1:\n            self.general_step_label.setText('Step 2: Tune fading and advanced parameters to improve Post processing')\n            self.general_step_label.setToolTip('Post processing is slower than Detection.\\nIt improves detection with the following optional algorithms:\\n - Fading detection\\n - Correct errors around initial shape\\n - Organism internal oscillation period\\n - Connect distant shape\\n - Appearing cell selection')\n            self.second_step_widget.setVisible(True)\n            self.fading.setVisible(self.parent().po.vars['do_fading'])\n            self.save_one_result.setVisible(False)\n        elif self.current_step == 2:\n            self.general_step_label.setText('Step 3: Run the full analysis or save the result of one arena.')\n            self.general_step_label.setToolTip('Once all settings are correct for a arena, click on \"Run All\" to start the full analysis.\\nIf the detection is unsatisfactory for a arena, you can repeat the detection for this\\narena and save the results by clicking \"Save One Result\".\\nRepeat the process for as many arenas as necessary.')\n            self.save_one_result.setVisible(True)\n            self.general_step_button.setVisible(False)\n\n    def reset_general_step(self):\n        \"\"\"\n        Reset the general step counter and update UI labels.\n        \"\"\"\n        self.current_step = 0\n        self.general_step_label.setText('Step 1: Tune parameters to improve Detection')\n        self.general_step_label.setToolTip('Detection uses only the visible parameters and those\\npreviously determined on the first or last image.')\n        self.general_step_button.setVisible(True)\n        self.second_step_widget.setVisible(False)\n\n    def full_screen_display(self, event):\n        \"\"\"\n        Full-screen display of an image.\n\n        This method creates a full-screen image popup and displays it. The\n        full-screen image is initialized with the current image to display,\n        and its size is set to match the screen dimensions.\n        \"\"\"\n        self.popup_img = FullScreenImage(self.parent().image_to_display, self.parent().screen_width, self.parent().screen_height)\n        self.popup_img.show()\n\n    def option_changed(self):\n        \"\"\"\n        Handles the logic for changing video option settings and logging the appropriate actions.\n\n        This method is responsible for updating various flags and configuration variables\n        based on the selected video option. It also logs informational messages regarding\n        the behavior of the segmentation algorithms being enabled or disabled.\n\n        Notes\n        -----\n        This function updates the parent object's configuration variables and logs messages\n        based on the selected video option. The behavior changes depending on the number of\n        colors detected and the specific video option chosen.\n        \"\"\"\n        self.parent().po.all['video_option'] = self.select_option.currentIndex()\n        self.parent().po.vars['frame_by_frame_segmentation'] = False\n        self.parent().po.vars['do_threshold_segmentation'] = False\n        self.parent().po.vars['do_slope_segmentation'] = False\n\n        if self.parent().po.vars['color_number'] &gt; 2 or self.parent().po.all['video_option'] == 0:\n            logging.info(f\"This option will detect {self.parent().po.vars['color_number']} distinct luminosity groups for each frame.\")\n            self.parent().po.vars['frame_by_frame_segmentation'] = True\n        else:\n            self.parent().po.vars['frame_by_frame_segmentation'] = False\n            if self.parent().po.all['video_option'] == 1:\n                logging.info(f\"This option will detect cell(s) using a dynamic threshold algorithm with a maximal growth factor of {self.parent().po.vars['maximal_growth_factor']}\")\n                self.parent().po.vars['do_threshold_segmentation'] = True\n            elif self.parent().po.all['video_option'] == 2:\n                logging.info(f\"This option will detect cell(s) using a dynamic slope algorithm with a maximal growth factor of {self.parent().po.vars['maximal_growth_factor']}\")\n                self.parent().po.vars['do_slope_segmentation'] = True\n            elif self.parent().po.all['video_option'] &gt; 2:\n                self.parent().po.vars['do_threshold_segmentation'] = True\n                self.parent().po.vars['do_slope_segmentation'] = True\n                if self.parent().po.all['video_option'] == 3:\n                    logging.info(f\"This option will detect cell(s) using the dynamic threshold AND slope algorithms with a maximal growth factor of {self.parent().po.vars['maximal_growth_factor']}\")\n                    self.parent().po.vars['true_if_use_light_AND_slope_else_OR'] = True\n                elif self.parent().po.all['video_option'] == 4:\n                    logging.info(f\"This option will detect cell(s) using the dynamic threshold OR slope algorithms with a maximal growth factor of {self.parent().po.vars['maximal_growth_factor']}\")\n                    self.parent().po.vars['true_if_use_light_AND_slope_else_OR'] = False\n\n    def data_tab_is_clicked(self):\n        \"\"\"\n        Handles the logic for when the \"Data specifications\" button is clicked in the interface,\n        leading to the FirstWindow.\n\n        Notes\n        -----\n        This function displays an error message when a thread relative to the current window is running.\n        This function also save the id of the following window for later use.\n        \"\"\"\n        if self.thread_dict['VideoReader'].isRunning() or self.thread_dict['OneArena'].isRunning() or self.thread_dict['ChangeOneRepResult'].isRunning() or self.parent().firstwindow.thread_dict[\"RunAll\"].isRunning():\n            self.message.setText(\"Wait for the analysis to end, or restart Cellects\")\n        else:\n            self.parent().last_tab = \"data_specifications\"\n            self.parent().change_widget(0)  # FirstWidget\n\n    def image_tab_is_clicked(self):\n        \"\"\"\n        Handles the logic for when the \"Image analysis\" button is clicked in the interface,\n        leading to the image analysis window.\n\n        Notes\n        -----\n        This function displays an error message when a thread relative to the current window is running.\n        This function also save the id of the following window for later use.\n        \"\"\"\n        if self.image_tab.state != \"not_usable\":\n            if self.thread_dict['VideoReader'].isRunning() or self.thread_dict['OneArena'].isRunning() or self.thread_dict[\n                'ChangeOneRepResult'].isRunning() or self.parent().firstwindow.thread_dict[\"RunAll\"].isRunning():\n                self.message.setText(\"Wait for the analysis to end, or restart Cellects\")\n            else:\n                self.parent().last_tab = \"video_analysis\"\n                self.parent().change_widget(2)\n\n\n    def required_outputs_is_clicked(self):\n        \"\"\"\n        Sets the required outputs flag and changes the widget to the \"Required Output\" window.\n        \"\"\"\n        self.parent().last_is_first = False\n        self.parent().change_widget(4)  # RequiredOutput\n\n    def advanced_parameters_is_clicked(self):\n        \"\"\"\n        Modifies the interface to display advanced parameters.\n        \"\"\"\n        self.parent().last_is_first = False\n        self.parent().widget(5).update_csc_editing_display()\n        self.parent().change_widget(5)  # AdvancedParameters\n\n    def previous_is_clicked(self):\n        \"\"\"\n        Transition to the previous tab based on current tab history.\n\n        This method handles the logic for navigating back through the\n        application's tabs when \"previous\" is clicked. It updates the current\n        tab to the one that was last visited, cycling through the predefined\n        order of tabs.\n\n        Notes\n        -----\n        This function is part of a state-machine-like navigation system that\n        tracks tab history. It assumes the parent widget has methods `last_tab`\n        and `change_widget` for managing the current view.\n        \"\"\"\n        if self.parent().last_tab == \"data_specifications\":\n            self.parent().change_widget(0)  # FirstWidget\n        elif self.parent().last_tab == \"image_analysis\":\n            self.parent().change_widget(2)  # ThirdWidget\n        self.parent().last_tab = \"video_analysis\"\n\n    def save_all_vars_thread(self):\n        \"\"\"\n        Start the 'SaveAllVars' thread if it is not already running.\n\n        This method is used to ensure that variable saving operations are performed\n        in a separate thread to avoid blocking the main application.\n        \"\"\"\n        if not self.parent().thread_dict['SaveAllVars'].isRunning():\n            self.parent().thread_dict['SaveAllVars'].start()  # SaveAllVarsThreadInThirdWidget\n\n    def save_current_settings(self):\n        \"\"\"\n        Saves the current settings from UI components to persistent storage.\n\n        This method captures the values of various UI components and stores\n        them in a persistent data structure to ensure settings are saved across\n        sessions.\n        \"\"\"\n        self.parent().po.vars['maximal_growth_factor'] = self.maximal_growth_factor.value()\n        self.parent().po.vars['repeat_video_smoothing'] = int(np.round(self.repeat_video_smoothing.value()))\n        self.parent().po.vars['do_fading'] = self.do_fading.isChecked()\n        self.parent().po.vars['fading'] = self.fading.value()\n        self.parent().po.all['compute_all_options'] = self.compute_all_options_cb.isChecked()\n        self.option_changed()\n        self.save_all_vars_thread()\n\n    def repeat_video_smoothing_changed(self):\n        \"\"\"\n        Save the repeat_video_smoothing spinbox value to set how many times the pixel intensity dynamics will be\n        smoothed.\n        \"\"\"\n        self.parent().po.vars['repeat_video_smoothing'] = int(np.round(self.repeat_video_smoothing.value()))\n\n    def do_fading_check(self):\n        \"\"\"\n        Save the fading checkbox value to allow cases where pixels can be left by the specimen(s).\n        \"\"\"\n        self.parent().po.vars['do_fading'] = self.do_fading.isChecked()\n        self.fading.setVisible(self.parent().po.vars['do_fading'])\n\n    def fading_changed(self):\n        \"\"\"\n        Save the fading spinbox value to modify how intensity must decrease to detect a pixel left by the specimen(s).\n        \"\"\"\n        self.parent().po.vars['fading'] = self.fading.value()\n\n    def maximal_growth_factor_changed(self):\n        \"\"\"\n        Save the maximal_growth_factor spinbox value to modulate the maximal growth between two frames.\n        \"\"\"\n        self.parent().po.vars['maximal_growth_factor'] = self.maximal_growth_factor.value()\n\n    def arena_changed(self):\n        \"\"\"\n        Resets the loaded arena when its video and processing threads are not running.\n\n        Notes\n        -----\n        This function is part of a larger class responsible for managing video and\n        arena processing threads. It should be called when all relevant threads are not\n        running to ensure the arena's state is properly reset.\n        \"\"\"\n        if not self.thread_dict['VideoReader'].isRunning() and not self.thread_dict['OneArena'].isRunning() and not self.thread_dict['ChangeOneRepResult'].isRunning():\n            self.parent().po.motion = None\n            self.reset_general_step()\n            self.parent().po.computed_video_options = np.zeros(5, bool)\n            self.parent().po.all['arena'] = int(np.round(self.arena.value()))\n\n    def load_one_arena_is_clicked(self):\n        \"\"\"\n        Load one arena if clicked.\n\n        Resets the general step, sets `load_quick_full` to 0, and runs the arena in a separate thread.\n        \"\"\"\n        self.reset_general_step()\n        self.parent().po.load_quick_full = 0\n        self.run_one_arena_thread()\n\n    def compute_all_options_check(self):\n        \"\"\"\n        Save the compute_all_options checkbox value to process every video segmentation algorithms during the next run.\n        \"\"\"\n        self.parent().po.all['compute_all_options'] = self.compute_all_options_cb.isChecked()\n\n    def detection_is_clicked(self):\n        \"\"\"\n        Trigger detection when a button is clicked.\n\n        This method handles the logic when the user clicks the \"detection\" button.\n        It resets certain states, sets a flag for quick full processing,\n        and starts a thread to run the detection in one arena.\n\n        Notes\n        -----\n        This method is part of a larger state machine for handling user interactions.\n        It assumes that the parent object has a `po` attribute with a `load_quick_full`\n        flag and a method to run an arena thread.\n        \"\"\"\n        self.reset_general_step()\n        self.parent().po.load_quick_full = 1\n        self.run_one_arena_thread()\n\n    def post_processing_is_clicked(self):\n        \"\"\"\n        Trigger post-processing when a button is clicked.\n\n        Extended Description\n        -------------------\n        This function updates the parent object's load_quick_full attribute,\n        logs a specific variable value, and runs an arena thread.\n        \"\"\"\n        self.parent().po.load_quick_full = 2\n        logging.info(self.parent().po.vars['maximal_growth_factor'])\n        self.run_one_arena_thread()\n\n    def run_one_arena_thread(self):\n        \"\"\"\n        Run the OneArena thread for processing.\n\n        Executes the OneArena thread to load video, initialize analysis,\n        stop any running instance of the thread, save settings, and connect\n        signals for displaying messages, images, and handling completion events.\n\n        Notes\n        -----\n        Ensures that the previous arena settings are cleared and connects signals\n        to display messages and images during thread execution.\n        \"\"\"\n        if self.thread_dict['OneArena']._isRunning:\n            self.thread_dict['OneArena'].stop()\n        self.save_current_settings()\n        if self.previous_arena != self.parent().po.all['arena']:\n            self.parent().po.motion = None\n        self.message.setText(\"Load the video and initialize analysis, wait...\")\n        self.thread_dict['OneArena'].start()  # OneArenaThreadInThirdWidget\n        self.thread_dict['OneArena'].message_from_thread_starting.connect(self.display_message_from_thread)\n        self.thread_dict['OneArena'].when_loading_finished.connect(self.when_loading_thread_finished)\n        self.thread_dict['OneArena'].when_detection_finished.connect(self.when_detection_finished)\n        self.thread_dict['OneArena'].image_from_thread.connect(self.display_image_during_thread)\n\n    def when_loading_thread_finished(self, save_loaded_video: bool):\n        \"\"\"\n        Ends the loading thread process and handles post-loading actions.\n\n        Notes\n        ----------\n        This method assumes that the parent object has a `po` attribute with an\n        'arena' key and a `load_quick_full` attribute. It also assumes that the\n        parent object has a 'thread' dictionary and a message UI component.\n        \"\"\"\n        self.previous_arena = self.parent().po.all['arena']\n        if save_loaded_video:\n            self.thread_dict['WriteVideo'] = WriteVideoThread(self.parent())\n            self.thread_dict['WriteVideo'].start()\n        if self.parent().po.load_quick_full == 0:\n            self.message.setText(\"Loading done, you can watch the video\")\n        self.read.setVisible(True)\n\n    def when_detection_finished(self, message: str):\n        \"\"\"\n        Handles the completion of video detection and updates the UI accordingly.\n\n        When the video detection is finished, this function waits for the\n        VideoReader thread to complete if it's running. It then processes the\n        last frame of the video based on the configured visualization and motion\n        detection settings. Finally, it updates the UI with the processed image\n        and sets appropriate labels' visibility.\n\n        Parameters\n        ----------\n        message : str\n            The message to display upon completion of detection.\n            This could be a status update or any relevant information.\n\n        Notes\n        -----\n        This function assumes that the parent object has attributes `po` and\n        `image_to_display`, and methods like `display_image.update_image`.\n        \"\"\"\n        self.previous_arena = self.parent().po.all['arena']\n        if self.thread_dict['VideoReader'].isRunning():  # VideoReaderThreadInThirdWidget\n            self.thread_dict['VideoReader'].wait()\n        if self.parent().po.load_quick_full &gt; 0:\n            image = self.parent().po.motion.segmented[-1, ...]\n        if self.parent().po.motion.visu is None:\n            image = self.parent().po.motion.converted_video[-1, ...] * (1 - image)\n            image = np.round(image).astype(np.uint8)\n            image = np.stack((image, image, image), axis=2)\n        else:\n            image = np.stack((image, image, image), axis=2)\n            image = self.parent().po.motion.visu[-1, ...] * (1 - image)\n        self.parent().image_to_display = image\n        self.display_image.update_image(image)\n        self.message.setText(message)\n        self.select_option_label.setVisible(self.parent().po.vars[\"color_number\"] == 2)\n        self.select_option.setVisible(self.parent().po.vars[\"color_number\"] == 2)\n        self.read.setVisible(True)\n\n    def display_image_during_thread(self, dictionary: dict):\n        \"\"\"\n        Display an image and set a message during a thread operation.\n\n        Parameters\n        ----------\n        dictionary : dict\n            A dictionary containing the 'message' and 'current_image'.\n                The message is a string to display.\n                The current_image is the image data that will be displayed.\n        \"\"\"\n        self.message.setText(dictionary['message'])\n        self.parent().image_to_display = dictionary['current_image']\n        self.display_image.update_image(dictionary['current_image'])\n\n    def save_one_result_is_clicked(self):\n        \"\"\"\n        Finalize one arena analysis and save the result if conditions are met.\n\n        This function checks various conditions before starting a thread to\n        finalize the analysis and save the result. It ensures that certain\n        threads are not running before proceeding.\n        \"\"\"\n        if self.parent().po.motion is not None:\n            if self.parent().po.load_quick_full == 2:\n                if not self.thread_dict['OneArena'].isRunning() and not self.thread_dict['ChangeOneRepResult'].isRunning():\n                    self.message.setText(f\"Arena {self.parent().po.all['arena']}: Finalize analysis and save, wait...\")\n                    self.thread_dict['ChangeOneRepResult'].start()  # ChangeOneRepResultThreadInThirdWidget\n                    self.thread_dict['ChangeOneRepResult'].message_from_thread.connect(self.display_message_from_thread)\n                    self.message.setText(\"Complete analysis + change that result\")\n                else:\n                    self.message.setText(\"Wait for the analysis to end\")\n            else:\n                self.message.setText(\"Run Post processing first\")\n        else:\n            self.message.setText(\"Run Post processing first\")\n\n    def read_is_clicked(self):\n        \"\"\"\n        Read a video corresponding to a numbered arena (numbered using natural sorting) in the image\n\n        This function checks if the detection has been run and if the video reader or analysis thread is running.\n        If both threads are idle, it starts the video reading process. Otherwise, it updates the message accordingly.\n        \"\"\"\n        if self.parent().po.motion is not None:\n            if self.parent().po.motion.segmented is not None:\n                if not self.thread_dict['OneArena'].isRunning() and not self.thread_dict['VideoReader'].isRunning():\n                    self.thread_dict['VideoReader'].start()  # VideoReaderThreadInThirdWidget\n                    self.thread_dict['VideoReader'].message_from_thread.connect(self.display_image_during_thread)\n                else:\n                    self.message.setText(\"Wait for the analysis to end\")\n            else:\n                self.message.setText(\"Run detection first\")\n        else:\n            self.message.setText(\"Run detection first\")\n\n    def run_all_is_clicked(self):\n        \"\"\"\n        Handle the click event to start the complete analysis.\n\n        This function checks if any threads are running and starts the\n        'RunAll' thread if none of them are active. It also updates\n        various attributes and messages related to the analysis process.\n\n        Notes\n        -----\n        This function will only start the analysis if no other threads\n        are running. It updates several attributes of the parent object.\n        \"\"\"\n        if self.thread_dict['OneArena'].isRunning() or self.thread_dict['ChangeOneRepResult'].isRunning():\n            self.message.setText(\"Wait for the current analysis to end\")\n        else:\n            if self.thread_dict['VideoReader'].isRunning():\n                self.thread_dict['VideoReader'].wait()\n            if self.parent().firstwindow.thread_dict[\"RunAll\"].isRunning():\n                self.message.setText('Analysis has already begun in the first window.')\n            else:\n                if not self.thread_dict['RunAll'].isRunning():\n                    self.save_current_settings()\n                    self.parent().po.motion = None\n                    self.parent().po.converted_video = None\n                    self.parent().po.converted_video2 = None\n                    self.parent().po.visu = None\n                    self.message.setText(\"Complete analysis has started, wait...\")\n                    self.thread_dict['RunAll'].start()  # RunAllThread\n                    self.thread_dict['RunAll'].message_from_thread.connect(self.display_message_from_thread)\n                    self.thread_dict['RunAll'].image_from_thread.connect(self.display_image_during_thread)\n\n    def display_message_from_thread(self, text_from_thread: str):\n        \"\"\"\n        Updates the message displayed in the UI with text from a thread.\n\n        Parameters\n        ----------\n        text_from_thread : str\n            The text to be displayed in the UI message.\n        \"\"\"\n        self.message.setText(text_from_thread)\n</code></pre>"},{"location":"api/cellects/gui/video_analysis_window/#cellects.gui.video_analysis_window.VideoAnalysisWindow.__init__","title":"<code>__init__(parent, night_mode)</code>","text":"<p>Initialize the VideoAnalysis window with a parent widget and night mode setting.</p> <p>Parameters:</p> Name Type Description Default <code>parent</code> <code>QWidget</code> <p>The parent widget to which this window will be attached.</p> required <code>night_mode</code> <code>bool</code> <p>A boolean indicating whether the night mode should be enabled.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from PySide6 import QtWidgets\n&gt;&gt;&gt; from cellects.gui.cellects import CellectsMainWidget\n&gt;&gt;&gt; from cellects.gui.video_analysis_window import VideoAnalysisWindow\n&gt;&gt;&gt; import sys\n&gt;&gt;&gt; app = QtWidgets.QApplication([])\n&gt;&gt;&gt; parent = CellectsMainWidget()\n&gt;&gt;&gt; session = VideoAnalysisWindow(parent, False)\n&gt;&gt;&gt; session.true_init()\n&gt;&gt;&gt; parent.insertWidget(0, session)\n&gt;&gt;&gt; parent.show()\n&gt;&gt;&gt; sys.exit(app.exec())\n</code></pre> Source code in <code>src/cellects/gui/video_analysis_window.py</code> <pre><code>def __init__(self, parent, night_mode):\n    \"\"\"\n    Initialize the VideoAnalysis window with a parent widget and night mode setting.\n\n    Parameters\n    ----------\n    parent : QWidget\n        The parent widget to which this window will be attached.\n    night_mode : bool\n        A boolean indicating whether the night mode should be enabled.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from PySide6 import QtWidgets\n    &gt;&gt;&gt; from cellects.gui.cellects import CellectsMainWidget\n    &gt;&gt;&gt; from cellects.gui.video_analysis_window import VideoAnalysisWindow\n    &gt;&gt;&gt; import sys\n    &gt;&gt;&gt; app = QtWidgets.QApplication([])\n    &gt;&gt;&gt; parent = CellectsMainWidget()\n    &gt;&gt;&gt; session = VideoAnalysisWindow(parent, False)\n    &gt;&gt;&gt; session.true_init()\n    &gt;&gt;&gt; parent.insertWidget(0, session)\n    &gt;&gt;&gt; parent.show()\n    &gt;&gt;&gt; sys.exit(app.exec())\n    \"\"\"\n    super().__init__(parent, night_mode)\n    logging.info(\"Initialize VideoAnalysisWindow\")\n    self.setParent(parent)\n    self.true_init()\n</code></pre>"},{"location":"api/cellects/gui/video_analysis_window/#cellects.gui.video_analysis_window.VideoAnalysisWindow.advanced_parameters_is_clicked","title":"<code>advanced_parameters_is_clicked()</code>","text":"<p>Modifies the interface to display advanced parameters.</p> Source code in <code>src/cellects/gui/video_analysis_window.py</code> <pre><code>def advanced_parameters_is_clicked(self):\n    \"\"\"\n    Modifies the interface to display advanced parameters.\n    \"\"\"\n    self.parent().last_is_first = False\n    self.parent().widget(5).update_csc_editing_display()\n    self.parent().change_widget(5)  # AdvancedParameters\n</code></pre>"},{"location":"api/cellects/gui/video_analysis_window/#cellects.gui.video_analysis_window.VideoAnalysisWindow.arena_changed","title":"<code>arena_changed()</code>","text":"<p>Resets the loaded arena when its video and processing threads are not running.</p> Notes <p>This function is part of a larger class responsible for managing video and arena processing threads. It should be called when all relevant threads are not running to ensure the arena's state is properly reset.</p> Source code in <code>src/cellects/gui/video_analysis_window.py</code> <pre><code>def arena_changed(self):\n    \"\"\"\n    Resets the loaded arena when its video and processing threads are not running.\n\n    Notes\n    -----\n    This function is part of a larger class responsible for managing video and\n    arena processing threads. It should be called when all relevant threads are not\n    running to ensure the arena's state is properly reset.\n    \"\"\"\n    if not self.thread_dict['VideoReader'].isRunning() and not self.thread_dict['OneArena'].isRunning() and not self.thread_dict['ChangeOneRepResult'].isRunning():\n        self.parent().po.motion = None\n        self.reset_general_step()\n        self.parent().po.computed_video_options = np.zeros(5, bool)\n        self.parent().po.all['arena'] = int(np.round(self.arena.value()))\n</code></pre>"},{"location":"api/cellects/gui/video_analysis_window/#cellects.gui.video_analysis_window.VideoAnalysisWindow.compute_all_options_check","title":"<code>compute_all_options_check()</code>","text":"<p>Save the compute_all_options checkbox value to process every video segmentation algorithms during the next run.</p> Source code in <code>src/cellects/gui/video_analysis_window.py</code> <pre><code>def compute_all_options_check(self):\n    \"\"\"\n    Save the compute_all_options checkbox value to process every video segmentation algorithms during the next run.\n    \"\"\"\n    self.parent().po.all['compute_all_options'] = self.compute_all_options_cb.isChecked()\n</code></pre>"},{"location":"api/cellects/gui/video_analysis_window/#cellects.gui.video_analysis_window.VideoAnalysisWindow.data_tab_is_clicked","title":"<code>data_tab_is_clicked()</code>","text":"<p>Handles the logic for when the \"Data specifications\" button is clicked in the interface, leading to the FirstWindow.</p> Notes <p>This function displays an error message when a thread relative to the current window is running. This function also save the id of the following window for later use.</p> Source code in <code>src/cellects/gui/video_analysis_window.py</code> <pre><code>def data_tab_is_clicked(self):\n    \"\"\"\n    Handles the logic for when the \"Data specifications\" button is clicked in the interface,\n    leading to the FirstWindow.\n\n    Notes\n    -----\n    This function displays an error message when a thread relative to the current window is running.\n    This function also save the id of the following window for later use.\n    \"\"\"\n    if self.thread_dict['VideoReader'].isRunning() or self.thread_dict['OneArena'].isRunning() or self.thread_dict['ChangeOneRepResult'].isRunning() or self.parent().firstwindow.thread_dict[\"RunAll\"].isRunning():\n        self.message.setText(\"Wait for the analysis to end, or restart Cellects\")\n    else:\n        self.parent().last_tab = \"data_specifications\"\n        self.parent().change_widget(0)  # FirstWidget\n</code></pre>"},{"location":"api/cellects/gui/video_analysis_window/#cellects.gui.video_analysis_window.VideoAnalysisWindow.detection_is_clicked","title":"<code>detection_is_clicked()</code>","text":"<p>Trigger detection when a button is clicked.</p> <p>This method handles the logic when the user clicks the \"detection\" button. It resets certain states, sets a flag for quick full processing, and starts a thread to run the detection in one arena.</p> Notes <p>This method is part of a larger state machine for handling user interactions. It assumes that the parent object has a <code>po</code> attribute with a <code>load_quick_full</code> flag and a method to run an arena thread.</p> Source code in <code>src/cellects/gui/video_analysis_window.py</code> <pre><code>def detection_is_clicked(self):\n    \"\"\"\n    Trigger detection when a button is clicked.\n\n    This method handles the logic when the user clicks the \"detection\" button.\n    It resets certain states, sets a flag for quick full processing,\n    and starts a thread to run the detection in one arena.\n\n    Notes\n    -----\n    This method is part of a larger state machine for handling user interactions.\n    It assumes that the parent object has a `po` attribute with a `load_quick_full`\n    flag and a method to run an arena thread.\n    \"\"\"\n    self.reset_general_step()\n    self.parent().po.load_quick_full = 1\n    self.run_one_arena_thread()\n</code></pre>"},{"location":"api/cellects/gui/video_analysis_window/#cellects.gui.video_analysis_window.VideoAnalysisWindow.display_conditionally_visible_widgets","title":"<code>display_conditionally_visible_widgets()</code>","text":"<p>Display Conditionally Visible Widgets</p> Source code in <code>src/cellects/gui/video_analysis_window.py</code> <pre><code>def display_conditionally_visible_widgets(self):\n    \"\"\"\n    Display Conditionally Visible Widgets\n    \"\"\"\n    self.select_option_label.setVisible(self.parent().po.vars[\"color_number\"] == 2)\n    self.select_option.setVisible(self.parent().po.vars[\"color_number\"] == 2)\n    self.fading.setVisible(self.parent().po.vars['do_fading'])\n</code></pre>"},{"location":"api/cellects/gui/video_analysis_window/#cellects.gui.video_analysis_window.VideoAnalysisWindow.display_image_during_thread","title":"<code>display_image_during_thread(dictionary)</code>","text":"<p>Display an image and set a message during a thread operation.</p> <p>Parameters:</p> Name Type Description Default <code>dictionary</code> <code>dict</code> <p>A dictionary containing the 'message' and 'current_image'.     The message is a string to display.     The current_image is the image data that will be displayed.</p> required Source code in <code>src/cellects/gui/video_analysis_window.py</code> <pre><code>def display_image_during_thread(self, dictionary: dict):\n    \"\"\"\n    Display an image and set a message during a thread operation.\n\n    Parameters\n    ----------\n    dictionary : dict\n        A dictionary containing the 'message' and 'current_image'.\n            The message is a string to display.\n            The current_image is the image data that will be displayed.\n    \"\"\"\n    self.message.setText(dictionary['message'])\n    self.parent().image_to_display = dictionary['current_image']\n    self.display_image.update_image(dictionary['current_image'])\n</code></pre>"},{"location":"api/cellects/gui/video_analysis_window/#cellects.gui.video_analysis_window.VideoAnalysisWindow.display_message_from_thread","title":"<code>display_message_from_thread(text_from_thread)</code>","text":"<p>Updates the message displayed in the UI with text from a thread.</p> <p>Parameters:</p> Name Type Description Default <code>text_from_thread</code> <code>str</code> <p>The text to be displayed in the UI message.</p> required Source code in <code>src/cellects/gui/video_analysis_window.py</code> <pre><code>def display_message_from_thread(self, text_from_thread: str):\n    \"\"\"\n    Updates the message displayed in the UI with text from a thread.\n\n    Parameters\n    ----------\n    text_from_thread : str\n        The text to be displayed in the UI message.\n    \"\"\"\n    self.message.setText(text_from_thread)\n</code></pre>"},{"location":"api/cellects/gui/video_analysis_window/#cellects.gui.video_analysis_window.VideoAnalysisWindow.do_fading_check","title":"<code>do_fading_check()</code>","text":"<p>Save the fading checkbox value to allow cases where pixels can be left by the specimen(s).</p> Source code in <code>src/cellects/gui/video_analysis_window.py</code> <pre><code>def do_fading_check(self):\n    \"\"\"\n    Save the fading checkbox value to allow cases where pixels can be left by the specimen(s).\n    \"\"\"\n    self.parent().po.vars['do_fading'] = self.do_fading.isChecked()\n    self.fading.setVisible(self.parent().po.vars['do_fading'])\n</code></pre>"},{"location":"api/cellects/gui/video_analysis_window/#cellects.gui.video_analysis_window.VideoAnalysisWindow.fading_changed","title":"<code>fading_changed()</code>","text":"<p>Save the fading spinbox value to modify how intensity must decrease to detect a pixel left by the specimen(s).</p> Source code in <code>src/cellects/gui/video_analysis_window.py</code> <pre><code>def fading_changed(self):\n    \"\"\"\n    Save the fading spinbox value to modify how intensity must decrease to detect a pixel left by the specimen(s).\n    \"\"\"\n    self.parent().po.vars['fading'] = self.fading.value()\n</code></pre>"},{"location":"api/cellects/gui/video_analysis_window/#cellects.gui.video_analysis_window.VideoAnalysisWindow.full_screen_display","title":"<code>full_screen_display(event)</code>","text":"<p>Full-screen display of an image.</p> <p>This method creates a full-screen image popup and displays it. The full-screen image is initialized with the current image to display, and its size is set to match the screen dimensions.</p> Source code in <code>src/cellects/gui/video_analysis_window.py</code> <pre><code>def full_screen_display(self, event):\n    \"\"\"\n    Full-screen display of an image.\n\n    This method creates a full-screen image popup and displays it. The\n    full-screen image is initialized with the current image to display,\n    and its size is set to match the screen dimensions.\n    \"\"\"\n    self.popup_img = FullScreenImage(self.parent().image_to_display, self.parent().screen_width, self.parent().screen_height)\n    self.popup_img.show()\n</code></pre>"},{"location":"api/cellects/gui/video_analysis_window/#cellects.gui.video_analysis_window.VideoAnalysisWindow.image_tab_is_clicked","title":"<code>image_tab_is_clicked()</code>","text":"<p>Handles the logic for when the \"Image analysis\" button is clicked in the interface, leading to the image analysis window.</p> Notes <p>This function displays an error message when a thread relative to the current window is running. This function also save the id of the following window for later use.</p> Source code in <code>src/cellects/gui/video_analysis_window.py</code> <pre><code>def image_tab_is_clicked(self):\n    \"\"\"\n    Handles the logic for when the \"Image analysis\" button is clicked in the interface,\n    leading to the image analysis window.\n\n    Notes\n    -----\n    This function displays an error message when a thread relative to the current window is running.\n    This function also save the id of the following window for later use.\n    \"\"\"\n    if self.image_tab.state != \"not_usable\":\n        if self.thread_dict['VideoReader'].isRunning() or self.thread_dict['OneArena'].isRunning() or self.thread_dict[\n            'ChangeOneRepResult'].isRunning() or self.parent().firstwindow.thread_dict[\"RunAll\"].isRunning():\n            self.message.setText(\"Wait for the analysis to end, or restart Cellects\")\n        else:\n            self.parent().last_tab = \"video_analysis\"\n            self.parent().change_widget(2)\n</code></pre>"},{"location":"api/cellects/gui/video_analysis_window/#cellects.gui.video_analysis_window.VideoAnalysisWindow.load_one_arena_is_clicked","title":"<code>load_one_arena_is_clicked()</code>","text":"<p>Load one arena if clicked.</p> <p>Resets the general step, sets <code>load_quick_full</code> to 0, and runs the arena in a separate thread.</p> Source code in <code>src/cellects/gui/video_analysis_window.py</code> <pre><code>def load_one_arena_is_clicked(self):\n    \"\"\"\n    Load one arena if clicked.\n\n    Resets the general step, sets `load_quick_full` to 0, and runs the arena in a separate thread.\n    \"\"\"\n    self.reset_general_step()\n    self.parent().po.load_quick_full = 0\n    self.run_one_arena_thread()\n</code></pre>"},{"location":"api/cellects/gui/video_analysis_window/#cellects.gui.video_analysis_window.VideoAnalysisWindow.maximal_growth_factor_changed","title":"<code>maximal_growth_factor_changed()</code>","text":"<p>Save the maximal_growth_factor spinbox value to modulate the maximal growth between two frames.</p> Source code in <code>src/cellects/gui/video_analysis_window.py</code> <pre><code>def maximal_growth_factor_changed(self):\n    \"\"\"\n    Save the maximal_growth_factor spinbox value to modulate the maximal growth between two frames.\n    \"\"\"\n    self.parent().po.vars['maximal_growth_factor'] = self.maximal_growth_factor.value()\n</code></pre>"},{"location":"api/cellects/gui/video_analysis_window/#cellects.gui.video_analysis_window.VideoAnalysisWindow.option_changed","title":"<code>option_changed()</code>","text":"<p>Handles the logic for changing video option settings and logging the appropriate actions.</p> <p>This method is responsible for updating various flags and configuration variables based on the selected video option. It also logs informational messages regarding the behavior of the segmentation algorithms being enabled or disabled.</p> Notes <p>This function updates the parent object's configuration variables and logs messages based on the selected video option. The behavior changes depending on the number of colors detected and the specific video option chosen.</p> Source code in <code>src/cellects/gui/video_analysis_window.py</code> <pre><code>def option_changed(self):\n    \"\"\"\n    Handles the logic for changing video option settings and logging the appropriate actions.\n\n    This method is responsible for updating various flags and configuration variables\n    based on the selected video option. It also logs informational messages regarding\n    the behavior of the segmentation algorithms being enabled or disabled.\n\n    Notes\n    -----\n    This function updates the parent object's configuration variables and logs messages\n    based on the selected video option. The behavior changes depending on the number of\n    colors detected and the specific video option chosen.\n    \"\"\"\n    self.parent().po.all['video_option'] = self.select_option.currentIndex()\n    self.parent().po.vars['frame_by_frame_segmentation'] = False\n    self.parent().po.vars['do_threshold_segmentation'] = False\n    self.parent().po.vars['do_slope_segmentation'] = False\n\n    if self.parent().po.vars['color_number'] &gt; 2 or self.parent().po.all['video_option'] == 0:\n        logging.info(f\"This option will detect {self.parent().po.vars['color_number']} distinct luminosity groups for each frame.\")\n        self.parent().po.vars['frame_by_frame_segmentation'] = True\n    else:\n        self.parent().po.vars['frame_by_frame_segmentation'] = False\n        if self.parent().po.all['video_option'] == 1:\n            logging.info(f\"This option will detect cell(s) using a dynamic threshold algorithm with a maximal growth factor of {self.parent().po.vars['maximal_growth_factor']}\")\n            self.parent().po.vars['do_threshold_segmentation'] = True\n        elif self.parent().po.all['video_option'] == 2:\n            logging.info(f\"This option will detect cell(s) using a dynamic slope algorithm with a maximal growth factor of {self.parent().po.vars['maximal_growth_factor']}\")\n            self.parent().po.vars['do_slope_segmentation'] = True\n        elif self.parent().po.all['video_option'] &gt; 2:\n            self.parent().po.vars['do_threshold_segmentation'] = True\n            self.parent().po.vars['do_slope_segmentation'] = True\n            if self.parent().po.all['video_option'] == 3:\n                logging.info(f\"This option will detect cell(s) using the dynamic threshold AND slope algorithms with a maximal growth factor of {self.parent().po.vars['maximal_growth_factor']}\")\n                self.parent().po.vars['true_if_use_light_AND_slope_else_OR'] = True\n            elif self.parent().po.all['video_option'] == 4:\n                logging.info(f\"This option will detect cell(s) using the dynamic threshold OR slope algorithms with a maximal growth factor of {self.parent().po.vars['maximal_growth_factor']}\")\n                self.parent().po.vars['true_if_use_light_AND_slope_else_OR'] = False\n</code></pre>"},{"location":"api/cellects/gui/video_analysis_window/#cellects.gui.video_analysis_window.VideoAnalysisWindow.post_processing_is_clicked","title":"<code>post_processing_is_clicked()</code>","text":"<p>Trigger post-processing when a button is clicked.</p> Extended Description <p>This function updates the parent object's load_quick_full attribute, logs a specific variable value, and runs an arena thread.</p> Source code in <code>src/cellects/gui/video_analysis_window.py</code> <pre><code>def post_processing_is_clicked(self):\n    \"\"\"\n    Trigger post-processing when a button is clicked.\n\n    Extended Description\n    -------------------\n    This function updates the parent object's load_quick_full attribute,\n    logs a specific variable value, and runs an arena thread.\n    \"\"\"\n    self.parent().po.load_quick_full = 2\n    logging.info(self.parent().po.vars['maximal_growth_factor'])\n    self.run_one_arena_thread()\n</code></pre>"},{"location":"api/cellects/gui/video_analysis_window/#cellects.gui.video_analysis_window.VideoAnalysisWindow.previous_is_clicked","title":"<code>previous_is_clicked()</code>","text":"<p>Transition to the previous tab based on current tab history.</p> <p>This method handles the logic for navigating back through the application's tabs when \"previous\" is clicked. It updates the current tab to the one that was last visited, cycling through the predefined order of tabs.</p> Notes <p>This function is part of a state-machine-like navigation system that tracks tab history. It assumes the parent widget has methods <code>last_tab</code> and <code>change_widget</code> for managing the current view.</p> Source code in <code>src/cellects/gui/video_analysis_window.py</code> <pre><code>def previous_is_clicked(self):\n    \"\"\"\n    Transition to the previous tab based on current tab history.\n\n    This method handles the logic for navigating back through the\n    application's tabs when \"previous\" is clicked. It updates the current\n    tab to the one that was last visited, cycling through the predefined\n    order of tabs.\n\n    Notes\n    -----\n    This function is part of a state-machine-like navigation system that\n    tracks tab history. It assumes the parent widget has methods `last_tab`\n    and `change_widget` for managing the current view.\n    \"\"\"\n    if self.parent().last_tab == \"data_specifications\":\n        self.parent().change_widget(0)  # FirstWidget\n    elif self.parent().last_tab == \"image_analysis\":\n        self.parent().change_widget(2)  # ThirdWidget\n    self.parent().last_tab = \"video_analysis\"\n</code></pre>"},{"location":"api/cellects/gui/video_analysis_window/#cellects.gui.video_analysis_window.VideoAnalysisWindow.read_is_clicked","title":"<code>read_is_clicked()</code>","text":"<p>Read a video corresponding to a numbered arena (numbered using natural sorting) in the image</p> <p>This function checks if the detection has been run and if the video reader or analysis thread is running. If both threads are idle, it starts the video reading process. Otherwise, it updates the message accordingly.</p> Source code in <code>src/cellects/gui/video_analysis_window.py</code> <pre><code>def read_is_clicked(self):\n    \"\"\"\n    Read a video corresponding to a numbered arena (numbered using natural sorting) in the image\n\n    This function checks if the detection has been run and if the video reader or analysis thread is running.\n    If both threads are idle, it starts the video reading process. Otherwise, it updates the message accordingly.\n    \"\"\"\n    if self.parent().po.motion is not None:\n        if self.parent().po.motion.segmented is not None:\n            if not self.thread_dict['OneArena'].isRunning() and not self.thread_dict['VideoReader'].isRunning():\n                self.thread_dict['VideoReader'].start()  # VideoReaderThreadInThirdWidget\n                self.thread_dict['VideoReader'].message_from_thread.connect(self.display_image_during_thread)\n            else:\n                self.message.setText(\"Wait for the analysis to end\")\n        else:\n            self.message.setText(\"Run detection first\")\n    else:\n        self.message.setText(\"Run detection first\")\n</code></pre>"},{"location":"api/cellects/gui/video_analysis_window/#cellects.gui.video_analysis_window.VideoAnalysisWindow.repeat_video_smoothing_changed","title":"<code>repeat_video_smoothing_changed()</code>","text":"<p>Save the repeat_video_smoothing spinbox value to set how many times the pixel intensity dynamics will be smoothed.</p> Source code in <code>src/cellects/gui/video_analysis_window.py</code> <pre><code>def repeat_video_smoothing_changed(self):\n    \"\"\"\n    Save the repeat_video_smoothing spinbox value to set how many times the pixel intensity dynamics will be\n    smoothed.\n    \"\"\"\n    self.parent().po.vars['repeat_video_smoothing'] = int(np.round(self.repeat_video_smoothing.value()))\n</code></pre>"},{"location":"api/cellects/gui/video_analysis_window/#cellects.gui.video_analysis_window.VideoAnalysisWindow.required_outputs_is_clicked","title":"<code>required_outputs_is_clicked()</code>","text":"<p>Sets the required outputs flag and changes the widget to the \"Required Output\" window.</p> Source code in <code>src/cellects/gui/video_analysis_window.py</code> <pre><code>def required_outputs_is_clicked(self):\n    \"\"\"\n    Sets the required outputs flag and changes the widget to the \"Required Output\" window.\n    \"\"\"\n    self.parent().last_is_first = False\n    self.parent().change_widget(4)  # RequiredOutput\n</code></pre>"},{"location":"api/cellects/gui/video_analysis_window/#cellects.gui.video_analysis_window.VideoAnalysisWindow.reset_general_step","title":"<code>reset_general_step()</code>","text":"<p>Reset the general step counter and update UI labels.</p> Source code in <code>src/cellects/gui/video_analysis_window.py</code> <pre><code>def reset_general_step(self):\n    \"\"\"\n    Reset the general step counter and update UI labels.\n    \"\"\"\n    self.current_step = 0\n    self.general_step_label.setText('Step 1: Tune parameters to improve Detection')\n    self.general_step_label.setToolTip('Detection uses only the visible parameters and those\\npreviously determined on the first or last image.')\n    self.general_step_button.setVisible(True)\n    self.second_step_widget.setVisible(False)\n</code></pre>"},{"location":"api/cellects/gui/video_analysis_window/#cellects.gui.video_analysis_window.VideoAnalysisWindow.run_all_is_clicked","title":"<code>run_all_is_clicked()</code>","text":"<p>Handle the click event to start the complete analysis.</p> <p>This function checks if any threads are running and starts the 'RunAll' thread if none of them are active. It also updates various attributes and messages related to the analysis process.</p> Notes <p>This function will only start the analysis if no other threads are running. It updates several attributes of the parent object.</p> Source code in <code>src/cellects/gui/video_analysis_window.py</code> <pre><code>def run_all_is_clicked(self):\n    \"\"\"\n    Handle the click event to start the complete analysis.\n\n    This function checks if any threads are running and starts the\n    'RunAll' thread if none of them are active. It also updates\n    various attributes and messages related to the analysis process.\n\n    Notes\n    -----\n    This function will only start the analysis if no other threads\n    are running. It updates several attributes of the parent object.\n    \"\"\"\n    if self.thread_dict['OneArena'].isRunning() or self.thread_dict['ChangeOneRepResult'].isRunning():\n        self.message.setText(\"Wait for the current analysis to end\")\n    else:\n        if self.thread_dict['VideoReader'].isRunning():\n            self.thread_dict['VideoReader'].wait()\n        if self.parent().firstwindow.thread_dict[\"RunAll\"].isRunning():\n            self.message.setText('Analysis has already begun in the first window.')\n        else:\n            if not self.thread_dict['RunAll'].isRunning():\n                self.save_current_settings()\n                self.parent().po.motion = None\n                self.parent().po.converted_video = None\n                self.parent().po.converted_video2 = None\n                self.parent().po.visu = None\n                self.message.setText(\"Complete analysis has started, wait...\")\n                self.thread_dict['RunAll'].start()  # RunAllThread\n                self.thread_dict['RunAll'].message_from_thread.connect(self.display_message_from_thread)\n                self.thread_dict['RunAll'].image_from_thread.connect(self.display_image_during_thread)\n</code></pre>"},{"location":"api/cellects/gui/video_analysis_window/#cellects.gui.video_analysis_window.VideoAnalysisWindow.run_one_arena_thread","title":"<code>run_one_arena_thread()</code>","text":"<p>Run the OneArena thread for processing.</p> <p>Executes the OneArena thread to load video, initialize analysis, stop any running instance of the thread, save settings, and connect signals for displaying messages, images, and handling completion events.</p> Notes <p>Ensures that the previous arena settings are cleared and connects signals to display messages and images during thread execution.</p> Source code in <code>src/cellects/gui/video_analysis_window.py</code> <pre><code>def run_one_arena_thread(self):\n    \"\"\"\n    Run the OneArena thread for processing.\n\n    Executes the OneArena thread to load video, initialize analysis,\n    stop any running instance of the thread, save settings, and connect\n    signals for displaying messages, images, and handling completion events.\n\n    Notes\n    -----\n    Ensures that the previous arena settings are cleared and connects signals\n    to display messages and images during thread execution.\n    \"\"\"\n    if self.thread_dict['OneArena']._isRunning:\n        self.thread_dict['OneArena'].stop()\n    self.save_current_settings()\n    if self.previous_arena != self.parent().po.all['arena']:\n        self.parent().po.motion = None\n    self.message.setText(\"Load the video and initialize analysis, wait...\")\n    self.thread_dict['OneArena'].start()  # OneArenaThreadInThirdWidget\n    self.thread_dict['OneArena'].message_from_thread_starting.connect(self.display_message_from_thread)\n    self.thread_dict['OneArena'].when_loading_finished.connect(self.when_loading_thread_finished)\n    self.thread_dict['OneArena'].when_detection_finished.connect(self.when_detection_finished)\n    self.thread_dict['OneArena'].image_from_thread.connect(self.display_image_during_thread)\n</code></pre>"},{"location":"api/cellects/gui/video_analysis_window/#cellects.gui.video_analysis_window.VideoAnalysisWindow.save_all_vars_thread","title":"<code>save_all_vars_thread()</code>","text":"<p>Start the 'SaveAllVars' thread if it is not already running.</p> <p>This method is used to ensure that variable saving operations are performed in a separate thread to avoid blocking the main application.</p> Source code in <code>src/cellects/gui/video_analysis_window.py</code> <pre><code>def save_all_vars_thread(self):\n    \"\"\"\n    Start the 'SaveAllVars' thread if it is not already running.\n\n    This method is used to ensure that variable saving operations are performed\n    in a separate thread to avoid blocking the main application.\n    \"\"\"\n    if not self.parent().thread_dict['SaveAllVars'].isRunning():\n        self.parent().thread_dict['SaveAllVars'].start()  # SaveAllVarsThreadInThirdWidget\n</code></pre>"},{"location":"api/cellects/gui/video_analysis_window/#cellects.gui.video_analysis_window.VideoAnalysisWindow.save_current_settings","title":"<code>save_current_settings()</code>","text":"<p>Saves the current settings from UI components to persistent storage.</p> <p>This method captures the values of various UI components and stores them in a persistent data structure to ensure settings are saved across sessions.</p> Source code in <code>src/cellects/gui/video_analysis_window.py</code> <pre><code>def save_current_settings(self):\n    \"\"\"\n    Saves the current settings from UI components to persistent storage.\n\n    This method captures the values of various UI components and stores\n    them in a persistent data structure to ensure settings are saved across\n    sessions.\n    \"\"\"\n    self.parent().po.vars['maximal_growth_factor'] = self.maximal_growth_factor.value()\n    self.parent().po.vars['repeat_video_smoothing'] = int(np.round(self.repeat_video_smoothing.value()))\n    self.parent().po.vars['do_fading'] = self.do_fading.isChecked()\n    self.parent().po.vars['fading'] = self.fading.value()\n    self.parent().po.all['compute_all_options'] = self.compute_all_options_cb.isChecked()\n    self.option_changed()\n    self.save_all_vars_thread()\n</code></pre>"},{"location":"api/cellects/gui/video_analysis_window/#cellects.gui.video_analysis_window.VideoAnalysisWindow.save_one_result_is_clicked","title":"<code>save_one_result_is_clicked()</code>","text":"<p>Finalize one arena analysis and save the result if conditions are met.</p> <p>This function checks various conditions before starting a thread to finalize the analysis and save the result. It ensures that certain threads are not running before proceeding.</p> Source code in <code>src/cellects/gui/video_analysis_window.py</code> <pre><code>def save_one_result_is_clicked(self):\n    \"\"\"\n    Finalize one arena analysis and save the result if conditions are met.\n\n    This function checks various conditions before starting a thread to\n    finalize the analysis and save the result. It ensures that certain\n    threads are not running before proceeding.\n    \"\"\"\n    if self.parent().po.motion is not None:\n        if self.parent().po.load_quick_full == 2:\n            if not self.thread_dict['OneArena'].isRunning() and not self.thread_dict['ChangeOneRepResult'].isRunning():\n                self.message.setText(f\"Arena {self.parent().po.all['arena']}: Finalize analysis and save, wait...\")\n                self.thread_dict['ChangeOneRepResult'].start()  # ChangeOneRepResultThreadInThirdWidget\n                self.thread_dict['ChangeOneRepResult'].message_from_thread.connect(self.display_message_from_thread)\n                self.message.setText(\"Complete analysis + change that result\")\n            else:\n                self.message.setText(\"Wait for the analysis to end\")\n        else:\n            self.message.setText(\"Run Post processing first\")\n    else:\n        self.message.setText(\"Run Post processing first\")\n</code></pre>"},{"location":"api/cellects/gui/video_analysis_window/#cellects.gui.video_analysis_window.VideoAnalysisWindow.step_done_is_clicked","title":"<code>step_done_is_clicked()</code>","text":"<p>Step the analysis progress when 'Done' button is clicked.</p> <p>Increments the current step and updates the UI accordingly based on the new step value. Updates labels, tooltips, and visibility of widgets.</p> Notes <p>This method is automatically called when the 'Done' button is clicked. It updates the GUI elements to reflect progress in a multi-step analysis process.</p> Source code in <code>src/cellects/gui/video_analysis_window.py</code> <pre><code>def step_done_is_clicked(self):\n    \"\"\"\n    Step the analysis progress when 'Done' button is clicked.\n\n    Increments the current step and updates the UI accordingly based on the\n    new step value. Updates labels, tooltips, and visibility of widgets.\n\n    Notes\n    -----\n    This method is automatically called when the 'Done' button is clicked.\n    It updates the GUI elements to reflect progress in a multi-step\n    analysis process.\n    \"\"\"\n    self.current_step += 1\n    if self.current_step == 1:\n        self.general_step_label.setText('Step 2: Tune fading and advanced parameters to improve Post processing')\n        self.general_step_label.setToolTip('Post processing is slower than Detection.\\nIt improves detection with the following optional algorithms:\\n - Fading detection\\n - Correct errors around initial shape\\n - Organism internal oscillation period\\n - Connect distant shape\\n - Appearing cell selection')\n        self.second_step_widget.setVisible(True)\n        self.fading.setVisible(self.parent().po.vars['do_fading'])\n        self.save_one_result.setVisible(False)\n    elif self.current_step == 2:\n        self.general_step_label.setText('Step 3: Run the full analysis or save the result of one arena.')\n        self.general_step_label.setToolTip('Once all settings are correct for a arena, click on \"Run All\" to start the full analysis.\\nIf the detection is unsatisfactory for a arena, you can repeat the detection for this\\narena and save the results by clicking \"Save One Result\".\\nRepeat the process for as many arenas as necessary.')\n        self.save_one_result.setVisible(True)\n        self.general_step_button.setVisible(False)\n</code></pre>"},{"location":"api/cellects/gui/video_analysis_window/#cellects.gui.video_analysis_window.VideoAnalysisWindow.true_init","title":"<code>true_init()</code>","text":"<p>Initialize the video tracking interface and set up its UI components.</p> Extended Description <p>This method initializes various tabs, threads, and UI elements for the video tracking interface. It sets up event handlers for tab clicks and configures layout components such as labels, buttons, spinboxes, and comboboxes.</p> Notes <p>This method assumes that the parent widget has a 'po' attribute with specific settings and variables.</p> Source code in <code>src/cellects/gui/video_analysis_window.py</code> <pre><code>def true_init(self):\n    \"\"\"\n    Initialize the video tracking interface and set up its UI components.\n\n    Extended Description\n    --------------------\n    This method initializes various tabs, threads, and UI elements for the video tracking interface. It sets up\n    event handlers for tab clicks and configures layout components such as labels, buttons, spinboxes, and\n    comboboxes.\n\n    Notes\n    -----\n    This method assumes that the parent widget has a 'po' attribute with specific settings and variables.\n    \"\"\"\n    self.data_tab.set_not_in_use()\n    self.image_tab.set_not_usable()\n    self.video_tab.set_in_use()\n    self.data_tab.clicked.connect(self.data_tab_is_clicked)\n    self.image_tab.clicked.connect(self.image_tab_is_clicked)\n    self.thread_dict = {}\n    self.thread_dict['VideoReader'] = VideoReaderThread(self.parent())\n    self.thread_dict['OneArena'] = OneArenaThread(self.parent())\n    self.thread_dict['ChangeOneRepResult'] = ChangeOneRepResultThread(self.parent())\n    self.thread_dict['RunAll'] = RunAllThread(self.parent())\n    self.previous_arena = 0\n    curr_row_main_layout = 0\n    ncol = 1\n    self.Vlayout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.Maximum, QtWidgets.QSizePolicy.MinimumExpanding))#, curr_row_main_layout, 0, 1, ncol)\n    curr_row_main_layout += 1\n\n    # Open subtitle\n    self.general_step_widget = QtWidgets.QWidget()\n    self.general_step_layout = QtWidgets.QHBoxLayout()\n    self.current_step = 0\n    self.general_step_label = FixedText('Step 1: Tune parameters to improve Detection', night_mode=self.parent().po.all['night_mode'])\n    self.general_step_button = PButton('Done', night_mode=self.parent().po.all['night_mode'])\n    self.general_step_button.clicked.connect(self.step_done_is_clicked)\n    self.general_step_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n    self.general_step_layout.addWidget(self.general_step_label)\n    self.general_step_layout.addWidget(self.general_step_button)\n    self.general_step_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n\n    self.general_step_widget.setLayout(self.general_step_layout)\n    self.Vlayout.addWidget(self.general_step_widget)#, curr_row_main_layout, 0, 1, ncol)\n    curr_row_main_layout += 1\n\n    # Open central widget\n    self.video_display_widget = QtWidgets.QWidget()\n    self.video_display_layout = QtWidgets.QHBoxLayout()\n    self.video_display_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n    #   Open left widget\n    self.left_options_widget = QtWidgets.QWidget()\n    self.left_options_layout = QtWidgets.QVBoxLayout()\n    self.left_options_widget.setSizePolicy(QtWidgets.QSizePolicy.Maximum, QtWidgets.QSizePolicy.Maximum)\n\n    self.arena_widget = QtWidgets.QWidget()\n    self.arena_layout = QtWidgets.QHBoxLayout()\n    self.arena_label = FixedText(VAW[\"Arena_to_analyze\"][\"label\"] + ':',\n                                   tip=VAW[\"Arena_to_analyze\"][\"tips\"],\n                                   night_mode=self.parent().po.all['night_mode'])\n    sample_size = self.parent().po.all['sample_number_per_folder'][0]\n    if self.parent().po.all['arena'] &gt; sample_size:\n        self.parent().po.all['arena'] = 1\n\n    self.arena = Spinbox(min=1, max=1000000, val=self.parent().po.all['arena'],\n                           night_mode=self.parent().po.all['night_mode'])\n    self.arena.valueChanged.connect(self.arena_changed)\n\n    self.arena_layout.addWidget(self.arena_label)\n    self.arena_layout.addWidget(self.arena)\n    self.arena_widget.setLayout(self.arena_layout)\n    self.left_options_layout.addWidget(self.arena_widget)\n\n\n    self.growth_per_frame_widget = QtWidgets.QWidget()\n    self.growth_per_frame_layout = QtWidgets.QHBoxLayout()\n    try:\n        self.parent().po.vars['maximal_growth_factor']\n    except KeyError:\n        self.parent().po.vars['maximal_growth_factor'] = 0.02\n        self.parent().po.vars['repeat_video_smoothing'] = self.parent().po.vars['iterate_smoothing']\n    self.maximal_growth_factor = Spinbox(min=0, max=0.5, val=self.parent().po.vars['maximal_growth_factor'],\n                                        decimals=3, night_mode=self.parent().po.all['night_mode'])\n    self.maximal_growth_factor_label = FixedText(VAW[\"Maximal_growth_factor\"][\"label\"] + ':',\n                                                tip=VAW[\"Maximal_growth_factor\"][\"tips\"],\n                                                night_mode=self.parent().po.all['night_mode'])\n    self.maximal_growth_factor.valueChanged.connect(self.maximal_growth_factor_changed)\n    self.growth_per_frame_layout.addWidget(self.maximal_growth_factor_label)\n    self.growth_per_frame_layout.addWidget(self.maximal_growth_factor)\n    self.growth_per_frame_widget.setLayout(self.growth_per_frame_layout)\n    self.left_options_layout.addWidget(self.growth_per_frame_widget)\n\n    self.iterate_widget = QtWidgets.QWidget()\n    self.iterate_layout = QtWidgets.QHBoxLayout()\n    self.repeat_video_smoothing = Spinbox(min=0, max=10, val=self.parent().po.vars['repeat_video_smoothing'],\n                                     night_mode=self.parent().po.all['night_mode'])\n    self.repeat_video_smoothing_label = FixedText(VAW[\"Temporal_smoothing\"][\"label\"] + ':',\n                                             tip=VAW[\"Temporal_smoothing\"][\"tips\"],\n                                             night_mode=self.parent().po.all['night_mode'])\n    self.repeat_video_smoothing.valueChanged.connect(self.repeat_video_smoothing_changed)\n    self.iterate_layout.addWidget(self.repeat_video_smoothing_label)\n    self.iterate_layout.addWidget(self.repeat_video_smoothing)\n    self.iterate_widget.setLayout(self.iterate_layout)\n    self.left_options_layout.addWidget(self.iterate_widget)\n\n\n    self.select_option_label = FixedText(VAW[\"Segmentation_method\"][\"label\"] + ':',\n                                         tip=VAW[\"Segmentation_method\"][\"tips\"],\n                                         night_mode=self.parent().po.all['night_mode'])\n    self.select_option = Combobox([], night_mode=self.parent().po.all['night_mode'])\n    self.select_option_label.setSizePolicy(QtWidgets.QSizePolicy.Maximum, QtWidgets.QSizePolicy.Maximum)\n    self.select_option.setSizePolicy(QtWidgets.QSizePolicy.Maximum, QtWidgets.QSizePolicy.Maximum)\n    self.select_option.setFixedWidth(175)\n    self.select_option.addItem(\"1. Frame by frame\")\n    self.select_option.addItem(\"2. Dynamical threshold\")\n    self.select_option.addItem(\"3. Dynamical slope\")\n    self.select_option.addItem(\"4. Threshold and Slope\")\n    self.select_option.addItem(\"5. Threshold or Slope\")\n    self.select_option.setCurrentIndex(self.parent().po.all['video_option'])\n    self.select_option.currentTextChanged.connect(self.option_changed)\n\n    # Open the choose best option row layout\n    self.options_row_widget = QtWidgets.QWidget()\n    self.options_row_layout = QtWidgets.QHBoxLayout()\n    self.options_row_layout.addWidget(self.select_option_label)\n    self.options_row_layout.addWidget(self.select_option)\n    self.options_row_layout.setAlignment(QtCore.Qt.AlignHCenter)\n    self.options_row_layout.setAlignment(QtCore.Qt.AlignVCenter)\n    self.options_row_widget.setLayout(self.options_row_layout)\n    self.left_options_layout.addWidget(self.options_row_widget)\n\n    #   Close left widget\n    self.left_options_widget.setLayout(self.left_options_layout)\n    self.video_display_layout.addWidget(self.left_options_widget)\n\n\n    # Add the central video display widget\n    self.display_image = np.zeros((self.parent().im_max_height, self.parent().im_max_width, 3), np.uint8)\n    self.display_image = InsertImage(self.display_image, self.parent().im_max_height, self.parent().im_max_width)\n    self.display_image.mousePressEvent = self.full_screen_display\n    self.video_display_layout.addWidget(self.display_image)\n\n\n    #   Open right widget\n    self.right_options_widget = QtWidgets.QWidget()\n    self.right_options_layout = QtWidgets.QVBoxLayout()\n    self.right_options_widget.setSizePolicy(QtWidgets.QSizePolicy.Maximum, QtWidgets.QSizePolicy.Maximum)\n\n    self.compute_all_options_label = FixedText('Compute all options',\n                                               tip=VAW[\"Segmentation_method\"][\"tips\"],\n                                               night_mode=self.parent().po.all['night_mode'])\n    self.compute_all_options_cb = Checkbox(self.parent().po.all['compute_all_options'])\n    self.compute_all_options_cb.setStyleSheet(\"QCheckBox::indicator {width: 12px;height: 12px;background-color: transparent;\"\n                        \"border-radius: 5px;border-style: solid;border-width: 1px;\"\n                        \"border-color: rgb(100,100,100);}\"\n                        \"QCheckBox::indicator:checked {background-color: rgb(70,130,180);}\"\n                        \"QCheckBox:checked, QCheckBox::indicator:checked {border-color: black black white white;}\"\n                        \"QCheckBox:checked {background-color: transparent;}\"\n                        \"QCheckBox:margin-left {0%}\"\n                        \"QCheckBox:margin-right {0%}\")\n    self.compute_all_options_cb.stateChanged.connect(self.compute_all_options_check)\n    self.all_options_row_widget = QtWidgets.QWidget()\n    self.all_options_row_layout = QtWidgets.QHBoxLayout()\n    self.all_options_row_layout.addWidget(self.compute_all_options_cb)\n    self.all_options_row_layout.addWidget(self.compute_all_options_label)\n    self.all_options_row_layout.setAlignment(QtCore.Qt.AlignHCenter)\n    self.all_options_row_layout.setAlignment(QtCore.Qt.AlignVCenter)\n    self.all_options_row_widget.setLayout(self.all_options_row_layout)\n    self.right_options_layout.addWidget(self.all_options_row_widget)\n\n    self.load_one_arena = PButton(VAW[\"Load_one_arena\"][\"label\"], tip=VAW[\"Load_one_arena\"][\"tips\"],\n                                  night_mode=self.parent().po.all['night_mode'])\n    self.load_one_arena.clicked.connect(self.load_one_arena_is_clicked)\n    self.detection = PButton(VAW[\"Detection\"][\"label\"], tip=VAW[\"Detection\"][\"tips\"],\n                             night_mode=self.parent().po.all['night_mode'])\n    self.detection.clicked.connect(self.detection_is_clicked)\n    self.read = PButton(VAW[\"Read\"][\"label\"], tip=VAW[\"Read\"][\"tips\"], night_mode=self.parent().po.all['night_mode'])\n    self.read.clicked.connect(self.read_is_clicked)\n    self.read.setVisible(False)\n    self.right_options_layout.addWidget(self.load_one_arena, alignment=QtCore.Qt.AlignCenter)\n    self.right_options_layout.addWidget(self.detection, alignment=QtCore.Qt.AlignCenter)\n    self.right_options_layout.addWidget(self.read, alignment=QtCore.Qt.AlignCenter)\n\n\n    self.right_options_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n    self.right_options_widget.setLayout(self.right_options_layout)\n    self.video_display_layout.addWidget(self.right_options_widget)\n    self.video_display_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n    # Close central widget\n    self.video_display_widget.setLayout(self.video_display_layout)\n    self.Vlayout.addWidget(self.video_display_widget)#, curr_row_main_layout, 0)\n    curr_row_main_layout += 1\n\n    # Open Second step row\n    self.second_step_widget = QtWidgets.QWidget()\n    self.second_step_layout = QtWidgets.QHBoxLayout()\n    self.second_step_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n    self.second_step_widget.setVisible(False)\n\n    self.fading_widget = QtWidgets.QWidget()\n    self.fading_layout = QtWidgets.QHBoxLayout()\n    self.do_fading = Checkbox(self.parent().po.vars['do_fading'])\n    self.do_fading.setStyleSheet(\"QCheckBox::indicator {width: 12px;height: 12px;background-color: transparent;\"\n                        \"border-radius: 5px;border-style: solid;border-width: 1px;\"\n                        \"border-color: rgb(100,100,100);}\"\n                        \"QCheckBox::indicator:checked {background-color: rgb(70,130,180);}\"\n                        \"QCheckBox:checked, QCheckBox::indicator:checked {border-color: black black white white;}\"\n                        \"QCheckBox:checked {background-color: transparent;}\"\n                        \"QCheckBox:margin-left {0%}\"\n                        \"QCheckBox:margin-right {0%}\")\n    self.do_fading.stateChanged.connect(self.do_fading_check)\n    self.fading = Spinbox(min=- 1, max=1, val=self.parent().po.vars['fading'], decimals=2,\n                           night_mode=self.parent().po.all['night_mode'])\n    self.fading_label = FixedText(VAW[\"Fading_detection\"][\"label\"],\n                                   tip=VAW[\"Fading_detection\"][\"tips\"],\n                                   night_mode=self.parent().po.all['night_mode'])\n    self.fading.valueChanged.connect(self.fading_changed)\n    self.fading_layout.addWidget(self.do_fading)\n    self.fading_layout.addWidget(self.fading_label)\n    self.fading_layout.addWidget(self.fading)\n    self.fading_layout.setAlignment(QtCore.Qt.AlignHCenter)\n    self.fading_widget.setLayout(self.fading_layout)\n    self.second_step_layout.addWidget(self.fading_widget)\n\n    self.post_processing = PButton(VAW[\"Post_processing\"][\"label\"], tip=VAW[\"Post_processing\"][\"tips\"],\n                                   night_mode=self.parent().po.all['night_mode'])\n    self.post_processing.clicked.connect(self.post_processing_is_clicked)\n    self.second_step_layout.addWidget(self.post_processing)\n\n    self.save_one_result = PButton(VAW[\"Save_one_result\"][\"label\"], tip=VAW[\"Save_one_result\"][\"tips\"],\n                                   night_mode=self.parent().po.all['night_mode'])\n    self.save_one_result.clicked.connect(self.save_one_result_is_clicked)\n    self.second_step_layout.addWidget(self.save_one_result)\n\n    # Close Second step row\n    self.second_step_layout.setAlignment(QtCore.Qt.AlignHCenter)\n    self.second_step_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n    self.second_step_widget.setLayout(self.second_step_layout)\n    self.Vlayout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.Maximum, QtWidgets.QSizePolicy.MinimumExpanding))#, curr_row_main_layout, 0, 1, ncol)\n    curr_row_main_layout += 1\n    self.Vlayout.addWidget(self.second_step_widget)#, curr_row_main_layout, 0)\n    curr_row_main_layout += 1\n\n    # Open last options row widget\n    self.last_options_widget = QtWidgets.QWidget()\n    self.last_options_layout = QtWidgets.QHBoxLayout()\n    self.last_options_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n\n    self.advanced_parameters = PButton(FW[\"Advanced_parameters\"][\"label\"], tip=FW[\"Advanced_parameters\"][\"tips\"],\n                                       night_mode=self.parent().po.all['night_mode'])\n    self.advanced_parameters.clicked.connect(self.advanced_parameters_is_clicked)\n    self.last_options_layout.addWidget(self.advanced_parameters)\n\n    #  Required Outputs widget\n    self.required_outputs = PButton(FW[\"Required_outputs\"][\"label\"], tip=FW[\"Required_outputs\"][\"tips\"],\n                                    night_mode=self.parent().po.all['night_mode'])\n    self.required_outputs.clicked.connect(self.required_outputs_is_clicked)\n    self.last_options_layout.addWidget(self.required_outputs)\n\n    #  Save all choices widget\n    self.save_all_vars = PButton(VAW[\"Save_all_choices\"][\"label\"], tip=VAW[\"Save_all_choices\"][\"tips\"],\n                                 night_mode=self.parent().po.all['night_mode'])\n    self.save_all_vars.clicked.connect(self.save_current_settings)\n    self.last_options_layout.addWidget(self.save_all_vars)\n\n    # Close last options widget\n    self.last_options_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n    self.last_options_widget.setLayout(self.last_options_layout)\n    self.Vlayout.addWidget(self.last_options_widget)#, curr_row_main_layout, 0)\n    curr_row_main_layout += 1\n\n    self.message = QtWidgets.QLabel(self)\n    self.message.setText('')\n    self.message.setStyleSheet(\"color: rgb(230, 145, 18)\")\n    self.message.setAlignment(QtCore.Qt.AlignLeft)\n\n    self.previous = PButton('Previous', night_mode=self.parent().po.all['night_mode'])\n    self.previous.clicked.connect(self.previous_is_clicked)\n\n    self.run_all = PButton(VAW[\"Run_All\"][\"label\"], tip=VAW[\"Run_All\"][\"tips\"],\n                           night_mode=self.parent().po.all['night_mode'])\n    self.run_all.clicked.connect(self.run_all_is_clicked)\n\n    # Open last row widget\n    self.last_row_widget = QtWidgets.QWidget()\n    self.last_row_layout = QtWidgets.QHBoxLayout()\n    self.last_row_layout.addWidget(self.previous)\n    self.last_row_layout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.MinimumExpanding, QtWidgets.QSizePolicy.Maximum))\n    self.last_row_layout.addWidget(self.message)\n    self.last_row_layout.addWidget(self.run_all)\n    # Close last row widget\n    self.last_row_widget.setLayout(self.last_row_layout)\n    self.Vlayout.addItem(QtWidgets.QSpacerItem(1, 1, QtWidgets.QSizePolicy.Maximum, QtWidgets.QSizePolicy.MinimumExpanding))#, curr_row_main_layout, 0, 1, ncol)\n    self.Vlayout.addWidget(self.last_row_widget)#, curr_row_main_layout + 1, 0)\n\n    self.setLayout(self.Vlayout)\n</code></pre>"},{"location":"api/cellects/gui/video_analysis_window/#cellects.gui.video_analysis_window.VideoAnalysisWindow.when_detection_finished","title":"<code>when_detection_finished(message)</code>","text":"<p>Handles the completion of video detection and updates the UI accordingly.</p> <p>When the video detection is finished, this function waits for the VideoReader thread to complete if it's running. It then processes the last frame of the video based on the configured visualization and motion detection settings. Finally, it updates the UI with the processed image and sets appropriate labels' visibility.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>The message to display upon completion of detection. This could be a status update or any relevant information.</p> required Notes <p>This function assumes that the parent object has attributes <code>po</code> and <code>image_to_display</code>, and methods like <code>display_image.update_image</code>.</p> Source code in <code>src/cellects/gui/video_analysis_window.py</code> <pre><code>def when_detection_finished(self, message: str):\n    \"\"\"\n    Handles the completion of video detection and updates the UI accordingly.\n\n    When the video detection is finished, this function waits for the\n    VideoReader thread to complete if it's running. It then processes the\n    last frame of the video based on the configured visualization and motion\n    detection settings. Finally, it updates the UI with the processed image\n    and sets appropriate labels' visibility.\n\n    Parameters\n    ----------\n    message : str\n        The message to display upon completion of detection.\n        This could be a status update or any relevant information.\n\n    Notes\n    -----\n    This function assumes that the parent object has attributes `po` and\n    `image_to_display`, and methods like `display_image.update_image`.\n    \"\"\"\n    self.previous_arena = self.parent().po.all['arena']\n    if self.thread_dict['VideoReader'].isRunning():  # VideoReaderThreadInThirdWidget\n        self.thread_dict['VideoReader'].wait()\n    if self.parent().po.load_quick_full &gt; 0:\n        image = self.parent().po.motion.segmented[-1, ...]\n    if self.parent().po.motion.visu is None:\n        image = self.parent().po.motion.converted_video[-1, ...] * (1 - image)\n        image = np.round(image).astype(np.uint8)\n        image = np.stack((image, image, image), axis=2)\n    else:\n        image = np.stack((image, image, image), axis=2)\n        image = self.parent().po.motion.visu[-1, ...] * (1 - image)\n    self.parent().image_to_display = image\n    self.display_image.update_image(image)\n    self.message.setText(message)\n    self.select_option_label.setVisible(self.parent().po.vars[\"color_number\"] == 2)\n    self.select_option.setVisible(self.parent().po.vars[\"color_number\"] == 2)\n    self.read.setVisible(True)\n</code></pre>"},{"location":"api/cellects/gui/video_analysis_window/#cellects.gui.video_analysis_window.VideoAnalysisWindow.when_loading_thread_finished","title":"<code>when_loading_thread_finished(save_loaded_video)</code>","text":"<p>Ends the loading thread process and handles post-loading actions.</p> Notes <p>This method assumes that the parent object has a <code>po</code> attribute with an 'arena' key and a <code>load_quick_full</code> attribute. It also assumes that the parent object has a 'thread' dictionary and a message UI component.</p> Source code in <code>src/cellects/gui/video_analysis_window.py</code> <pre><code>def when_loading_thread_finished(self, save_loaded_video: bool):\n    \"\"\"\n    Ends the loading thread process and handles post-loading actions.\n\n    Notes\n    ----------\n    This method assumes that the parent object has a `po` attribute with an\n    'arena' key and a `load_quick_full` attribute. It also assumes that the\n    parent object has a 'thread' dictionary and a message UI component.\n    \"\"\"\n    self.previous_arena = self.parent().po.all['arena']\n    if save_loaded_video:\n        self.thread_dict['WriteVideo'] = WriteVideoThread(self.parent())\n        self.thread_dict['WriteVideo'].start()\n    if self.parent().po.load_quick_full == 0:\n        self.message.setText(\"Loading done, you can watch the video\")\n    self.read.setVisible(True)\n</code></pre>"},{"location":"api/cellects/image_analysis/","title":"<code>cellects.image_analysis</code>","text":""},{"location":"api/cellects/image_analysis/#cellects.image_analysis","title":"<code>cellects.image_analysis</code>","text":""},{"location":"api/cellects/image_analysis/cell_leaving_detection/","title":"<code>cellects.image_analysis.cell_leaving_detection</code>","text":""},{"location":"api/cellects/image_analysis/cell_leaving_detection/#cellects.image_analysis.cell_leaving_detection","title":"<code>cellects.image_analysis.cell_leaving_detection</code>","text":"<p>Contains the function: cell_leaving_detection This function considers the pixel intensity curve of each covered pixel and assesesed whether a covered pixel retrieved -partially at least- its initial intensity.</p>"},{"location":"api/cellects/image_analysis/cell_leaving_detection/#cellects.image_analysis.cell_leaving_detection.cell_leaving_detection","title":"<code>cell_leaving_detection(new_shape, covering_intensity, previous_binary, greyscale_image, fading_coefficient, lighter_background, several_blob_per_arena, erodila_disk, protect_from_fading=None, add_to_fading=None)</code>","text":"<p>Perform cell leaving detection based on shape changes and intensity variations.</p> <p>Checks for fading pixels by considering the internal contour of a previous binary image, applies erosion and subtraction operations, and updates the shape based on fading detection. It handles cases where the background is lighter or darker and ensures that detected fading regions do not fragment the shape into multiple components, unless specified otherwise.</p> <p>Parameters:</p> Name Type Description Default <code>new_shape</code> <code>NDArray[uint8]</code> <p>The current shape to be updated based on fading detection.</p> required <code>covering_intensity</code> <code>NDArray</code> <p>Intensity values used to determine if pixels are fading. Should have the same dimensions as new_shape.</p> required <code>previous_binary</code> <code>NDArray[uint8]</code> <p>Binary representation of the shape at the previous time step. Should have the same dimensions as new_shape.</p> required <code>greyscale_image</code> <code>NDArray</code> <p>Greyscale image used for intensity comparison. Should have the same dimensions as new_shape.</p> required <code>fading_coefficient</code> <code>float</code> <p>A coefficient to determine fading thresholds based on covering intensity. Should be between 0 and 1.</p> required <code>lighter_background</code> <code>bool</code> <p>Flag indicating if the background is lighter. True if background is lighter, False otherwise.</p> required <code>several_blob_per_arena</code> <code>bool</code> <p>Flag indicating if multiple blobs per arena are allowed. True to allow fragmentation, False otherwise.</p> required <code>erodila_disk</code> <code>NDArray[uint8]</code> <p>Disk used for erosion operations. Should be a valid structuring element.</p> required <code>protect_from_fading</code> <code>NDArray</code> <p>An optional array to prevent certain pixels from being marked as fading. Should have the same dimensions as new_shape.</p> <code>None</code> <code>add_to_fading</code> <code>NDArray</code> <p>An optional array to mark additional pixels as fading. Should have the same dimensions as new_shape.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>new_shape</code> <code>NDArray[uint8]</code> <p>Updated shape after applying fading detection and morphological operations.</p> <code>covering_intensity</code> <code>NDArray</code> <p>Updated intensity values.</p> Source code in <code>src/cellects/image_analysis/cell_leaving_detection.py</code> <pre><code>def cell_leaving_detection(new_shape: NDArray[np.uint8], covering_intensity:NDArray, previous_binary: NDArray[np.uint8], greyscale_image: NDArray, fading_coefficient: float, lighter_background: bool, several_blob_per_arena: bool, erodila_disk: NDArray[np.uint8], protect_from_fading: NDArray=None, add_to_fading: NDArray=None) -&gt; Tuple[NDArray[np.uint8], NDArray]:\n    \"\"\"\n    Perform cell leaving detection based on shape changes and intensity variations.\n\n    Checks for fading pixels by considering the internal contour of a previous binary\n    image, applies erosion and subtraction operations, and updates the shape based on\n    fading detection. It handles cases where the background is lighter or darker and\n    ensures that detected fading regions do not fragment the shape into multiple components,\n    unless specified otherwise.\n\n    Parameters\n    ----------\n    new_shape : NDArray[np.uint8]\n        The current shape to be updated based on fading detection.\n\n    covering_intensity : NDArray\n        Intensity values used to determine if pixels are fading.\n        Should have the same dimensions as new_shape.\n\n    previous_binary : NDArray[np.uint8]\n        Binary representation of the shape at the previous time step.\n        Should have the same dimensions as new_shape.\n\n    greyscale_image : NDArray\n        Greyscale image used for intensity comparison.\n        Should have the same dimensions as new_shape.\n\n    fading_coefficient : float\n        A coefficient to determine fading thresholds based on covering intensity.\n        Should be between 0 and 1.\n\n    lighter_background : bool\n        Flag indicating if the background is lighter.\n        True if background is lighter, False otherwise.\n\n    several_blob_per_arena : bool\n        Flag indicating if multiple blobs per arena are allowed.\n        True to allow fragmentation, False otherwise.\n\n    erodila_disk : NDArray[np.uint8]\n        Disk used for erosion operations.\n        Should be a valid structuring element.\n\n    protect_from_fading : NDArray, optional\n        An optional array to prevent certain pixels from being marked as fading.\n        Should have the same dimensions as new_shape.\n\n    add_to_fading : NDArray, optional\n        An optional array to mark additional pixels as fading.\n        Should have the same dimensions as new_shape.\n\n    Returns\n    -------\n    new_shape : NDArray[np.uint8]\n        Updated shape after applying fading detection and morphological operations.\n\n    covering_intensity : NDArray\n        Updated intensity values.\n    \"\"\"\n    # To look for fading pixels, only consider the internal contour of the shape at t-1\n    fading = cv2.erode(previous_binary, erodila_disk)\n    fading = previous_binary - fading\n    # If the origin state is considered constant: origin pixels will never be fading\n    if protect_from_fading is not None:\n        fading *= (1 - protect_from_fading)\n    if add_to_fading is not None:\n        if protect_from_fading is not None:\n            add_to_fading[np.nonzero(protect_from_fading)] = 0\n        add_to_fading_coord = np.nonzero(add_to_fading)\n        fading[add_to_fading_coord] = 1\n        if lighter_background:\n            covering_intensity[add_to_fading_coord] = 1  # 0.9 * covering_intensity[add_to_fading_coord]  #\n        else:\n            covering_intensity[add_to_fading_coord] = 255  # 1.1 * covering_intensity[add_to_fading_coord]\n    # With a lighter background, fading them if their intensity gets higher than the covering intensity\n    if lighter_background:\n        fading = fading * np.greater(greyscale_image, (1 - fading_coefficient) * covering_intensity).astype(np.uint8)\n    else:\n        fading = fading * np.less(greyscale_image, (1 + fading_coefficient) * covering_intensity).astype(np.uint8)\n\n    if np.any(fading):\n        fading = cv2.morphologyEx(fading, cv2.MORPH_CLOSE, cross_33, iterations=1)\n        if not several_blob_per_arena:\n            # Check if uncov_potentials does not break the shape into several components\n            uncov_nb, uncov_shapes = cv2.connectedComponents(fading, ltype=cv2.CV_16U)\n            nb, garbage_img = cv2.connectedComponents(new_shape, ltype=cv2.CV_16U)\n            i = 0\n            while i &lt;= uncov_nb:\n                i += 1\n                prev_nb = nb\n                new_shape[np.nonzero(uncov_shapes == i)] = 0\n                nb, garbage_img = cv2.connectedComponents(new_shape, ltype=cv2.CV_16U)\n                if nb &gt; prev_nb:\n                    new_shape[np.nonzero(uncov_shapes == i)] = 1\n                    nb, garbage_img = cv2.connectedComponents(new_shape, ltype=cv2.CV_16U)\n            uncov_shapes = None\n        else:\n            new_shape[np.nonzero(fading)] = 0\n        new_shape = cv2.morphologyEx(new_shape, cv2.MORPH_OPEN, cross_33, iterations=0)\n        new_shape = cv2.morphologyEx(new_shape, cv2.MORPH_CLOSE, cross_33)\n\n    return new_shape, covering_intensity\n</code></pre>"},{"location":"api/cellects/image_analysis/image_segmentation/","title":"<code>cellects.image_analysis.image_segmentation</code>","text":""},{"location":"api/cellects/image_analysis/image_segmentation/#cellects.image_analysis.image_segmentation","title":"<code>cellects.image_analysis.image_segmentation</code>","text":"<p>Module for image segmentation operations including filtering, color space conversion, thresholding, and quality assessment.</p> <p>This module provides tools to process images through grayscale conversion, apply various filters (e.g., Gaussian, Median, Butterworth), perform thresholding methods like Otsu's algorithm, combine color spaces for enhanced segmentation, and evaluate binary image quality. Key functionalities include dynamic background subtraction, rolling window segmentation with localized thresholds, and optimization of segmentation masks using shape descriptors.</p> <p>Functions apply_filter : Apply skimage or OpenCV-based filters to grayscale images. get_color_spaces : Convert BGR images into specified color space representations (e.g., LAB, HSV). combine_color_spaces : Merge multiple color channels with coefficients to produce a segmented image. generate_color_space_combination : Create custom grayscale combinations using two sets of channel weights and backgrounds. otsu_thresholding : Binarize an image using histogram-based Otsu thresholding. segment_with_lum_value : Segment video frames using luminance thresholds adjusted for background variation. rolling_window_segmentation : Apply localized Otsu thresholding across overlapping patches to improve segmentation accuracy. binary_quality_index : Calculate a quality metric based on perimeter and connected components in binary images. find_threshold_given_mask : Binary search optimization to determine optimal threshold between masked regions.</p> <p>Notes Uses Numba's @njit decorator for JIT compilation of performance-critical functions like combine_color_spaces and _get_counts_jit.</p>"},{"location":"api/cellects/image_analysis/image_segmentation/#cellects.image_analysis.image_segmentation.apply_filter","title":"<code>apply_filter(image, filter_type, param, rescale_to_uint8=False)</code>","text":"<p>Apply various filters to an image based on the specified filter type.</p> <p>This function applies a filter to the input image according to the specified <code>filter_type</code> and associated parameters. Supported filters include Gaussian, Median, Butterworth, Frangi, Sato, Meijering, Hessian, Laplace, Mexican hat, Farid, Prewitt, Roberts, Scharr, and Sobel. Except from Sharpen and Mexican hat, these filters are implemented using the skimage.filters module. Additionally, the function can rescale the output image to uint8 format if specified.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>NDArray</code> <p>The input image to which the filter will be applied.</p> required <code>filter_type</code> <code>str</code> <p>The type of filter to apply. Supported values include: \"Gaussian\", \"Median\", \"Butterworth\", \"Frangi\", \"Sato\", \"Meijering\", \"Hessian\", \"Laplace\", \"Mexican hat\", \"Sharpen\", \"Farid\", \"Prewitt\", \"Roberts\", \"Scharr\", and \"Sobel\".</p> required <code>param</code> <code>list or tuple</code> <p>Parameters specific to the filter type. The structure of <code>param</code> depends on the chosen filter.</p> required <code>rescale_to_uint8</code> <code>bool</code> <p>Whether to rescale the output image to uint8 format. Default is False.</p> <code>False</code> Notes <p>The Sharpen filter is implemented through: cv2.filter2D(image, -1, np.array([[-1, -1, -1], [-1, 9, -1], [-1, -1, -1]])) The Maxican hat filter is implemented through: cv2.filter2D(image, -1, np.array(         [[0, 0, -1, 0, 0], [0, -1, -2, -1, 0], [-1, -2, 16, -2, -1], [0, -1, -2, -1, 0], [0, 0, -1, 0, 0]])) All other filters are skimage filters.</p> <p>Returns:</p> Type Description <code>NDArray</code> <p>The filtered image. If <code>rescale_to_uint8</code> is True and the output image's dtype is not uint8, it will be rescaled accordingly.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; image = np.zeros((3, 3))\n&gt;&gt;&gt; image[1, 1] = 1\n&gt;&gt;&gt; filtered_image = apply_filter(image, \"Gaussian\", [1.0])\n&gt;&gt;&gt; print(filtered_image)\n[[0.05855018 0.09653293 0.05855018]\n [0.09653293 0.15915589 0.09653293]\n [0.05855018 0.09653293 0.05855018]]\nFiltered image with Gaussian filter.\n</code></pre> <pre><code>&gt;&gt;&gt; image = np.zeros((3, 3))\n&gt;&gt;&gt; image[1, 1] = 1\n&gt;&gt;&gt; filtered_image = apply_filter(image, \"Median\", [])\n&gt;&gt;&gt; print(filtered_image)\n[[0. 0. 0.]\n [0. 0. 0.]\n [0. 0. 0.]]\nFiltered image with Median filter.\n</code></pre> <pre><code>&gt;&gt;&gt; image = np.zeros((3, 3))\n&gt;&gt;&gt; image[1, 1] = 1\n&gt;&gt;&gt; filtered_image = apply_filter(image, \"Butterworth\", [0.005, 2])\n&gt;&gt;&gt; print(filtered_image)\n[[-0.1111111  -0.11111111 -0.1111111 ]\n[-0.11111111  0.88888886 -0.11111111]\n[-0.1111111  -0.11111111 -0.1111111 ]]\nFiltered image with Butterworth filter.\n</code></pre> Source code in <code>src/cellects/image_analysis/image_segmentation.py</code> <pre><code>def apply_filter(image: NDArray, filter_type: str, param, rescale_to_uint8=False) -&gt; NDArray:\n    \"\"\"\n    Apply various filters to an image based on the specified filter type.\n\n    This function applies a filter to the input image according to the\n    specified `filter_type` and associated parameters. Supported filters\n    include Gaussian, Median, Butterworth, Frangi, Sato, Meijering,\n    Hessian, Laplace, Mexican hat, Farid, Prewitt, Roberts, Scharr, and Sobel.\n    Except from Sharpen and Mexican hat, these filters are implemented using the skimage.filters module.\n    Additionally, the function can rescale the output image to uint8\n    format if specified.\n\n    Parameters\n    ----------\n    image : NDArray\n        The input image to which the filter will be applied.\n    filter_type : str\n        The type of filter to apply. Supported values include:\n        \"Gaussian\", \"Median\", \"Butterworth\", \"Frangi\",\n        \"Sato\", \"Meijering\", \"Hessian\", \"Laplace\", \"Mexican hat\",\n        \"Sharpen\", \"Farid\", \"Prewitt\", \"Roberts\", \"Scharr\", and \"Sobel\".\n    param : list or tuple\n        Parameters specific to the filter type. The structure of `param`\n        depends on the chosen filter.\n    rescale_to_uint8 : bool, optional\n        Whether to rescale the output image to uint8 format. Default is False.\n\n    Notes\n    -----\n    The Sharpen filter is implemented through:\n    cv2.filter2D(image, -1, np.array([[-1, -1, -1], [-1, 9, -1], [-1, -1, -1]]))\n    The Maxican hat filter is implemented through:\n    cv2.filter2D(image, -1, np.array(\n            [[0, 0, -1, 0, 0], [0, -1, -2, -1, 0], [-1, -2, 16, -2, -1], [0, -1, -2, -1, 0], [0, 0, -1, 0, 0]]))\n    All other filters are skimage filters.\n\n    Returns\n    -------\n    NDArray\n        The filtered image. If `rescale_to_uint8` is True and the output\n        image's dtype is not uint8, it will be rescaled accordingly.\n\n    Examples\n    --------\n    &gt;&gt;&gt; image = np.zeros((3, 3))\n    &gt;&gt;&gt; image[1, 1] = 1\n    &gt;&gt;&gt; filtered_image = apply_filter(image, \"Gaussian\", [1.0])\n    &gt;&gt;&gt; print(filtered_image)\n    [[0.05855018 0.09653293 0.05855018]\n     [0.09653293 0.15915589 0.09653293]\n     [0.05855018 0.09653293 0.05855018]]\n    Filtered image with Gaussian filter.\n\n    &gt;&gt;&gt; image = np.zeros((3, 3))\n    &gt;&gt;&gt; image[1, 1] = 1\n    &gt;&gt;&gt; filtered_image = apply_filter(image, \"Median\", [])\n    &gt;&gt;&gt; print(filtered_image)\n    [[0. 0. 0.]\n     [0. 0. 0.]\n     [0. 0. 0.]]\n    Filtered image with Median filter.\n\n    &gt;&gt;&gt; image = np.zeros((3, 3))\n    &gt;&gt;&gt; image[1, 1] = 1\n    &gt;&gt;&gt; filtered_image = apply_filter(image, \"Butterworth\", [0.005, 2])\n    &gt;&gt;&gt; print(filtered_image)\n    [[-0.1111111  -0.11111111 -0.1111111 ]\n    [-0.11111111  0.88888886 -0.11111111]\n    [-0.1111111  -0.11111111 -0.1111111 ]]\n    Filtered image with Butterworth filter.\n    \"\"\"\n    if filter_type == \"Gaussian\":\n        image = gaussian(image, sigma=param[0])\n    elif filter_type == \"Median\":\n        image = median(image)\n    elif filter_type == \"Butterworth\":\n        image = butterworth(image, cutoff_frequency_ratio=param[0], order=param[1])\n    elif filter_type == \"Frangi\":\n        image = frangi(image, sigmas=np.linspace(param[0], param[1], num=3))\n    elif filter_type == \"Sato\":\n        image = sato(image, sigmas=np.linspace(param[0], param[1], num=3))\n    elif filter_type == \"Meijering\":\n        image = meijering(image, sigmas=np.linspace(param[0], param[1], num=3))\n    elif filter_type == \"Hessian\":\n        image = hessian(image, sigmas=np.linspace(param[0], param[1], num=3))\n    elif filter_type == \"Laplace\":\n        image = laplace(image, ksize=np.max((3, int(np.ceil(param[0])))))\n    elif filter_type == \"Sharpen\":\n        image = cv2.filter2D(image, -1, np.array([[-1, -1, -1], [-1, 9, -1], [-1, -1, -1]]))\n    elif filter_type == \"Mexican hat\":\n        image = cv2.filter2D(image, -1, np.array(\n            [[0, 0, -1, 0, 0], [0, -1, -2, -1, 0], [-1, -2, 16, -2, -1], [0, -1, -2, -1, 0], [0, 0, -1, 0, 0]]))\n    elif filter_type == \"Farid\":\n        image = farid(image)\n    elif filter_type == \"Prewitt\":\n        image = prewitt(image)\n    elif filter_type == \"Roberts\":\n        image = roberts(image)\n    elif filter_type == \"Scharr\":\n        image = scharr(image)\n    elif filter_type == \"Sobel\":\n        image = sobel(image)\n    if rescale_to_uint8 and image.dtype != np.uint8:\n        image = bracket_to_uint8_image_contrast(image)\n    return image\n</code></pre>"},{"location":"api/cellects/image_analysis/image_segmentation/#cellects.image_analysis.image_segmentation.binary_quality_index","title":"<code>binary_quality_index(binary_img)</code>","text":"<p>Calculate the binary quality index for a binary image.</p> <p>The binary quality index is computed based on the perimeter of the largest connected component in the binary image, normalized by the total number of pixels.</p> <p>Parameters:</p> Name Type Description Default <code>binary_img</code> <code>ndarray of uint8</code> <p>Input binary image array.</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>float</code> <p>The binary quality index value.</p> Source code in <code>src/cellects/image_analysis/image_segmentation.py</code> <pre><code>def binary_quality_index(binary_img: NDArray[np.uint8]) -&gt; float:\n    \"\"\"\n    Calculate the binary quality index for a binary image.\n\n    The binary quality index is computed based on the perimeter of the largest\n    connected component in the binary image, normalized by the total number of\n    pixels.\n\n    Parameters\n    ----------\n    binary_img : ndarray of uint8\n        Input binary image array.\n\n    Returns\n    -------\n    out : float\n        The binary quality index value.\n    \"\"\"\n    if np.any(binary_img):\n        # SD = ShapeDescriptors(binary_img, [\"euler_number\"])\n        # index = - SD.descriptors['euler_number']\n        size, largest_cc = get_largest_connected_component(binary_img)\n        index = np.square(perimeter(largest_cc)) / binary_img.sum()\n        # index = (largest_cc.sum() * perimeter(largest_cc)) / binary_img.sum()\n    else:\n        index = 0.\n    return index\n</code></pre>"},{"location":"api/cellects/image_analysis/image_segmentation/#cellects.image_analysis.image_segmentation.combine_color_spaces","title":"<code>combine_color_spaces(c_space_dict, all_c_spaces, subtract_background=None)</code>","text":"<p>Combine color spaces from a dictionary and generate an analyzable image.</p> <p>This function processes multiple color spaces defined in <code>c_space_dict</code>, combines them according to given coefficients, and produces a normalized image that can be converted to uint8. Optionally subtracts background from the resultant image.</p> <p>Parameters:</p> Name Type Description Default <code>c_space_dict</code> <code>dict</code> <p>Dictionary containing color spaces and their respective coefficients.</p> required <code>all_c_spaces</code> <code>Dict</code> <p>Dictionary of all available color spaces in the image.</p> required <code>subtract_background</code> <code>NDArray</code> <p>Background image to subtract from the resultant image. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>out</code> <code>NDArray</code> <p>Processed and normalized image in float64 format, ready for uint8 conversion.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; c_space_dict = Dict()\n&gt;&gt;&gt; c_space_dict['hsv'] = np.array((0, 1, 1))\n&gt;&gt;&gt; all_c_spaces = Dict()\n&gt;&gt;&gt; all_c_spaces['bgr'] = np.random.rand(5, 5, 3)\n&gt;&gt;&gt; all_c_spaces['hsv'] = np.random.rand(5, 5, 3)\n&gt;&gt;&gt; background = np.zeros((5, 5))\n&gt;&gt;&gt; result = combine_color_spaces(c_space_dict, all_c_spaces)\n&gt;&gt;&gt; print(result.shape)\n(5, 5)\n</code></pre> Source code in <code>src/cellects/image_analysis/image_segmentation.py</code> <pre><code>@njit()\ndef combine_color_spaces(c_space_dict: Dict, all_c_spaces: Dict, subtract_background: NDArray=None) -&gt; NDArray:\n    \"\"\"\n    Combine color spaces from a dictionary and generate an analyzable image.\n\n    This function processes multiple color spaces defined in `c_space_dict`, combines\n    them according to given coefficients, and produces a normalized image that can be\n    converted to uint8. Optionally subtracts background from the resultant image.\n\n    Parameters\n    ----------\n    c_space_dict : dict\n        Dictionary containing color spaces and their respective coefficients.\n    all_c_spaces : Dict\n        Dictionary of all available color spaces in the image.\n    subtract_background : NDArray, optional\n        Background image to subtract from the resultant image. Defaults to None.\n\n    Returns\n    -------\n    out : NDArray\n        Processed and normalized image in float64 format, ready for uint8 conversion.\n\n    Examples\n    --------\n    &gt;&gt;&gt; c_space_dict = Dict()\n    &gt;&gt;&gt; c_space_dict['hsv'] = np.array((0, 1, 1))\n    &gt;&gt;&gt; all_c_spaces = Dict()\n    &gt;&gt;&gt; all_c_spaces['bgr'] = np.random.rand(5, 5, 3)\n    &gt;&gt;&gt; all_c_spaces['hsv'] = np.random.rand(5, 5, 3)\n    &gt;&gt;&gt; background = np.zeros((5, 5))\n    &gt;&gt;&gt; result = combine_color_spaces(c_space_dict, all_c_spaces)\n    &gt;&gt;&gt; print(result.shape)\n    (5, 5)\n    \"\"\"\n    image = np.zeros((all_c_spaces['bgr'].shape[0], all_c_spaces['bgr'].shape[1]), dtype=np.float64)\n    for space, channels in c_space_dict.items():\n        image += c_space_dict[space][0] * all_c_spaces[space][:, :, 0] + c_space_dict[space][1] * \\\n                 all_c_spaces[space][:, :, 1] + c_space_dict[space][2] * all_c_spaces[space][:, :, 2]\n    if subtract_background is not None:\n        # add (resp. subtract) the most negative (resp. smallest) value to the whole matrix to get a min = 0\n        image -= np.min(image)\n        # Make analysable this image by bracketing its values between 0 and 255 and converting it to uint8\n        max_im = np.max(image)\n        if max_im != 0:\n            image = 255 * (image / np.max(image))\n        if image.sum() &gt; subtract_background.sum():\n            image -= subtract_background\n        else:\n            image = subtract_background - image\n    # add (resp. subtract) the most negative (resp. smallest) value to the whole matrix to get a min = 0\n    image -= np.min(image)\n    # Make analysable this image by bracketing its values between 0 and 255\n    max_im = np.max(image)\n    if max_im != 0:\n        image = 255 * (image / max_im)\n    return image\n</code></pre>"},{"location":"api/cellects/image_analysis/image_segmentation/#cellects.image_analysis.image_segmentation.convert_subtract_and_filter_video","title":"<code>convert_subtract_and_filter_video(video, color_space_combination, background=None, background2=None, lose_accuracy_to_save_memory=False, filter_spec=None)</code>","text":"<p>Convert a video to grayscale, subtract the background, and apply filters.</p> <p>Parameters:</p> Name Type Description Default <code>video</code> <code>NDArray</code> <p>The input video as a 4D NumPy array.</p> required <code>color_space_combination</code> <code>dict</code> <p>A dictionary containing the combinations of color space transformations.</p> required <code>background</code> <code>NDArray</code> <p>The first background image for subtraction. If <code>None</code>, no subtraction is performed.</p> <code>None</code> <code>background2</code> <code>NDArray</code> <p>The second background image for subtraction. If <code>None</code>, no subtraction is performed.</p> <code>None</code> <code>lose_accuracy_to_save_memory</code> <code>bool</code> <p>Flag to reduce accuracy and save memory by using <code>uint8</code> instead of <code>float64</code>.</p> <code>False</code> <code>filter_spec</code> <code>dict</code> <p>A dictionary containing the specifications for filters to apply.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[NDArray, NDArray]</code> <p>A tuple containing: - <code>converted_video</code>: The converted grayscale video. - <code>converted_video2</code>: The second converted grayscale video if logical operation is not 'None'.</p> Notes <pre><code>- The function reduces accuracy of the converted video when `lose_accuracy_to_save_memory` is set to True.\n- If `color_space_combination['logical']` is not 'None', a second converted video will be created.\n- This function uses the `generate_color_space_combination` and `apply_filter` functions internally.\n</code></pre> Source code in <code>src/cellects/image_analysis/image_segmentation.py</code> <pre><code>def convert_subtract_and_filter_video(video: NDArray, color_space_combination: dict, background: NDArray=None,\n                                      background2: NDArray=None, lose_accuracy_to_save_memory:bool=False,\n                                      filter_spec: dict=None) -&gt; Tuple[NDArray, NDArray]:\n    \"\"\"\n    Convert a video to grayscale, subtract the background, and apply filters.\n\n    Parameters\n    ----------\n    video : NDArray\n        The input video as a 4D NumPy array.\n    color_space_combination : dict\n        A dictionary containing the combinations of color space transformations.\n    background : NDArray, optional\n        The first background image for subtraction. If `None`, no subtraction is performed.\n    background2 : NDArray, optional\n        The second background image for subtraction. If `None`, no subtraction is performed.\n    lose_accuracy_to_save_memory : bool\n        Flag to reduce accuracy and save memory by using `uint8` instead of `float64`.\n    filter_spec : dict\n        A dictionary containing the specifications for filters to apply.\n\n    Returns\n    -------\n    Tuple[NDArray, NDArray]\n        A tuple containing:\n        - `converted_video`: The converted grayscale video.\n        - `converted_video2`: The second converted grayscale video if logical operation is not 'None'.\n\n    Notes\n    -----\n        - The function reduces accuracy of the converted video when `lose_accuracy_to_save_memory` is set to True.\n        - If `color_space_combination['logical']` is not 'None', a second converted video will be created.\n        - This function uses the `generate_color_space_combination` and `apply_filter` functions internally.\n    \"\"\"\n\n    converted_video2 = None\n    if len(video.shape) == 3:\n        converted_video = video\n    else:\n        if lose_accuracy_to_save_memory:\n            array_type = np.uint8\n        else:\n            array_type = np.float64\n        first_dict, second_dict, c_spaces = split_dict(color_space_combination)\n        if 'PCA' in first_dict:\n            greyscale_image, var_ratio, first_pc_vector = extract_first_pc(video[0])\n            first_dict = Dict()\n            first_dict['bgr'] = bracket_to_uint8_image_contrast(first_pc_vector)\n            c_spaces = ['bgr']\n        if 'PCA' in second_dict:\n            greyscale_image, var_ratio, first_pc_vector = extract_first_pc(video[0])\n            second_dict = Dict()\n            second_dict['bgr'] = bracket_to_uint8_image_contrast(first_pc_vector)\n            c_spaces = ['bgr']\n\n        converted_video = np.zeros(video.shape[:3], dtype=array_type)\n        if color_space_combination['logical'] != 'None':\n            converted_video2 = converted_video.copy()\n        for im_i in range(video.shape[0]):\n            if im_i == 0 and background is not None:\n                # when doing background subtraction, the first and the second image are equal\n                image_i = video[1, ...]\n            else:\n                image_i = video[im_i, ...]\n            results = generate_color_space_combination(image_i, c_spaces, first_dict, second_dict, background,\n                                                       background2, lose_accuracy_to_save_memory)\n            greyscale_image, greyscale_image2, all_c_spaces, first_pc_vector = results\n            if filter_spec is not None and filter_spec['filter1_type'] != \"\":\n                greyscale_image = apply_filter(greyscale_image, filter_spec['filter1_type'],\n                                               filter_spec['filter1_param'],lose_accuracy_to_save_memory)\n                if greyscale_image2 is not None and filter_spec['filter2_type'] != \"\":\n                    greyscale_image2 = apply_filter(greyscale_image2,\n                                                    filter_spec['filter2_type'], filter_spec['filter2_param'],\n                                                    lose_accuracy_to_save_memory)\n            converted_video[im_i, ...] = greyscale_image\n            if color_space_combination['logical'] != 'None':\n                converted_video2[im_i, ...] = greyscale_image2\n    return converted_video, converted_video2\n</code></pre>"},{"location":"api/cellects/image_analysis/image_segmentation/#cellects.image_analysis.image_segmentation.extract_first_pc","title":"<code>extract_first_pc(bgr_image, standardize=True)</code>","text":"<p>Extract the first principal component from a BGR image.</p> <p>Parameters:</p> Name Type Description Default <code>bgr_image</code> <code>ndarray</code> <p>A 3D or 2D array representing the BGR image. Expected shape is either (height, width, 3) or (3, height, width).</p> required <code>standardize</code> <code>bool</code> <p>If True, standardizes the image pixel values by subtracting the mean and dividing by the standard deviation before computing the principal components. Default is True.</p> <code>True</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The first principal component image, reshaped to the original image height and width.</p> <code>float</code> <p>The explained variance ratio of the first principal component.</p> <code>ndarray</code> <p>The first principal component vector.</p> Notes <p>The principal component analysis (PCA) is performed using Singular Value Decomposition (SVD). Standardization helps in scenarios where the pixel values have different scales. Pixels with zero standard deviation are handled by setting their standardization denominator to 1.0 to avoid division by zero.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; bgr_image = np.random.rand(100, 200, 3)  # Example BGR image\n&gt;&gt;&gt; first_pc_image, explained_variance_ratio, first_pc_vector = extract_first_pc(bgr_image)\n&gt;&gt;&gt; print(first_pc_image.shape)\n(100, 200)\n&gt;&gt;&gt; print(explained_variance_ratio)\n0.339\n&gt;&gt;&gt; print(first_pc_vector.shape)\n(3,)\n</code></pre> Source code in <code>src/cellects/image_analysis/image_segmentation.py</code> <pre><code>def extract_first_pc(bgr_image: np.ndarray, standardize: bool=True) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"\n\n    Extract the first principal component from a BGR image.\n\n    Parameters\n    ----------\n    bgr_image : numpy.ndarray\n        A 3D or 2D array representing the BGR image. Expected shape is either\n        (height, width, 3) or (3, height, width).\n    standardize : bool, optional\n        If True, standardizes the image pixel values by subtracting the mean and\n        dividing by the standard deviation before computing the principal\n        components. Default is True.\n\n    Returns\n    -------\n    numpy.ndarray\n        The first principal component image, reshaped to the original image height and width.\n    float\n        The explained variance ratio of the first principal component.\n    numpy.ndarray\n        The first principal component vector.\n\n    Notes\n    -----\n    The principal component analysis (PCA) is performed using Singular Value Decomposition (SVD).\n    Standardization helps in scenarios where the pixel values have different scales.\n    Pixels with zero standard deviation are handled by setting their standardization\n    denominator to 1.0 to avoid division by zero.\n\n    Examples\n    --------\n    &gt;&gt;&gt; bgr_image = np.random.rand(100, 200, 3)  # Example BGR image\n    &gt;&gt;&gt; first_pc_image, explained_variance_ratio, first_pc_vector = extract_first_pc(bgr_image)\n    &gt;&gt;&gt; print(first_pc_image.shape)\n    (100, 200)\n    &gt;&gt;&gt; print(explained_variance_ratio)\n    0.339\n    &gt;&gt;&gt; print(first_pc_vector.shape)\n    (3,)\n    \"\"\"\n    height, width, channels = bgr_image.shape\n    pixels = bgr_image.reshape(-1, channels)  # Flatten to Nx3 matrix\n\n    if standardize:\n        mean = np.mean(pixels, axis=0)\n        std = np.std(pixels, axis=0)\n        std[std == 0] = 1.0  # Avoid division by zero\n        pixels_scaled = (pixels - mean) / std\n    else:\n        pixels_scaled = pixels\n\n    # Perform SVD on standardized data to get principal components\n    U, d, Vt = np.linalg.svd(pixels_scaled, full_matrices=False)\n\n    # First PC is the projection of each pixel onto first right singular vector (Vt[0])\n    first_pc_vector = Vt[0]  # Shape: (3,)\n    eigen = d ** 2\n    total_variance = np.sum(eigen)\n\n    explained_variance_ratio = np.zeros_like(eigen)\n    np.divide(eigen, total_variance, out=explained_variance_ratio, where=total_variance != 0)\n\n    # Compute first principal component scores\n    first_pc_scores = U[:, 0] * d[0]\n\n    # Reshape to image shape and threshold negative values\n    first_pc_image = first_pc_scores.reshape(height, width)\n    # first_pc_thresholded = np.maximum(first_pc_image, 0)\n\n    return first_pc_image, explained_variance_ratio[0], first_pc_vector\n</code></pre>"},{"location":"api/cellects/image_analysis/image_segmentation/#cellects.image_analysis.image_segmentation.find_threshold_given_mask","title":"<code>find_threshold_given_mask(greyscale, mask, min_threshold=0)</code>","text":"<p>Find the optimal threshold value for a greyscale image given a mask.</p> <p>This function performs a binary search to find the optimal threshold that maximizes the separation between two regions defined by the mask. The search is bounded by a minimum threshold value.</p> <p>Parameters:</p> Name Type Description Default <code>greyscale</code> <code>ndarray of uint8</code> <p>The greyscale image array.</p> required <code>mask</code> <code>ndarray of uint8</code> <p>The binary mask array where positive values define region A and zero values define region B.</p> required <code>min_threshold</code> <code>uint8</code> <p>The minimum threshold value for the search. Defaults to 0.</p> <code>0</code> <p>Returns:</p> Name Type Description <code>out</code> <code>uint8</code> <p>The optimal threshold value found.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; greyscale = np.array([[255, 128, 54], [0, 64, 20]], dtype=np.uint8)\n&gt;&gt;&gt; mask = np.array([[1, 1, 0], [0, 0, 0]], dtype=np.uint8)\n&gt;&gt;&gt; find_threshold_given_mask(greyscale, mask)\n54\n</code></pre> Source code in <code>src/cellects/image_analysis/image_segmentation.py</code> <pre><code>def find_threshold_given_mask(greyscale: NDArray[np.uint8], mask: np.uint8, min_threshold: np.uint8=0) -&gt; np.uint8:\n    \"\"\"\n    Find the optimal threshold value for a greyscale image given a mask.\n\n    This function performs a binary search to find the optimal threshold\n    that maximizes the separation between two regions defined by the mask.\n    The search is bounded by a minimum threshold value.\n\n    Parameters\n    ----------\n    greyscale : ndarray of uint8\n        The greyscale image array.\n    mask : ndarray of uint8\n        The binary mask array where positive values define region A and zero values define region B.\n    min_threshold : uint8, optional\n        The minimum threshold value for the search. Defaults to 0.\n\n    Returns\n    -------\n    out : uint8\n        The optimal threshold value found.\n\n    Examples\n    --------\n    &gt;&gt;&gt; greyscale = np.array([[255, 128, 54], [0, 64, 20]], dtype=np.uint8)\n    &gt;&gt;&gt; mask = np.array([[1, 1, 0], [0, 0, 0]], dtype=np.uint8)\n    &gt;&gt;&gt; find_threshold_given_mask(greyscale, mask)\n    54\n    \"\"\"\n    region_a = greyscale[mask &gt; 0]\n    if len(region_a) == 0:\n        return np.uint8(255)\n    region_b = greyscale[mask == 0]\n    if len(region_b) == 0:\n        return min_threshold\n    else:\n        low = min_threshold\n        high = 255\n        best_thresh = low\n\n        while 0 &lt;= low &lt;= high:\n            mid = (low + high) // 2\n            count_a, count_b = _get_counts_jit(mid, region_a, region_b)\n\n            if count_a &gt; count_b:\n                # Try to find a lower threshold that still satisfies the condition\n                best_thresh = mid\n                high = mid - 1\n            else:\n                if count_a == 0 and count_b == 0:\n                    best_thresh = greyscale.mean()\n                    break\n                # Need higher threshold\n                low = mid + 1\n    return best_thresh\n</code></pre>"},{"location":"api/cellects/image_analysis/image_segmentation/#cellects.image_analysis.image_segmentation.generate_color_space_combination","title":"<code>generate_color_space_combination(bgr_image, c_spaces, first_dict, second_dict={}, background=None, background2=None, convert_to_uint8=False, all_c_spaces={})</code>","text":"<p>Generate color space combinations for an input image.</p> <p>This function generates a grayscale image by combining multiple color spaces from an input BGR image and provided dictionaries. Optionally, it can also generate a second grayscale image using another dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>bgr_image</code> <code>ndarray of uint8</code> <p>The input image in BGR color space.</p> required <code>c_spaces</code> <code>list</code> <p>List of color spaces to consider for combination.</p> required <code>first_dict</code> <code>Dict</code> <p>Dictionary containing color space and transformation details for the first grayscale image.</p> required <code>second_dict</code> <code>Dict</code> <p>Dictionary containing color space and transformation details for the second grayscale image.</p> <code>{}</code> <code>background</code> <code>ndarray</code> <p>Background image to be used. Default is None.</p> <code>None</code> <code>background2</code> <code>ndarray</code> <p>Second background image to be used for the second grayscale image. Default is None.</p> <code>None</code> <code>convert_to_uint8</code> <code>bool</code> <p>Flag indicating whether to convert the output images to uint8. Default is False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>out</code> <code>tuple of ndarray of uint8</code> <p>A tuple containing the first and second grayscale images.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; bgr_image = np.random.randint(0, 256, (100, 100, 3), dtype=np.uint8)\n&gt;&gt;&gt; c_spaces = ['bgr', 'hsv']\n&gt;&gt;&gt; first_dict = Dict()\n&gt;&gt;&gt; first_dict['bgr'] = np.array((0, 1, 1))\n&gt;&gt;&gt; second_dict = Dict()\n&gt;&gt;&gt; second_dict['hsv'] = np.array((0, 0, 1))\n&gt;&gt;&gt; greyscale_image1, greyscale_image2 = generate_color_space_combination(bgr_image, c_spaces, first_dict, second_dict)\n&gt;&gt;&gt; print(greyscale_image1.shape)\n(100, 100)\n</code></pre> Source code in <code>src/cellects/image_analysis/image_segmentation.py</code> <pre><code>def generate_color_space_combination(bgr_image: NDArray[np.uint8], c_spaces: list, first_dict: Dict, second_dict: Dict={}, background: NDArray=None, background2: NDArray=None, convert_to_uint8: bool=False, all_c_spaces: dict={}) -&gt; NDArray[np.uint8]:\n    \"\"\"\n    Generate color space combinations for an input image.\n\n    This function generates a grayscale image by combining multiple color spaces\n    from an input BGR image and provided dictionaries. Optionally, it can also generate\n    a second grayscale image using another dictionary.\n\n    Parameters\n    ----------\n    bgr_image : ndarray of uint8\n        The input image in BGR color space.\n    c_spaces : list\n        List of color spaces to consider for combination.\n    first_dict : Dict\n        Dictionary containing color space and transformation details for the first grayscale image.\n    second_dict : Dict, optional\n        Dictionary containing color space and transformation details for the second grayscale image.\n    background : ndarray, optional\n        Background image to be used. Default is None.\n    background2 : ndarray, optional\n        Second background image to be used for the second grayscale image. Default is None.\n    convert_to_uint8 : bool, optional\n        Flag indicating whether to convert the output images to uint8. Default is False.\n\n    Returns\n    -------\n    out : tuple of ndarray of uint8\n        A tuple containing the first and second grayscale images.\n\n    Examples\n    --------\n    &gt;&gt;&gt; bgr_image = np.random.randint(0, 256, (100, 100, 3), dtype=np.uint8)\n    &gt;&gt;&gt; c_spaces = ['bgr', 'hsv']\n    &gt;&gt;&gt; first_dict = Dict()\n    &gt;&gt;&gt; first_dict['bgr'] = np.array((0, 1, 1))\n    &gt;&gt;&gt; second_dict = Dict()\n    &gt;&gt;&gt; second_dict['hsv'] = np.array((0, 0, 1))\n    &gt;&gt;&gt; greyscale_image1, greyscale_image2 = generate_color_space_combination(bgr_image, c_spaces, first_dict, second_dict)\n    &gt;&gt;&gt; print(greyscale_image1.shape)\n    (100, 100)\n    \"\"\"\n    greyscale_image2 = None\n    first_pc_vector = None\n    if \"PCA\" in c_spaces:\n        greyscale_image, var_ratio, first_pc_vector = extract_first_pc(bgr_image)\n    else:\n        if len(all_c_spaces) == 0:\n            all_c_spaces = get_color_spaces(bgr_image, c_spaces)\n        try:\n            greyscale_image = combine_color_spaces(first_dict, all_c_spaces, background)\n        except:\n            first_dict = translate_dict(first_dict)\n            greyscale_image = combine_color_spaces(first_dict, all_c_spaces, background)\n        if len(second_dict) &gt; 0:\n            greyscale_image2 = combine_color_spaces(second_dict, all_c_spaces, background2)\n\n    if convert_to_uint8:\n        greyscale_image = bracket_to_uint8_image_contrast(greyscale_image)\n        if greyscale_image2 is not None and len(second_dict) &gt; 0:\n            greyscale_image2 = bracket_to_uint8_image_contrast(greyscale_image2)\n    return greyscale_image, greyscale_image2, all_c_spaces, first_pc_vector\n</code></pre>"},{"location":"api/cellects/image_analysis/image_segmentation/#cellects.image_analysis.image_segmentation.get_color_spaces","title":"<code>get_color_spaces(bgr_image, space_names='')</code>","text":"<p>Convert a BGR image into various color spaces.</p> <p>Converts the input BGR image to specified color spaces and returns them as a dictionary. If no space names are provided, converts to all default color spaces (LAB, HSV, LUV, HLS, YUV). If 'logical' is in the space names, it will be removed before conversion.</p> <p>Parameters:</p> Name Type Description Default <code>bgr_image</code> <code>ndarray of uint8</code> <p>Input image in BGR color space.</p> required <code>space_names</code> <code>list of str</code> <p>List of color spaces to convert the image to. Defaults to none.</p> <code>''</code> <p>Returns:</p> Name Type Description <code>out</code> <code>dict</code> <p>Dictionary with keys as color space names and values as the converted images.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; bgr_image = np.zeros((5, 5, 3), dtype=np.uint8)\n&gt;&gt;&gt; c_spaces = get_color_spaces(bgr_image, ['lab', 'hsv'])\n&gt;&gt;&gt; print(list(c_spaces.keys()))\n['bgr', 'lab', 'hsv']\n</code></pre> Source code in <code>src/cellects/image_analysis/image_segmentation.py</code> <pre><code>def get_color_spaces(bgr_image: NDArray[np.uint8], space_names: list=\"\") -&gt; Dict:\n    \"\"\"\n    Convert a BGR image into various color spaces.\n\n    Converts the input BGR image to specified color spaces and returns them\n    as a dictionary. If no space names are provided, converts to all default\n    color spaces (LAB, HSV, LUV, HLS, YUV). If 'logical' is in the space names,\n    it will be removed before conversion.\n\n    Parameters\n    ----------\n    bgr_image : ndarray of uint8\n        Input image in BGR color space.\n    space_names : list of str, optional\n        List of color spaces to convert the image to. Defaults to none.\n\n    Returns\n    -------\n    out : dict\n        Dictionary with keys as color space names and values as the converted images.\n\n    Examples\n    --------\n    &gt;&gt;&gt; bgr_image = np.zeros((5, 5, 3), dtype=np.uint8)\n    &gt;&gt;&gt; c_spaces = get_color_spaces(bgr_image, ['lab', 'hsv'])\n    &gt;&gt;&gt; print(list(c_spaces.keys()))\n    ['bgr', 'lab', 'hsv']\n    \"\"\"\n    if 'logical' in space_names:\n        space_names.pop(np.nonzero(np.array(space_names, dtype=str) == 'logical')[0][0])\n    c_spaces = Dict()\n    c_spaces['bgr'] = bgr_image.astype(np.float64)\n    if len(space_names) == 0:\n        c_spaces['lab'] = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2LAB).astype(np.float64)\n        c_spaces['hsv'] = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2HSV).astype(np.float64)\n        c_spaces['luv'] = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2LUV).astype(np.float64)\n        c_spaces['hls'] = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2HLS).astype(np.float64)\n        c_spaces['yuv'] = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2YUV).astype(np.float64)\n    else:\n        if np.isin('lab', space_names):\n            c_spaces['lab'] = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2LAB).astype(np.float64)\n        if np.isin('hsv', space_names):\n            c_spaces['hsv'] = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2HSV).astype(np.float64)\n        if np.isin('luv', space_names):\n            c_spaces['luv'] = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2LUV).astype(np.float64)\n        if np.isin('hls', space_names):\n            c_spaces['hls'] = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2HLS).astype(np.float64)\n        if np.isin('yuv', space_names):\n            c_spaces['yuv'] = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2YUV).astype(np.float64)\n    return c_spaces\n</code></pre>"},{"location":"api/cellects/image_analysis/image_segmentation/#cellects.image_analysis.image_segmentation.get_otsu_threshold","title":"<code>get_otsu_threshold(image)</code>","text":"<p>Calculate the optimal threshold value for an image using Otsu's method.</p> <p>This function computes the Otsu's thresholding which automatically performs histogram shape analysis for threshold selection.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>NDArray</code> <p>The input grayscale image, represented as a NumPy array.</p> required <p>Returns:</p> Type Description <code>int or float</code> <p>The computed Otsu's threshold value.</p> Source code in <code>src/cellects/image_analysis/image_segmentation.py</code> <pre><code>@njit()\ndef get_otsu_threshold(image: NDArray):\n    \"\"\"\n    Calculate the optimal threshold value for an image using Otsu's method.\n\n    This function computes the Otsu's thresholding which automatically\n    performs histogram shape analysis for threshold selection.\n\n    Parameters\n    ----------\n    image : NDArray\n        The input grayscale image, represented as a NumPy array.\n\n    Returns\n    -------\n    int or float\n        The computed Otsu's threshold value.\n    \"\"\"\n    # Set total number of bins in the histogram\n    bins_num = 256\n\n    # Get the image histogram\n    hist, bin_edges = np.histogram(image, bins=bins_num)\n\n    # Calculate centers of bins\n    bin_mids = (bin_edges[:-1] + bin_edges[1:]) / 2.\n\n    # Iterate over all thresholds (indices) and get the probabilities w1(t), w2(t)\n    weight1 = np.cumsum(hist)\n    weight2 = np.cumsum(hist[::-1])[::-1]\n\n    # Get the class means mu0(t)\n    if weight1.all():\n        mean1 = np.cumsum(hist * bin_mids) / weight1\n    else:\n        mean1 = np.zeros_like(bin_mids)\n\n    # Get the class means mu1(t)\n    if weight2.all():\n        mean2 = (np.cumsum((hist * bin_mids)[::-1]) / weight2[::-1])[::-1]\n    else:\n        mean2 = np.zeros_like(bin_mids)\n\n    inter_class_variance = weight1[:-1] * weight2[1:] * (mean1[:-1] - mean2[1:]) ** 2\n\n    # Maximize the inter_class_variance function val\n    index_of_max_val = np.argmax(inter_class_variance)\n\n    threshold = bin_mids[:-1][index_of_max_val]\n    return threshold\n</code></pre>"},{"location":"api/cellects/image_analysis/image_segmentation/#cellects.image_analysis.image_segmentation.kmeans","title":"<code>kmeans(greyscale, greyscale2=None, kmeans_clust_nb=2, bio_mask=None, back_mask=None, logical='None', bio_label=None, bio_label2=None, previous_binary_image=None)</code>","text":"<p>Perform K-means clustering on a greyscale image to generate binary images.</p> Extended Description <p>This function applies the K-means algorithm to a greyscale image or pair of images to segment them into binary images. It supports optional masks and previous segmentation labels for refining the clustering.</p> <p>Parameters:</p> Name Type Description Default <code>greyscale</code> <code>NDArray</code> <p>The input greyscale image to segment.</p> required <code>greyscale2</code> <code>NDArray</code> <p>A second greyscale image for logical operations. Default is <code>None</code>.</p> <code>None</code> <code>kmeans_clust_nb</code> <code>int</code> <p>Number of clusters for K-means. Default is <code>2</code>.</p> <code>2</code> <code>bio_mask</code> <code>NDArray[uint8]</code> <p>Mask for selecting biological objects. Default is <code>None</code>.</p> <code>None</code> <code>back_mask</code> <code>NDArray[uint8]</code> <p>Mask for selecting background regions. Default is <code>None</code>.</p> <code>None</code> <code>logical</code> <code>str</code> <p>Logical operation flag to enable processing of the second image. Default is <code>'None'</code>.</p> <code>'None'</code> <code>bio_label</code> <code>int</code> <p>Label for biological objects in the first segmentation. Default is <code>None</code>.</p> <code>None</code> <code>bio_label2</code> <code>int</code> <p>Label for biological objects in the second segmentation. Default is <code>None</code>.</p> <code>None</code> <code>previous_binary_image</code> <code>NDArray[uint8]</code> <p>Previous binary image for refinement. Default is <code>None</code>.</p> <code>None</code> <p>Other Parameters:</p> Name Type Description <code>**greyscale2</code> <code>NDArray</code> <code>logical</code> <code>NDArray</code> <code>bio_label2</code> <code>NDArray</code> <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing: - <code>binary_image</code>: Binary image derived from the first input. - <code>binary_image2</code>: Binary image for the second input if processed, else <code>None</code>. - <code>new_bio_label</code>: New biological label for the first segmentation. - <code>new_bio_label2</code>: New biological label for the second segmentation, if applicable.</p> Notes <ul> <li>The function performs K-means clustering with random centers.</li> <li>If <code>logical</code> is not <code>'None'</code>, both images are processed.</li> <li>Default clustering uses 2 clusters, modify <code>kmeans_clust_nb</code> for different needs.</li> </ul> Source code in <code>src/cellects/image_analysis/image_segmentation.py</code> <pre><code>def kmeans(greyscale: NDArray, greyscale2: NDArray=None, kmeans_clust_nb: int=2,\n           bio_mask: NDArray[np.uint8]=None, back_mask: NDArray[np.uint8]=None, logical: str='None',\n           bio_label=None, bio_label2=None, previous_binary_image: NDArray[np.uint8]=None):\n    \"\"\"\n\n    Perform K-means clustering on a greyscale image to generate binary images.\n\n    Extended Description\n    --------------------\n    This function applies the K-means algorithm to a greyscale image or pair of images to segment them into binary images. It supports optional masks and previous segmentation labels for refining the clustering.\n\n    Parameters\n    ----------\n    greyscale : NDArray\n        The input greyscale image to segment.\n    greyscale2 : NDArray, optional\n        A second greyscale image for logical operations. Default is `None`.\n    kmeans_clust_nb : int, optional\n        Number of clusters for K-means. Default is `2`.\n    bio_mask : NDArray[np.uint8], optional\n        Mask for selecting biological objects. Default is `None`.\n    back_mask : NDArray[np.uint8], optional\n        Mask for selecting background regions. Default is `None`.\n    logical : str, optional\n        Logical operation flag to enable processing of the second image. Default is `'None'`.\n    bio_label : int, optional\n        Label for biological objects in the first segmentation. Default is `None`.\n    bio_label2 : int, optional\n        Label for biological objects in the second segmentation. Default is `None`.\n    previous_binary_image : NDArray[np.uint8], optional\n        Previous binary image for refinement. Default is `None`.\n\n    Other Parameters\n    ----------------\n    **greyscale2, logical, bio_label2**: Optional parameters for processing a second image with logical operations.\n\n    Returns\n    -------\n    tuple\n        A tuple containing:\n        - `binary_image`: Binary image derived from the first input.\n        - `binary_image2`: Binary image for the second input if processed, else `None`.\n        - `new_bio_label`: New biological label for the first segmentation.\n        - `new_bio_label2`: New biological label for the second segmentation, if applicable.\n\n    Notes\n    -----\n    - The function performs K-means clustering with random centers.\n    - If `logical` is not `'None'`, both images are processed.\n    - Default clustering uses 2 clusters, modify `kmeans_clust_nb` for different needs.\n\n    \"\"\"\n    if isinstance(bio_mask, np.ndarray):\n        bio_mask = np.nonzero(bio_mask)\n    if isinstance(back_mask, np.ndarray):\n        back_mask = np.nonzero(back_mask)\n    new_bio_label = None\n    new_bio_label2 = None\n    binary_image2 = None\n    image = greyscale.reshape((-1, 1))\n    image = np.float32(image)\n    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)\n    compactness, label, center = cv2.kmeans(image, kmeans_clust_nb, None, criteria, attempts=10, flags=cv2.KMEANS_RANDOM_CENTERS)\n    kmeans_image = np.uint8(label.flatten().reshape(greyscale.shape[:2]))\n    sum_per_label = np.zeros(kmeans_clust_nb)\n    binary_image = np.zeros(greyscale.shape[:2], np.uint8)\n    if previous_binary_image is not None:\n        binary_images = []\n        image_scores = np.zeros(kmeans_clust_nb, np.uint64)\n        for i in range(kmeans_clust_nb):\n            binary_image_i = np.zeros(greyscale.shape[:2], np.uint8)\n            binary_image_i[kmeans_image == i] = 1\n            image_scores[i] = (binary_image_i * previous_binary_image).sum()\n            binary_images.append(binary_image_i)\n        binary_image[kmeans_image == np.argmax(image_scores)] = 1\n    elif bio_label is not None:\n        binary_image[kmeans_image == bio_label] = 1\n        new_bio_label = bio_label\n    else:\n        if bio_mask is not None:\n            all_labels = kmeans_image[bio_mask[0], bio_mask[1]]\n            for i in range(kmeans_clust_nb):\n                sum_per_label[i] = (all_labels == i).sum()\n            new_bio_label = np.argsort(sum_per_label)[1]\n        elif back_mask is not None:\n            all_labels = kmeans_image[back_mask[0], back_mask[1]]\n            for i in range(kmeans_clust_nb):\n                sum_per_label[i] = (all_labels == i).sum()\n            new_bio_label = np.argsort(sum_per_label)[-2]\n        else:\n            for i in range(kmeans_clust_nb):\n                sum_per_label[i] = (kmeans_image == i).sum()\n            new_bio_label = np.argsort(sum_per_label)[-2]\n        binary_image += np.isin(kmeans_image, new_bio_label)\n\n    if logical != 'None' and greyscale2 is not None:\n        image = greyscale2.reshape((-1, 1))\n        image = np.float32(image)\n        criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)\n        compactness, label, center = cv2.kmeans(image, kmeans_clust_nb, None, criteria, attempts=10,\n                                                flags=cv2.KMEANS_RANDOM_CENTERS)\n        kmeans_image = np.uint8(label.flatten().reshape(greyscale.shape[:2]))\n        sum_per_label = np.zeros(kmeans_clust_nb)\n        binary_image2 = np.zeros(greyscale.shape[:2], np.uint8)\n        if previous_binary_image is not None:\n            binary_images = []\n            image_scores = np.zeros(kmeans_clust_nb, np.uint64)\n            for i in range(kmeans_clust_nb):\n                binary_image_i = np.zeros(greyscale.shape[:2], np.uint8)\n                binary_image_i[kmeans_image == i] = 1\n                image_scores[i] = (binary_image_i * previous_binary_image).sum()\n                binary_images.append(binary_image_i)\n            binary_image2[kmeans_image == np.argmax(image_scores)] = 1\n        elif bio_label2 is not None:\n            binary_image2[kmeans_image == bio_label2] = 1\n            new_bio_label2 = bio_label2\n        else:\n            if bio_mask is not None:\n                all_labels = kmeans_image[bio_mask[0], bio_mask[1]]\n                for i in range(kmeans_clust_nb):\n                    sum_per_label[i] = (all_labels == i).sum()\n                new_bio_label2 = np.argsort(sum_per_label)[1]\n            elif back_mask is not None:\n                all_labels = kmeans_image[back_mask[0], back_mask[1]]\n                for i in range(kmeans_clust_nb):\n                    sum_per_label[i] = (all_labels == i).sum()\n                new_bio_label2 = np.argsort(sum_per_label)[-2]\n            else:\n                for i in range(kmeans_clust_nb):\n                    sum_per_label[i] = (kmeans_image == i).sum()\n                new_bio_label2 = np.argsort(sum_per_label)[-2]\n            binary_image2[kmeans_image == new_bio_label2] = 1\n    return binary_image, binary_image2, new_bio_label, new_bio_label2\n</code></pre>"},{"location":"api/cellects/image_analysis/image_segmentation/#cellects.image_analysis.image_segmentation.otsu_thresholding","title":"<code>otsu_thresholding(image)</code>","text":"<p>Apply Otsu's thresholding to a grayscale image.</p> <p>This function calculates the optimal threshold using Otsu's method and applies it to binarize the input image. The output is a binary image where pixel values are either 0 or 1.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>Input grayscale image with any kind of value.</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>ndarray of uint8</code> <p>Binarized image with pixel values 0 or 1.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; image = np.array([10, 20, 30])\n&gt;&gt;&gt; result = otsu_thresholding(image)\n&gt;&gt;&gt; print(result)\n[1 0 0]\n</code></pre> Source code in <code>src/cellects/image_analysis/image_segmentation.py</code> <pre><code>@njit()\ndef otsu_thresholding(image: NDArray) -&gt; NDArray[np.uint8]:\n    \"\"\"\n    Apply Otsu's thresholding to a grayscale image.\n\n    This function calculates the optimal threshold using\n    Otsu's method and applies it to binarize the input image.\n    The output is a binary image where pixel values are either\n    0 or 1.\n\n    Parameters\n    ----------\n    image : ndarray\n        Input grayscale image with any kind of value.\n\n    Returns\n    -------\n    out : ndarray of uint8\n        Binarized image with pixel values 0 or 1.\n\n    Examples\n    --------\n    &gt;&gt;&gt; image = np.array([10, 20, 30])\n    &gt;&gt;&gt; result = otsu_thresholding(image)\n    &gt;&gt;&gt; print(result)\n    [1 0 0]\n    \"\"\"\n    threshold = get_otsu_threshold(image)\n    binary_image = (image &gt; threshold)\n    if binary_image.sum() &lt; binary_image.size / 2:\n        return binary_image.astype(np.uint8)\n    else:\n        return np.logical_not(binary_image).astype(np.uint8)\n</code></pre>"},{"location":"api/cellects/image_analysis/image_segmentation/#cellects.image_analysis.image_segmentation.rolling_window_segmentation","title":"<code>rolling_window_segmentation(greyscale_image, possibly_filled_pixels, patch_size=(10, 10))</code>","text":"<p>Perform rolling window segmentation on a greyscale image, using potentially filled pixels and a specified patch size.</p> <p>The function divides the input greyscale image into overlapping patches defined by <code>patch_size</code>, and applies Otsu's thresholding method to each patch. The thresholds can be optionally refined using a minimization algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>greyscale_image</code> <code>ndarray of uint8</code> <p>The input greyscale image to segment.</p> required <code>possibly_filled_pixels</code> <code>ndarray of uint8</code> <p>An array indicating which pixels are possibly filled.</p> required <code>patch_size</code> <code>tuple</code> <p>The dimensions of the patches to segment. Default is (10, 10). Must be superior to (1, 1).</p> <code>(10, 10)</code> <p>Returns:</p> Name Type Description <code>output</code> <code>ndarray of uint8</code> <p>The segmented binary image where the network is marked as True.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; greyscale_image = np.array([[1, 2, 1, 1], [1, 3, 4, 1], [2, 4, 3, 1], [2, 1, 2, 1]])\n&gt;&gt;&gt; possibly_filled_pixels = greyscale_image &gt; 1\n&gt;&gt;&gt; patch_size = (2, 2)\n&gt;&gt;&gt; result = rolling_window_segmentation(greyscale_image, possibly_filled_pixels, patch_size)\n&gt;&gt;&gt; print(result)\n[[0 1 0 0]\n [0 1 1 0]\n [0 1 1 0]\n [0 0 1 0]]\n</code></pre> Source code in <code>src/cellects/image_analysis/image_segmentation.py</code> <pre><code>def rolling_window_segmentation(greyscale_image: NDArray, possibly_filled_pixels: NDArray, patch_size: tuple=(10, 10)) -&gt; NDArray[np.uint8]:\n    \"\"\"\n    Perform rolling window segmentation on a greyscale image, using potentially filled pixels and a specified patch size.\n\n    The function divides the input greyscale image into overlapping patches defined by `patch_size`,\n    and applies Otsu's thresholding method to each patch. The thresholds can be optionally\n    refined using a minimization algorithm.\n\n    Parameters\n    ----------\n    greyscale_image : ndarray of uint8\n        The input greyscale image to segment.\n    possibly_filled_pixels : ndarray of uint8\n        An array indicating which pixels are possibly filled.\n    patch_size : tuple, optional\n        The dimensions of the patches to segment. Default is (10, 10).\n        Must be superior to (1, 1).\n\n    Returns\n    -------\n    output : ndarray of uint8\n        The segmented binary image where the network is marked as True.\n\n    Examples\n    --------\n    &gt;&gt;&gt; greyscale_image = np.array([[1, 2, 1, 1], [1, 3, 4, 1], [2, 4, 3, 1], [2, 1, 2, 1]])\n    &gt;&gt;&gt; possibly_filled_pixels = greyscale_image &gt; 1\n    &gt;&gt;&gt; patch_size = (2, 2)\n    &gt;&gt;&gt; result = rolling_window_segmentation(greyscale_image, possibly_filled_pixels, patch_size)\n    &gt;&gt;&gt; print(result)\n    [[0 1 0 0]\n     [0 1 1 0]\n     [0 1 1 0]\n     [0 0 1 0]]\n    \"\"\"\n    patch_centers = [\n        np.floor(np.linspace(\n            p // 2, s - p // 2, int(np.ceil(s / (p // 2))) - 1\n        )).astype(int)\n        for s, p in zip(greyscale_image.shape, patch_size)\n    ]\n    patch_centers = np.transpose(np.meshgrid(*patch_centers), (1, 2, 0)).reshape((-1, 2))\n\n    patch_slices = [\n        tuple(slice(c - p // 2, c + p // 2, 1)\n              for c, p in zip(p_c, patch_size)) for p_c in patch_centers\n    ]\n    maximize_parameter = False\n\n    network_patches = []\n    patch_thresholds = []\n    # for patch in tqdm(patch_slices):\n    for patch in patch_slices:\n        v = greyscale_image[patch] * possibly_filled_pixels[patch]\n        if v.max() &gt; 0 and np.ptp(v) &gt; 0.5:\n            t = threshold_otsu(v)\n\n            if maximize_parameter:\n                res = minimize(_network_perimeter, x0=t, args=(v,), method='Nelder-Mead')\n                t = res.x[0]\n\n            network_patches.append(v &gt; t)\n            patch_thresholds.append(t)\n        else:\n            network_patches.append(np.zeros_like(v))\n            patch_thresholds.append(0)\n\n    network_img = np.zeros(greyscale_image.shape, dtype=np.float64)\n    count_img = np.zeros_like(greyscale_image)\n    for patch, network_patch, t in zip(patch_slices, network_patches, patch_thresholds):\n        network_img[patch] += network_patch\n        count_img[patch] += np.ones_like(network_patch)\n\n    # Safe in-place division: zeros remain where count_img == 0\n    np.divide(network_img, count_img, out=network_img, where=count_img != 0)\n\n    return (network_img &gt; 0.5).astype(np.uint8)\n</code></pre>"},{"location":"api/cellects/image_analysis/image_segmentation/#cellects.image_analysis.image_segmentation.segment_with_lum_value","title":"<code>segment_with_lum_value(converted_video, basic_bckgrnd_values, l_threshold, lighter_background)</code>","text":"<p>Segment video frames based on luminance threshold.</p> <p>This function segments the input video frames by comparing against a dynamic luminance threshold. The segmentation can be based on either lighter or darker background.</p> <p>Parameters:</p> Name Type Description Default <code>converted_video</code> <code>ndarray</code> <p>The input video frames in numpy array format.</p> required <code>basic_bckgrnd_values</code> <code>ndarray</code> <p>Array containing background values for each frame.</p> required <code>l_threshold</code> <code>int or float</code> <p>The luminance threshold value for segmentation.</p> required <code>lighter_background</code> <code>bool</code> <p>If True, the segmentation is done assuming a lighter background. Defaults to False.</p> required <p>Returns:</p> Name Type Description <code>segmentation</code> <code>ndarray</code> <p>Array containing the segmented video frames.</p> <code>l_threshold_over_time</code> <code>ndarray</code> <p>Computed threshold over time for each frame.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; converted_video = np.array([[[100, 120], [130, 140]], [[160, 170], [180, 200]]], dtype=np.uint8)\n&gt;&gt;&gt; basic_bckgrnd_values = np.array([100, 120])\n&gt;&gt;&gt; lighter_background = False\n&gt;&gt;&gt; l_threshold = 130\n&gt;&gt;&gt; segmentation, threshold_over_time = segment_with_lum_value(converted_video, basic_bckgrnd_values, l_threshold, lighter_background)\n&gt;&gt;&gt; print(segmentation)\n[[[0 1]\n  [1 1]]\n [[1 1]\n  [1 1]]]\n</code></pre> Source code in <code>src/cellects/image_analysis/image_segmentation.py</code> <pre><code>def segment_with_lum_value(converted_video: NDArray, basic_bckgrnd_values: NDArray, l_threshold, lighter_background: bool) -&gt; Tuple[NDArray, NDArray]:\n    \"\"\"\n    Segment video frames based on luminance threshold.\n\n    This function segments the input video frames by comparing against a dynamic\n    luminance threshold. The segmentation can be based on either lighter or darker\n    background.\n\n    Parameters\n    ----------\n    converted_video : ndarray\n        The input video frames in numpy array format.\n\n    basic_bckgrnd_values : ndarray\n        Array containing background values for each frame.\n\n    l_threshold : int or float\n        The luminance threshold value for segmentation.\n\n    lighter_background : bool, optional\n        If True, the segmentation is done assuming a lighter background.\n        Defaults to False.\n\n    Returns\n    -------\n    segmentation : ndarray\n        Array containing the segmented video frames.\n    l_threshold_over_time : ndarray\n        Computed threshold over time for each frame.\n\n    Examples\n    --------\n    &gt;&gt;&gt; converted_video = np.array([[[100, 120], [130, 140]], [[160, 170], [180, 200]]], dtype=np.uint8)\n    &gt;&gt;&gt; basic_bckgrnd_values = np.array([100, 120])\n    &gt;&gt;&gt; lighter_background = False\n    &gt;&gt;&gt; l_threshold = 130\n    &gt;&gt;&gt; segmentation, threshold_over_time = segment_with_lum_value(converted_video, basic_bckgrnd_values, l_threshold, lighter_background)\n    &gt;&gt;&gt; print(segmentation)\n    [[[0 1]\n      [1 1]]\n     [[1 1]\n      [1 1]]]\n\n    \"\"\"\n    # segmentation = None\n    if lighter_background:\n        l_threshold_over_time = l_threshold - (basic_bckgrnd_values[-1] - basic_bckgrnd_values)\n        if np.all(np.logical_and(0 &lt;= l_threshold_over_time, l_threshold_over_time &lt;= 255)):\n            segmentation = less_along_first_axis(converted_video, l_threshold_over_time)\n        else:\n            segmentation = np.zeros_like(converted_video)\n            if l_threshold &gt; 255:\n                l_threshold = 255\n            segmentation += converted_video &gt; l_threshold\n    else:\n        l_threshold_over_time = l_threshold - (basic_bckgrnd_values[-1] - basic_bckgrnd_values)\n        if np.all(np.logical_and(0 &lt;= l_threshold_over_time, l_threshold_over_time &lt;= 255)):\n            segmentation = greater_along_first_axis(converted_video, l_threshold_over_time)\n        else:\n            segmentation = np.zeros_like(converted_video)\n            if l_threshold &gt; 255:\n                l_threshold = 255\n            segmentation += converted_video &gt; l_threshold\n    return segmentation, l_threshold_over_time\n</code></pre>"},{"location":"api/cellects/image_analysis/image_segmentation/#cellects.image_analysis.image_segmentation.windowed_thresholding","title":"<code>windowed_thresholding(image, lighter_background=None, side_length=None, step=None, min_int_var=None)</code>","text":"<p>Perform grid segmentation on the image.</p> <p>This method applies a sliding window approach to segment the image into a grid-like pattern based on intensity variations and optionally uses a mask. The segmented regions are stored in <code>self.binary_image</code>.</p> <p>Args:     lighter_background (bool): If True, areas lighter than the Otsu threshold are considered;         otherwise, darker areas are considered.     side_length (int, optional): The size of each grid square. Default is None.     step (int, optional): The step size for the sliding window. Default is None.     min_int_var (int, optional): Threshold for intensity variation within a grid.         Default is 20.     mask (NDArray, optional): A binary mask to restrict the segmentation area. Default is None.</p> Source code in <code>src/cellects/image_analysis/image_segmentation.py</code> <pre><code>def windowed_thresholding(image:NDArray, lighter_background: bool=None, side_length: int=None, step: int=None, min_int_var: float=None):\n    \"\"\"\n    Perform grid segmentation on the image.\n\n    This method applies a sliding window approach to segment the image into\n    a grid-like pattern based on intensity variations and optionally uses a mask.\n    The segmented regions are stored in `self.binary_image`.\n\n    Args:\n        lighter_background (bool): If True, areas lighter than the Otsu threshold are considered;\n            otherwise, darker areas are considered.\n        side_length (int, optional): The size of each grid square. Default is None.\n        step (int, optional): The step size for the sliding window. Default is None.\n        min_int_var (int, optional): Threshold for intensity variation within a grid.\n            Default is 20.\n        mask (NDArray, optional): A binary mask to restrict the segmentation area. Default is None.\n    \"\"\"\n    if lighter_background is None:\n        binary_image = otsu_thresholding(image)\n        lighter_background = binary_image.sum() &gt; (binary_image.size / 2)\n    if min_int_var is None:\n        min_int_var = np.ptp(image).astype(np.float64) * 0.1\n    if side_length is None:\n        side_length = int(np.min(image.shape) // 10)\n    if step is None:\n        step = side_length // 2\n    grid_image = np.zeros(image.shape, np.uint64)\n    homogeneities = np.zeros(image.shape, np.uint64)\n    mask = np.ones(image.shape, np.uint64)\n    for to_add in np.arange(0, side_length, step):\n        y_windows = np.arange(0, image.shape[0], side_length)\n        x_windows = np.arange(0, image.shape[1], side_length)\n        y_windows += to_add\n        x_windows += to_add\n        for y_start in y_windows:\n            if y_start &lt; image.shape[0]:\n                y_end = y_start + side_length\n                if y_end &lt; image.shape[0]:\n                    for x_start in x_windows:\n                        if x_start &lt; image.shape[1]:\n                            x_end = x_start + side_length\n                            if x_end &lt; image.shape[1]:\n                                if np.any(mask[y_start:y_end, x_start:x_end]):\n                                    potential_detection = image[y_start:y_end, x_start:x_end]\n                                    if np.any(potential_detection):\n                                        if np.ptp(potential_detection[np.nonzero(potential_detection)]) &lt; min_int_var:\n                                            homogeneities[y_start:y_end, x_start:x_end] += 1\n                                        threshold = get_otsu_threshold(potential_detection)\n                                        if lighter_background:\n                                            net_coord = np.nonzero(potential_detection &lt; threshold)\n                                        else:\n                                            net_coord = np.nonzero(potential_detection &gt; threshold)\n                                        grid_image[y_start + net_coord[0], x_start + net_coord[1]] += 1\n\n    binary_image = (grid_image &gt;= (side_length // step)).astype(np.uint8)\n    binary_image[homogeneities &gt;= (((side_length // step) // 2) + 1)] = 0\n    return binary_image\n</code></pre>"},{"location":"api/cellects/image_analysis/morphological_operations/","title":"<code>cellects.image_analysis.morphological_operations</code>","text":""},{"location":"api/cellects/image_analysis/morphological_operations/#cellects.image_analysis.morphological_operations","title":"<code>cellects.image_analysis.morphological_operations</code>","text":"<p>This module provides methods to analyze and modify shapes in binary images. It includes functions for comparing neighboring pixels, generating shape descriptors, and performing morphological operations like expanding shapes and filling holes.</p> <p>Classes:</p> Name Description <code>CompareNeighborsWithValue : Class to compare neighboring pixels to a specified value</code> <p>Functions:</p> Name Description <code>cc : Sort connected components according to size</code> <code>make_gravity_field : Create a gradient field around shapes</code> <code>find_median_shape : Generate median shape from multiple inputs</code> <code>make_numbered_rays : Create numbered rays for analysis</code> <code>CompareNeighborsWithFocal : Compare neighboring pixels to focal values</code> <code>ShapeDescriptors : Generate shape descriptors using provided functions</code> <code>get_radius_distance_against_time : Calculate radius distances over time</code> <code>expand_until_one : Expand shapes until a single connected component remains</code> <code>expand_and_rate_until_one : Expand and rate shapes until one remains</code> <code>expand_until_overlap : Expand shapes until overlap occurs</code> <code>dynamically_expand_to_fill_holes : Dynamically expand to fill holes in shapes</code> <code>expand_smalls_toward_biggest : Expand smaller shapes toward largest component</code> <code>change_thresh_until_one : Change threshold until one connected component remains</code> <code>create_ellipse : Generate ellipse shape descriptors</code> <code>get_rolling_window_coordinates_list : Get coordinates for rolling window operations</code>"},{"location":"api/cellects/image_analysis/morphological_operations/#cellects.image_analysis.morphological_operations.CompareNeighborsWithValue","title":"<code>CompareNeighborsWithValue</code>","text":"<p>CompareNeighborsWithValue class to summarize each pixel by comparing its neighbors to a value.</p> <p>This class analyzes pixels in a 2D array, comparing each pixel's neighbors to a specified value. The comparison can be equality, superiority, or inferiority, and neighbors can be the 4 or 8 nearest pixels based on the connectivity parameter.</p> Source code in <code>src/cellects/image_analysis/morphological_operations.py</code> <pre><code>class CompareNeighborsWithValue:\n    \"\"\"\n    CompareNeighborsWithValue class to summarize each pixel by comparing its neighbors to a value.\n\n    This class analyzes pixels in a 2D array, comparing each pixel's neighbors\n    to a specified value. The comparison can be equality, superiority,\n    or inferiority, and neighbors can be the 4 or 8 nearest pixels based on\n    the connectivity parameter.\n    \"\"\"\n    def __init__(self, array: np.ndarray, connectivity: int=None, data_type: np.dtype=np.int8):\n        \"\"\"\n        Initialize a class for array connectivity processing.\n\n        This class processes arrays based on given connectivities, creating\n        windows around the original data for both 1D and 2D arrays. Depending on\n        the connectivity value (4 or 8), it creates different windows with borders.\n\n        Parameters\n        ----------\n        array : ndarray\n            Input array to process, can be 1D or 2D.\n        connectivity : int, optional\n            Connectivity type for processing (4 or 8), by default None.\n        data_type : dtype, optional\n            Data type for the array elements, by default np.int8.\n\n        Attributes\n        ----------\n        array : ndarray\n            The processed array based on the given data type.\n        connectivity : int\n            Connectivity value used for processing.\n        on_the_right : ndarray\n            Array with shifted elements to the right.\n        on_the_left : ndarray\n            Array with shifted elements to the left.\n        on_the_bot : ndarray, optional\n            Array with shifted elements to the bottom (for 2D arrays).\n        on_the_top : ndarray, optional\n            Array with shifted elements to the top (for 2D arrays).\n        on_the_topleft : ndarray, optional\n            Array with shifted elements to the top left (for 2D arrays).\n        on_the_topright : ndarray, optional\n            Array with shifted elements to the top right (for 2D arrays).\n        on_the_botleft : ndarray, optional\n            Array with shifted elements to the bottom left (for 2D arrays).\n        on_the_botright : ndarray, optional\n            Array with shifted elements to the bottom right (for 2D arrays).\n        \"\"\"\n        array = array.astype(data_type)\n        self.array = array\n        self.connectivity = connectivity\n        if len(self.array.shape) == 1:\n            self.on_the_right = np.append(array[1:], array[-1])\n            self.on_the_left = np.append(array[0], array[:-1])\n        else:\n            # Build 4 window of the original array, each missing one of the four borders\n            # Grow each window with a copy of the last border at the opposite of the side a border have been deleted\n            if self.connectivity == 4 or self.connectivity == 8:\n                self.on_the_right = np.column_stack((array[:, 1:], array[:, -1]))\n                self.on_the_left = np.column_stack((array[:, 0], array[:, :-1]))\n                self.on_the_bot = np.vstack((array[1:, :], array[-1, :]))\n                self.on_the_top = np.vstack((array[0, :], array[:-1, :]))\n            if self.connectivity != 4:\n                self.on_the_topleft = array[:-1, :-1]\n                self.on_the_topright = array[:-1, 1:]\n                self.on_the_botleft = array[1:, :-1]\n                self.on_the_botright = array[1:, 1:]\n\n                self.on_the_topleft = np.vstack((self.on_the_topleft[0, :], self.on_the_topleft))\n                self.on_the_topleft = np.column_stack((self.on_the_topleft[:, 0], self.on_the_topleft))\n\n                self.on_the_topright = np.vstack((self.on_the_topright[0, :], self.on_the_topright))\n                self.on_the_topright = np.column_stack((self.on_the_topright, self.on_the_topright[:, -1]))\n\n                self.on_the_botleft = np.vstack((self.on_the_botleft, self.on_the_botleft[-1, :]))\n                self.on_the_botleft = np.column_stack((self.on_the_botleft[:, 0], self.on_the_botleft))\n\n                self.on_the_botright = np.vstack((self.on_the_botright, self.on_the_botright[-1, :]))\n                self.on_the_botright = np.column_stack((self.on_the_botright, self.on_the_botright[:, -1]))\n\n    def is_equal(self, value, and_itself: bool=False):\n        \"\"\"\n        Check equality of neighboring values in an array.\n\n        This method compares the neighbors of each element in `self.array` to a given value.\n        Depending on the dimensionality and connectivity settings, it checks different neighboring\n        elements.\n\n        Parameters\n        ----------\n        value : int or float\n            The value to check equality with neighboring elements.\n        and_itself : bool, optional\n            If True, also check equality with the element itself. Defaults to False.\n\n        Returns\n        -------\n        None\n\n        Attributes (not standard Qt properties)\n        --------------------------------------\n        equal_neighbor_nb : ndarray of uint8\n            Array that holds the number of equal neighbors for each element.\n\n        Examples\n        --------\n        &gt;&gt;&gt; matrix = np.array([[9, 0, 4, 6], [4, 9, 1, 3], [7, 2, 1, 4], [9, 0, 8, 5]], dtype=np.int8)\n        &gt;&gt;&gt; compare = CompareNeighborsWithValue(matrix, connectivity=4)\n        &gt;&gt;&gt; compare.is_equal(1)\n        &gt;&gt;&gt; print(compare.equal_neighbor_nb)\n        [[0 0 1 0]\n        [0 1 1 1]\n        [0 1 1 1]\n        [0 0 1 0]]\n        \"\"\"\n\n        if len(self.array.shape) == 1:\n            self.equal_neighbor_nb = np.sum((np.equal(self.on_the_right, value), np.equal(self.on_the_left, value)), axis=0)\n        else:\n            if self.connectivity == 4:\n                self.equal_neighbor_nb =  np.dstack((np.equal(self.on_the_right, value), np.equal(self.on_the_left, value),\n                                                 np.equal(self.on_the_bot, value), np.equal(self.on_the_top, value)))\n            elif self.connectivity == 8:\n                self.equal_neighbor_nb =  np.dstack(\n                    (np.equal(self.on_the_right, value), np.equal(self.on_the_left, value),\n                     np.equal(self.on_the_bot, value), np.equal(self.on_the_top, value),\n                     np.equal(self.on_the_topleft, value), np.equal(self.on_the_topright, value),\n                     np.equal(self.on_the_botleft, value), np.equal(self.on_the_botright, value)))\n            else:\n                self.equal_neighbor_nb =  np.dstack(\n                    (np.equal(self.on_the_topleft, value), np.equal(self.on_the_topright, value),\n                     np.equal(self.on_the_botleft, value), np.equal(self.on_the_botright, value)))\n            self.equal_neighbor_nb = np.sum(self.equal_neighbor_nb, 2, dtype=np.uint8)\n\n        if and_itself:\n            self.equal_neighbor_nb[np.not_equal(self.array, value)] = 0\n\n    def is_sup(self, value, and_itself=False):\n        \"\"\"\n        Determine if pixels have more neighbors with higher values than a given threshold.\n\n        This method computes the number of neighboring pixels that have values greater\n        than a specified `value` for each pixel in the array. Optionally, it can exclude\n        the pixel itself if its value is less than or equal to `value`.\n\n        Parameters\n        ----------\n        value : int\n            The threshold value used to determine if a neighboring pixel's value is greater.\n        and_itself : bool, optional\n            If True, exclude the pixel itself if its value is less than or equal to `value`.\n            Defaults to False.\n\n        Examples\n        --------\n        &gt;&gt;&gt; matrix = np.array([[9, 0, 4, 6], [4, 9, 1, 3], [7, 2, 1, 4], [9, 0, 8, 5]], dtype=np.int8)\n        &gt;&gt;&gt; compare = CompareNeighborsWithValue(matrix, connectivity=4)\n        &gt;&gt;&gt; compare.is_sup(1)\n        &gt;&gt;&gt; print(compare.sup_neighbor_nb)\n        [[3 3 2 4]\n         [4 2 3 3]\n         [4 2 3 3]\n         [3 3 2 4]]\n        \"\"\"\n        if len(self.array.shape) == 1:\n            self.sup_neighbor_nb = (self.on_the_right &gt; value).astype(self.array.dtype) + (self.on_the_left &gt; value).astype(self.array.dtype)\n        else:\n            if self.connectivity == 4:\n                self.sup_neighbor_nb =  np.dstack((self.on_the_right &gt; value, self.on_the_left &gt; value,\n                                               self.on_the_bot &gt; value, self.on_the_top &gt; value))\n            elif self.connectivity == 8:\n                self.sup_neighbor_nb =  np.dstack((self.on_the_right &gt; value, self.on_the_left &gt; value,\n                                               self.on_the_bot &gt; value, self.on_the_top &gt; value,\n                                               self.on_the_topleft &gt; value, self.on_the_topright &gt; value,\n                                               self.on_the_botleft &gt; value, self.on_the_botright &gt; value))\n            else:\n                self.sup_neighbor_nb =  np.dstack((self.on_the_topleft &gt; value, self.on_the_topright &gt; value,\n                                               self.on_the_botleft &gt; value, self.on_the_botright &gt; value))\n\n            self.sup_neighbor_nb = np.sum(self.sup_neighbor_nb, 2, dtype=np.uint8)\n        if and_itself:\n            self.sup_neighbor_nb[np.less_equal(self.array, value)] = 0\n\n    def is_inf(self, value, and_itself=False):\n        \"\"\"\n        is_inf(value and_itself=False)\n\n        Determine the number of neighbors that are infinitely small relative to a given value,\n        considering optional connectivity and exclusion of the element itself.\n\n        Parameters\n        ----------\n        value : numeric\n            The value to compare neighbor elements against.\n        and_itself : bool, optional\n            If True, excludes the element itself from being counted. Default is False.\n\n        Examples\n        --------\n        &gt;&gt;&gt; matrix = np.array([[9, 0, 4, 6], [4, 9, 1, 3], [7, 2, 1, 4], [9, 0, 8, 5]], dtype=np.int8)\n        &gt;&gt;&gt; compare = CompareNeighborsWithValue(matrix, connectivity=4)\n        &gt;&gt;&gt; compare.is_inf(1)\n        &gt;&gt;&gt; print(compare.inf_neighbor_nb)\n        [[1 1 1 0]\n         [0 1 0 0]\n         [0 1 0 0]\n         [1 1 1 0]]\n        \"\"\"\n        if len(self.array.shape) == 1:\n            self.inf_neighbor_nb = (self.on_the_right &lt; value).astype(self.array.dtype) + (self.on_the_left &lt; value).astype(self.array.dtype)\n        else:\n            if self.connectivity == 4:\n                self.inf_neighbor_nb =  np.dstack((self.on_the_right &lt; value, self.on_the_left &lt; value,\n                                               self.on_the_bot &lt; value, self.on_the_top &lt; value))\n            elif self.connectivity == 8:\n                self.inf_neighbor_nb =  np.dstack((self.on_the_right &lt; value, self.on_the_left &lt; value,\n                                               self.on_the_bot &lt; value, self.on_the_top &lt; value,\n                                               self.on_the_topleft &lt; value, self.on_the_topright &lt; value,\n                                               self.on_the_botleft &lt; value, self.on_the_botright &lt; value))\n            else:\n                self.inf_neighbor_nb =  np.dstack((self.on_the_topleft &lt; value, self.on_the_topright &lt; value,\n                                               self.on_the_botleft &lt; value, self.on_the_botright &lt; value))\n\n            self.inf_neighbor_nb = np.sum(self.inf_neighbor_nb, 2, dtype=np.uint8)\n        if and_itself:\n            self.inf_neighbor_nb[np.greater_equal(self.array, value)] = 0\n</code></pre>"},{"location":"api/cellects/image_analysis/morphological_operations/#cellects.image_analysis.morphological_operations.CompareNeighborsWithValue.__init__","title":"<code>__init__(array, connectivity=None, data_type=np.int8)</code>","text":"<p>Initialize a class for array connectivity processing.</p> <p>This class processes arrays based on given connectivities, creating windows around the original data for both 1D and 2D arrays. Depending on the connectivity value (4 or 8), it creates different windows with borders.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>ndarray</code> <p>Input array to process, can be 1D or 2D.</p> required <code>connectivity</code> <code>int</code> <p>Connectivity type for processing (4 or 8), by default None.</p> <code>None</code> <code>data_type</code> <code>dtype</code> <p>Data type for the array elements, by default np.int8.</p> <code>int8</code> <p>Attributes:</p> Name Type Description <code>array</code> <code>ndarray</code> <p>The processed array based on the given data type.</p> <code>connectivity</code> <code>int</code> <p>Connectivity value used for processing.</p> <code>on_the_right</code> <code>ndarray</code> <p>Array with shifted elements to the right.</p> <code>on_the_left</code> <code>ndarray</code> <p>Array with shifted elements to the left.</p> <code>on_the_bot</code> <code>(ndarray, optional)</code> <p>Array with shifted elements to the bottom (for 2D arrays).</p> <code>on_the_top</code> <code>(ndarray, optional)</code> <p>Array with shifted elements to the top (for 2D arrays).</p> <code>on_the_topleft</code> <code>(ndarray, optional)</code> <p>Array with shifted elements to the top left (for 2D arrays).</p> <code>on_the_topright</code> <code>(ndarray, optional)</code> <p>Array with shifted elements to the top right (for 2D arrays).</p> <code>on_the_botleft</code> <code>(ndarray, optional)</code> <p>Array with shifted elements to the bottom left (for 2D arrays).</p> <code>on_the_botright</code> <code>(ndarray, optional)</code> <p>Array with shifted elements to the bottom right (for 2D arrays).</p> Source code in <code>src/cellects/image_analysis/morphological_operations.py</code> <pre><code>def __init__(self, array: np.ndarray, connectivity: int=None, data_type: np.dtype=np.int8):\n    \"\"\"\n    Initialize a class for array connectivity processing.\n\n    This class processes arrays based on given connectivities, creating\n    windows around the original data for both 1D and 2D arrays. Depending on\n    the connectivity value (4 or 8), it creates different windows with borders.\n\n    Parameters\n    ----------\n    array : ndarray\n        Input array to process, can be 1D or 2D.\n    connectivity : int, optional\n        Connectivity type for processing (4 or 8), by default None.\n    data_type : dtype, optional\n        Data type for the array elements, by default np.int8.\n\n    Attributes\n    ----------\n    array : ndarray\n        The processed array based on the given data type.\n    connectivity : int\n        Connectivity value used for processing.\n    on_the_right : ndarray\n        Array with shifted elements to the right.\n    on_the_left : ndarray\n        Array with shifted elements to the left.\n    on_the_bot : ndarray, optional\n        Array with shifted elements to the bottom (for 2D arrays).\n    on_the_top : ndarray, optional\n        Array with shifted elements to the top (for 2D arrays).\n    on_the_topleft : ndarray, optional\n        Array with shifted elements to the top left (for 2D arrays).\n    on_the_topright : ndarray, optional\n        Array with shifted elements to the top right (for 2D arrays).\n    on_the_botleft : ndarray, optional\n        Array with shifted elements to the bottom left (for 2D arrays).\n    on_the_botright : ndarray, optional\n        Array with shifted elements to the bottom right (for 2D arrays).\n    \"\"\"\n    array = array.astype(data_type)\n    self.array = array\n    self.connectivity = connectivity\n    if len(self.array.shape) == 1:\n        self.on_the_right = np.append(array[1:], array[-1])\n        self.on_the_left = np.append(array[0], array[:-1])\n    else:\n        # Build 4 window of the original array, each missing one of the four borders\n        # Grow each window with a copy of the last border at the opposite of the side a border have been deleted\n        if self.connectivity == 4 or self.connectivity == 8:\n            self.on_the_right = np.column_stack((array[:, 1:], array[:, -1]))\n            self.on_the_left = np.column_stack((array[:, 0], array[:, :-1]))\n            self.on_the_bot = np.vstack((array[1:, :], array[-1, :]))\n            self.on_the_top = np.vstack((array[0, :], array[:-1, :]))\n        if self.connectivity != 4:\n            self.on_the_topleft = array[:-1, :-1]\n            self.on_the_topright = array[:-1, 1:]\n            self.on_the_botleft = array[1:, :-1]\n            self.on_the_botright = array[1:, 1:]\n\n            self.on_the_topleft = np.vstack((self.on_the_topleft[0, :], self.on_the_topleft))\n            self.on_the_topleft = np.column_stack((self.on_the_topleft[:, 0], self.on_the_topleft))\n\n            self.on_the_topright = np.vstack((self.on_the_topright[0, :], self.on_the_topright))\n            self.on_the_topright = np.column_stack((self.on_the_topright, self.on_the_topright[:, -1]))\n\n            self.on_the_botleft = np.vstack((self.on_the_botleft, self.on_the_botleft[-1, :]))\n            self.on_the_botleft = np.column_stack((self.on_the_botleft[:, 0], self.on_the_botleft))\n\n            self.on_the_botright = np.vstack((self.on_the_botright, self.on_the_botright[-1, :]))\n            self.on_the_botright = np.column_stack((self.on_the_botright, self.on_the_botright[:, -1]))\n</code></pre>"},{"location":"api/cellects/image_analysis/morphological_operations/#cellects.image_analysis.morphological_operations.CompareNeighborsWithValue.is_equal","title":"<code>is_equal(value, and_itself=False)</code>","text":"<p>Check equality of neighboring values in an array.</p> <p>This method compares the neighbors of each element in <code>self.array</code> to a given value. Depending on the dimensionality and connectivity settings, it checks different neighboring elements.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>int or float</code> <p>The value to check equality with neighboring elements.</p> required <code>and_itself</code> <code>bool</code> <p>If True, also check equality with the element itself. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> Attributes (not standard Qt properties) <p>equal_neighbor_nb : ndarray of uint8     Array that holds the number of equal neighbors for each element.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; matrix = np.array([[9, 0, 4, 6], [4, 9, 1, 3], [7, 2, 1, 4], [9, 0, 8, 5]], dtype=np.int8)\n&gt;&gt;&gt; compare = CompareNeighborsWithValue(matrix, connectivity=4)\n&gt;&gt;&gt; compare.is_equal(1)\n&gt;&gt;&gt; print(compare.equal_neighbor_nb)\n[[0 0 1 0]\n[0 1 1 1]\n[0 1 1 1]\n[0 0 1 0]]\n</code></pre> Source code in <code>src/cellects/image_analysis/morphological_operations.py</code> <pre><code>def is_equal(self, value, and_itself: bool=False):\n    \"\"\"\n    Check equality of neighboring values in an array.\n\n    This method compares the neighbors of each element in `self.array` to a given value.\n    Depending on the dimensionality and connectivity settings, it checks different neighboring\n    elements.\n\n    Parameters\n    ----------\n    value : int or float\n        The value to check equality with neighboring elements.\n    and_itself : bool, optional\n        If True, also check equality with the element itself. Defaults to False.\n\n    Returns\n    -------\n    None\n\n    Attributes (not standard Qt properties)\n    --------------------------------------\n    equal_neighbor_nb : ndarray of uint8\n        Array that holds the number of equal neighbors for each element.\n\n    Examples\n    --------\n    &gt;&gt;&gt; matrix = np.array([[9, 0, 4, 6], [4, 9, 1, 3], [7, 2, 1, 4], [9, 0, 8, 5]], dtype=np.int8)\n    &gt;&gt;&gt; compare = CompareNeighborsWithValue(matrix, connectivity=4)\n    &gt;&gt;&gt; compare.is_equal(1)\n    &gt;&gt;&gt; print(compare.equal_neighbor_nb)\n    [[0 0 1 0]\n    [0 1 1 1]\n    [0 1 1 1]\n    [0 0 1 0]]\n    \"\"\"\n\n    if len(self.array.shape) == 1:\n        self.equal_neighbor_nb = np.sum((np.equal(self.on_the_right, value), np.equal(self.on_the_left, value)), axis=0)\n    else:\n        if self.connectivity == 4:\n            self.equal_neighbor_nb =  np.dstack((np.equal(self.on_the_right, value), np.equal(self.on_the_left, value),\n                                             np.equal(self.on_the_bot, value), np.equal(self.on_the_top, value)))\n        elif self.connectivity == 8:\n            self.equal_neighbor_nb =  np.dstack(\n                (np.equal(self.on_the_right, value), np.equal(self.on_the_left, value),\n                 np.equal(self.on_the_bot, value), np.equal(self.on_the_top, value),\n                 np.equal(self.on_the_topleft, value), np.equal(self.on_the_topright, value),\n                 np.equal(self.on_the_botleft, value), np.equal(self.on_the_botright, value)))\n        else:\n            self.equal_neighbor_nb =  np.dstack(\n                (np.equal(self.on_the_topleft, value), np.equal(self.on_the_topright, value),\n                 np.equal(self.on_the_botleft, value), np.equal(self.on_the_botright, value)))\n        self.equal_neighbor_nb = np.sum(self.equal_neighbor_nb, 2, dtype=np.uint8)\n\n    if and_itself:\n        self.equal_neighbor_nb[np.not_equal(self.array, value)] = 0\n</code></pre>"},{"location":"api/cellects/image_analysis/morphological_operations/#cellects.image_analysis.morphological_operations.CompareNeighborsWithValue.is_inf","title":"<code>is_inf(value, and_itself=False)</code>","text":"<p>is_inf(value and_itself=False)</p> <p>Determine the number of neighbors that are infinitely small relative to a given value, considering optional connectivity and exclusion of the element itself.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>numeric</code> <p>The value to compare neighbor elements against.</p> required <code>and_itself</code> <code>bool</code> <p>If True, excludes the element itself from being counted. Default is False.</p> <code>False</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; matrix = np.array([[9, 0, 4, 6], [4, 9, 1, 3], [7, 2, 1, 4], [9, 0, 8, 5]], dtype=np.int8)\n&gt;&gt;&gt; compare = CompareNeighborsWithValue(matrix, connectivity=4)\n&gt;&gt;&gt; compare.is_inf(1)\n&gt;&gt;&gt; print(compare.inf_neighbor_nb)\n[[1 1 1 0]\n [0 1 0 0]\n [0 1 0 0]\n [1 1 1 0]]\n</code></pre> Source code in <code>src/cellects/image_analysis/morphological_operations.py</code> <pre><code>def is_inf(self, value, and_itself=False):\n    \"\"\"\n    is_inf(value and_itself=False)\n\n    Determine the number of neighbors that are infinitely small relative to a given value,\n    considering optional connectivity and exclusion of the element itself.\n\n    Parameters\n    ----------\n    value : numeric\n        The value to compare neighbor elements against.\n    and_itself : bool, optional\n        If True, excludes the element itself from being counted. Default is False.\n\n    Examples\n    --------\n    &gt;&gt;&gt; matrix = np.array([[9, 0, 4, 6], [4, 9, 1, 3], [7, 2, 1, 4], [9, 0, 8, 5]], dtype=np.int8)\n    &gt;&gt;&gt; compare = CompareNeighborsWithValue(matrix, connectivity=4)\n    &gt;&gt;&gt; compare.is_inf(1)\n    &gt;&gt;&gt; print(compare.inf_neighbor_nb)\n    [[1 1 1 0]\n     [0 1 0 0]\n     [0 1 0 0]\n     [1 1 1 0]]\n    \"\"\"\n    if len(self.array.shape) == 1:\n        self.inf_neighbor_nb = (self.on_the_right &lt; value).astype(self.array.dtype) + (self.on_the_left &lt; value).astype(self.array.dtype)\n    else:\n        if self.connectivity == 4:\n            self.inf_neighbor_nb =  np.dstack((self.on_the_right &lt; value, self.on_the_left &lt; value,\n                                           self.on_the_bot &lt; value, self.on_the_top &lt; value))\n        elif self.connectivity == 8:\n            self.inf_neighbor_nb =  np.dstack((self.on_the_right &lt; value, self.on_the_left &lt; value,\n                                           self.on_the_bot &lt; value, self.on_the_top &lt; value,\n                                           self.on_the_topleft &lt; value, self.on_the_topright &lt; value,\n                                           self.on_the_botleft &lt; value, self.on_the_botright &lt; value))\n        else:\n            self.inf_neighbor_nb =  np.dstack((self.on_the_topleft &lt; value, self.on_the_topright &lt; value,\n                                           self.on_the_botleft &lt; value, self.on_the_botright &lt; value))\n\n        self.inf_neighbor_nb = np.sum(self.inf_neighbor_nb, 2, dtype=np.uint8)\n    if and_itself:\n        self.inf_neighbor_nb[np.greater_equal(self.array, value)] = 0\n</code></pre>"},{"location":"api/cellects/image_analysis/morphological_operations/#cellects.image_analysis.morphological_operations.CompareNeighborsWithValue.is_sup","title":"<code>is_sup(value, and_itself=False)</code>","text":"<p>Determine if pixels have more neighbors with higher values than a given threshold.</p> <p>This method computes the number of neighboring pixels that have values greater than a specified <code>value</code> for each pixel in the array. Optionally, it can exclude the pixel itself if its value is less than or equal to <code>value</code>.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>int</code> <p>The threshold value used to determine if a neighboring pixel's value is greater.</p> required <code>and_itself</code> <code>bool</code> <p>If True, exclude the pixel itself if its value is less than or equal to <code>value</code>. Defaults to False.</p> <code>False</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; matrix = np.array([[9, 0, 4, 6], [4, 9, 1, 3], [7, 2, 1, 4], [9, 0, 8, 5]], dtype=np.int8)\n&gt;&gt;&gt; compare = CompareNeighborsWithValue(matrix, connectivity=4)\n&gt;&gt;&gt; compare.is_sup(1)\n&gt;&gt;&gt; print(compare.sup_neighbor_nb)\n[[3 3 2 4]\n [4 2 3 3]\n [4 2 3 3]\n [3 3 2 4]]\n</code></pre> Source code in <code>src/cellects/image_analysis/morphological_operations.py</code> <pre><code>def is_sup(self, value, and_itself=False):\n    \"\"\"\n    Determine if pixels have more neighbors with higher values than a given threshold.\n\n    This method computes the number of neighboring pixels that have values greater\n    than a specified `value` for each pixel in the array. Optionally, it can exclude\n    the pixel itself if its value is less than or equal to `value`.\n\n    Parameters\n    ----------\n    value : int\n        The threshold value used to determine if a neighboring pixel's value is greater.\n    and_itself : bool, optional\n        If True, exclude the pixel itself if its value is less than or equal to `value`.\n        Defaults to False.\n\n    Examples\n    --------\n    &gt;&gt;&gt; matrix = np.array([[9, 0, 4, 6], [4, 9, 1, 3], [7, 2, 1, 4], [9, 0, 8, 5]], dtype=np.int8)\n    &gt;&gt;&gt; compare = CompareNeighborsWithValue(matrix, connectivity=4)\n    &gt;&gt;&gt; compare.is_sup(1)\n    &gt;&gt;&gt; print(compare.sup_neighbor_nb)\n    [[3 3 2 4]\n     [4 2 3 3]\n     [4 2 3 3]\n     [3 3 2 4]]\n    \"\"\"\n    if len(self.array.shape) == 1:\n        self.sup_neighbor_nb = (self.on_the_right &gt; value).astype(self.array.dtype) + (self.on_the_left &gt; value).astype(self.array.dtype)\n    else:\n        if self.connectivity == 4:\n            self.sup_neighbor_nb =  np.dstack((self.on_the_right &gt; value, self.on_the_left &gt; value,\n                                           self.on_the_bot &gt; value, self.on_the_top &gt; value))\n        elif self.connectivity == 8:\n            self.sup_neighbor_nb =  np.dstack((self.on_the_right &gt; value, self.on_the_left &gt; value,\n                                           self.on_the_bot &gt; value, self.on_the_top &gt; value,\n                                           self.on_the_topleft &gt; value, self.on_the_topright &gt; value,\n                                           self.on_the_botleft &gt; value, self.on_the_botright &gt; value))\n        else:\n            self.sup_neighbor_nb =  np.dstack((self.on_the_topleft &gt; value, self.on_the_topright &gt; value,\n                                           self.on_the_botleft &gt; value, self.on_the_botright &gt; value))\n\n        self.sup_neighbor_nb = np.sum(self.sup_neighbor_nb, 2, dtype=np.uint8)\n    if and_itself:\n        self.sup_neighbor_nb[np.less_equal(self.array, value)] = 0\n</code></pre>"},{"location":"api/cellects/image_analysis/morphological_operations/#cellects.image_analysis.morphological_operations.box_counting_dimension","title":"<code>box_counting_dimension(zoomed_binary, side_lengths, display=False)</code>","text":"<p>Box counting dimension calculation.</p> <p>This function calculates the box-counting dimension of a binary image by analyzing the number of boxes (of varying sizes) that contain at least one pixel of the image. The function also provides the R-squared value from linear regression and the number of boxes used.</p> <p>Parameters:</p> Name Type Description Default <code>zoomed_binary</code> <code>NDArray[uint8]</code> <p>Binary image (0 or 255 values) for which the box-counting dimension is calculated.</p> required <code>side_lengths</code> <code>NDArray</code> <p>Array of side lengths for the boxes used in the box-counting calculation.</p> required <code>display</code> <code>bool</code> <p>If True, displays a scatter plot of the log-transformed box counts and diameters, along with the linear regression fit. Default is False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>out</code> <code>Tuple[float, float, float]</code> <p>A tuple containing the calculated box-counting dimension (<code>d</code>), R-squared value (<code>r_value</code>), and the number of boxes used (<code>box_nb</code>).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; binary_image = np.zeros((10, 10), dtype=np.uint8)\n&gt;&gt;&gt; binary_image[2:4, 2:6] = 1\n&gt;&gt;&gt; binary_image[7:9, 4:7] = 1\n&gt;&gt;&gt; binary_image[4:7, 5] = 1\n&gt;&gt;&gt; zoomed_binary, side_lengths = prepare_box_counting(binary_image, min_im_side=2, min_mesh_side=2)\n&gt;&gt;&gt; dimension, r_value, box_nb = box_counting_dimension(zoomed_binary, side_lengths)\n&gt;&gt;&gt; print(dimension, r_value, box_nb)\n(np.float64(1.1699250014423126), np.float64(0.9999999999999998), 2)\n</code></pre> Source code in <code>src/cellects/image_analysis/morphological_operations.py</code> <pre><code>def box_counting_dimension(zoomed_binary: NDArray[np.uint8], side_lengths: NDArray, display: bool=False) -&gt; Tuple[float, float, float]:\n    \"\"\"\n    Box counting dimension calculation.\n\n    This function calculates the box-counting dimension of a binary image by analyzing the number\n    of boxes (of varying sizes) that contain at least one pixel of the image. The function also\n    provides the R-squared value from linear regression and the number of boxes used.\n\n    Parameters\n    ----------\n    zoomed_binary : NDArray[np.uint8]\n        Binary image (0 or 255 values) for which the box-counting dimension is calculated.\n    side_lengths : NDArray\n        Array of side lengths for the boxes used in the box-counting calculation.\n    display : bool, optional\n        If True, displays a scatter plot of the log-transformed box counts and diameters,\n        along with the linear regression fit. Default is False.\n\n    Returns\n    -------\n    out : Tuple[float, float, float]\n        A tuple containing the calculated box-counting dimension (`d`), R-squared value (`r_value`),\n        and the number of boxes used (`box_nb`).\n\n    Examples\n    --------\n    &gt;&gt;&gt; binary_image = np.zeros((10, 10), dtype=np.uint8)\n    &gt;&gt;&gt; binary_image[2:4, 2:6] = 1\n    &gt;&gt;&gt; binary_image[7:9, 4:7] = 1\n    &gt;&gt;&gt; binary_image[4:7, 5] = 1\n    &gt;&gt;&gt; zoomed_binary, side_lengths = prepare_box_counting(binary_image, min_im_side=2, min_mesh_side=2)\n    &gt;&gt;&gt; dimension, r_value, box_nb = box_counting_dimension(zoomed_binary, side_lengths)\n    &gt;&gt;&gt; print(dimension, r_value, box_nb)\n    (np.float64(1.1699250014423126), np.float64(0.9999999999999998), 2)\n    \"\"\"\n    dimension:float = 0.\n    r_value:float = 0.\n    box_nb:float = 0.\n    if side_lengths is not None:\n        box_counts = np.zeros(len(side_lengths), dtype=np.uint64)\n        # Loop through side_lengths and compute block counts\n        for idx, side_length in enumerate(side_lengths):\n            S = np.add.reduceat(\n                np.add.reduceat(zoomed_binary, np.arange(0, zoomed_binary.shape[0], side_length), axis=0),\n                np.arange(0, zoomed_binary.shape[1], side_length),\n                axis=1\n            )\n            box_counts[idx] = len(np.where(S &gt; 0)[0])\n\n        valid_indices = box_counts &gt; 0\n        if valid_indices.sum() &gt;= 2:\n            log_box_counts = np.log(box_counts)\n            log_reciprocal_lengths = np.log(1 / side_lengths)\n            slope, intercept, r_value, p_value, stderr = linregress(log_reciprocal_lengths, log_box_counts)\n            # coefficients = np.polyfit(log_reciprocal_lengths, log_box_counts, 1)\n            dimension = slope\n            box_nb = len(side_lengths)\n            if display:\n                plt.scatter(log_reciprocal_lengths, log_box_counts, label=\"Box counting\")\n                plt.plot([0, log_reciprocal_lengths.min()], [intercept, intercept + slope * log_reciprocal_lengths.min()], label=\"Linear regression\")\n                plt.plot([], [], ' ', label=f\"D = {slope:.2f}\")\n                plt.plot([], [], ' ', label=f\"R2 = {r_value:.6f}\")\n                plt.plot([], [], ' ', label=f\"p-value = {p_value:.2e}\")\n                plt.legend(loc='best')\n                plt.xlabel(f\"log(1/Diameter) | Diameter \u2286 [{side_lengths[0]}:{side_lengths[-1]}] (n={box_nb})\")\n                plt.ylabel(f\"log(Box number) | Box number \u2286 [{box_counts[0]}:{box_counts[-1]}]\")\n                plt.show()\n                # plt.close()\n\n    return dimension, r_value, box_nb\n</code></pre>"},{"location":"api/cellects/image_analysis/morphological_operations/#cellects.image_analysis.morphological_operations.cc","title":"<code>cc(binary_img)</code>","text":"<p>Processes a binary image to reorder and label connected components.</p> <p>This function takes a binary image, analyses the connected components, reorders them by size, ensures background is correctly labeled as 0, and returns the new ordered labels along with their statistics and centers.</p> <p>Parameters:</p> Name Type Description Default <code>binary_img</code> <code>ndarray of uint8</code> <p>Input binary image with connected components.</p> required <p>Returns:</p> Name Type Description <code>new_order</code> <code>ndarray of uint8, uint16 or uint32</code> <p>Image with reordered labels for connected components.</p> <code>stats</code> <code>ndarray of ints</code> <p>Statistics for each component (x, y, width, height, area).</p> <code>centers</code> <code>ndarray of floats</code> <p>Centers for each component (x, y).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; binary_img = np.array([[0, 1, 0], [0, 1, 0]], dtype=np.uint8)\n&gt;&gt;&gt; new_order, stats, centers = cc(binary_img)\n&gt;&gt;&gt; print(stats)\narray([[0, 0, 3, 2, 4],\n   [1, 0, 2, 2, 2]], dtype=int32)\n</code></pre> Source code in <code>src/cellects/image_analysis/morphological_operations.py</code> <pre><code>def cc(binary_img: NDArray[np.uint8]) -&gt; Tuple[NDArray, NDArray, NDArray]:\n    \"\"\"\n    Processes a binary image to reorder and label connected components.\n\n    This function takes a binary image, analyses the connected components,\n    reorders them by size, ensures background is correctly labeled as 0,\n    and returns the new ordered labels along with their statistics and centers.\n\n    Parameters\n    ----------\n    binary_img : ndarray of uint8\n        Input binary image with connected components.\n\n    Returns\n    -------\n    new_order : ndarray of uint8, uint16 or uint32\n        Image with reordered labels for connected components.\n    stats : ndarray of ints\n        Statistics for each component (x, y, width, height, area).\n    centers : ndarray of floats\n        Centers for each component (x, y).\n\n    Examples\n    --------\n    &gt;&gt;&gt; binary_img = np.array([[0, 1, 0], [0, 1, 0]], dtype=np.uint8)\n    &gt;&gt;&gt; new_order, stats, centers = cc(binary_img)\n    &gt;&gt;&gt; print(stats)\n    array([[0, 0, 3, 2, 4],\n       [1, 0, 2, 2, 2]], dtype=int32)\n    \"\"\"\n    number, img, stats, centers = cv2.connectedComponentsWithStats(binary_img, ltype=cv2.CV_16U)\n    if number &gt; 255:\n        img_dtype = np.uint16\n        if number &gt; 65535:\n            img_dtype = np.uint32\n    else:\n        img_dtype = np.uint8\n    stats[:, 2] = stats[:, 0] + stats[:, 2]\n    stats[:, 3] = stats[:, 1] + stats[:, 3]\n    sorted_idx = np.argsort(stats[:, 4])[::-1]\n\n    # Make sure that the first connected component (labelled 0) is the background and not the main shape\n    size_ranked_stats = stats[sorted_idx, :]\n    background = (size_ranked_stats[:, 0] == 0).astype(np.uint8) + (size_ranked_stats[:, 1] == 0).astype(np.uint8) + (\n            size_ranked_stats[:, 2] == img.shape[1]).astype(np.uint8) + (\n                         size_ranked_stats[:, 3] == img.shape[0]).astype(np.uint8)\n\n    # background = ((size_ranked_stats[:, 0] == 0) &amp; (size_ranked_stats[:, 1] == 0) &amp; (size_ranked_stats[:, 2] == img.shape[1]) &amp; (size_ranked_stats[:, 3] == img.shape[0]))\n\n    touch_borders = np.nonzero(background &gt; 2)[0]\n    # if not isinstance(touch_borders, np.int64):\n    #     touch_borders = touch_borders[0]\n    # Most of the time, the background should be the largest shape and therefore has the index 0,\n    # Then, if there is at least one shape touching more than 2 borders and having not the index 0, solve:\n    if np.any(touch_borders != 0):\n        # If there is only one shape touching borders, it means that background is not at its right position (i.e. 0)\n        if len(touch_borders) == 1:\n            # Then exchange that shape position with background position\n            shape = sorted_idx[0]  # Store shape position in the first place\n            back = sorted_idx[touch_borders[0]]  # Store back position in the first place\n            sorted_idx[touch_borders[0]] = shape  # Put shape position at the previous place of back and conversely\n            sorted_idx[0] = back\n        # If there are two shapes, it means that the main shape grew sufficiently to reach at least 3 borders\n        # We assume that it grew larger than background\n        else:\n            shape = sorted_idx[0]\n            back = sorted_idx[1]\n            sorted_idx[1] = shape\n            sorted_idx[0] = back\n            # Put shape position at the previous place of back and conversely\n\n\n    stats = stats[sorted_idx, :]\n    centers = centers[sorted_idx, :]\n\n    new_order = np.zeros_like(binary_img, dtype=img_dtype)\n\n    for i, val in enumerate(sorted_idx):\n        new_order[img == val] = i\n    return new_order, stats, centers\n</code></pre>"},{"location":"api/cellects/image_analysis/morphological_operations/#cellects.image_analysis.morphological_operations.close_holes","title":"<code>close_holes(binary_img)</code>","text":"<p>Close holes in a binary image using connected components analysis.</p> <p>This function identifies and closes small holes within the foreground objects of a binary image. It uses connected component analysis to find and fill holes that are smaller than the main object.</p> <p>Parameters:</p> Name Type Description Default <code>binary_img</code> <code>ndarray of uint8</code> <p>Binary input image where holes need to be closed.</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>ndarray of uint8</code> <p>Binary image with closed holes.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; binary_img = np.zeros((10, 10), dtype=np.uint8)\n&gt;&gt;&gt; binary_img[2:8, 2:8] = 1\n&gt;&gt;&gt; binary_img[4:6, 4:6] = 0  # Creating a hole\n&gt;&gt;&gt; result = close_holes(binary_img)\n&gt;&gt;&gt; print(result)\n[[0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0]\n [0 0 1 1 1 1 1 1 0 0]\n [0 0 1 1 1 1 1 1 0 0]\n [0 0 1 1 1 1 1 1 0 0]\n [0 0 1 1 1 1 1 1 0 0]\n [0 0 1 1 1 1 1 1 0 0]\n [0 0 1 1 1 1 1 1 0 0]\n [0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0]]\n</code></pre> Source code in <code>src/cellects/image_analysis/morphological_operations.py</code> <pre><code>def close_holes(binary_img: NDArray[np.uint8]) -&gt; NDArray[np.uint8]:\n    \"\"\"\n    Close holes in a binary image using connected components analysis.\n\n    This function identifies and closes small holes within the foreground objects of a binary image. It uses connected component analysis to find and fill holes that are smaller than the main object.\n\n    Parameters\n    ----------\n    binary_img : ndarray of uint8\n        Binary input image where holes need to be closed.\n\n    Returns\n    -------\n    out : ndarray of uint8\n        Binary image with closed holes.\n\n    Examples\n    --------\n    &gt;&gt;&gt; binary_img = np.zeros((10, 10), dtype=np.uint8)\n    &gt;&gt;&gt; binary_img[2:8, 2:8] = 1\n    &gt;&gt;&gt; binary_img[4:6, 4:6] = 0  # Creating a hole\n    &gt;&gt;&gt; result = close_holes(binary_img)\n    &gt;&gt;&gt; print(result)\n    [[0 0 0 0 0 0 0 0 0 0]\n     [0 0 0 0 0 0 0 0 0 0]\n     [0 0 1 1 1 1 1 1 0 0]\n     [0 0 1 1 1 1 1 1 0 0]\n     [0 0 1 1 1 1 1 1 0 0]\n     [0 0 1 1 1 1 1 1 0 0]\n     [0 0 1 1 1 1 1 1 0 0]\n     [0 0 1 1 1 1 1 1 0 0]\n     [0 0 0 0 0 0 0 0 0 0]\n     [0 0 0 0 0 0 0 0 0 0]]\n    \"\"\"\n    #### Third version ####\n    nb, new_order = cv2.connectedComponents(1 - binary_img)\n    if nb &gt; 2:\n        binary_img[new_order &gt; 1] = 1\n    return binary_img\n</code></pre>"},{"location":"api/cellects/image_analysis/morphological_operations/#cellects.image_analysis.morphological_operations.create_ellipse","title":"<code>create_ellipse(vsize, hsize, min_size=0)</code>","text":"<p>Create a 2D array representing an ellipse with given vertical and horizontal sizes.</p> <p>This function generates a NumPy boolean array where each element is <code>True</code> if the point lies within or on the boundary of an ellipse defined by its vertical and horizontal radii. The ellipse is centered at the center of the array, which corresponds to the midpoint of the given dimensions.</p> <p>Parameters:</p> Name Type Description Default <code>vsize</code> <code>int</code> <p>Vertical size (number of rows) in the output 2D array.</p> required <code>hsize</code> <code>int</code> <p>Horizontal size (number of columns) in the output 2D array.</p> required <p>Returns:</p> Type Description <code>NDArray[bool]</code> <p>A boolean NumPy array of shape <code>(vsize, hsize)</code> where <code>True</code> indicates that a pixel lies within or on the boundary of an ellipse centered at the image's center with radii determined by half of the dimensions.</p> Source code in <code>src/cellects/image_analysis/morphological_operations.py</code> <pre><code>@njit()\ndef create_ellipse(vsize: int, hsize: int, min_size: int=0) -&gt; NDArray[np.uint8]:\n    \"\"\"\n    Create a 2D array representing an ellipse with given vertical and horizontal sizes.\n\n    This function generates a NumPy boolean array where each element is `True` if the point lies within or on\n    the boundary of an ellipse defined by its vertical and horizontal radii. The ellipse is centered at the center\n    of the array, which corresponds to the midpoint of the given dimensions.\n\n    Parameters\n    ----------\n    vsize : int\n        Vertical size (number of rows) in the output 2D array.\n    hsize : int\n        Horizontal size (number of columns) in the output 2D array.\n\n    Returns\n    -------\n    NDArray[bool]\n        A boolean NumPy array of shape `(vsize, hsize)` where `True` indicates that a pixel lies within or on\n        the boundary of an ellipse centered at the image's center with radii determined by half of the dimensions.\n    \"\"\"\n    # Use default values if input sizes are zero\n    vsize = min_size if vsize == 0 else vsize\n    hsize = min_size if hsize == 0 else hsize\n\n    # Compute radii (half of each size)\n    vr = hsize // 2\n    hr = vsize // 2\n\n    result = np.empty((vsize, hsize), dtype=np.bool_)\n    if vr &gt; 0 and hr &gt; 0:\n        for i in range(vsize):\n            for j in range(hsize):\n                x = i\n                y = j\n                lhs = ((x - hr) ** 2 / (hr ** 2)) + ((y - vr) ** 2 / (vr ** 2))\n                result[i, j] = lhs &lt;= 1\n    else:\n        result[hr, vr] = True\n    return result\n</code></pre>"},{"location":"api/cellects/image_analysis/morphological_operations/#cellects.image_analysis.morphological_operations.create_mask","title":"<code>create_mask(dims, minmax, shape)</code>","text":"<p>Create a boolean mask based on given dimensions and min/max coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>dims</code> <code>Tuple[int, int]</code> <p>The dimensions of the mask (height and width).</p> required <code>minmax</code> <code>Tuple[int, int, int, int]</code> <p>The minimum and maximum coordinates for the mask (x_min, x_max, y_min, y_max).</p> required <code>shape</code> <code>str</code> <p>The shape of the mask. Should be either 'circle' or any other value for a rectangular mask.</p> required <p>Returns:</p> Type Description <code>ndarray[bool]</code> <p>A boolean NumPy array with the same dimensions as <code>dims</code>, initialized to False, where the specified region (or circle) is set to True.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the shape is 'circle' and the ellipse creation fails.</p> Notes <p>If <code>shape</code> is not 'circle', a rectangular mask will be created. The ellipse creation method used may have specific performance considerations.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; mask = create_mask((5, 6), (0, 5, 1, 5), 'circle')\n&gt;&gt;&gt; print(mask)\n[[False False False  True False False]\n [False False  True  True  True False]\n [False  True  True  True  True False]\n [False False  True  True  True False]\n [False False False  True False False]]\n</code></pre> Source code in <code>src/cellects/image_analysis/morphological_operations.py</code> <pre><code>def create_mask(dims: Tuple, minmax: Tuple, shape: str):\n    \"\"\"\n\n    Create a boolean mask based on given dimensions and min/max coordinates.\n\n    Parameters\n    ----------\n    dims : Tuple[int, int]\n        The dimensions of the mask (height and width).\n    minmax : Tuple[int, int, int, int]\n        The minimum and maximum coordinates for the mask (x_min, x_max,\n        y_min, y_max).\n    shape : str\n        The shape of the mask. Should be either 'circle' or any other value for a rectangular mask.\n\n    Returns\n    -------\n    np.ndarray[bool]\n        A boolean NumPy array with the same dimensions as `dims`, initialized to False,\n        where the specified region (or circle) is set to True.\n\n    Raises\n    ------\n    ValueError\n        If the shape is 'circle' and the ellipse creation fails.\n\n    Notes\n    -----\n    If `shape` is not 'circle', a rectangular mask will be created. The ellipse\n    creation method used may have specific performance considerations.\n\n    Examples\n    --------\n    &gt;&gt;&gt; mask = create_mask((5, 6), (0, 5, 1, 5), 'circle')\n    &gt;&gt;&gt; print(mask)\n    [[False False False  True False False]\n     [False False  True  True  True False]\n     [False  True  True  True  True False]\n     [False False  True  True  True False]\n     [False False False  True False False]]\n     \"\"\"\n    mask = np.zeros(dims[:2], dtype=bool)\n    if shape == 'circle':\n        ellipse = create_ellipse(minmax[1] - minmax[0], minmax[3] - minmax[2])\n        mask[minmax[0]:minmax[1], minmax[2]:minmax[3], ...] = ellipse\n    else:\n        mask[minmax[0]:minmax[1], minmax[2]:minmax[3]] = 1\n    return mask\n</code></pre>"},{"location":"api/cellects/image_analysis/morphological_operations/#cellects.image_analysis.morphological_operations.draw_img_with_mask","title":"<code>draw_img_with_mask(img, dims, minmax, shape, drawing, only_contours=False, dilate_mask=0)</code>","text":"<p>Draw an image with a mask and optional contours.</p> <p>Draws a subregion of the input image using a specified shape (circle or rectangle), which can be dilated. The mask can be limited to contours only, and an optional drawing (overlay) can be applied within the masked region.</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>NDArray</code> <p>The input image to draw on.</p> required <code>dims</code> <code>Tuple[int, int]</code> <p>Dimensions of the subregion (width, height).</p> required <code>minmax</code> <code>Tuple[int, int, int, int]</code> <p>Coordinates of the subregion (x_start, x_end, y_start, y_end).</p> required <code>shape</code> <code>str</code> <p>Shape of the mask to draw ('circle' or 'rectangle').</p> required <code>drawing</code> <code>Tuple[NDArray, NDArray, NDArray]</code> <p>Optional drawing (overlay) to apply within the masked region.</p> required <code>only_contours</code> <code>bool</code> <p>If True, draw only the contours of the shape. Default is False.</p> <code>False</code> <code>dilate_mask</code> <code>int</code> <p>Number of iterations for dilating the mask. Default is 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>NDArray</code> <p>The modified image with the applied mask and drawing.</p> Notes <p>This function assumes that the input image is in BGR format (OpenCV style).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; dim = (100, 100, 3)\n&gt;&gt;&gt; img = np.zeros(dim)\n&gt;&gt;&gt; result = draw_img_with_mask(img, dim, (50, 75, 50, 75), 'circle', (0, 255, 0))\n&gt;&gt;&gt; print((result == 255).sum())\n441\n</code></pre> Source code in <code>src/cellects/image_analysis/morphological_operations.py</code> <pre><code>def draw_img_with_mask(img:NDArray, dims: Tuple, minmax: Tuple, shape: str, drawing: Tuple, only_contours: bool=False,\n                       dilate_mask: int=0) -&gt; NDArray:\n    \"\"\"\n\n    Draw an image with a mask and optional contours.\n\n    Draws a subregion of the input image using a specified shape (circle or rectangle),\n    which can be dilated. The mask can be limited to contours only, and an optional\n    drawing (overlay) can be applied within the masked region.\n\n    Parameters\n    ----------\n    img : NDArray\n        The input image to draw on.\n    dims : Tuple[int, int]\n        Dimensions of the subregion (width, height).\n    minmax : Tuple[int, int, int, int]\n        Coordinates of the subregion (x_start, x_end, y_start, y_end).\n    shape : str\n        Shape of the mask to draw ('circle' or 'rectangle').\n    drawing : Tuple[NDArray, NDArray, NDArray]\n        Optional drawing (overlay) to apply within the masked region.\n    only_contours : bool, optional\n        If True, draw only the contours of the shape. Default is False.\n    dilate_mask : int, optional\n        Number of iterations for dilating the mask. Default is 0.\n\n    Returns\n    -------\n    NDArray\n        The modified image with the applied mask and drawing.\n\n    Notes\n    -----\n    This function assumes that the input image is in BGR format (OpenCV style).\n\n    Examples\n    --------\n    &gt;&gt;&gt; dim = (100, 100, 3)\n    &gt;&gt;&gt; img = np.zeros(dim)\n    &gt;&gt;&gt; result = draw_img_with_mask(img, dim, (50, 75, 50, 75), 'circle', (0, 255, 0))\n    &gt;&gt;&gt; print((result == 255).sum())\n    441\n    \"\"\"\n    if shape == 'circle':\n        mask = create_ellipse(minmax[1] - minmax[0], minmax[3] - minmax[2]).astype(np.uint8)\n        if only_contours:\n            mask = get_contours(mask)\n    else:\n        if only_contours:\n            mask = 1 - image_borders((minmax[1] - minmax[0], minmax[3] - minmax[2]))\n        else:\n            mask = np.ones((minmax[1] - minmax[0], minmax[3] - minmax[2]), np.uint8)\n    if dilate_mask:\n        mask = cv2.dilate(mask, kernel=cross_33, iterations=dilate_mask)\n    anti_mask = 1 - mask\n    img[minmax[0]:minmax[1], minmax[2]:minmax[3], 0] *= anti_mask\n    img[minmax[0]:minmax[1], minmax[2]:minmax[3], 1] *= anti_mask\n    img[minmax[0]:minmax[1], minmax[2]:minmax[3], 2] *= anti_mask\n    if isinstance(drawing, np.ndarray):\n        if drawing.dtype != np.uint8:\n            drawing = bracket_to_uint8_image_contrast(drawing)\n        drawing = [drawing[:, :, 0], drawing[:, :, 1], drawing[:, :, 2]]\n    img[minmax[0]:minmax[1], minmax[2]:minmax[3], 0] += mask * drawing[0]\n    img[minmax[0]:minmax[1], minmax[2]:minmax[3], 1] += mask * drawing[1]\n    img[minmax[0]:minmax[1], minmax[2]:minmax[3], 2] += mask * drawing[2]\n    return img\n</code></pre>"},{"location":"api/cellects/image_analysis/morphological_operations/#cellects.image_analysis.morphological_operations.draw_me_a_sun","title":"<code>draw_me_a_sun(main_shape, ray_length_coef=4)</code>","text":"<p>Draw a sun-shaped pattern on an image based on the main shape and ray length coefficient.</p> <p>This function takes an input binary image (main_shape) and draws sun rays from the perimeter of that shape. The length of the rays is controlled by a coefficient. The function ensures that rays do not extend beyond the image borders.</p> <p>Parameters:</p> Name Type Description Default <code>main_shape</code> <code>ndarray of bool or int</code> <p>Binary input image where the main shape is defined.</p> required <code>ray_length_coef</code> <code>float</code> <p>Coefficient to control the length of sun rays. Defaults to 2.</p> <code>4</code> <p>Returns:</p> Name Type Description <code>rays</code> <code>ndarray</code> <p>Indices of the rays drawn.</p> <code>sun</code> <code>ndarray</code> <p>Image with sun rays drawn on it.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; main_shape = np.zeros((10, 10), dtype=np.uint8)\n&gt;&gt;&gt; main_shape[4:7, 3:6] = 1\n&gt;&gt;&gt; rays, sun = draw_me_a_sun(main_shape)\n&gt;&gt;&gt; print(sun)\n</code></pre> Source code in <code>src/cellects/image_analysis/morphological_operations.py</code> <pre><code>def draw_me_a_sun(main_shape: NDArray, ray_length_coef: int=4) -&gt; Tuple[NDArray, NDArray]:\n    \"\"\"\n    Draw a sun-shaped pattern on an image based on the main shape and ray length coefficient.\n\n    This function takes an input binary image (main_shape) and draws sun rays\n    from the perimeter of that shape. The length of the rays is controlled by a coefficient.\n    The function ensures that rays do not extend beyond the image borders.\n\n    Parameters\n    ----------\n    main_shape : ndarray of bool or int\n        Binary input image where the main shape is defined.\n    ray_length_coef : float, optional\n        Coefficient to control the length of sun rays. Defaults to 2.\n\n    Returns\n    -------\n    rays : ndarray\n        Indices of the rays drawn.\n    sun : ndarray\n        Image with sun rays drawn on it.\n\n    Examples\n    --------\n    &gt;&gt;&gt; main_shape = np.zeros((10, 10), dtype=np.uint8)\n    &gt;&gt;&gt; main_shape[4:7, 3:6] = 1\n    &gt;&gt;&gt; rays, sun = draw_me_a_sun(main_shape)\n    &gt;&gt;&gt; print(sun)\n\n    \"\"\"\n    nb, shapes, stats, center = cv2.connectedComponentsWithStats(main_shape)\n    sun = np.zeros(main_shape.shape, np.uint32)\n    rays = []\n    r = 0\n    for i in range(1, nb):\n        shape_i = cv2.dilate((shapes == i).astype(np.uint8), kernel=cross_33)\n        # shape_i = (shapes == i).astype(np.uint8)\n        contours = get_contours(shape_i)\n        first_ring_idx = np.nonzero(contours)\n        centroid = np.round((center[i, 1], center[i, 0])).astype(np.int64)\n        second_ring_y = centroid[0] + ((first_ring_idx[0] - centroid[0]) * ray_length_coef)\n        second_ring_x = centroid[1] + ((first_ring_idx[1] - centroid[1]) * ray_length_coef)\n\n        second_ring_y[second_ring_y &lt; 0] = 0\n        second_ring_x[second_ring_x &lt; 0] = 0\n\n        second_ring_y[second_ring_y &gt; main_shape.shape[0] - 1] = main_shape.shape[0] - 1\n        second_ring_x[second_ring_x &gt; main_shape.shape[1] - 1] = main_shape.shape[1] - 1\n        for j in range(len(second_ring_y)):\n            r += 1\n            fy, fx, sy, sx = first_ring_idx[0][j], first_ring_idx[1][j], second_ring_y[j], second_ring_x[j]\n            line = get_line_points((fy, fx), (sy, sx))\n            sun[line[:, 1], line[:, 0]] = r\n            rays.append(r)\n    return np.array(rays), sun\n</code></pre>"},{"location":"api/cellects/image_analysis/morphological_operations/#cellects.image_analysis.morphological_operations.dynamically_expand_to_fill_holes","title":"<code>dynamically_expand_to_fill_holes(binary_video, holes)</code>","text":"<p>Fill the holes in a binary video by progressively expanding the shape made of ones.</p> <p>Parameters:</p> Name Type Description Default <code>binary_video</code> <code>ndarray of uint8</code> <p>The binary video where holes need to be filled.</p> required <code>holes</code> <code>ndarray of uint8</code> <p>Array representing the holes in the binary video.</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>tuple of ndarray of uint8, int, and ndarray of float32</code> <p>The modified binary video with filled holes, the end time when all holes are filled, and an array of distances against time used to fill the holes.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; binary_video = np.zeros((10, 640, 480), dtype=np.uint8)\n&gt;&gt;&gt; binary_video[:, 300:400, 220:240] = 1\n&gt;&gt;&gt; holes = np.zeros((640, 480), dtype=np.uint8)\n&gt;&gt;&gt; holes[340:360, 228:232] = 1\n&gt;&gt;&gt; filled_video, end_time, distances = dynamically_expand_to_fill_holes(binary_video, holes)\n&gt;&gt;&gt; print(filled_video.shape)  # Should print (10, 640, 480)\n(10, 640, 480)\n</code></pre> Source code in <code>src/cellects/image_analysis/morphological_operations.py</code> <pre><code>def dynamically_expand_to_fill_holes(binary_video: NDArray[np.uint8], holes: NDArray[np.uint8]) -&gt; Tuple[NDArray[np.uint8], int, NDArray[np.float32]]:\n    \"\"\"\n    Fill the holes in a binary video by progressively expanding the shape made of ones.\n\n    Parameters\n    ----------\n    binary_video : ndarray of uint8\n        The binary video where holes need to be filled.\n    holes : ndarray of uint8\n        Array representing the holes in the binary video.\n\n    Returns\n    -------\n    out : tuple of ndarray of uint8, int, and ndarray of float32\n        The modified binary video with filled holes,\n        the end time when all holes are filled, and\n        an array of distances against time used to fill the holes.\n\n    Examples\n    --------\n    &gt;&gt;&gt; binary_video = np.zeros((10, 640, 480), dtype=np.uint8)\n    &gt;&gt;&gt; binary_video[:, 300:400, 220:240] = 1\n    &gt;&gt;&gt; holes = np.zeros((640, 480), dtype=np.uint8)\n    &gt;&gt;&gt; holes[340:360, 228:232] = 1\n    &gt;&gt;&gt; filled_video, end_time, distances = dynamically_expand_to_fill_holes(binary_video, holes)\n    &gt;&gt;&gt; print(filled_video.shape)  # Should print (10, 640, 480)\n    (10, 640, 480)\n    \"\"\"\n    #first move should be the time at wich the first pixel hole could have been covered\n    #it should ask how much time the shape made to cross a distance long enough to overlap all holes\n    holes_contours = cv2.dilate(holes, cross_33, borderType=cv2.BORDER_CONSTANT, borderValue=0)\n    field = rounded_inverted_distance_transform(binary_video[0, :, :], (binary_video.shape[0] - 1))\n    field2 = inverted_distance_transform(binary_video[0, :, :], (binary_video.shape[0] - 1))\n    holes_contours = holes_contours * field * binary_video[- 1, :, :]\n    holes[np.nonzero(holes)] = field[np.nonzero(holes)]\n    if np.any(holes_contours):\n        # Find the relationship between distance and time\n        distance_against_time, holes_time_start, holes_time_end = get_radius_distance_against_time(binary_video, holes_contours)\n        # Use that vector to progressively fill holes at the same speed as shape grows\n        for t in np.arange(len(distance_against_time)):\n            new_order, stats, centers = cc((holes &gt;= distance_against_time[t]).astype(np.uint8))\n            for comp_i in np.arange(1, stats.shape[0]):\n                past_image = deepcopy(binary_video[holes_time_start + t, :, :])\n                with_new_comp = new_order == comp_i\n                past_image[with_new_comp] = 1\n                nb_comp, image_garbage = cv2.connectedComponents(past_image)\n                if nb_comp == 2:\n                    binary_video[holes_time_start + t, :, :][with_new_comp] = 1\n        # Make sure that holes remain filled from holes_time_end to the end of the video\n        for t in np.arange((holes_time_end + 1), binary_video.shape[0]):\n            past_image = binary_video[t, :, :]\n            past_image[holes &gt;= distance_against_time[-1]] = 1\n            binary_video[t, :, :] = past_image\n    else:\n        holes_time_end = None\n        distance_against_time = np.array([1, 2], dtype=np.float32)\n\n    return binary_video, holes_time_end, distance_against_time\n</code></pre>"},{"location":"api/cellects/image_analysis/morphological_operations/#cellects.image_analysis.morphological_operations.expand_until_neighbor_center_gets_nearer_than_own","title":"<code>expand_until_neighbor_center_gets_nearer_than_own(shape_to_expand, without_shape_i, shape_original_centroid, ref_centroids, kernel)</code>","text":"<p>Expand a shape until its neighbor's centroid is closer than its own.</p> <p>This function takes in several numpy arrays representing shapes and their centroids, and expands the input shape until the distance to the nearest neighboring centroid is less than or equal to the distance between the shape's contour and its own centroid.</p> <p>Parameters:</p> Name Type Description Default <code>shape_to_expand</code> <code>ndarray of uint8</code> <p>The binary shape to be expanded.</p> required <code>without_shape_i</code> <code>ndarray of uint8</code> <p>A binary array representing the area without the shape.</p> required <code>shape_original_centroid</code> <code>ndarray</code> <p>The centroid of the original shape.</p> required <code>ref_centroids</code> <code>ndarray</code> <p>Reference centroids to compare distances with.</p> required <code>kernel</code> <code>ndarray</code> <p>The kernel for dilation operation.</p> required <p>Returns:</p> Type Description <code>ndarray of uint8</code> <p>The expanded shape.</p> Source code in <code>src/cellects/image_analysis/morphological_operations.py</code> <pre><code>def expand_until_neighbor_center_gets_nearer_than_own(shape_to_expand: NDArray[np.uint8], without_shape_i: NDArray[np.uint8],\n                                                      shape_original_centroid: NDArray,\n                                                      ref_centroids: NDArray, kernel: NDArray) -&gt; NDArray[np.uint8]:\n    \"\"\"\n    Expand a shape until its neighbor's centroid is closer than its own.\n\n    This function takes in several numpy arrays representing shapes and their\n    centroids, and expands the input shape until the distance to the nearest\n    neighboring centroid is less than or equal to the distance between the shape's\n    contour and its own centroid.\n\n    Parameters\n    ----------\n    shape_to_expand : ndarray of uint8\n        The binary shape to be expanded.\n    without_shape_i : ndarray of uint8\n        A binary array representing the area without the shape.\n    shape_original_centroid : ndarray\n        The centroid of the original shape.\n    ref_centroids : ndarray\n        Reference centroids to compare distances with.\n    kernel : ndarray\n        The kernel for dilation operation.\n\n    Returns\n    -------\n    ndarray of uint8\n        The expanded shape.\n    \"\"\"\n    # shape_to_expand=test_shape\n    # shape_i=0\n    # shape_original_centroid=ordered_centroids[shape_i, :]\n    # ref_centroids=np.delete(ordered_centroids, shape_i, axis=0)\n    # kernel=self.small_kernels\n    previous_shape_to_expand = shape_to_expand.copy()\n    without_shape = deepcopy(without_shape_i)\n    if ref_centroids.shape[0] &gt; 1:\n        # Calculate the distance between the focal shape centroid and its 10% nearest neighbor centroids\n        centroid_distances = np.sqrt(np.square(ref_centroids[1:, 0] - shape_original_centroid[0]) + np.square(\n            ref_centroids[1:, 1] - shape_original_centroid[1]))\n        nearest_shapes = np.where(np.greater_equal(np.quantile(centroid_distances, 0.1), centroid_distances))[0]\n\n        # Use the nearest neighbor distance as a maximal reference to get the minimal distance between the border of the shape and the neighboring centroids\n        neighbor_mindist = np.min(centroid_distances)\n        idx = np.nonzero(shape_to_expand)\n        for shape_j in nearest_shapes:\n            neighbor_mindist = np.minimum(neighbor_mindist, np.min(\n                np.sqrt(np.square(ref_centroids[shape_j, 0] - idx[1]) + np.square(ref_centroids[shape_j, 1] - idx[0]))))\n        neighbor_mindist *= 0.5\n        # Get the maximal distance of the focal shape between its contour and its centroids\n        itself_maxdist = np.max(\n            np.sqrt(np.square(shape_original_centroid[0] - idx[1]) + np.square(shape_original_centroid[1] - idx[0])))\n    else:\n        itself_maxdist = np.max(shape_to_expand.shape)\n        neighbor_mindist = itself_maxdist\n        nearest_shapes = []\n    # Put 1 at the border of the reference image in order to be able to stop the while loop once border reached\n    without_shape[0, :] = 1\n    without_shape[:, 0] = 1\n    without_shape[without_shape.shape[0] - 1, :] = 1\n    without_shape[:, without_shape.shape[1] - 1] = 1\n\n    # Compare the distance between the contour of the shape and its centroid with this contour with the centroids of neighbors\n    # Continue as the distance made by the shape (from its centroid) keeps being smaller than its distance with the nearest centroid.\n    while np.logical_and(np.any(np.less_equal(itself_maxdist, neighbor_mindist)),\n                         np.count_nonzero(shape_to_expand * without_shape) == 0):\n        previous_shape_to_expand = shape_to_expand.copy()\n        # Dilate the shape by the kernel size\n        shape_to_expand = cv2.dilate(shape_to_expand, kernel, iterations=1,\n                                     borderType=cv2.BORDER_CONSTANT | cv2.BORDER_ISOLATED)\n        # Extract the new connected component\n        shape_nb, shape_to_expand = cv2.connectedComponents(shape_to_expand, ltype=cv2.CV_16U)\n        shape_to_expand = shape_to_expand.astype(np.uint8)\n        # Use the nex shape coordinates to calculate the new distances of the shape with its centroid and with neighboring centroids\n        idx = np.nonzero(shape_to_expand)\n        for shape_j in nearest_shapes:\n            neighbor_mindist = np.minimum(neighbor_mindist, np.min(\n                np.sqrt(np.square(ref_centroids[shape_j, 0] - idx[1]) + np.square(ref_centroids[shape_j, 1] - idx[0]))))\n        itself_maxdist = np.max(\n            np.sqrt(np.square(shape_original_centroid[0] - idx[1]) + np.square(shape_original_centroid[1] - idx[0])))\n    return previous_shape_to_expand\n</code></pre>"},{"location":"api/cellects/image_analysis/morphological_operations/#cellects.image_analysis.morphological_operations.find_major_incline","title":"<code>find_major_incline(vector, natural_noise)</code>","text":"<p>Find the major incline section in a vector.</p> <p>This function identifies the segment of a vector that exhibits the most significant change in values, considering a specified natural noise level. It returns the left and right indices that define this segment.</p> <p>Parameters:</p> Name Type Description Default <code>vector</code> <code>ndarray of float64</code> <p>Input data vector where the incline needs to be detected.</p> required <code>natural_noise</code> <code>float</code> <p>The acceptable noise level for determining the incline.</p> required <p>Returns:</p> Type Description <code>Tuple[int, int]</code> <p>A tuple containing two integers: the left and right indices of the major incline section in the vector.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; vector = np.array([3, 5, 7, 9, 10])\n&gt;&gt;&gt; natural_noise = 2.5\n&gt;&gt;&gt; left, right = find_major_incline(vector, natural_noise)\n&gt;&gt;&gt; (left, right)\n(0, 1)\n</code></pre> Source code in <code>src/cellects/image_analysis/morphological_operations.py</code> <pre><code>def find_major_incline(vector: NDArray, natural_noise: float) -&gt; Tuple[int, int]:\n    \"\"\"\n    Find the major incline section in a vector.\n\n    This function identifies the segment of a vector that exhibits\n    the most significant change in values, considering a specified\n    natural noise level. It returns the left and right indices that\n    define this segment.\n\n    Parameters\n    ----------\n    vector : ndarray of float64\n        Input data vector where the incline needs to be detected.\n    natural_noise : float\n        The acceptable noise level for determining the incline.\n\n    Returns\n    -------\n    Tuple[int, int]\n        A tuple containing two integers: the left and right indices\n        of the major incline section in the vector.\n\n    Examples\n    --------\n    &gt;&gt;&gt; vector = np.array([3, 5, 7, 9, 10])\n    &gt;&gt;&gt; natural_noise = 2.5\n    &gt;&gt;&gt; left, right = find_major_incline(vector, natural_noise)\n    &gt;&gt;&gt; (left, right)\n    (0, 1)\n    \"\"\"\n    left = 0\n    right = 1\n    ref_length = np.max((5, 2 * natural_noise))\n    vector = moving_average(vector, 5)\n    ref_extent = np.ptp(vector)\n    extent = ref_extent\n    # Find the left limit:\n    while len(vector) &gt; ref_length and extent &gt; (ref_extent - (natural_noise / 4)):\n        vector = vector[1:]\n        extent = np.ptp(vector)\n        left += 1\n    # And the right one:\n    extent = ref_extent\n    while len(vector) &gt; ref_length and extent &gt; (ref_extent - natural_noise / 2):\n        vector = vector[:-1]\n        extent = np.ptp(vector)\n        right += 1\n    # And the left again, with stronger stringency:\n    extent = ref_extent\n    while len(vector) &gt; ref_length and extent &gt; (ref_extent - natural_noise):\n        vector = vector[1:]\n        extent = np.ptp(vector)\n        left += 1\n    # When there is no incline, put back left and right to 0\n    if len(vector) &lt;= ref_length:\n        left = 0\n        right = 1\n    return left, right\n</code></pre>"},{"location":"api/cellects/image_analysis/morphological_operations/#cellects.image_analysis.morphological_operations.find_median_shape","title":"<code>find_median_shape(binary_3d_matrix)</code>","text":"<p>Find the median shape from a binary 3D matrix.</p> <p>This function computes the median 2D slice of a binary (0/1) 3D matrix by finding which voxels appear in at least half of the slices.</p> <p>Parameters:</p> Name Type Description Default <code>binary_3d_matrix</code> <code>ndarray of uint8</code> <p>Input 3D binary matrix where each slice is a 2D array.</p> required <p>Returns:</p> Type Description <code>ndarray of uint8</code> <p>Median shape as a 2D binary matrix where the same voxels that appear in at least half of the input slices are set to 1.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; binary_3d_matrix = np.random.randint(0, 2, (10, 5, 5), dtype=np.uint8)\n&gt;&gt;&gt; median_shape = find_median_shape(binary_3d_matrix)\n&gt;&gt;&gt; print(median_shape)\n</code></pre> Source code in <code>src/cellects/image_analysis/morphological_operations.py</code> <pre><code>def find_median_shape(binary_3d_matrix: NDArray[np.uint8]) -&gt; NDArray[np.uint8]:\n    \"\"\"\n    Find the median shape from a binary 3D matrix.\n\n    This function computes the median 2D slice of a binary (0/1) 3D matrix\n    by finding which voxels appear in at least half of the slices.\n\n    Parameters\n    ----------\n    binary_3d_matrix : ndarray of uint8\n        Input 3D binary matrix where each slice is a 2D array.\n\n    Returns\n    -------\n    ndarray of uint8\n        Median shape as a 2D binary matrix where the same voxels\n        that appear in at least half of the input slices are set to 1.\n\n    Examples\n    --------\n    &gt;&gt;&gt; binary_3d_matrix = np.random.randint(0, 2, (10, 5, 5), dtype=np.uint8)\n    &gt;&gt;&gt; median_shape = find_median_shape(binary_3d_matrix)\n    &gt;&gt;&gt; print(median_shape)\n    \"\"\"\n    binary_2d_matrix = np.apply_along_axis(np.sum, 0, binary_3d_matrix)\n    median_shape = np.zeros(binary_2d_matrix.shape, dtype=np.uint8)\n    median_shape[np.greater_equal(binary_2d_matrix, binary_3d_matrix.shape[0] // 2)] = 1\n    return median_shape\n</code></pre>"},{"location":"api/cellects/image_analysis/morphological_operations/#cellects.image_analysis.morphological_operations.get_all_line_coordinates","title":"<code>get_all_line_coordinates(start_point, end_points)</code>","text":"<p>Get all line coordinates between start point and end points.</p> <p>This function computes the coordinates of lines connecting a start point to multiple end points, converting input arrays to float if necessary before processing.</p> <p>Parameters:</p> Name Type Description Default <code>start_point</code> <code>NDArray[float]</code> <p>Starting coordinate point for the lines. Can be of any numeric type, will be converted to float if needed.</p> required <code>end_points</code> <code>NDArray[float]</code> <p>Array of end coordinate points for the lines. Can be of any numeric type, will be converted to float if needed.</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>List[NDArray[int]]</code> <p>A list of numpy arrays containing the coordinates of each line as integer values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; start_point = np.array([0, 0])\n&gt;&gt;&gt; end_points = np.array([[1, 2], [3, 4]])\n&gt;&gt;&gt; get_all_line_coordinates(start_point, end_points)\n[array([[0, 0],\n   [0, 1],\n   [1, 2]], dtype=uint64), array([[0, 0],\n   [1, 1],\n   [1, 2],\n   [2, 3],\n   [3, 4]], dtype=uint64)]\n</code></pre> Source code in <code>src/cellects/image_analysis/morphological_operations.py</code> <pre><code>def get_all_line_coordinates(start_point: NDArray[int], end_points: NDArray[int]) -&gt; NDArray[int]:\n    \"\"\"\n    Get all line coordinates between start point and end points.\n\n    This function computes the coordinates of lines connecting a\n    start point to multiple end points, converting input arrays to float\n    if necessary before processing.\n\n    Parameters\n    ----------\n    start_point : NDArray[float]\n        Starting coordinate point for the lines. Can be of any numeric type,\n        will be converted to float if needed.\n    end_points : NDArray[float]\n        Array of end coordinate points for the lines. Can be of any\n        numeric type, will be converted to float if needed.\n\n    Returns\n    -------\n    out : List[NDArray[int]]\n        A list of numpy arrays containing the coordinates of each line\n        as integer values.\n\n    Examples\n    --------\n    &gt;&gt;&gt; start_point = np.array([0, 0])\n    &gt;&gt;&gt; end_points = np.array([[1, 2], [3, 4]])\n    &gt;&gt;&gt; get_all_line_coordinates(start_point, end_points)\n    [array([[0, 0],\n       [0, 1],\n       [1, 2]], dtype=uint64), array([[0, 0],\n       [1, 1],\n       [1, 2],\n       [2, 3],\n       [3, 4]], dtype=uint64)]\n    \"\"\"\n    lines = []\n    for end_point in end_points:\n        line_coords = get_line_points(start_point, end_point)\n        lines.append(np.array(line_coords, dtype=np.uint64))\n    return lines\n</code></pre>"},{"location":"api/cellects/image_analysis/morphological_operations/#cellects.image_analysis.morphological_operations.get_bb_with_moving_centers","title":"<code>get_bb_with_moving_centers(motion_list, all_specimens_have_same_direction, original_shape_hsize, binary_image, y_boundaries)</code>","text":"<p>Get the bounding boxes with moving centers.</p> <p>Parameters:</p> Name Type Description Default <code>motion_list</code> <code>list</code> <p>List of binary images representing the motion frames.</p> required <code>all_specimens_have_same_direction</code> <code>bool</code> <p>Boolean indicating if all specimens move in the same direction.</p> required <code>original_shape_hsize</code> <code>int or None</code> <p>Original height size of the shape. If <code>None</code>, a default kernel size is used.</p> required <code>binary_image</code> <code>NDArray</code> <p>Binary image of the initial frame.</p> required <code>y_boundaries</code> <code>NDArray</code> <p>Array defining the y-boundaries for ranking shapes.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing: - top : NDArray     Array of top coordinates for each bounding box. - bot : NDArray     Array of bottom coordinates for each bounding box. - left : NDArray     Array of left coordinates for each bounding box. - right : NDArray     Array of right coordinates for each bounding box. - ordered_image_i : NDArray     Updated binary image with the final ranked shapes.</p> Notes <p>This function processes each frame to expand and confirm shapes, updating centroids if necessary. It uses morphological operations like dilation to detect shape changes over frames.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; top, bot, left, right, ordered_image = _get_bb_with_moving_centers(motion_frames, True, None, binary_img, y_bounds)\n&gt;&gt;&gt; print(\"Top coordinates:\", top)\n&gt;&gt;&gt; print(\"Bottom coordinates:\", bot)\n</code></pre> Source code in <code>src/cellects/image_analysis/morphological_operations.py</code> <pre><code>def get_bb_with_moving_centers(motion_list: list, all_specimens_have_same_direction: bool,\n                                original_shape_hsize: int, binary_image: NDArray,\n                                y_boundaries: NDArray):\n    \"\"\"\n    Get the bounding boxes with moving centers.\n\n    Parameters\n    ----------\n    motion_list : list\n        List of binary images representing the motion frames.\n    all_specimens_have_same_direction : bool\n        Boolean indicating if all specimens move in the same direction.\n    original_shape_hsize : int or None\n        Original height size of the shape. If `None`, a default kernel size is used.\n    binary_image : NDArray\n        Binary image of the initial frame.\n    y_boundaries : NDArray\n        Array defining the y-boundaries for ranking shapes.\n\n    Returns\n    -------\n    tuple\n        A tuple containing:\n        - top : NDArray\n            Array of top coordinates for each bounding box.\n        - bot : NDArray\n            Array of bottom coordinates for each bounding box.\n        - left : NDArray\n            Array of left coordinates for each bounding box.\n        - right : NDArray\n            Array of right coordinates for each bounding box.\n        - ordered_image_i : NDArray\n            Updated binary image with the final ranked shapes.\n\n    Notes\n    -----\n    This function processes each frame to expand and confirm shapes, updating centroids if necessary.\n    It uses morphological operations like dilation to detect shape changes over frames.\n\n    Examples\n    --------\n    &gt;&gt;&gt; top, bot, left, right, ordered_image = _get_bb_with_moving_centers(motion_frames, True, None, binary_img, y_bounds)\n    &gt;&gt;&gt; print(\"Top coordinates:\", top)\n    &gt;&gt;&gt; print(\"Bottom coordinates:\", bot)\n    \"\"\"\n    print(\"Read and segment each sample image and rank shapes from top to bot and from left to right\")\n    k_size = 3\n    if original_shape_hsize is not None:\n        k_size = int((np.ceil(original_shape_hsize / 5) * 2) + 1)\n    big_kernel = create_ellipse(k_size, k_size, min_size=3).astype(np.uint8)\n\n    ordered_stats, ordered_centroids, ordered_image = rank_from_top_to_bottom_from_left_to_right(\n        binary_image, y_boundaries, get_ordered_image=True)\n    blob_number = ordered_stats.shape[0]\n\n    ordered_image_i = deepcopy(ordered_image)\n    logging.info(\"For each frame, expand each previously confirmed shape to add area to its maximal bounding box\")\n    for step_i in np.arange(1, len(motion_list)):\n        previously_ordered_centroids = deepcopy(ordered_centroids)\n        new_image_i = motion_list[step_i].copy()\n        detected_shape_number = blob_number + 1\n        c = 0\n        while c &lt; 5 and detected_shape_number == blob_number + 1:\n            c += 1\n            image_i = new_image_i\n            new_image_i = cv2.dilate(image_i, cross_33, iterations=1)\n            detected_shape_number, _ = cv2.connectedComponents(new_image_i, connectivity=8)\n        if c == 0:\n            break\n        else:\n            for shape_i in range(blob_number):\n                shape_to_expand = np.zeros(image_i.shape, dtype=np.uint8)\n                shape_to_expand[ordered_image_i == (shape_i + 1)] = 1\n                without_shape_i = ordered_image_i.copy()\n                without_shape_i[ordered_image_i == (shape_i + 1)] = 0\n                if k_size != 3:\n                    test_shape = expand_until_neighbor_center_gets_nearer_than_own(shape_to_expand, without_shape_i,\n                                                                                   ordered_centroids[shape_i, :],\n                                                                                   np.delete(ordered_centroids, shape_i,\n                                                                                             axis=0), big_kernel)\n                else:\n                    test_shape = shape_to_expand\n                test_shape = expand_until_neighbor_center_gets_nearer_than_own(test_shape, without_shape_i,\n                                                                               ordered_centroids[shape_i, :],\n                                                                               np.delete(ordered_centroids, shape_i,\n                                                                                         axis=0), cross_33)\n                confirmed_shape = test_shape * image_i\n                ordered_image_i[confirmed_shape &gt; 0] = shape_i + 1\n\n\n            mask_to_display = np.zeros(image_i.shape, dtype=np.uint8)\n            mask_to_display[ordered_image_i &gt; 0] = 1\n\n            # If the blob moves enough to drastically change its gravity center,\n            # update the ordered centroids at each frame.\n            detected_shape_number, mask_to_display = cv2.connectedComponents(mask_to_display,\n                                                                             connectivity=8)\n\n            mask_to_display = mask_to_display.astype(np.uint8)\n            while np.logical_and(detected_shape_number - 1 != blob_number,\n                                 np.sum(mask_to_display &gt; 0) &lt; mask_to_display.size):\n                mask_to_display = cv2.dilate(mask_to_display, cross_33, iterations=1)\n                detected_shape_number, mask_to_display = cv2.connectedComponents(mask_to_display,\n                                                                                 connectivity=8)\n                mask_to_display[np.nonzero(mask_to_display)] = 1\n                mask_to_display = mask_to_display.astype(np.uint8)\n            ordered_stats, ordered_centroids = rank_from_top_to_bottom_from_left_to_right(mask_to_display, y_boundaries)\n\n            if all_specimens_have_same_direction:\n                # Adjust each centroid position according to the maximal centroid displacement.\n                x_diffs = ordered_centroids[:, 0] - previously_ordered_centroids[:, 0]\n                if np.mean(x_diffs) &gt; 0: # They moved left, we add to x\n                    add_to_x = np.max(x_diffs) - x_diffs\n                else: #They moved right, we remove from x\n                    add_to_x = np.min(x_diffs) - x_diffs\n                ordered_centroids[:, 0] = ordered_centroids[:, 0] + add_to_x\n\n                y_diffs = ordered_centroids[:, 1] - previously_ordered_centroids[:, 1]\n                if np.mean(y_diffs) &gt; 0:  # They moved down, we add to y\n                    add_to_y = np.max(y_diffs) - y_diffs\n                else:  # They moved up, we remove from y\n                    add_to_y = np.min(y_diffs) - y_diffs\n                ordered_centroids[:, 1] = ordered_centroids[:, 1] + add_to_y\n\n            ordered_image_i = mask_to_display\n\n    # Save each bounding box\n    top = np.zeros(blob_number, dtype=np.int64)\n    bot = np.repeat(binary_image.shape[0], blob_number)\n    left = np.zeros(blob_number, dtype=np.int64)\n    right = np.repeat(binary_image.shape[1], blob_number)\n    for shape_i in range(blob_number):\n        shape_i_indices = np.where(ordered_image_i == shape_i + 1)\n        left[shape_i] = np.min(shape_i_indices[1])\n        right[shape_i] = np.max(shape_i_indices[1])\n        top[shape_i] = np.min(shape_i_indices[0])\n        bot[shape_i] = np.max(shape_i_indices[0])\n    return top, bot, left, right, ordered_image_i\n</code></pre>"},{"location":"api/cellects/image_analysis/morphological_operations/#cellects.image_analysis.morphological_operations.get_contours","title":"<code>get_contours(binary_image)</code>","text":"<p>Find and return the contours of a binary image.</p> <p>This function erodes the input binary image using a 3x3 cross-shaped structuring element and then subtracts the eroded image from the original to obtain the contours.</p> <p>Parameters:</p> Name Type Description Default <code>binary_image</code> <code>ndarray of uint8</code> <p>Input binary image from which to extract contours.</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>ndarray of uint8</code> <p>Image containing only the contours extracted from <code>binary_image</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; binary_image = np.zeros((10, 10), dtype=np.uint8)\n&gt;&gt;&gt; binary_image[2:8, 2:8] = 1\n&gt;&gt;&gt; result = get_contours(binary_image)\n&gt;&gt;&gt; print(result)\n[[0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0]\n [0 0 1 1 1 1 1 1 0 0]\n [0 0 1 0 0 0 0 1 0 0]\n [0 0 1 0 0 0 0 1 0 0]\n [0 0 1 0 0 0 0 1 0 0]\n [0 0 1 0 0 0 0 1 0 0]\n [0 0 1 1 1 1 1 1 0 0]\n [0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0]]\n</code></pre> Source code in <code>src/cellects/image_analysis/morphological_operations.py</code> <pre><code>def get_contours(binary_image: NDArray[np.uint8]) -&gt; NDArray[np.uint8]:\n    \"\"\"\n    Find and return the contours of a binary image.\n\n    This function erodes the input binary image using a 3x3 cross-shaped\n    structuring element and then subtracts the eroded image from the original to obtain the contours.\n\n    Parameters\n    ----------\n    binary_image : ndarray of uint8\n        Input binary image from which to extract contours.\n\n    Returns\n    -------\n    out : ndarray of uint8\n        Image containing only the contours extracted from `binary_image`.\n\n    Examples\n    --------\n    &gt;&gt;&gt; binary_image = np.zeros((10, 10), dtype=np.uint8)\n    &gt;&gt;&gt; binary_image[2:8, 2:8] = 1\n    &gt;&gt;&gt; result = get_contours(binary_image)\n    &gt;&gt;&gt; print(result)\n    [[0 0 0 0 0 0 0 0 0 0]\n     [0 0 0 0 0 0 0 0 0 0]\n     [0 0 1 1 1 1 1 1 0 0]\n     [0 0 1 0 0 0 0 1 0 0]\n     [0 0 1 0 0 0 0 1 0 0]\n     [0 0 1 0 0 0 0 1 0 0]\n     [0 0 1 0 0 0 0 1 0 0]\n     [0 0 1 1 1 1 1 1 0 0]\n     [0 0 0 0 0 0 0 0 0 0]\n     [0 0 0 0 0 0 0 0 0 0]]\n    \"\"\"\n    if not isinstance(binary_image.dtype, np.uint8):\n        binary_image = binary_image.astype(np.uint8)\n    if np.all(binary_image):\n        contours = 1 - image_borders(binary_image.shape)\n    elif np.any(binary_image):\n        eroded_binary = cv2.erode(binary_image, cross_33, borderType=cv2.BORDER_CONSTANT, borderValue=0)\n        contours = binary_image - eroded_binary\n    else:\n        contours = binary_image\n    return contours\n</code></pre>"},{"location":"api/cellects/image_analysis/morphological_operations/#cellects.image_analysis.morphological_operations.get_largest_connected_component","title":"<code>get_largest_connected_component(segmentation)</code>","text":"<p>Find the largest connected component in a segmentation image.</p> <p>This function labels all connected components in a binary segmentation image, determines the size of each component, and returns information about the largest connected component.</p> <p>Parameters:</p> Name Type Description Default <code>segmentation</code> <code>ndarray of uint8</code> <p>Binary segmentation image where different integer values represent different connected components.</p> required <p>Returns:</p> Type Description <code>Tuple[int, ndarray of bool]</code> <p>A tuple containing: - The size of the largest connected component. - A boolean mask representing the largest connected   component in the input segmentation image.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; segmentation = np.zeros((10, 10), dtype=np.uint8)\n&gt;&gt;&gt; segmentation[2:6, 2:5] = 1\n&gt;&gt;&gt; segmentation[6:9, 6:9] = 1\n&gt;&gt;&gt; size, mask = get_largest_connected_component(segmentation)\n&gt;&gt;&gt; print(size)\n12\n</code></pre> Source code in <code>src/cellects/image_analysis/morphological_operations.py</code> <pre><code>def get_largest_connected_component(segmentation: NDArray[np.uint8]) -&gt; Tuple[np.int64, NDArray[bool]]:\n    \"\"\"\n    Find the largest connected component in a segmentation image.\n\n    This function labels all connected components in a binary\n    segmentation image, determines the size of each component,\n    and returns information about the largest connected component.\n\n    Parameters\n    ----------\n    segmentation : ndarray of uint8\n        Binary segmentation image where different integer values represent\n        different connected components.\n\n    Returns\n    -------\n    Tuple[int, ndarray of bool]\n        A tuple containing:\n        - The size of the largest connected component.\n        - A boolean mask representing the largest connected\n          component in the input segmentation image.\n\n    Examples\n    --------\n    &gt;&gt;&gt; segmentation = np.zeros((10, 10), dtype=np.uint8)\n    &gt;&gt;&gt; segmentation[2:6, 2:5] = 1\n    &gt;&gt;&gt; segmentation[6:9, 6:9] = 1\n    &gt;&gt;&gt; size, mask = get_largest_connected_component(segmentation)\n    &gt;&gt;&gt; print(size)\n    12\n    \"\"\"\n    labels = label(segmentation)\n    assert(labels.max() != 0) # assume at least 1 CC\n    con_comp_sizes = np.bincount(labels.flat)[1:]\n    largest_idx = np.argmax(con_comp_sizes)\n    largest_connected_component = labels == largest_idx + 1\n    return con_comp_sizes[largest_idx], largest_connected_component\n</code></pre>"},{"location":"api/cellects/image_analysis/morphological_operations/#cellects.image_analysis.morphological_operations.get_line_points","title":"<code>get_line_points(start, end)</code>","text":"<p>Get line points between two endpoints using Bresenham's line algorithm.</p> <p>This function calculates all the integer coordinate points that form a line between two endpoints using Bresenham's line algorithm. It is optimized for performance using Numba's just-in-time compilation.</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>tuple of int</code> <p>The starting point coordinates (y0, x0).</p> required <code>end</code> <code>tuple of int</code> <p>The ending point coordinates (y1, x1).</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>ndarray of int</code> <p>Array of points representing the line, with shape (N, 2), where N is the number of points on the line.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; start = (0, 0)\n&gt;&gt;&gt; end = (1, 2)\n&gt;&gt;&gt; points = get_line_points(start, end)\n&gt;&gt;&gt; print(points)\n[[0 0]\n[0 1]\n[1 2]]\n</code></pre> Source code in <code>src/cellects/image_analysis/morphological_operations.py</code> <pre><code>@njit()\ndef get_line_points(start, end) -&gt; NDArray[int]:\n    \"\"\"\n    Get line points between two endpoints using Bresenham's line algorithm.\n\n    This function calculates all the integer coordinate points that form a\n    line between two endpoints using Bresenham's line algorithm. It is\n    optimized for performance using Numba's just-in-time compilation.\n\n    Parameters\n    ----------\n    start : tuple of int\n        The starting point coordinates (y0, x0).\n    end : tuple of int\n        The ending point coordinates (y1, x1).\n\n    Returns\n    -------\n    out : ndarray of int\n        Array of points representing the line, with shape (N, 2), where N is\n        the number of points on the line.\n\n    Examples\n    --------\n    &gt;&gt;&gt; start = (0, 0)\n    &gt;&gt;&gt; end = (1, 2)\n    &gt;&gt;&gt; points = get_line_points(start, end)\n    &gt;&gt;&gt; print(points)\n    [[0 0]\n    [0 1]\n    [1 2]]\n    \"\"\"\n    y0, x0 = start\n    y1, x1 = end\n\n    # Calculate differences\n    dx = np.abs(x1 - x0)\n    dy = np.abs(y1 - y0)\n\n    # Determine step direction\n    sx = 1 if x0 &lt; x1 else -1\n    sy = 1 if y0 &lt; y1 else -1\n\n    # Initialize\n    err = dx - dy\n    points = []\n    x, y = x0, y0\n\n    while True:\n        points.append([y, x])\n\n        # Check if we've reached the end\n        if x == x1 and y == y1:\n            break\n\n        # Calculate error for next step\n        e2 = 2 * err\n\n        if e2 &gt; -dy:\n            err -= dy\n            x += sx\n\n        if e2 &lt; dx:\n            err += dx\n            y += sy\n\n    return np.array(points)\n</code></pre>"},{"location":"api/cellects/image_analysis/morphological_operations/#cellects.image_analysis.morphological_operations.get_min_or_max_euclidean_pair","title":"<code>get_min_or_max_euclidean_pair(coords, min_or_max='max')</code>","text":"<p>Find the pair of points in a given set with the minimum or maximum Euclidean distance.</p> <p>Parameters:</p> Name Type Description Default <code>coords</code> <code>Union[ndarray, Tuple]</code> <p>An Nx2 numpy array or a tuple of two arrays, each containing the x and y coordinates of points.</p> required <code>min_or_max</code> <code>str</code> <p>Whether to find the 'min' or 'max' distance pair. Default is 'max'.</p> <code>'max'</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>A tuple containing the coordinates of the two points that form the minimum or maximum distance pair.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>min_or_max</code> is not 'min' or 'max'.</p> Notes <ul> <li>The function first computes all pairwise distances in condensed form using <code>pdist</code>.</li> <li>Then, it finds the index of the minimum or maximum distance.</li> <li>Finally, it maps this index to the actual point indices using a binary search method.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; coords = np.array([[0, 1], [2, 3], [4, 5]])\n&gt;&gt;&gt; point1, point2 = get_min_or_max_euclidean_pair(coords, min_or_max=\"max\")\n&gt;&gt;&gt; print(point1)\n[0 1]\n&gt;&gt;&gt; print(point2)\n[4 5]\n&gt;&gt;&gt; coords = (np.array([0, 2, 4, 8, 1, 5]), np.array([0, 2, 4, 8, 0, 5]))\n&gt;&gt;&gt; point1, point2 = get_min_or_max_euclidean_pair(coords, min_or_max=\"min\")\n&gt;&gt;&gt; print(point1)\n[0 0]\n&gt;&gt;&gt; print(point2)\n[1 0]\n</code></pre> Source code in <code>src/cellects/image_analysis/morphological_operations.py</code> <pre><code>def get_min_or_max_euclidean_pair(coords, min_or_max: str=\"max\") -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Find the pair of points in a given set with the minimum or maximum Euclidean distance.\n\n    Parameters\n    ----------\n    coords : Union[np.ndarray, Tuple]\n        An Nx2 numpy array or a tuple of two arrays, each containing the x and y coordinates of points.\n    min_or_max : str, optional\n        Whether to find the 'min' or 'max' distance pair. Default is 'max'.\n\n    Returns\n    -------\n    Tuple[np.ndarray, np.ndarray]\n        A tuple containing the coordinates of the two points that form the minimum or maximum distance pair.\n\n    Raises\n    ------\n    ValueError\n        If `min_or_max` is not 'min' or 'max'.\n\n    Notes\n    -----\n    - The function first computes all pairwise distances in condensed form using `pdist`.\n    - Then, it finds the index of the minimum or maximum distance.\n    - Finally, it maps this index to the actual point indices using a binary search method.\n\n    Examples\n    --------\n    &gt;&gt;&gt; coords = np.array([[0, 1], [2, 3], [4, 5]])\n    &gt;&gt;&gt; point1, point2 = get_min_or_max_euclidean_pair(coords, min_or_max=\"max\")\n    &gt;&gt;&gt; print(point1)\n    [0 1]\n    &gt;&gt;&gt; print(point2)\n    [4 5]\n    &gt;&gt;&gt; coords = (np.array([0, 2, 4, 8, 1, 5]), np.array([0, 2, 4, 8, 0, 5]))\n    &gt;&gt;&gt; point1, point2 = get_min_or_max_euclidean_pair(coords, min_or_max=\"min\")\n    &gt;&gt;&gt; print(point1)\n    [0 0]\n    &gt;&gt;&gt; print(point2)\n    [1 0]\n\n    \"\"\"\n    if isinstance(coords, Tuple):\n        coords = np.column_stack(coords)\n    N = coords.shape[0]\n    if N &lt;= 1:\n        return (coords[0], coords[0]) if N == 1 else None\n\n    # Step 1: Compute all pairwise distances in condensed form\n    distances = pdist(coords)\n\n    # Step 2: Find the index of the maximum distance\n    if min_or_max == \"max\":\n        idx = np.argmax(distances)\n    elif min_or_max == \"min\":\n        idx = np.argmin(distances)\n    else:\n        raise ValueError\n\n    # Step 3: Map this index to (i, j) using a binary search method\n\n    def get_pair_index(k):\n        low, high = 0, N\n        while low &lt; high:\n            mid = (low + high) // 2\n            total = mid * (2 * N - mid - 1) // 2\n            if total &lt;= k:\n                low = mid + 1\n            else:\n                high = mid\n\n        i = low - 1\n        prev_sum = i * (2 * N - i - 1) // 2\n        j_index_in_row = k - prev_sum\n        return i, i + j_index_in_row + 1  # Ensure j &gt; i\n\n    i, j = get_pair_index(idx)\n    return coords[i], coords[j]\n</code></pre>"},{"location":"api/cellects/image_analysis/morphological_operations/#cellects.image_analysis.morphological_operations.get_minimal_distance_between_2_shapes","title":"<code>get_minimal_distance_between_2_shapes(image_of_2_shapes, increase_speed=True)</code>","text":"<p>Get the minimal distance between two shapes in an image.</p> <p>This function calculates the minimal Euclidean distance between two different shapes represented by binary values 1 and 2 in a given image. It can optionally reduce the image size for faster processing.</p> <p>Parameters:</p> Name Type Description Default <code>image_of_2_shapes</code> <code>ndarray of int8</code> <p>Binary image containing two shapes to measure distance between.</p> required <code>increase_speed</code> <code>bool</code> <p>Flag to reduce image size for faster computation. Default is True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>min_distance</code> <code>float64</code> <p>The minimal Euclidean distance between the two shapes.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; image = np.array([[1, 0], [0, 2]])\n&gt;&gt;&gt; distance = get_minimal_distance_between_2_shapes(image)\n&gt;&gt;&gt; print(distance)\nexpected output\n</code></pre> Source code in <code>src/cellects/image_analysis/morphological_operations.py</code> <pre><code>def get_minimal_distance_between_2_shapes(image_of_2_shapes: NDArray[np.uint8], increase_speed: bool=True) -&gt; float:\n    \"\"\"\n    Get the minimal distance between two shapes in an image.\n\n    This function calculates the minimal Euclidean distance between\n    two different shapes represented by binary values 1 and 2 in a given image.\n    It can optionally reduce the image size for faster processing.\n\n    Parameters\n    ----------\n    image_of_2_shapes : ndarray of int8\n        Binary image containing two shapes to measure distance between.\n    increase_speed : bool, optional\n        Flag to reduce image size for faster computation. Default is True.\n\n    Returns\n    -------\n    min_distance : float64\n        The minimal Euclidean distance between the two shapes.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; image = np.array([[1, 0], [0, 2]])\n    &gt;&gt;&gt; distance = get_minimal_distance_between_2_shapes(image)\n    &gt;&gt;&gt; print(distance)\n    expected output\n    \"\"\"\n    if increase_speed:\n        shape1_idx, shape2_idx = reduce_image_size_for_speed(image_of_2_shapes)\n    else:\n        shape1_idx, shape2_idx = np.nonzero(image_of_2_shapes == 1), np.nonzero(image_of_2_shapes == 2)\n    t = KDTree(np.transpose(shape1_idx))\n    dists, nns = t.query(np.transpose(shape2_idx), 1)\n    return np.min(dists)\n</code></pre>"},{"location":"api/cellects/image_analysis/morphological_operations/#cellects.image_analysis.morphological_operations.get_quick_bounding_boxes","title":"<code>get_quick_bounding_boxes(binary_image, ordered_image, ordered_stats)</code>","text":"<p>Compute bounding boxes for shapes in a binary image.</p> <p>Parameters:</p> Name Type Description Default <code>binary_image</code> <code>NDArray[uint8]</code> <p>A 2D array representing the binary image.</p> required <code>ordered_image</code> <code>NDArray</code> <p>An array containing the ordered image data.</p> required <code>ordered_stats</code> <code>NDArray</code> <p>A 2D array with statistics about the shapes in the image.</p> required <p>Returns:</p> Type Description <code>Tuple[NDArray, NDArray, NDArray, NDArray]</code> <p>A tuple containing four arrays: - top: Array of y-coordinates for the top edge of bounding boxes. - bot: Array of y-coordinates for the bottom edge of bounding boxes. - left: Array of x-coordinates for the left edge of bounding boxes. - right: Array of x-coordinates for the right edge of bounding boxes.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; binary_image = np.array([[0, 1], [0, 0], [1, 0]], dtype=np.uint8)\n&gt;&gt;&gt; ordered_image = np.array([[0, 1], [0, 0], [2, 0]], dtype=np.uint8)\n&gt;&gt;&gt; ordered_stats = np.array([[1, 0, 1, 1, 1], [0, 2, 1, 1, 1]], dtype=np.int32)\n&gt;&gt;&gt; top, bot, left, right = get_quick_bounding_boxes(binary_image, ordered_image, ordered_stats)\n&gt;&gt;&gt; print(top)\n[-1  1]\n&gt;&gt;&gt; print(bot)\n[2 4]\n&gt;&gt;&gt; print(left)\n[0 -1]\n&gt;&gt;&gt; print(right)\n[3 2]\n</code></pre> Source code in <code>src/cellects/image_analysis/morphological_operations.py</code> <pre><code>def get_quick_bounding_boxes(binary_image: NDArray[np.uint8], ordered_image: NDArray, ordered_stats: NDArray) -&gt; Tuple[NDArray, NDArray, NDArray, NDArray]:\n    \"\"\"\n    Compute bounding boxes for shapes in a binary image.\n\n    Parameters\n    ----------\n    binary_image : NDArray[np.uint8]\n        A 2D array representing the binary image.\n    ordered_image : NDArray\n        An array containing the ordered image data.\n    ordered_stats : NDArray\n        A 2D array with statistics about the shapes in the image.\n\n    Returns\n    -------\n    Tuple[NDArray, NDArray, NDArray, NDArray]\n        A tuple containing four arrays:\n        - top: Array of y-coordinates for the top edge of bounding boxes.\n        - bot: Array of y-coordinates for the bottom edge of bounding boxes.\n        - left: Array of x-coordinates for the left edge of bounding boxes.\n        - right: Array of x-coordinates for the right edge of bounding boxes.\n\n    Examples\n    --------\n    &gt;&gt;&gt; binary_image = np.array([[0, 1], [0, 0], [1, 0]], dtype=np.uint8)\n    &gt;&gt;&gt; ordered_image = np.array([[0, 1], [0, 0], [2, 0]], dtype=np.uint8)\n    &gt;&gt;&gt; ordered_stats = np.array([[1, 0, 1, 1, 1], [0, 2, 1, 1, 1]], dtype=np.int32)\n    &gt;&gt;&gt; top, bot, left, right = get_quick_bounding_boxes(binary_image, ordered_image, ordered_stats)\n    &gt;&gt;&gt; print(top)\n    [-1  1]\n    &gt;&gt;&gt; print(bot)\n    [2 4]\n    &gt;&gt;&gt; print(left)\n    [0 -1]\n    &gt;&gt;&gt; print(right)\n    [3 2]\n    \"\"\"\n    shapes = get_contours(binary_image)\n    x_min = ordered_stats[:, 0]\n    y_min = ordered_stats[:, 1]\n    x_max = ordered_stats[:, 0] + ordered_stats[:, 2]\n    y_max = ordered_stats[:, 1] + ordered_stats[:, 3]\n    x_min_dist = shapes.shape[1]\n    y_min_dist = shapes.shape[0]\n\n    shapes *= ordered_image\n    shape_nb = (len(np.unique(shapes)) - 1)\n    i = 0\n    a_indices, b_indices = np.triu_indices(shape_nb, 1)\n    a_indices, b_indices = a_indices + 1, b_indices + 1\n    all_distances = np.zeros((len(a_indices), 3), dtype=float)\n    # For every pair of components, find the minimal distance\n    for (a, b) in zip(a_indices, b_indices):\n        x_dist = np.absolute(x_max[a - 1] - x_min[b - 1])\n        y_dist = np.absolute(y_max[a - 1] - y_min[b - 1])\n        if x_dist &lt; 2 * x_min_dist and y_dist &lt; 2 * y_min_dist:\n            sub_shapes = np.logical_or(shapes == a, shapes == b) * shapes\n            sub_shapes = sub_shapes[np.min((y_min[a - 1], y_min[b - 1])):np.max((y_max[a - 1], y_max[b - 1])),\n                         np.min((x_min[a - 1], x_min[b - 1])):np.max((x_max[a - 1], x_max[b - 1]))]\n            sub_shapes[sub_shapes == a] = 1\n            sub_shapes[sub_shapes == b] = 2\n            if np.any(sub_shapes == 1) and np.any(sub_shapes == 2):\n                all_distances[i, :] = a, b, get_minimal_distance_between_2_shapes(sub_shapes, False)\n\n                if x_dist &gt; y_dist:\n                    x_min_dist = np.min((x_min_dist, x_dist))\n                else:\n                    y_min_dist = np.min((y_min_dist, y_dist))\n                i += 1\n    shape_number = ordered_stats.shape[0]\n    top = np.zeros(shape_number, dtype=np.int64)\n    bot = np.repeat(binary_image.shape[0], shape_number)\n    left = np.zeros(shape_number, dtype=np.int64)\n    right = np.repeat(binary_image.shape[1], shape_number)\n    for shape_i in np.arange(1, shape_nb + 1):\n        # Get where the shape i appear in pairwise comparisons\n        idx = np.nonzero(np.logical_or(all_distances[:, 0] == shape_i, all_distances[:, 1] == shape_i))\n        # Compute the minimal distance related to shape i and divide by 2\n        if len(all_distances[idx, 2]) &gt; 0:\n            dist = all_distances[idx, 2].min() // 2\n        else:\n            dist = 1\n            # Save the coordinates of the arena around shape i\n        top[shape_i - 1] = y_min[shape_i - 1] - dist.astype(np.int64)\n        bot[shape_i - 1] = y_max[shape_i - 1] + dist.astype(np.int64)\n        left[shape_i - 1] = x_min[shape_i - 1] - dist.astype(np.int64)\n        right[shape_i - 1] = x_max[shape_i - 1] + dist.astype(np.int64)\n    return top, bot, left, right\n</code></pre>"},{"location":"api/cellects/image_analysis/morphological_operations/#cellects.image_analysis.morphological_operations.get_radius_distance_against_time","title":"<code>get_radius_distance_against_time(binary_video, field)</code>","text":"<p>Calculate the radius distance against time from a binary video and field.</p> <p>This function computes the change in radius distances over time by analyzing a binary video and mapping it to corresponding field values.</p> <p>Parameters:</p> Name Type Description Default <code>binary_video</code> <code>ndarray of uint8</code> <p>Binary video data.</p> required <code>field</code> <code>ndarray</code> <p>Field values to analyze the radius distances against.</p> required <p>Returns:</p> Name Type Description <code>distance_against_time</code> <code>ndarray of float32</code> <p>Radius distances over time.</p> <code>time_start</code> <code>int</code> <p>Starting time index where the radius distance measurement begins.</p> <code>time_end</code> <code>int</code> <p>Ending time index where the radius distance measurement ends.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; binary_video = np.ones((10, 5, 5), dtype=np.uint8)\n</code></pre> <pre><code>&gt;&gt;&gt; distance_against_time, time_start, time_end = get_radius_distance_against_time(binary_video, field)\n</code></pre> Source code in <code>src/cellects/image_analysis/morphological_operations.py</code> <pre><code>def get_radius_distance_against_time(binary_video: NDArray[np.uint8], field) -&gt; Tuple[NDArray[np.float32], int, int]:\n    \"\"\"\n    Calculate the radius distance against time from a binary video and field.\n\n    This function computes the change in radius distances over time\n    by analyzing a binary video and mapping it to corresponding field values.\n\n    Parameters\n    ----------\n    binary_video : ndarray of uint8\n        Binary video data.\n    field : ndarray\n        Field values to analyze the radius distances against.\n\n    Returns\n    -------\n    distance_against_time : ndarray of float32\n        Radius distances over time.\n    time_start : int\n        Starting time index where the radius distance measurement begins.\n    time_end : int\n        Ending time index where the radius distance measurement ends.\n\n    Examples\n    --------\n    &gt;&gt;&gt; binary_video = np.ones((10, 5, 5), dtype=np.uint8)\n\n    &gt;&gt;&gt; distance_against_time, time_start, time_end = get_radius_distance_against_time(binary_video, field)\n    \"\"\"\n    pixel_start = np.max(field[field &gt; 0])\n    pixel_end = np.min(field[field &gt; 0])\n    time_span = np.arange(binary_video.shape[0])\n    time_start = 0\n    time_end = time_span[-1]\n    start_not_found: bool = True\n    for t in time_span:\n        if start_not_found:\n            if np.any((field == pixel_start) * binary_video[t, :, :]):\n                start_not_found = False\n                time_start = t\n        if np.any((field == pixel_end) * binary_video[t, :, :]):\n            time_end = t\n            break\n    distance_against_time = np.linspace(pixel_start, pixel_end, (time_end - time_start + 1))\n    distance_against_time = np.round(distance_against_time).astype(np.float32)\n    return distance_against_time, time_start, time_end\n</code></pre>"},{"location":"api/cellects/image_analysis/morphological_operations/#cellects.image_analysis.morphological_operations.image_borders","title":"<code>image_borders(dimensions, shape='rectangular')</code>","text":"<p>Create an image with borders, either rectangular or circular.</p> <p>Parameters:</p> Name Type Description Default <code>dimensions</code> <code>tuple</code> <p>The dimensions of the image (height, width).</p> required <code>shape</code> <code>str</code> <p>The shape of the borders. Options are \"rectangular\" or \"circular\". Defaults to \"rectangular\".</p> <code>'rectangular'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>ndarray of uint8</code> <p>The image with borders. If the shape is \"circular\", an ellipse border; if \"rectangular\", a rectangular border.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; borders = image_borders((3, 3), \"rectangular\")\n&gt;&gt;&gt; print(borders)\n[[0 0 0]\n [0 1 0]\n [0 0 0]]\n</code></pre> Source code in <code>src/cellects/image_analysis/morphological_operations.py</code> <pre><code>def image_borders(dimensions: tuple, shape: str=\"rectangular\") -&gt; NDArray[np.uint8]:\n    \"\"\"\n    Create an image with borders, either rectangular or circular.\n\n    Parameters\n    ----------\n    dimensions : tuple\n        The dimensions of the image (height, width).\n    shape : str, optional\n        The shape of the borders. Options are \"rectangular\" or \"circular\".\n        Defaults to \"rectangular\".\n\n    Returns\n    -------\n    out : ndarray of uint8\n        The image with borders. If the shape is \"circular\", an ellipse border;\n        if \"rectangular\", a rectangular border.\n\n    Examples\n    --------\n    &gt;&gt;&gt; borders = image_borders((3, 3), \"rectangular\")\n    &gt;&gt;&gt; print(borders)\n    [[0 0 0]\n     [0 1 0]\n     [0 0 0]]\n    \"\"\"\n    if shape == \"circular\":\n        borders = create_ellipse(dimensions[0], dimensions[0])\n        img_contours = image_borders(dimensions)\n        borders = borders * img_contours\n    else:\n        borders = np.ones(dimensions, dtype=np.uint8)\n        borders[0, :] = 0\n        borders[:, 0] = 0\n        borders[- 1, :] = 0\n        borders[:, - 1] = 0\n    return borders\n</code></pre>"},{"location":"api/cellects/image_analysis/morphological_operations/#cellects.image_analysis.morphological_operations.inverted_distance_transform","title":"<code>inverted_distance_transform(original_shape, max_distance=None, with_erosion=0)</code>","text":"<p>Calculate the distance transform around ones in a binary image, with optional erosion.</p> <p>This function computes the Euclidean distance transform where zero values represent the background and ones represent the foreground. Optionally, it erodes the input image before computing the distance transform, and limits distances based on a maximum value.</p> <p>Parameters:</p> Name Type Description Default <code>original_shape</code> <code>ndarray of uint8</code> <p>Input binary image where ones represent the foreground.</p> required <code>max_distance</code> <code>int</code> <p>Maximum distance value to threshold. If None (default), no thresholding is applied.</p> <code>None</code> <code>with_erosion</code> <code>int</code> <p>Number of iterations for erosion. If 0 (default), no erosion is applied.</p> <code>0</code> <p>Returns:</p> Name Type Description <code>out</code> <code>ndarray of uint32</code> <p>Distance transform array where each element represents the distance to the nearest zero value in the input image.</p> See also <p>rounded_distance_transform : less precise (outputs int) and faster for small max_distance values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; segmentation = np.zeros((4, 4), dtype=np.uint8)\n&gt;&gt;&gt; segmentation[1:3, 1:3] = 1\n&gt;&gt;&gt; gravity = inverted_distance_transform(segmentation, max_distance=2)\n&gt;&gt;&gt; print(gravity)\n[[1.         1.41421356 1.41421356 1.        ]\n [1.41421356 0.         0.         1.41421356]\n [1.41421356 0.         0.         1.41421356]\n [1.         1.41421356 1.41421356 1.        ]]\n</code></pre> Source code in <code>src/cellects/image_analysis/morphological_operations.py</code> <pre><code>def inverted_distance_transform(original_shape: NDArray[np.uint8], max_distance: int=None, with_erosion: int=0) -&gt; NDArray[np.uint32]:\n    \"\"\"\n    Calculate the distance transform around ones in a binary image, with optional erosion.\n\n    This function computes the Euclidean distance transform where zero values\n    represent the background and ones represent the foreground. Optionally,\n    it erodes the input image before computing the distance transform, and\n    limits distances based on a maximum value.\n\n    Parameters\n    ----------\n    original_shape : ndarray of uint8\n        Input binary image where ones represent the foreground.\n    max_distance : int, optional\n        Maximum distance value to threshold. If None (default), no thresholding is applied.\n    with_erosion : int, optional\n        Number of iterations for erosion. If 0 (default), no erosion is applied.\n\n    Returns\n    -------\n    out : ndarray of uint32\n        Distance transform array where each element represents the distance\n        to the nearest zero value in the input image.\n\n    See also\n    --------\n    rounded_distance_transform : less precise (outputs int) and faster for small max_distance values.\n\n    Examples\n    --------\n    &gt;&gt;&gt; segmentation = np.zeros((4, 4), dtype=np.uint8)\n    &gt;&gt;&gt; segmentation[1:3, 1:3] = 1\n    &gt;&gt;&gt; gravity = inverted_distance_transform(segmentation, max_distance=2)\n    &gt;&gt;&gt; print(gravity)\n    [[1.         1.41421356 1.41421356 1.        ]\n     [1.41421356 0.         0.         1.41421356]\n     [1.41421356 0.         0.         1.41421356]\n     [1.         1.41421356 1.41421356 1.        ]]\n    \"\"\"\n    if with_erosion:\n        original_shape = cv2.erode(original_shape, cross_33, iterations=with_erosion, borderType=cv2.BORDER_CONSTANT, borderValue=0)\n    gravity_field = distance_transform_edt(1 - original_shape)\n    if max_distance is not None:\n        if max_distance &gt; np.min(original_shape.shape) / 2:\n            max_distance = (np.min(original_shape.shape) // 2).astype(np.uint32)\n        gravity_field[gravity_field &gt;= max_distance] = 0\n    gravity_field[gravity_field &gt; 0] = 1 + gravity_field.max() - gravity_field[gravity_field &gt; 0]\n    return gravity_field\n</code></pre>"},{"location":"api/cellects/image_analysis/morphological_operations/#cellects.image_analysis.morphological_operations.keep_largest_shape","title":"<code>keep_largest_shape(indexed_shapes)</code>","text":"<p>Keep the largest shape from an array of indexed shapes.</p> <p>This function identifies the most frequent non-zero shape in the input array and returns a binary mask where elements matching this shape are set to 1, and others are set to 0. The function uses NumPy's bincount to count occurrences of each shape and assumes that the first element (index 0) is not part of any shape classification.</p> <p>Parameters:</p> Name Type Description Default <code>indexed_shapes</code> <code>ndarray of int32</code> <p>Input array containing indexed shapes.</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>ndarray of uint8</code> <p>Binary mask where the largest shape is marked as 1.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; indexed_shapes = np.array([0, 2, 2, 3, 1], dtype=np.int32)\n&gt;&gt;&gt; keep_largest_shape(indexed_shapes)\narray([0, 1, 1, 0, 0], dtype=uint8)\n</code></pre> Source code in <code>src/cellects/image_analysis/morphological_operations.py</code> <pre><code>@njit()\ndef keep_largest_shape(indexed_shapes: NDArray[np.int32]) -&gt; NDArray[np.uint8]:\n    \"\"\"\n    Keep the largest shape from an array of indexed shapes.\n\n    This function identifies the most frequent non-zero shape in the input\n    array and returns a binary mask where elements matching this shape are set to 1,\n    and others are set to 0. The function uses NumPy's bincount to count occurrences\n    of each shape and assumes that the first element (index 0) is not part of any\n    shape classification.\n\n    Parameters\n    ----------\n    indexed_shapes : ndarray of int32\n        Input array containing indexed shapes.\n\n    Returns\n    -------\n    out : ndarray of uint8\n        Binary mask where the largest shape is marked as 1.\n\n    Examples\n    --------\n    &gt;&gt;&gt; indexed_shapes = np.array([0, 2, 2, 3, 1], dtype=np.int32)\n    &gt;&gt;&gt; keep_largest_shape(indexed_shapes)\n    array([0, 1, 1, 0, 0], dtype=uint8)\n    \"\"\"\n    label_counts = np.bincount(indexed_shapes.flatten())\n    largest_label = 1 + np.argmax(label_counts[1:])\n    return (indexed_shapes == largest_label).astype(np.uint8)\n</code></pre>"},{"location":"api/cellects/image_analysis/morphological_operations/#cellects.image_analysis.morphological_operations.keep_one_connected_component","title":"<code>keep_one_connected_component(binary_image)</code>","text":"<p>Keep only one connected component in a binary image.</p> <p>This function filters out all but the largest connected component in a binary image, effectively isolating it from other noise or objects. The function ensures the input is in uint8 format before processing.</p> <p>Parameters:</p> Name Type Description Default <code>binary_image</code> <code>ndarray of uint8</code> <p>Binary image containing one or more connected components.</p> required <p>Returns:</p> Type Description <code>ndarray of uint8</code> <p>Image with only the largest connected component retained.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; all_shapes = np.zeros((5, 5), dtype=np.uint8)\n&gt;&gt;&gt; all_shapes[0:2, 0:2] = 1\n&gt;&gt;&gt; all_shapes[3:4, 3:4] = 1\n&gt;&gt;&gt; res = keep_one_connected_component(all_shapes)\n&gt;&gt;&gt; print(res)\n[[1 1 0 0 0]\n [1 1 0 0 0]\n [0 0 0 0 0]\n [0 0 0 0 0]\n [0 0 0 0 0]]\n</code></pre> Source code in <code>src/cellects/image_analysis/morphological_operations.py</code> <pre><code>def keep_one_connected_component(binary_image: NDArray[np.uint8])-&gt; NDArray[np.uint8]:\n    \"\"\"\n    Keep only one connected component in a binary image.\n\n    This function filters out all but the largest connected component in\n    a binary image, effectively isolating it from other noise or objects.\n    The function ensures the input is in uint8 format before processing.\n\n    Parameters\n    ----------\n    binary_image : ndarray of uint8\n        Binary image containing one or more connected components.\n\n    Returns\n    -------\n    ndarray of uint8\n        Image with only the largest connected component retained.\n\n    Examples\n    -------\n    &gt;&gt;&gt; all_shapes = np.zeros((5, 5), dtype=np.uint8)\n    &gt;&gt;&gt; all_shapes[0:2, 0:2] = 1\n    &gt;&gt;&gt; all_shapes[3:4, 3:4] = 1\n    &gt;&gt;&gt; res = keep_one_connected_component(all_shapes)\n    &gt;&gt;&gt; print(res)\n    [[1 1 0 0 0]\n     [1 1 0 0 0]\n     [0 0 0 0 0]\n     [0 0 0 0 0]\n     [0 0 0 0 0]]\n    \"\"\"\n    if binary_image.dtype != np.uint8:\n        binary_image = binary_image.astype(np.uint8)\n    num_labels, sh = cv2.connectedComponents(binary_image)\n    if num_labels &lt;= 1:\n        return binary_image.astype(np.uint8)\n    else:\n        return keep_largest_shape(sh)\n</code></pre>"},{"location":"api/cellects/image_analysis/morphological_operations/#cellects.image_analysis.morphological_operations.keep_shape_connected_with_ref","title":"<code>keep_shape_connected_with_ref(all_shapes, reference_shape)</code>","text":"<p>Keep shape connected with reference.</p> <p>This function analyzes the connected components of a binary image represented by <code>all_shapes</code> and returns the first component that intersects with the <code>reference_shape</code>. If no such component is found, it returns None.</p> <p>Parameters:</p> Name Type Description Default <code>all_shapes</code> <code>ndarray of uint8</code> <p>Binary image containing all shapes to analyze.</p> required <code>reference_shape</code> <code>ndarray of uint8</code> <p>Binary reference shape used for intersection check.</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>ndarray of uint8 or None</code> <p>The first connected component that intersects with the reference shape, or None if no such component is found.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; all_shapes = np.zeros((5, 5), dtype=np.uint8)\n&gt;&gt;&gt; reference_shape = np.zeros((5, 5), dtype=np.uint8)\n&gt;&gt;&gt; reference_shape[3, 3] = 1\n&gt;&gt;&gt; all_shapes[0:2, 0:2] = 1\n&gt;&gt;&gt; all_shapes[3:4, 3:4] = 1\n&gt;&gt;&gt; res = keep_shape_connected_with_ref(all_shapes, reference_shape)\n&gt;&gt;&gt; print(res)\n[[0 0 0 0 0]\n [0 0 0 0 0]\n [0 0 0 0 0]\n [0 0 0 1 0]\n [0 0 0 0 0]]\n</code></pre> Source code in <code>src/cellects/image_analysis/morphological_operations.py</code> <pre><code>def keep_shape_connected_with_ref(all_shapes: NDArray[np.uint8], reference_shape: NDArray[np.uint8]) -&gt; NDArray[np.uint8]:\n    \"\"\"\n    Keep shape connected with reference.\n\n    This function analyzes the connected components of a binary image represented by `all_shapes`\n    and returns the first component that intersects with the `reference_shape`.\n    If no such component is found, it returns None.\n\n    Parameters\n    ----------\n    all_shapes : ndarray of uint8\n        Binary image containing all shapes to analyze.\n    reference_shape : ndarray of uint8\n        Binary reference shape used for intersection check.\n\n    Returns\n    -------\n    out : ndarray of uint8 or None\n        The first connected component that intersects with the reference shape,\n        or None if no such component is found.\n\n    Examples\n    -------\n    &gt;&gt;&gt; all_shapes = np.zeros((5, 5), dtype=np.uint8)\n    &gt;&gt;&gt; reference_shape = np.zeros((5, 5), dtype=np.uint8)\n    &gt;&gt;&gt; reference_shape[3, 3] = 1\n    &gt;&gt;&gt; all_shapes[0:2, 0:2] = 1\n    &gt;&gt;&gt; all_shapes[3:4, 3:4] = 1\n    &gt;&gt;&gt; res = keep_shape_connected_with_ref(all_shapes, reference_shape)\n    &gt;&gt;&gt; print(res)\n    [[0 0 0 0 0]\n     [0 0 0 0 0]\n     [0 0 0 0 0]\n     [0 0 0 1 0]\n     [0 0 0 0 0]]\n    \"\"\"\n    number, order = cv2.connectedComponents(all_shapes, ltype=cv2.CV_16U)\n    expanded_shape = None\n    if number &gt; 1:\n        for i in np.arange(1, number):\n            expanded_shape_test = np.zeros(order.shape, np.uint8)\n            expanded_shape_test[order == i] = 1\n            if np.any(expanded_shape_test * reference_shape):\n                break\n        if np.any(expanded_shape_test * reference_shape):\n            expanded_shape = expanded_shape_test\n        else:\n            expanded_shape = reference_shape\n    return expanded_shape\n</code></pre>"},{"location":"api/cellects/image_analysis/morphological_operations/#cellects.image_analysis.morphological_operations.prepare_box_counting","title":"<code>prepare_box_counting(binary_image, min_im_side=128, min_mesh_side=8, zoom_step=0, contours=True)</code>","text":"<p>Prepare box counting parameters for image analysis.</p> <p>Prepares parameters for box counting method based on binary image input. Adjusts image size, computes side lengths, and applies contour extraction if specified.</p> <p>Parameters:</p> Name Type Description Default <code>binary_image</code> <code>ndarray of uint8</code> <p>Binary image for analysis.</p> required <code>min_im_side</code> <code>int</code> <p>Minimum side length threshold. Default is 128.</p> <code>128</code> <code>min_mesh_side</code> <code>int</code> <p>Minimum mesh side length. Default is 8.</p> <code>8</code> <code>zoom_step</code> <code>int</code> <p>Zoom step for side lengths computation. Default is 0.</p> <code>0</code> <code>contours</code> <code>bool</code> <p>Whether to apply contour extraction. Default is True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>out</code> <code>tuple of ndarray of uint8, ndarray (or None)</code> <p>Cropped binary image and computed side lengths.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; binary_image = np.zeros((10, 10), dtype=np.uint8)\n&gt;&gt;&gt; binary_image[2:4, 2:6] = 1\n&gt;&gt;&gt; binary_image[7:9, 4:7] = 1\n&gt;&gt;&gt; binary_image[4:7, 5] = 1\n&gt;&gt;&gt; cropped_img, side_lengths = prepare_box_counting(binary_image, min_im_side=2, min_mesh_side=2)\n&gt;&gt;&gt; print(cropped_img), print(side_lengths)\n[[0 0 0 0 0 0 0]\n [0 1 1 1 1 0 0]\n [0 1 1 1 1 0 0]\n [0 0 0 0 1 0 0]\n [0 0 0 0 1 0 0]\n [0 0 0 0 1 0 0]\n [0 0 0 1 0 1 0]\n [0 0 0 1 1 1 0]\n [0 0 0 0 0 0 0]]\n[4 2]\n</code></pre> Source code in <code>src/cellects/image_analysis/morphological_operations.py</code> <pre><code>def prepare_box_counting(binary_image: NDArray[np.uint8], min_im_side: int=128, min_mesh_side: int=8, zoom_step: int=0, contours: bool=True)-&gt; Tuple[NDArray[np.uint8], NDArray[np.uint8]]:\n    \"\"\"Prepare box counting parameters for image analysis.\n\n    Prepares parameters for box counting method based on binary\n    image input. Adjusts image size, computes side lengths, and applies\n    contour extraction if specified.\n\n    Parameters\n    ----------\n    binary_image : ndarray of uint8\n        Binary image for analysis.\n    min_im_side : int, optional\n        Minimum side length threshold. Default is 128.\n    min_mesh_side : int, optional\n        Minimum mesh side length. Default is 8.\n    zoom_step : int, optional\n        Zoom step for side lengths computation. Default is 0.\n    contours : bool, optional\n        Whether to apply contour extraction. Default is True.\n\n    Returns\n    -------\n    out : tuple of ndarray of uint8, ndarray (or None)\n        Cropped binary image and computed side lengths.\n\n    Examples\n    --------\n    &gt;&gt;&gt; binary_image = np.zeros((10, 10), dtype=np.uint8)\n    &gt;&gt;&gt; binary_image[2:4, 2:6] = 1\n    &gt;&gt;&gt; binary_image[7:9, 4:7] = 1\n    &gt;&gt;&gt; binary_image[4:7, 5] = 1\n    &gt;&gt;&gt; cropped_img, side_lengths = prepare_box_counting(binary_image, min_im_side=2, min_mesh_side=2)\n    &gt;&gt;&gt; print(cropped_img), print(side_lengths)\n    [[0 0 0 0 0 0 0]\n     [0 1 1 1 1 0 0]\n     [0 1 1 1 1 0 0]\n     [0 0 0 0 1 0 0]\n     [0 0 0 0 1 0 0]\n     [0 0 0 0 1 0 0]\n     [0 0 0 1 0 1 0]\n     [0 0 0 1 1 1 0]\n     [0 0 0 0 0 0 0]]\n    [4 2]\n    \"\"\"\n    side_lengths = None\n    zoomed_binary = binary_image\n    binary_idx = np.nonzero(binary_image)\n    if binary_idx[0].size:\n        min_y = np.min(binary_idx[0])\n        min_y = np.max((min_y - 1, 0))\n\n        min_x = np.min(binary_idx[1])\n        min_x = np.max((min_x - 1, 0))\n\n        max_y = np.max(binary_idx[0])\n        max_y = np.min((max_y + 1, binary_image.shape[0] - 1))\n\n        max_x = np.max(binary_idx[1])\n        max_x = np.min((max_x + 1, binary_image.shape[1] - 1))\n\n        zoomed_binary = deepcopy(binary_image[min_y:(max_y + 1), min_x: (max_x + 1)])\n        min_side = np.min(zoomed_binary.shape)\n        if min_side &gt;= min_im_side:\n            if contours:\n                zoomed_binary = get_contours(zoomed_binary)\n            if zoom_step == 0:\n                max_power = int(np.floor(np.log2(min_side)))  # Largest integer power of 2\n                side_lengths = 2 ** np.arange(max_power, int(np.log2(min_mesh_side // 2)), -1)\n            else:\n                side_lengths = np.arange(min_mesh_side, min_side, zoom_step)\n    return zoomed_binary, side_lengths\n</code></pre>"},{"location":"api/cellects/image_analysis/morphological_operations/#cellects.image_analysis.morphological_operations.rank_from_top_to_bottom_from_left_to_right","title":"<code>rank_from_top_to_bottom_from_left_to_right(binary_image, y_boundaries, get_ordered_image=False)</code>","text":"<p>Rank components in a binary image from top to bottom and from left to right.</p> <p>This function processes a binary image to rank its components based on their centroids. It first sorts the components row by row and then orders them within each row from left to right. If the ordering fails, it attempts an alternative algorithm and returns the ordered statistics and centroids.</p> <p>Parameters:</p> Name Type Description Default <code>binary_image</code> <code>ndarray of uint8</code> <p>The input binary image to process.</p> required <code>y_boundaries</code> <code>ndarray of int</code> <p>Boundary information for the y-coordinates.</p> required <code>get_ordered_image</code> <code>bool</code> <p>If True, returns an ordered image in addition to the statistics and centroids. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple</code> <p>If <code>get_ordered_image</code> is True, returns a tuple containing: - ordered_stats : ndarray of int     Statistics for the ordered components. - ordered_centroids : ndarray of float64     Centroids for the ordered components. - ordered_image : ndarray of uint8     The binary image with ordered component labels.</p> <p>If <code>get_ordered_image</code> is False, returns a tuple containing: - ordered_stats : ndarray of int     Statistics for the ordered components. - ordered_centroids : ndarray of float64     Centroids for the ordered components.</p> Source code in <code>src/cellects/image_analysis/morphological_operations.py</code> <pre><code>def rank_from_top_to_bottom_from_left_to_right(binary_image: NDArray[np.uint8], y_boundaries: NDArray[int], get_ordered_image: bool=False) -&gt; Tuple:\n    \"\"\"\n    Rank components in a binary image from top to bottom and from left to right.\n\n    This function processes a binary image to rank its components based on\n    their centroids. It first sorts the components row by row and then orders them\n    within each row from left to right. If the ordering fails, it attempts an alternative\n    algorithm and returns the ordered statistics and centroids.\n\n    Parameters\n    ----------\n    binary_image : ndarray of uint8\n        The input binary image to process.\n    y_boundaries : ndarray of int\n        Boundary information for the y-coordinates.\n    get_ordered_image : bool, optional\n        If True, returns an ordered image in addition to the statistics and centroids.\n        Default is False.\n\n    Returns\n    -------\n    tuple\n        If `get_ordered_image` is True, returns a tuple containing:\n        - ordered_stats : ndarray of int\n            Statistics for the ordered components.\n        - ordered_centroids : ndarray of float64\n            Centroids for the ordered components.\n        - ordered_image : ndarray of uint8\n            The binary image with ordered component labels.\n\n        If `get_ordered_image` is False, returns a tuple containing:\n        - ordered_stats : ndarray of int\n            Statistics for the ordered components.\n        - ordered_centroids : ndarray of float64\n            Centroids for the ordered components.\n    \"\"\"\n    nb_components, output, stats, centroids = cv2.connectedComponentsWithStats(binary_image.astype(np.uint8),\n                                                                               connectivity=8)\n\n    centroids = centroids[1:, :]\n    final_order = np.zeros(centroids.shape[0], dtype=np.uint8)\n    sorted_against_y = np.argsort(centroids[:, 1])\n    # row_nb = (y_boundaries == 1).sum()\n    row_nb = np.max(((y_boundaries == 1).sum(), (y_boundaries == - 1).sum()))\n    if row_nb &gt; 0:\n        component_per_row = int(np.ceil((nb_components - 1) / row_nb))\n        for row_i in range(row_nb):\n            row_i_start = row_i * component_per_row\n            if row_i == (row_nb - 1):\n                sorted_against_x = np.argsort(centroids[sorted_against_y[row_i_start:], 0])\n                final_order[row_i_start:] = sorted_against_y[row_i_start:][sorted_against_x]\n            else:\n                row_i_end = (row_i + 1) * component_per_row\n                sorted_against_x = np.argsort(centroids[sorted_against_y[row_i_start:row_i_end], 0])\n                final_order[row_i_start:row_i_end] = sorted_against_y[row_i_start:row_i_end][sorted_against_x]\n    else:\n        final_order = np.argsort(centroids[:, 0])\n    ordered_centroids = centroids[final_order, :]\n    ordered_stats = stats[1:, :]\n    ordered_stats = ordered_stats[final_order, :]\n\n    if get_ordered_image:\n        old_to_new = np.zeros_like(final_order)\n        old_to_new[final_order] = np.arange(len(final_order))\n        mapping_array = np.zeros(binary_image.shape, dtype=np.uint8)\n        mapping_array[output != 0] = old_to_new[output[output != 0] - 1] + 1\n        ordered_image = mapping_array.copy()\n        return ordered_stats, ordered_centroids, ordered_image\n    else:\n        return ordered_stats, ordered_centroids\n</code></pre>"},{"location":"api/cellects/image_analysis/morphological_operations/#cellects.image_analysis.morphological_operations.reduce_image_size_for_speed","title":"<code>reduce_image_size_for_speed(image_of_2_shapes)</code>","text":"<p>Reduces the size of an image containing two shapes for faster processing.</p> <p>The function iteratively divides the image into quadrants and keeps only those that contain both shapes until a minimal size is reached.</p> <p>Parameters:</p> Name Type Description Default <code>image_of_2_shapes</code> <code>ndarray of uint8</code> <p>The input image containing two shapes.</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>tuple of tuples</code> <p>The indices of the first and second shape in the reduced image.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; image_of_2_shapes = np.zeros((10, 10), dtype=np.uint8)\n&gt;&gt;&gt; image_of_2_shapes[1:3, 1:3] = 1\n&gt;&gt;&gt; image_of_2_shapes[1:3, 4:6] = 2\n&gt;&gt;&gt; shape1_idx, shape2_idx = reduce_image_size_for_speed(image_of_2_shapes)\n&gt;&gt;&gt; print(shape1_idx)\n(array([1, 1, 2, 2]), array([1, 2, 1, 2]))\n</code></pre> Source code in <code>src/cellects/image_analysis/morphological_operations.py</code> <pre><code>@njit()\ndef reduce_image_size_for_speed(image_of_2_shapes: NDArray[np.uint8]) -&gt; Tuple[Tuple, Tuple]:\n    \"\"\"\n    Reduces the size of an image containing two shapes for faster processing.\n\n    The function iteratively divides the image into quadrants and keeps only\n    those that contain both shapes until a minimal size is reached.\n\n    Parameters\n    ----------\n    image_of_2_shapes : ndarray of uint8\n        The input image containing two shapes.\n\n    Returns\n    -------\n    out : tuple of tuples\n        The indices of the first and second shape in the reduced image.\n\n    Examples\n    --------\n    &gt;&gt;&gt; image_of_2_shapes = np.zeros((10, 10), dtype=np.uint8)\n    &gt;&gt;&gt; image_of_2_shapes[1:3, 1:3] = 1\n    &gt;&gt;&gt; image_of_2_shapes[1:3, 4:6] = 2\n    &gt;&gt;&gt; shape1_idx, shape2_idx = reduce_image_size_for_speed(image_of_2_shapes)\n    &gt;&gt;&gt; print(shape1_idx)\n    (array([1, 1, 2, 2]), array([1, 2, 1, 2]))\n    \"\"\"\n    sub_image = image_of_2_shapes.copy()\n    y_size, x_size = sub_image.shape\n    images_list = [sub_image]\n    good_images = [0]\n    sub_image = images_list[good_images[0]]\n    while (len(good_images) == 1 or len(good_images) == 2) and y_size &gt; 3 and x_size &gt; 3:\n        y_size, x_size = sub_image.shape\n        images_list = []\n        images_list.append(sub_image[:((y_size // 2) + 1), :((x_size // 2) + 1)])\n        images_list.append(sub_image[:((y_size // 2) + 1), (x_size // 2):])\n        images_list.append(sub_image[(y_size // 2):, :((x_size // 2) + 1)])\n        images_list.append(sub_image[(y_size // 2):, (x_size // 2):])\n        good_images = []\n        for idx, image in enumerate(images_list):\n            if np.any(image == 2):\n                if np.any(image == 1):\n                    good_images.append(idx)\n        if len(good_images) == 0:\n            break\n        elif len(good_images) == 2:\n            if good_images == [0, 1]:\n                sub_image = np.concatenate((images_list[good_images[0]], images_list[good_images[1]]), axis=1)\n            elif good_images == [0, 2]:\n                sub_image = np.concatenate((images_list[good_images[0]], images_list[good_images[1]]), axis=0)\n            elif good_images == [1, 3]:\n                sub_image = np.concatenate((images_list[good_images[0]], images_list[good_images[1]]), axis=0)\n            elif good_images == [2, 3]:\n                sub_image = np.concatenate((images_list[good_images[0]], images_list[good_images[1]]), axis=1)\n            else:\n                pass\n        else:\n            sub_image = images_list[good_images[0]]\n\n    shape1_idx = np.nonzero(sub_image == 1)\n    shape2_idx = np.nonzero(sub_image == 2)\n    return shape1_idx, shape2_idx\n</code></pre>"},{"location":"api/cellects/image_analysis/morphological_operations/#cellects.image_analysis.morphological_operations.rounded_inverted_distance_transform","title":"<code>rounded_inverted_distance_transform(original_shape, max_distance=None, with_erosion=0)</code>","text":"<p>Perform rounded inverted distance transform on a binary image.</p> <p>This function computes the inverse of the Euclidean distance transform, where each pixel value represents its distance to the nearest zero pixel. The operation can include erosion and will stop either at a given max distance or until no further expansion is needed.</p> <p>Parameters:</p> Name Type Description Default <code>original_shape</code> <code>ndarray of uint8</code> <p>Input binary image to be processed.</p> required <code>max_distance</code> <code>int</code> <p>Maximum distance for the expansion. If None, no limit is applied.</p> <code>None</code> <code>with_erosion</code> <code>int</code> <p>Number of erosion iterations to apply before the transform. Default is 0.</p> <code>0</code> <p>Returns:</p> Name Type Description <code>out</code> <code>ndarray of uint32</code> <p>Output image containing the rounded inverted distance transform.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; segmentation = np.zeros((4, 4), dtype=np.uint8)\n&gt;&gt;&gt; segmentation[1:3, 1:3] = 1\n&gt;&gt;&gt; gravity = rounded_inverted_distance_transform(segmentation, max_distance=2)\n&gt;&gt;&gt; print(gravity)\n[[1 2 2 1]\n [2 0 0 2]\n [2 0 0 2]\n [1 2 2 1]]\n</code></pre> Source code in <code>src/cellects/image_analysis/morphological_operations.py</code> <pre><code>def rounded_inverted_distance_transform(original_shape: NDArray[np.uint8], max_distance: int=None, with_erosion: int=0) -&gt; NDArray[np.uint32]:\n    \"\"\"\n    Perform rounded inverted distance transform on a binary image.\n\n    This function computes the inverse of the Euclidean distance transform,\n    where each pixel value represents its distance to the nearest zero\n    pixel. The operation can include erosion and will stop either at a given\n    max distance or until no further expansion is needed.\n\n    Parameters\n    ----------\n    original_shape : ndarray of uint8\n        Input binary image to be processed.\n    max_distance : int, optional\n        Maximum distance for the expansion. If None, no limit is applied.\n    with_erosion : int, optional\n        Number of erosion iterations to apply before the transform. Default is 0.\n\n    Returns\n    -------\n    out : ndarray of uint32\n        Output image containing the rounded inverted distance transform.\n\n    Examples\n    --------\n    &gt;&gt;&gt; segmentation = np.zeros((4, 4), dtype=np.uint8)\n    &gt;&gt;&gt; segmentation[1:3, 1:3] = 1\n    &gt;&gt;&gt; gravity = rounded_inverted_distance_transform(segmentation, max_distance=2)\n    &gt;&gt;&gt; print(gravity)\n    [[1 2 2 1]\n     [2 0 0 2]\n     [2 0 0 2]\n     [1 2 2 1]]\n    \"\"\"\n    if with_erosion &gt; 0:\n        original_shape = cv2.erode(original_shape, cross_33, iterations=with_erosion, borderType=cv2.BORDER_CONSTANT, borderValue=0)\n    expand = deepcopy(original_shape)\n    if max_distance is not None:\n        if max_distance &gt; np.max(original_shape.shape):\n            max_distance = np.max(original_shape.shape).astype(np.uint32)\n        gravity_field = np.zeros(original_shape.shape , np.uint32)\n        for gravi in np.arange(max_distance):\n            expand = cv2.dilate(expand, cross_33, iterations=1, borderType=cv2.BORDER_CONSTANT, borderValue=0)\n            gravity_field[np.logical_xor(expand, original_shape)] += 1\n    else:\n        gravity_field = np.zeros(original_shape.shape , np.uint32)\n        while np.any(np.equal(original_shape + expand, 0)):\n            expand = cv2.dilate(expand, cross_33, iterations=1, borderType=cv2.BORDER_CONSTANT, borderValue=0)\n            gravity_field[np.logical_xor(expand, original_shape)] += 1\n    return gravity_field\n</code></pre>"},{"location":"api/cellects/image_analysis/morphological_operations/#cellects.image_analysis.morphological_operations.shape_selection","title":"<code>shape_selection(binary_image, several_blob_per_arena, true_shape_number=None, horizontal_size=None, spot_shape=None, bio_mask=None, back_mask=None)</code>","text":"<p>Process the binary image to identify and validate shapes.</p> <p>This method processes a binary image to detect connected components, validate their sizes, and handle bio and back masks if specified. It ensures that the number of validated shapes matches the expected sample number or applies additional filtering if necessary.</p> <p>Args:     use_bio_and_back_masks (bool): Whether to use bio and back masks         during the processing. Default is False.</p> <p>Selects and validates the shapes of stains based on their size and shape.</p> <p>This method performs two main tasks: 1. Removes stains whose horizontal size varies too much from a reference value. 2. Determines the shape of each remaining stain and only keeps those that correspond to a reference shape.</p> <p>The method first removes stains whose horizontal size is outside the specified confidence interval. Then, it identifies shapes that do not correspond to a predefined reference shape and removes them as well.</p> <p>Args:     horizontal_size (int): The expected horizontal size of the stains to use as a reference.     shape (str): The shape type ('circle' or 'rectangle')         that the stains should match. Other shapes are not currently supported.     confint (float): The confidence interval as a decimal         representing the percentage within which the size of the stains should fall.     do_not_delete (NDArray, optional): An array of stain indices that should not be deleted.         Default is None.</p> Source code in <code>src/cellects/image_analysis/morphological_operations.py</code> <pre><code>def shape_selection(binary_image:NDArray, several_blob_per_arena: bool, true_shape_number: int=None,\n                    horizontal_size: int=None, spot_shape: str=None, bio_mask:NDArray=None, back_mask:NDArray=None):\n    \"\"\"\n    Process the binary image to identify and validate shapes.\n\n    This method processes a binary image to detect connected components,\n    validate their sizes, and handle bio and back masks if specified.\n    It ensures that the number of validated shapes matches the expected\n    sample number or applies additional filtering if necessary.\n\n    Args:\n        use_bio_and_back_masks (bool): Whether to use bio and back masks\n            during the processing. Default is False.\n\n    Selects and validates the shapes of stains based on their size and shape.\n\n    This method performs two main tasks:\n    1. Removes stains whose horizontal size varies too much from a reference value.\n    2. Determines the shape of each remaining stain and only keeps those that correspond to a reference shape.\n\n    The method first removes stains whose horizontal size is outside the specified confidence interval. Then, it identifies shapes that do not correspond to a predefined reference shape and removes them as well.\n\n    Args:\n        horizontal_size (int): The expected horizontal size of the stains to use as a reference.\n        shape (str): The shape type ('circle' or 'rectangle')\n            that the stains should match. Other shapes are not currently supported.\n        confint (float): The confidence interval as a decimal\n            representing the percentage within which the size of the stains should fall.\n        do_not_delete (NDArray, optional): An array of stain indices that should not be deleted.\n            Default is None.\n\n    \"\"\"\n\n    shape_number, shapes, stats, centroids = cv2.connectedComponentsWithStats(binary_image, connectivity=8)\n    do_not_delete = None\n    if bio_mask is not None or back_mask is not None:\n        if back_mask is not None:\n            if np.any(shapes[back_mask]):\n                shapes[np.isin(shapes, np.unique(shapes[back_mask]))] = 0\n                shape_number, shapes, stats, centroids = cv2.connectedComponentsWithStats(\n                    (shapes &gt; 0).astype(np.uint8), connectivity=8)\n        if bio_mask is not None:\n            if np.any(shapes[bio_mask]):\n                do_not_delete = np.unique(shapes[bio_mask])\n                do_not_delete = do_not_delete[do_not_delete != 0]\n    shape_number -= 1\n\n    if not several_blob_per_arena and horizontal_size is not None:\n        ordered_shapes = shapes.copy()\n        if spot_shape is None:\n            c_spot_shapes = spot_shapes\n            c_spot_sizes = spot_sizes\n        else:\n            if spot_shape == 'circle':\n                c_spot_shapes = spot_shapes[::2]\n            else:\n                c_spot_shapes = spot_shapes[::-2]\n            c_spot_sizes = spot_sizes[::2]\n\n        # shape_number = stats.shape[0]\n        counter = 0\n        while shape_number != true_shape_number and counter &lt; len(spot_size_coefficients):\n            shape = c_spot_shapes[counter]\n            confint = c_spot_sizes[counter]\n            # counter+=1;horizontal_size = self.spot_size; shape = self.parent.spot_shapes[counter];confint = self.parent.spot_size_confints[::-1][counter]\n            # stats columns contain in that order:\n            # - x leftmost coordinate of boundingbox\n            # - y topmost coordinate of boundingbox\n            # - The horizontal size of the bounding box.\n            # - The vertical size of the bounding box.\n            # - The total area (in pixels) of the connected component.\n\n            # First, remove each stain which horizontal size varies too much from reference\n            size_interval = [horizontal_size * (1 - confint), horizontal_size * (1 + confint)]\n            cc_to_remove = np.argwhere(np.logical_or(stats[:, 2] &lt; size_interval[0], stats[:, 2] &gt; size_interval[1]))\n\n            if do_not_delete is None:\n                ordered_shapes[np.isin(ordered_shapes, cc_to_remove)] = 0\n            else:\n                ordered_shapes[np.logical_and(np.isin(ordered_shapes, cc_to_remove),\n                                              np.logical_not(np.isin(ordered_shapes, do_not_delete)))] = 0\n\n            # Second, determine the shape of each stain to only keep the ones corresponding to the reference shape\n            validated_shapes = np.zeros(ordered_shapes.shape, dtype=np.uint8)\n            validated_shapes[ordered_shapes &gt; 0] = 1\n            nb_components, ordered_shapes, stats, centroids = cv2.connectedComponentsWithStats(validated_shapes,\n                                                                                               connectivity=8)\n            if nb_components &gt; 1:\n                if shape == 'circle':\n                    surf_interval = [np.pi * np.square(horizontal_size // 2) * (1 - confint),\n                                     np.pi * np.square(horizontal_size // 2) * (1 + confint)]\n                    cc_to_remove = np.argwhere(\n                        np.logical_or(stats[:, 4] &lt; surf_interval[0], stats[:, 4] &gt; surf_interval[1]))\n                elif shape == 'rectangle':\n                    # If the smaller side is the horizontal one, use the user provided horizontal side\n                    if np.argmin((np.mean(stats[1:, 2]), np.mean(stats[1:, 3]))) == 0:\n                        surf_interval = [np.square(horizontal_size) * (1 - confint),\n                                         np.square(horizontal_size) * (1 + confint)]\n                        cc_to_remove = np.argwhere(\n                            np.logical_or(stats[:, 4] &lt; surf_interval[0], stats[:, 4] &gt; surf_interval[1]))\n                    # If the smaller side is the vertical one, use the median vertical length shape\n                    else:\n                        surf_interval = [np.square(np.median(stats[1:, 3])) * (1 - confint),\n                                         np.square(np.median(stats[1:, 3])) * (1 + confint)]\n                        cc_to_remove = np.argwhere(\n                            np.logical_or(stats[:, 4] &lt; surf_interval[0], stats[:, 4] &gt; surf_interval[1]))\n                else:\n                    logging.info(\"Original blob shape not well written\")\n\n                if do_not_delete is None:\n                    ordered_shapes[np.isin(ordered_shapes, cc_to_remove)] = 0\n                else:\n                    ordered_shapes[np.logical_and(np.isin(ordered_shapes, cc_to_remove),\n                                                  np.logical_not(np.isin(ordered_shapes, do_not_delete)))] = 0\n                # There was only that before:\n                validated_shapes = np.zeros(ordered_shapes.shape, dtype=np.uint8)\n                validated_shapes[np.nonzero(ordered_shapes)] = 1\n                nb_components, ordered_shapes, stats, centroids = cv2.connectedComponentsWithStats(validated_shapes,\n                                                                                                   connectivity=8)\n\n            shape_number = nb_components - 1\n            counter += 1\n\n        if shape_number == true_shape_number:\n            shapes = ordered_shapes\n    if true_shape_number is None or shape_number == true_shape_number:\n        validated_shapes = np.zeros(shapes.shape, dtype=np.uint8)\n        validated_shapes[shapes &gt; 0] = 1\n    else:\n        max_size = binary_image.size * 0.75\n        min_size = 10\n        cc_to_remove = np.argwhere(np.logical_or(stats[1:, 4] &lt; min_size, stats[1:, 4] &gt; max_size)) + 1\n        shapes[np.isin(shapes, cc_to_remove)] = 0\n        validated_shapes = np.zeros(shapes.shape, dtype=np.uint8)\n        validated_shapes[shapes &gt; 0] = 1\n        shape_number, shapes, stats, centroids = cv2.connectedComponentsWithStats(validated_shapes, connectivity=8)\n        if not several_blob_per_arena and true_shape_number is not None and shape_number &gt; true_shape_number:\n            # Sort shapes by size and compare the largest with the second largest\n            # If the difference is too large, remove that largest shape.\n            cc_to_remove = np.array([], dtype=np.uint8)\n            to_remove = np.array([], dtype=np.uint8)\n            stats = stats[1:, :]\n            while stats.shape[0] &gt; true_shape_number and to_remove is not None:\n                # 1) rank by height\n                sorted_height = np.argsort(stats[:, 2])\n                # and only consider the number of shapes we want to detect\n                standard_error = np.std(stats[sorted_height, 2][-true_shape_number:])\n                differences = np.diff(stats[sorted_height, 2])\n                # Look for very big changes from one height to the next\n                if differences.any() and np.max(differences) &gt; 2 * standard_error:\n                    # Within these, remove shapes that are too large\n                    to_remove = sorted_height[np.argmax(differences)]\n                    cc_to_remove = np.append(cc_to_remove, to_remove + 1)\n                    stats = np.delete(stats, to_remove, 0)\n\n                else:\n                    to_remove = None\n            shapes[np.isin(shapes, cc_to_remove)] = 0\n            validated_shapes = np.zeros(shapes.shape, dtype=np.uint8)\n            validated_shapes[shapes &gt; 0] = 1\n            shape_number, shapes, stats, centroids = cv2.connectedComponentsWithStats(validated_shapes, connectivity=8)\n\n        shape_number -= 1\n    return validated_shapes, shape_number, stats, centroids\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/","title":"<code>cellects.image_analysis.network_functions</code>","text":""},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions","title":"<code>cellects.image_analysis.network_functions</code>","text":"<p>Network detection and skeleton analysis for biological networks (such as Physarum polycephalum's) images.</p> <p>This module provides tools for analyzing network structures in grayscale images of biological networks. It implements vessel detection using Frangi/Sato filters, thresholding methods, and quality metrics to select optimal network representations. Additional functionality includes pseudopod detection, skeletonization, loop removal, edge identification, and network topology analysis through vertex/edge tracking.</p> <p>Classes:</p> Name Description <code>NetworkDetection : Detects vessels in images using multi-scale filters with parameter variations.</code> <code>EdgeIdentification : Identifies edges between vertices in a skeletonized network structure.</code> <p>Functions:</p> Name Description <code>get_skeleton_and_widths: Computes medial axis skeleton and distance transforms for networks.</code> <code>remove_small_loops: Eliminates small loops from skeletons while preserving topology.</code> <code>get_neighbor_comparisons: Analyzes pixel connectivity patterns in skeletons.</code> <code>get_vertices_and_tips_from_skeleton: Identifies junctions and endpoints in network skeletons.</code> <code>merge_network_with_pseudopods: Combines detected network structures with identified pseudopods.</code> Notes <p>Uses morphological operations for network refinement, including hole closing, component labeling, and distance transform analysis. Implements both Otsu thresholding and rolling window segmentation methods for image processing workflows.</p>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.EdgeIdentification","title":"<code>EdgeIdentification</code>","text":"<p>Initialize the class with skeleton and distance arrays.</p> <p>This class is used to identify edges within a skeleton structure based on provided skeleton and distance arrays. It performs various operations to refine and label edges, ultimately producing a fully identified network.</p> Source code in <code>src/cellects/image_analysis/network_functions.py</code> <pre><code>class EdgeIdentification:\n    \"\"\"Initialize the class with skeleton and distance arrays.\n\n    This class is used to identify edges within a skeleton structure based on\n    provided skeleton and distance arrays. It performs various operations to\n    refine and label edges, ultimately producing a fully identified network.\n    \"\"\"\n    def __init__(self, pad_skeleton: NDArray[np.uint8], pad_distances: NDArray[np.float64], t: int=0):\n        \"\"\"\n        Initialize the class with skeleton and distance arrays.\n\n        Parameters\n        ----------\n        pad_skeleton : ndarray of uint8\n            Array representing the skeleton to pad.\n        pad_distances : ndarray of float64\n            Array representing distances corresponding to the skeleton.\n\n        Attributes\n        ----------\n        remaining_vertices : None\n            Remaining vertices. Initialized as `None`.\n        vertices : None\n            Vertices. Initialized as `None`.\n        growing_vertices : None\n            Growing vertices. Initialized as `None`.\n        im_shape : tuple of ints\n            Shape of the skeleton array.\n        \"\"\"\n        self.pad_skeleton = pad_skeleton\n        self.pad_distances = pad_distances\n        self.t = t\n        self.remaining_vertices = None\n        self.vertices = None\n        self.growing_vertices = None\n        self.im_shape = pad_skeleton.shape\n\n    def run_edge_identification(self):\n        \"\"\"\n        Run the edge identification process.\n\n        This method orchestrates a series of steps to identify and label edges\n        within the graph structure. Each step handles a specific aspect of edge\n        identification, ultimately leading to a clearer and more refined edge network.\n\n        Steps involved:\n        1. Get vertices and tips coordinates.\n        2. Identify tipped edges.\n        3. Remove tipped edges smaller than branch width.\n        4. Label tipped edges and their vertices.\n        5. Label edges connected with vertex clusters.\n        6. Label edges connecting vertex clusters.\n        7. Label edges from known vertices iteratively.\n        8. Label edges looping on 1 vertex.\n        9. Clear areas with 1 or 2 unidentified pixels.\n        10. Clear edge duplicates.\n        11. Clear vertices connecting 2 edges.\n        \"\"\"\n        self.get_vertices_and_tips_coord()\n        self.get_tipped_edges()\n        self.remove_tipped_edge_smaller_than_branch_width()\n        self.label_tipped_edges_and_their_vertices()\n        self.check_vertex_existence()\n        self.label_edges_connected_with_vertex_clusters()\n        self.label_edges_connecting_vertex_clusters()\n        self.label_edges_from_known_vertices_iteratively()\n        self.label_edges_looping_on_1_vertex()\n        self.clear_areas_of_1_or_2_unidentified_pixels()\n        self.clear_edge_duplicates()\n        self.clear_vertices_connecting_2_edges()\n\n    def get_vertices_and_tips_coord(self):\n        \"\"\"Process skeleton data to extract non-tip vertices and tip coordinates.\n\n        This method processes the skeleton stored in `self.pad_skeleton` by first\n        extracting all vertices and tips. It then separates these into branch points\n        (non-tip vertices) and specific tip coordinates using internal processing.\n\n        Attributes\n        ----------\n        self.non_tip_vertices : array-like\n            Coordinates of non-tip (branch) vertices.\n        self.tips_coord : array-like\n            Coordinates of identified tips in the skeleton.\n        \"\"\"\n        pad_vertices, pad_tips = get_vertices_and_tips_from_skeleton(self.pad_skeleton)\n        self.non_tip_vertices, self.tips_coord = get_branches_and_tips_coord(pad_vertices, pad_tips)\n\n    def get_tipped_edges(self):\n        \"\"\"\n        get_tipped_edges : method to extract skeleton edges connecting branching points and tips.\n\n        Makes sure that there is only one connected component constituting the skeleton of the network and\n        identifies all edges that are connected to a tip.\n\n        Attributes\n        ----------\n        pad_skeleton : ndarray of bool, modified\n            Boolean mask representing the pruned skeleton after isolating the largest connected component.\n        vertices_branching_tips : ndarray of int, shape (N, 2)\n            Coordinates of branching points that connect to tips in the skeleton structure.\n        edge_lengths : ndarray of float, shape (M,)\n            Lengths of edges connecting non-tip vertices to identified tip locations.\n        edge_pix_coord : list of array of int\n            Pixel coordinates for each edge path between connected skeleton elements.\n\n        \"\"\"\n        self.pad_skeleton = keep_one_connected_component(self.pad_skeleton)\n        self.vertices_branching_tips, self.edge_lengths, self.edge_pix_coord = _find_closest_vertices(self.pad_skeleton,\n                                                                                        self.non_tip_vertices,\n                                                                                        self.tips_coord[:, :2])\n\n    def remove_tipped_edge_smaller_than_branch_width(self):\n        \"\"\"Remove very short edges from the skeleton.\n\n        This method focuses on edges connecting tips. When too short, they are considered are noise and\n        removed from the skeleton and distances matrices. These edges are considered too short when their length is\n        smaller than the width of the nearest network branch (an information included in pad_distances).\n        This method also updates internal data structures (skeleton, edge coordinates, vertex/tip positions)\n        accordingly through pixel-wise analysis and connectivity checks.\n        \"\"\"\n        # Identify edges that are smaller than the width of the branch it is attached to\n        tipped_edges_to_remove = np.zeros(self.edge_lengths.shape[0], dtype=bool)\n        # connecting_vertices_to_remove = np.zeros(self.vertices_branching_tips.shape[0], dtype=bool)\n        branches_to_remove = np.zeros(self.non_tip_vertices.shape[0], dtype=bool)\n        new_edge_pix_coord = []\n        remaining_tipped_edges_nb = 0\n        for i in range(len(self.edge_lengths)): # i = 3142 #1096 # 974 # 222\n            Y, X = self.vertices_branching_tips[i, 0], self.vertices_branching_tips[i, 1]\n            edge_bool = self.edge_pix_coord[:, 2] == i + 1\n            eY, eX = self.edge_pix_coord[edge_bool, 0], self.edge_pix_coord[edge_bool, 1]\n            if np.nanmax(self.pad_distances[(Y - 1): (Y + 2), (X - 1): (X + 2)]) &gt;= self.edge_lengths[i]:\n                tipped_edges_to_remove[i] = True\n                # Remove the edge\n                self.pad_skeleton[eY, eX] = 0\n                # Remove the tip\n                self.pad_skeleton[self.tips_coord[i, 0], self.tips_coord[i, 1]] = 0\n\n                # Remove the coordinates corresponding to that edge\n                self.edge_pix_coord = np.delete(self.edge_pix_coord, edge_bool, 0)\n\n                # check whether the connecting vertex remains a vertex of not\n                pad_sub_skeleton = np.pad(self.pad_skeleton[(Y - 2): (Y + 3), (X - 2): (X + 3)], [(1,), (1,)],\n                                          mode='constant')\n                sub_vertices, sub_tips = get_vertices_and_tips_from_skeleton(pad_sub_skeleton)\n                # If the vertex does not connect at least 3 edges anymore, remove its vertex label\n                if sub_vertices[3, 3] == 0:\n                    vertex_to_remove = np.nonzero(np.logical_and(self.non_tip_vertices[:, 0] == Y, self.non_tip_vertices[:, 1] == X))[0]\n                    branches_to_remove[vertex_to_remove] = True\n                # If that pixel became a tip connected to another vertex remove it from the skeleton\n                if sub_tips[3, 3]:\n                    if sub_vertices[2:5, 2:5].sum() &gt; 1:\n                        self.pad_skeleton[Y, X] = 0\n                        self.edge_pix_coord = np.delete(self.edge_pix_coord, np.all(self.edge_pix_coord[:, :2] == [Y, X], axis=1), 0)\n                        vertex_to_remove = np.nonzero(np.logical_and(self.non_tip_vertices[:, 0] == Y, self.non_tip_vertices[:, 1] == X))[0]\n                        branches_to_remove[vertex_to_remove] = True\n            else:\n                remaining_tipped_edges_nb += 1\n                new_edge_pix_coord.append(np.stack((eY, eX, np.repeat(remaining_tipped_edges_nb, len(eY))), axis=1))\n\n        # Check that excedent connected components are 1 pixel size, if so:\n        # It means that they were neighbors to removed tips and not necessary for the skeleton\n        nb, sh = cv2.connectedComponents(self.pad_skeleton)\n        if nb &gt; 2:\n            logging.error(\"Removing small tipped edges split the skeleton\")\n            # for i in range(2, nb):\n            #     excedent = sh == i\n            #     if (excedent).sum() == 1:\n            #         self.pad_skeleton[excedent] = 0\n\n        # Remove in distances the pixels removed in skeleton:\n        self.pad_distances *= self.pad_skeleton\n\n        # update edge_pix_coord\n        if len(new_edge_pix_coord) &gt; 0:\n            self.edge_pix_coord = np.vstack(new_edge_pix_coord)\n\n        # # Remove tips connected to very small edges\n        # self.tips_coord = np.delete(self.tips_coord, tipped_edges_to_remove, 0)\n        # # Add corresponding edge names\n        # self.tips_coord = np.hstack((self.tips_coord, np.arange(1, len(self.tips_coord) + 1)[:, None]))\n\n        # # Within all branching (non-tip) vertices, keep those that did not lose their vertex status because of the edge removal\n        # self.non_tip_vertices = np.delete(self.non_tip_vertices, branches_to_remove, 0)\n\n        # # Get the branching vertices who kept their typped edge\n        # self.vertices_branching_tips = np.delete(self.vertices_branching_tips, tipped_edges_to_remove, 0)\n\n        # Within all branching (non-tip) vertices, keep those that do not connect a tipped edge.\n        # v_branching_tips_in_branching_v = find_common_coord(self.non_tip_vertices, self.vertices_branching_tips[:, :2])\n        # self.remaining_vertices = np.delete(self.non_tip_vertices, v_branching_tips_in_branching_v, 0)\n        # ordered_v_coord = np.vstack((self.tips_coord[:, :2], self.vertices_branching_tips[:, :2], self.remaining_vertices))\n\n        # tips = self.tips_coord\n        # branching_any_edge = self.non_tip_vertices\n        # branching_typped_edges = self.vertices_branching_tips\n        # branching_no_typped_edges = self.remaining_vertices\n\n        self.get_vertices_and_tips_coord()\n        self.get_tipped_edges()\n\n    def label_tipped_edges_and_their_vertices(self):\n        \"\"\"Label edges connecting tip vertices to branching vertices and assign unique labels to all relevant vertices.\n\n        Processes vertex coordinates by stacking tips, vertices branching from tips, and remaining non-tip vertices.\n        Assigns unique sequential identifiers to these vertices in a new array. Constructs an array of edge-label information,\n        where each row contains the edge label (starting at 1), corresponding tip label, and connected vertex label.\n\n        Attributes\n        ----------\n        tip_number : int\n            The number of tip coordinates available in `tips_coord`.\n\n        ordered_v_coord : ndarray of float\n            Stack of unique vertex coordinates ordered by: tips first, vertices branching tips second, non-tip vertices third.\n\n        numbered_vertices : ndarray of uint32\n            2D array where each coordinate position is labeled with a sequential integer (starting at 1) based on the order in `ordered_v_coord`.\n\n        edges_labels : ndarray of uint32\n            Array of shape (n_edges, 3). Each row contains:\n            - Edge label (sequential from 1 to n_edges)\n            - Label of the tip vertex for that edge.\n            - Label of the vertex branching the tip.\n\n        vertices_branching_tips : ndarray of float\n            Unique coordinates of vertices directly connected to tips after removing duplicates.\n        \"\"\"\n        self.tip_number = self.tips_coord.shape[0]\n\n        # Stack vertex coordinates in that order: 1. Tips, 2. Vertices branching tips, 3. All remaining vertices\n        ordered_v_coord = np.vstack((self.tips_coord[:, :2], self.vertices_branching_tips[:, :2], self.non_tip_vertices))\n        ordered_v_coord = np.unique(ordered_v_coord, axis=0)\n\n        # Create arrays to store edges and vertices labels\n        self.numbered_vertices = np.zeros(self.im_shape, dtype=np.uint32)\n        self.numbered_vertices[ordered_v_coord[:, 0], ordered_v_coord[:, 1]] = np.arange(1, ordered_v_coord.shape[0] + 1)\n        self.vertices = None\n        self.vertex_index_map = {}\n        for idx, (y, x) in enumerate(ordered_v_coord):\n            self.vertex_index_map[idx + 1] = tuple((np.uint32(y), np.uint32(x)))\n\n        # Name edges from 1 to the number of edges connecting tips and set the vertices labels from all tips to their connected vertices:\n        self.edges_labels = np.zeros((self.tip_number, 3), dtype=np.uint32)\n        # edge label:\n        self.edges_labels[:, 0] = np.arange(self.tip_number) + 1\n        # tip label:\n        self.edges_labels[:, 1] = self.numbered_vertices[self.tips_coord[:, 0], self.tips_coord[:, 1]]\n        # vertex branching tip label:\n        self.edges_labels[:, 2] = self.numbered_vertices[self.vertices_branching_tips[:, 0], self.vertices_branching_tips[:, 1]]\n\n        # Remove duplicates in vertices_branching_tips\n        self.vertices_branching_tips = np.unique(self.vertices_branching_tips[:, :2], axis=0)\n\n    def check_vertex_existence(self):\n        if self.tips_coord.shape[0] == 0 and self.non_tip_vertices.shape[0] == 0:\n            loop_coord = np.nonzero(self.pad_skeleton)\n            start = 1\n            end = 1\n            vertex_coord = loop_coord[0][0], loop_coord[1][0]\n            self.numbered_vertices[vertex_coord[0], vertex_coord[1]] = 1\n            self.vertex_index_map[1] = vertex_coord\n            self.non_tip_vertices = np.array(vertex_coord)[None, :]\n            new_edge_lengths = len(loop_coord[0]) - 1\n            new_edge_pix_coord = np.transpose(np.vstack(((loop_coord[0][1:], loop_coord[1][1:], np.zeros(new_edge_lengths, dtype=np.int32)))))\n            self.edge_pix_coord = np.zeros((0, 3), dtype=np.int32)\n            self._update_edge_data(start, end, new_edge_lengths, new_edge_pix_coord)\n\n    def label_edges_connected_with_vertex_clusters(self):\n        \"\"\"\n        Identify edges connected to touching vertices by processing vertex clusters.\n\n        This function processes the skeleton to identify edges connecting vertices\n        that are part of touching clusters. It creates a cropped version of the skeleton\n        by removing already detected edges and their tips, then iterates through vertex\n        clusters to explore and identify nearby edges.\n        \"\"\"\n        # I.1. Identify edges connected to touching vertices:\n        # First, create another version of these arrays, where we remove every already detected edge and their tips\n        cropped_skeleton = self.pad_skeleton.copy()\n        cropped_skeleton[self.edge_pix_coord[:, 0], self.edge_pix_coord[:, 1]] = 0\n        cropped_skeleton[self.tips_coord[:, 0], self.tips_coord[:, 1]] = 0\n\n        # non_tip_vertices does not need to be updated yet, because it only contains verified branching vertices\n        cropped_non_tip_vertices = self.non_tip_vertices.copy()\n\n        self.new_level_vertices = None\n        # The problem with vertex_to_vertex_connexion is that since they are not separated by zeros,\n        # they always atract each other instead of exploring other paths.\n        # To fix this, we loop over each vertex group to\n        # 1. Add one edge per inter-vertex connexion inside the group\n        # 2. Remove all except one, and loop as many time as necessary.\n        # Inside that second loop, we explore and identify every edge nearby.\n        # Find every vertex_to_vertex_connexion\n        v_cluster_nb, self.v_cluster_lab, self.v_cluster_stats, vgc = cv2.connectedComponentsWithStats(\n            (self.numbered_vertices &gt; 0).astype(np.uint8), connectivity=8)\n        if v_cluster_nb &gt; 0:\n            max_v_nb = np.max(self.v_cluster_stats[1:, 4])\n            cropped_skeleton_list = []\n            starting_vertices_list = []\n            for v_nb in range(2, max_v_nb + 1):\n                labels = np.nonzero(self.v_cluster_stats[:, 4] == v_nb)[0]\n                coord_list = []\n                for lab in labels:  # lab=labels[0]\n                    coord_list.append(np.nonzero(self.v_cluster_lab == lab))\n                for iter in range(v_nb):\n                    for lab_ in range(labels.shape[0]): # lab=labels[0]\n                        cs = cropped_skeleton.copy()\n                        sv = []\n                        v_c = coord_list[lab_]\n                        # Save the current coordinate in the starting vertices array of this iteration\n                        sv.append([v_c[0][iter], v_c[1][iter]])\n                        # Remove one vertex coordinate to keep it from cs\n                        v_y, v_x = np.delete(v_c[0], iter), np.delete(v_c[1], iter)\n                        cs[v_y, v_x] = 0\n                        cropped_skeleton_list.append(cs)\n                        starting_vertices_list.append(np.array(sv))\n            for cropped_skeleton, starting_vertices in zip(cropped_skeleton_list, starting_vertices_list):\n                _, _ = self._identify_edges_connecting_a_vertex_list(cropped_skeleton, cropped_non_tip_vertices, starting_vertices)\n\n    def label_edges_connecting_vertex_clusters(self):\n        \"\"\"\n        Label edges connecting vertex clusters.\n\n        This method identifies the connections between connected vertices within\n        vertex clusters and labels these edges. It uses the previously found connected\n        vertices, creates an image of the connections, and then identifies\n        and labels the edges between these touching vertices.\n        \"\"\"\n        # I.2. Identify the connexions between connected vertices:\n        all_connected_vertices = np.nonzero(self.v_cluster_stats[:, 4] &gt; 1)[0][1:]\n        all_con_v_im = np.zeros_like(self.pad_skeleton)\n        for v_group in all_connected_vertices:\n            all_con_v_im[self.v_cluster_lab == v_group] = 1\n        cropped_skeleton = all_con_v_im\n        self.vertex_clusters_coord = np.transpose(np.array(np.nonzero(cropped_skeleton)))\n        _, _ = self._identify_edges_connecting_a_vertex_list(cropped_skeleton, self.vertex_clusters_coord, self.vertex_clusters_coord)\n        # self.edges_labels\n        del self.v_cluster_stats\n        del self.v_cluster_lab\n\n    def label_edges_from_known_vertices_iteratively(self):\n        \"\"\"\n        Label edges iteratively from known vertices.\n\n        This method labels edges in an iterative process starting from\n        known vertices. It handles the removal of detected edges and\n        updates the skeleton accordingly, to avoid detecting edges twice.\n        \"\"\"\n        # II/ Identify all remaining edges\n        if self.new_level_vertices is not None:\n            starting_vertices_coord = np.vstack((self.new_level_vertices[:, :2], self.vertices_branching_tips))\n            starting_vertices_coord = np.unique(starting_vertices_coord, axis=0)\n        else:\n            # We start from the vertices connecting tips\n            starting_vertices_coord = self.vertices_branching_tips.copy()\n        # Remove the detected edges from cropped_skeleton:\n        cropped_skeleton = self.pad_skeleton.copy()\n        cropped_skeleton[self.edge_pix_coord[:, 0], self.edge_pix_coord[:, 1]] = 0\n        cropped_skeleton[self.tips_coord[:, 0], self.tips_coord[:, 1]] = 0\n        cropped_skeleton[self.vertex_clusters_coord[:, 0], self.vertex_clusters_coord[:, 1]] = 0\n\n        # Reinitialize cropped_non_tip_vertices to browse all vertices except tips and groups\n        cropped_non_tip_vertices = self.non_tip_vertices.copy()\n        cropped_non_tip_vertices = remove_coordinates(cropped_non_tip_vertices, self.vertex_clusters_coord)\n        del self.vertex_clusters_coord\n        remaining_v = cropped_non_tip_vertices.shape[0] + 1\n        while remaining_v &gt; cropped_non_tip_vertices.shape[0]:\n            remaining_v = cropped_non_tip_vertices.shape[0]\n            cropped_skeleton, cropped_non_tip_vertices = self._identify_edges_connecting_a_vertex_list(cropped_skeleton, cropped_non_tip_vertices, starting_vertices_coord)\n            if self.new_level_vertices is not None:\n                starting_vertices_coord = np.unique(self.new_level_vertices[:, :2], axis=0)\n\n    def label_edges_looping_on_1_vertex(self):\n        \"\"\"\n        Identify and handle edges that form loops around a single vertex.\n        This method processes the skeleton image to find looping edges and updates\n        the edge data structure accordingly.\n        \"\"\"\n        self.identified = np.zeros_like(self.numbered_vertices)\n        self.identified[self.edge_pix_coord[:, 0], self.edge_pix_coord[:, 1]] = 1\n        self.identified[self.non_tip_vertices[:, 0], self.non_tip_vertices[:, 1]] = 1\n        self.identified[self.tips_coord[:, 0], self.tips_coord[:, 1]] = 1\n        unidentified = (1 - self.identified) * self.pad_skeleton\n\n        # Find out the remaining non-identified pixels\n        nb, self.unidentified_shapes, self.unidentified_stats, ce = cv2.connectedComponentsWithStats(unidentified.astype(np.uint8))\n        # Handle the cases where edges are loops over only one vertex\n        looping_edges = np.nonzero(self.unidentified_stats[:, 4 ] &gt; 2)[0][1:]\n        for loop_i in looping_edges: # loop_i = looping_edges[0] loop_i=11 #  zoom_on_nonzero(unique_vertices_im, return_coord=False)\n            edge_i = (self.unidentified_shapes == loop_i).astype(np.uint8)\n            dil_edge_i = cv2.dilate(edge_i, square_33)\n            unique_vertices_im = self.numbered_vertices.copy()\n            unique_vertices_im[self.tips_coord[:, 0], self.tips_coord[:, 1]] = 0\n            unique_vertices_im = dil_edge_i * unique_vertices_im\n            unique_vertices = np.unique(unique_vertices_im)\n            unique_vertices = unique_vertices[unique_vertices &gt; 0]\n            v_nb = len(unique_vertices)\n            new_edge_lengths = edge_i.sum()\n            new_edge_pix_coord = np.transpose(np.vstack((np.nonzero(edge_i))))\n            new_edge_pix_coord = np.hstack((new_edge_pix_coord, np.repeat(1, new_edge_pix_coord.shape[0])[:, None]))\n            if v_nb == 1:\n                start, end = unique_vertices[0], unique_vertices[0]\n                self._update_edge_data(start, end, new_edge_lengths, new_edge_pix_coord)\n            elif v_nb == 2:\n                # The edge loops around a group of connected vertices\n                start, end = unique_vertices[0], unique_vertices[1]\n                self._update_edge_data(start, end, new_edge_lengths, new_edge_pix_coord)\n                # conn_v_nb, conn_v = cv2.connectedComponents((unique_vertices_im &gt; 0).astype(np.uint8))\n                # if len(unique_vertices) == 2 and conn_v_nb == 2:\n            elif v_nb &gt; 2: # The question is: How to choose two vertices so that they link all missing pixels?\n                # 1. Find every edge pixel connected to these vertices\n                vertex_connected_pixels = np.nonzero(cv2.dilate((unique_vertices_im &gt; 0).astype(np.uint8), square_33) * edge_i)\n                # 2. Find the most distant pair of these\n                pix1, pix2 = get_min_or_max_euclidean_pair(vertex_connected_pixels, \"max\")\n                # 3. The two best vertices are the two nearest to these two most distant edge pixels\n                dist_to_pix1 = np.zeros(v_nb, np.float64)\n                dist_to_pix2 = np.zeros(v_nb, np.float64)\n                for _i, v_i in enumerate(unique_vertices):\n                    v_coord = self.vertex_index_map[v_i]\n                    dist_to_pix1[_i] = eudist(pix1, v_coord)\n                    dist_to_pix2[_i] = eudist(pix2, v_coord)\n                start, end = unique_vertices[np.argmin(dist_to_pix1)], unique_vertices[np.argmin(dist_to_pix2)]\n                self._update_edge_data(start, end, new_edge_lengths, new_edge_pix_coord)\n            else:\n                logging.error(f\"t={self.t}, One long edge is not identified: i={loop_i} of length={edge_i.sum()} close to {len(unique_vertices)} vertices.\")\n        self.identified[self.edge_pix_coord[:, 0], self.edge_pix_coord[:, 1]] = 1\n\n    def clear_areas_of_1_or_2_unidentified_pixels(self):\n        \"\"\"Removes 1 or 2 pixel size non-identified areas from the skeleton.\n\n        This function checks whether small non-identified areas (1 or 2 pixels)\n        can be removed without breaking the skeleton structure. It performs\n        a series of operations to ensure only safe removals are made, logging\n        errors if the final skeleton is not fully connected or if some unidentified pixels remain.\n        \"\"\"\n        # Check whether the 1 or 2 pixel size non-identified areas can be removed without breaking the skel\n        one_pix = np.nonzero(self.unidentified_stats[:, 4 ] &lt;= 2)[0] # == 1)[0]\n        cutting_removal = []\n        for pix_i in one_pix: #pix_i=one_pix[0]\n            skel_copy = self.pad_skeleton.copy()\n            y1, y2, x1, x2 = self.unidentified_stats[pix_i, 1], self.unidentified_stats[pix_i, 1] + self.unidentified_stats[pix_i, 3], self.unidentified_stats[pix_i, 0], self.unidentified_stats[pix_i, 0] + self.unidentified_stats[pix_i, 2]\n            skel_copy[y1:y2, x1:x2][self.unidentified_shapes[y1:y2, x1:x2] == pix_i] = 0\n            nb1, sh1 = cv2.connectedComponents(skel_copy.astype(np.uint8), connectivity=8)\n            if nb1 &gt; 2:\n                cutting_removal.append(pix_i)\n            else:\n                self.pad_skeleton[y1:y2, x1:x2][self.unidentified_shapes[y1:y2, x1:x2] == pix_i] = 0\n        if len(cutting_removal) &gt; 0:\n            logging.error(f\"t={self.t}, These pixels break the skeleton when removed: {cutting_removal}\")\n        if (self.identified &gt; 0).sum() != self.pad_skeleton.sum():\n            logging.error(f\"t={self.t}, Proportion of identified pixels in the skeleton: {(self.identified &gt; 0).sum() / self.pad_skeleton.sum()}\")\n        self.pad_distances *= self.pad_skeleton\n        del self.identified\n        del self.unidentified_stats\n        del self.unidentified_shapes\n\n\n    def _identify_edges_connecting_a_vertex_list(self, cropped_skeleton: NDArray[np.uint8], cropped_non_tip_vertices: NDArray, starting_vertices_coord: NDArray) -&gt; Tuple[NDArray[np.uint8], NDArray]:\n        \"\"\"Identify edges connecting a list of vertices within a cropped skeleton.\n\n        This function iteratively connects the closest vertices from starting_vertices_coord to their nearest neighbors,\n        updating the skeleton and removing already connected vertices until no new connections can be made or\n        a maximum number of connections is reached.\n\n        Parameters\n        ----------\n        cropped_skeleton : ndarray of uint8\n            A binary skeleton image where skeletal pixels are marked as 1.\n        cropped_non_tip_vertices : ndarray of int\n            Coordinates of non-tip vertices in the cropped skeleton.\n        starting_vertices_coord : ndarray of int\n            Coordinates of vertices from which to find connections.\n\n        Returns\n        -------\n        cropped_skeleton : ndarray of uint8\n            Updated skeleton with edges marked as 0.\n        cropped_non_tip_vertices : ndarray of int\n            Updated list of non-tip vertices after removing those that have been connected.\n        \"\"\"\n        explored_connexions_per_vertex = 0  # the maximal edge number that can connect a vertex\n        new_connexions = True\n        while new_connexions and explored_connexions_per_vertex &lt; 5 and np.any(cropped_non_tip_vertices) and np.any(starting_vertices_coord):\n\n            explored_connexions_per_vertex += 1\n            # 1. Find the ith closest vertex to each focal vertex\n            ending_vertices_coord, new_edge_lengths, new_edge_pix_coord = _find_closest_vertices(\n                cropped_skeleton, cropped_non_tip_vertices, starting_vertices_coord)\n            if np.isnan(new_edge_lengths).sum() + (new_edge_lengths == 0).sum() == new_edge_lengths.shape[0]:\n                new_connexions = False\n            else:\n                # In new_edge_lengths, zeros are duplicates and nan are lone vertices (from starting_vertices_coord)\n                # Find out which starting_vertices_coord should be kept and which one should be used to save edges\n                no_new_connexion = np.isnan(new_edge_lengths)\n                no_found_connexion = np.logical_or(no_new_connexion, new_edge_lengths == 0)\n                found_connexion = np.logical_not(no_found_connexion)\n\n                # Any vertex_to_vertex_connexions must be analyzed only once. We remove them with the non-connectable vertices\n                vertex_to_vertex_connexions = new_edge_lengths == 1\n\n                # Save edge data\n                start = self.numbered_vertices[\n                    starting_vertices_coord[found_connexion, 0], starting_vertices_coord[found_connexion, 1]]\n                end = self.numbered_vertices[\n                    ending_vertices_coord[found_connexion, 0], ending_vertices_coord[found_connexion, 1]]\n                new_edge_lengths = new_edge_lengths[found_connexion]\n                self._update_edge_data(start, end, new_edge_lengths, new_edge_pix_coord)\n\n                no_new_connexion = np.logical_or(no_new_connexion, vertex_to_vertex_connexions)\n                vertices_to_crop = starting_vertices_coord[no_new_connexion, :]\n\n                # Remove non-connectable and connected_vertices from:\n                cropped_non_tip_vertices = remove_coordinates(cropped_non_tip_vertices, vertices_to_crop)\n                starting_vertices_coord = remove_coordinates(starting_vertices_coord, vertices_to_crop)\n\n                if new_edge_pix_coord.shape[0] &gt; 0:\n                    # Update cropped_skeleton to not identify each edge more than once\n                    cropped_skeleton[new_edge_pix_coord[:, 0], new_edge_pix_coord[:, 1]] = 0\n                # And the starting vertices that cannot connect anymore\n                cropped_skeleton[vertices_to_crop[:, 0], vertices_to_crop[:, 1]] = 0\n\n                if self.new_level_vertices is None:\n                    self.new_level_vertices = ending_vertices_coord[found_connexion, :].copy()\n                else:\n                    self.new_level_vertices = np.vstack((self.new_level_vertices, ending_vertices_coord[found_connexion, :]))\n\n        return cropped_skeleton, cropped_non_tip_vertices\n\n    def _update_edge_data(self, start, end, new_edge_lengths: NDArray, new_edge_pix_coord: NDArray):\n        \"\"\"\n        Update edge data by expanding existing arrays with new edges.\n\n        This method updates the internal edge data structures (lengths,\n        labels, and pixel coordinates) by appending new edges.\n\n        Parameters\n        ----------\n        start : int or ndarray of int\n            The starting vertex label(s) for the new edges.\n        end : int or ndarray of int\n            The ending vertex label(s) for the new edges.\n        new_edge_lengths : ndarray of float\n            The lengths of the new edges to be added.\n        new_edge_pix_coord : ndarray of float\n            The pixel coordinates of the new edges.\n\n        Attributes\n        ----------\n        edge_lengths : ndarray of float\n            The lengths of all edges.\n        edges_labels : ndarray of int\n            The labels for each edge (start and end vertices).\n        edge_pix_coord : ndarray of float\n            The pixel coordinates for all edges.\n        \"\"\"\n        if isinstance(start, np.ndarray):\n            end_idx = len(start)\n            self.edge_lengths = np.concatenate((self.edge_lengths, new_edge_lengths))\n        else:\n            end_idx = 1\n            self.edge_lengths = np.append(self.edge_lengths, new_edge_lengths)\n        start_idx = self.edges_labels.shape[0]\n        new_edges = np.zeros((end_idx, 3), dtype=np.uint32)\n        new_edges[:, 0] = np.arange(start_idx, start_idx + end_idx) + 1  # edge label\n        new_edges[:, 1] = start  # starting vertex label\n        new_edges[:, 2] = end  # ending vertex label\n        self.edges_labels = np.vstack((self.edges_labels, new_edges))\n        # Add the new edge coord\n        if new_edge_pix_coord.shape[0] &gt; 0:\n            # Add the new edge pixel coord\n            new_edge_pix_coord[:, 2] += start_idx\n            self.edge_pix_coord = np.vstack((self.edge_pix_coord, new_edge_pix_coord))\n\n    def clear_edge_duplicates(self):\n        \"\"\"\n        Remove duplicate edges by checking vertices and coordinates.\n\n        This method identifies and removes duplicate edges based on their vertex labels\n        and pixel coordinates. It scans through the edge attributes, compares them,\n        and removes duplicates if they are found.\n        \"\"\"\n        edges_to_remove = []\n        duplicates = find_duplicates_coord(np.vstack((self.edges_labels[:, 1:], self.edges_labels[:, :0:-1])))\n        duplicates = np.logical_or(duplicates[:len(duplicates)//2], duplicates[len(duplicates)//2:])\n        for v in self.edges_labels[duplicates, 1:]: #v = self.edges_labels[duplicates, 1:][4]\n            edge_lab1 = self.edges_labels[np.all(self.edges_labels[:, 1:] == v, axis=1), 0]\n            edge_lab2 = self.edges_labels[np.all(self.edges_labels[:, 1:] == v[::-1], axis=1), 0]\n            edge_labs = np.unique(np.concatenate((edge_lab1, edge_lab2)))\n            for edge_i in range(0, len(edge_labs) - 1):  #  edge_i = 0\n                edge_i_coord = self.edge_pix_coord[self.edge_pix_coord[:, 2] == edge_labs[edge_i], :2]\n                for edge_j in range(edge_i + 1, len(edge_labs)):  #  edge_j = 1\n                    edge_j_coord = self.edge_pix_coord[self.edge_pix_coord[:, 2] == edge_labs[edge_j], :2]\n                    if np.array_equal(edge_i_coord, edge_j_coord):\n                        edges_to_remove.append(edge_labs[edge_j])\n        edges_to_remove = np.unique(edges_to_remove)\n        for edge in edges_to_remove:\n            edge_bool = self.edges_labels[:, 0] != edge\n            self.edges_labels = self.edges_labels[edge_bool, :]\n            self.edge_lengths = self.edge_lengths[edge_bool]\n            self.edge_pix_coord = self.edge_pix_coord[self.edge_pix_coord[:, 2] != edge, :]\n\n\n    def clear_vertices_connecting_2_edges(self):\n        \"\"\"\n        Remove vertices connecting exactly two edges and update edge-related attributes.\n\n        This method identifies vertices that are connected to exactly 2 edges,\n        renames edges, updates edge lengths and vertex coordinates accordingly.\n        It also removes the corresponding vertices from non-tip vertices list.\n        \"\"\"\n        v_labels, v_counts = np.unique(self.edges_labels[:, 1:], return_counts=True)\n        vertices2 = v_labels[v_counts == 2]\n        for vertex2 in vertices2:  # vertex2 = vertices2[0]\n            edge_indices = np.nonzero(self.edges_labels[:, 1:] == vertex2)[0]\n            edge_names = [self.edges_labels[edge_indices[0], 0], self.edges_labels[edge_indices[1], 0]]\n            v_names = np.concatenate((self.edges_labels[edge_indices[0], 1:], self.edges_labels[edge_indices[1], 1:]))\n            v_names = v_names[v_names != vertex2]\n            if len(v_names) &gt; 0: # Otherwise it's a vertex between a normal edge and a loop\n                kept_edge = int(self.edge_lengths[edge_indices[1]] &gt;= self.edge_lengths[edge_indices[0]])\n                # Rename the removed edge by the kept edge name in pix_coord:\n                self.edge_pix_coord[self.edge_pix_coord[:, 2] == edge_names[1 - kept_edge], 2] = edge_names[kept_edge]\n                # Add the removed edge length to the kept edge length (minus 2, corresponding to the removed vertex)\n                self.edge_lengths[self.edges_labels[:, 0] == edge_names[kept_edge]] += self.edge_lengths[self.edges_labels[:, 0] == edge_names[1 - kept_edge]] - 1\n                # Remove the corresponding edge length from the list\n                self.edge_lengths = self.edge_lengths[self.edges_labels[:, 0] != edge_names[1 - kept_edge]]\n                # Rename the vertex of the kept edge in edges_labels\n                self.edges_labels[self.edges_labels[:, 0] == edge_names[kept_edge], 1:] = v_names[1 - kept_edge], v_names[kept_edge]\n                # Remove the removed edge from the edges_labels array\n                self.edges_labels = self.edges_labels[self.edges_labels[:, 0] != edge_names[1 - kept_edge], :]\n                # vY, vX = np.nonzero(self.numbered_vertices == vertex2)\n                # v_idx = np.nonzero(np.all(self.non_tip_vertices == [vY[0], vX[0]], axis=1))\n                vY, vX = self.vertex_index_map[vertex2]\n                v_idx = np.nonzero(np.all(self.non_tip_vertices == [vY, vX], axis=1))\n                self.non_tip_vertices = np.delete(self.non_tip_vertices, v_idx, axis=0)\n        # Sometimes, clearing vertices connecting 2 edges can create edge duplicates, so:\n        self.clear_edge_duplicates()\n\n    def _remove_padding(self):\n        \"\"\"\n        Removes padding from various coordinate arrays.\n\n        This method adjusts the coordinates of edge pixels, tips,\n        and non-tip vertices by subtracting 1 from their x and y values.\n        It also removes padding from the skeleton, distances, and vertices\n        using the `remove_padding` function.\n        \"\"\"\n        self.edge_pix_coord[:, :2] -= 1\n        self.tips_coord[:, :2] -= 1\n        self.non_tip_vertices[:, :2] -= 1\n        del self.vertex_index_map\n        self.skeleton, self.distances, self.vertices = remove_padding(\n            [self.pad_skeleton, self.pad_distances, self.numbered_vertices])\n\n\n    def make_vertex_table(self, origin_contours: NDArray[np.uint8]=None, growing_areas: NDArray=None):\n        \"\"\"\n        Generate a table for the vertices.\n\n        This method constructs and returns a 2D NumPy array holding information\n        about all vertices. Each row corresponds to one vertex identified either\n        by its coordinates in `self.tips_coord` or `self.non_tip_vertices`. The\n        array includes additional information about each vertex, including whether\n        they are food vertices, growing areas, and connected components.\n\n        Parameters\n        ----------\n        origin_contours : ndarray of uint8, optional\n            Binary map to identify food vertices. Default is `None`.\n        growing_areas : ndarray, optional\n            Binary map to identify growing regions. Default is `None`.\n\n        Notes\n        -----\n            The method updates the instance attribute `self.vertex_table` with\n            the generated vertex information.\n        \"\"\"\n        if self.vertices is None:\n            self._remove_padding()\n        self.vertex_table = np.zeros((self.tips_coord.shape[0] + self.non_tip_vertices.shape[0], 6), dtype=self.vertices.dtype)\n        self.vertex_table[:self.tips_coord.shape[0], :2] = self.tips_coord\n        self.vertex_table[self.tips_coord.shape[0]:, :2] = self.non_tip_vertices\n        self.vertex_table[:self.tips_coord.shape[0], 2] = self.vertices[self.tips_coord[:, 0], self.tips_coord[:, 1]]\n        self.vertex_table[self.tips_coord.shape[0]:, 2] = self.vertices[self.non_tip_vertices[:, 0], self.non_tip_vertices[:, 1]]\n        self.vertex_table[:self.tips_coord.shape[0], 3] = 1\n        if origin_contours is not None:\n            food_vertices = self.vertices[origin_contours &gt; 0]\n            food_vertices = food_vertices[food_vertices &gt; 0]\n            self.vertex_table[np.isin(self.vertex_table[:, 2], food_vertices), 4] = 1\n\n        if growing_areas is not None and growing_areas.shape[1] &gt; 0:\n            # growing = np.unique(self.vertices * growing_areas)[1:]\n            growing = np.unique(self.vertices[growing_areas[0], growing_areas[1]])\n            growing = growing[growing &gt; 0]\n            if len(growing) &gt; 0:\n                growing = np.isin(self.vertex_table[:, 2], growing)\n                self.vertex_table[growing, 4] = 2\n\n        nb, sh, stats, cent = cv2.connectedComponentsWithStats((self.vertices &gt; 0).astype(np.uint8))\n        for i, v_i in enumerate(np.nonzero(stats[:, 4] &gt; 1)[0][1:]):\n            v_labs = self.vertices[sh == v_i]\n            for v_lab in v_labs: # v_lab = v_labs[0]\n                self.vertex_table[self.vertex_table[:, 2] == v_lab, 5] = 1\n\n\n    def make_edge_table(self, greyscale: NDArray[np.uint8], compute_BC: bool=False):\n        \"\"\"\n        Generate edge table with length and average intensity information.\n\n        This method processes the vertex coordinates, calculates lengths\n        between vertices for each edge, and computes average width and intensity\n        along the edges. Additionally, it computes edge betweenness centrality\n        for each vertex pair.\n\n        Parameters\n        ----------\n        greyscale : ndarray of uint8\n            Grayscale image.\n        \"\"\"\n        if self.vertices is None:\n            self._remove_padding()\n        self.edge_table = np.zeros((self.edges_labels.shape[0], 7), float) # edge_id, vertex1, vertex2, length, average_width, int, BC\n        self.edge_table[:, :3] = self.edges_labels[:, :]\n        self.edge_table[:, 3] = self.edge_lengths\n        for idx, edge_lab in enumerate(self.edges_labels[:, 0]):\n            edge_coord = self.edge_pix_coord[self.edge_pix_coord[:, 2] == edge_lab, :]\n            pix_widths = self.distances[edge_coord[:, 0], edge_coord[:, 1]]\n            v_id = self.edges_labels[self.edges_labels[:, 0] == edge_lab, 1:][0]\n            v1_coord = self.vertex_table[self.vertex_table[:, 2] == v_id[0], :2][0]#\n            v2_coord = self.vertex_table[self.vertex_table[:, 2] == v_id[1], :2][0]#\n            v1_width, v2_width = self.distances[v1_coord[0], v1_coord[1]], self.distances[v2_coord[0], v2_coord[1]]\n\n            if not np.isnan(v1_width):\n                pix_widths = np.append(pix_widths, v1_width)\n            if not np.isnan(v2_width):\n                pix_widths = np.append(pix_widths, v2_width)\n            if pix_widths.size &gt; 0:\n                self.edge_table[idx, 4] = pix_widths.mean()\n            else:\n                self.edge_table[idx, 4] = np.nan\n            pix_ints = greyscale[edge_coord[:, 0], edge_coord[:, 1]]\n            v1_int, v2_int = greyscale[v1_coord[0], v1_coord[1]], greyscale[v2_coord[0], v2_coord[1]]\n            pix_ints = np.append(pix_ints, (v1_int, v2_int))\n            self.edge_table[idx, 5] = pix_ints.mean()\n\n        if compute_BC:\n            G = nx.from_edgelist(self.edges_labels[:, 1:])\n            e_BC = nx.edge_betweenness_centrality(G, seed=0)\n            self.BC_net = np.zeros_like(self.distances)\n            for v, k in e_BC.items(): # v=(81, 80)\n                v1_coord = self.vertex_table[self.vertex_table[:, 2] == v[0], :2]\n                v2_coord = self.vertex_table[self.vertex_table[:, 2] == v[1], :2]\n                full_coord = np.concatenate((v1_coord, v2_coord))\n                edge_lab1 = self.edges_labels[np.all(self.edges_labels[:, 1:] == v[::-1], axis=1), 0]\n                edge_lab2 = self.edges_labels[np.all(self.edges_labels[:, 1:] == v, axis=1), 0]\n                edge_lab = np.unique(np.concatenate((edge_lab1, edge_lab2)))\n                if len(edge_lab) == 1:\n                    edge_coord = self.edge_pix_coord[self.edge_pix_coord[:, 2] == edge_lab, :2]\n                    full_coord = np.concatenate((full_coord, edge_coord))\n                    self.BC_net[full_coord[:, 0], full_coord[:, 1]] = k\n                    self.edge_table[self.edge_table[:, 0] == edge_lab, 6] = k\n                elif len(edge_lab) &gt; 1:\n                    edge_coord0 = self.edge_pix_coord[self.edge_pix_coord[:, 2] == edge_lab[0], :2]\n                    for edge_i in range(len(edge_lab)): #  edge_i=1\n                        edge_coord = self.edge_pix_coord[self.edge_pix_coord[:, 2] == edge_lab[edge_i], :2]\n                        self.edge_table[self.edge_table[:, 0] == edge_lab[edge_i], 6] = k\n                        full_coord = np.concatenate((full_coord, edge_coord))\n                        self.BC_net[full_coord[:, 0], full_coord[:, 1]] = k\n                        if edge_i &gt; 0 and np.array_equal(edge_coord0, edge_coord):\n                            logging.error(f\"There still is two identical edges: {edge_lab} of len: {len(edge_coord)} linking vertices {v}\")\n                            break\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.EdgeIdentification.__init__","title":"<code>__init__(pad_skeleton, pad_distances, t=0)</code>","text":"<p>Initialize the class with skeleton and distance arrays.</p> <p>Parameters:</p> Name Type Description Default <code>pad_skeleton</code> <code>ndarray of uint8</code> <p>Array representing the skeleton to pad.</p> required <code>pad_distances</code> <code>ndarray of float64</code> <p>Array representing distances corresponding to the skeleton.</p> required <p>Attributes:</p> Name Type Description <code>remaining_vertices</code> <code>None</code> <p>Remaining vertices. Initialized as <code>None</code>.</p> <code>vertices</code> <code>None</code> <p>Vertices. Initialized as <code>None</code>.</p> <code>growing_vertices</code> <code>None</code> <p>Growing vertices. Initialized as <code>None</code>.</p> <code>im_shape</code> <code>tuple of ints</code> <p>Shape of the skeleton array.</p> Source code in <code>src/cellects/image_analysis/network_functions.py</code> <pre><code>def __init__(self, pad_skeleton: NDArray[np.uint8], pad_distances: NDArray[np.float64], t: int=0):\n    \"\"\"\n    Initialize the class with skeleton and distance arrays.\n\n    Parameters\n    ----------\n    pad_skeleton : ndarray of uint8\n        Array representing the skeleton to pad.\n    pad_distances : ndarray of float64\n        Array representing distances corresponding to the skeleton.\n\n    Attributes\n    ----------\n    remaining_vertices : None\n        Remaining vertices. Initialized as `None`.\n    vertices : None\n        Vertices. Initialized as `None`.\n    growing_vertices : None\n        Growing vertices. Initialized as `None`.\n    im_shape : tuple of ints\n        Shape of the skeleton array.\n    \"\"\"\n    self.pad_skeleton = pad_skeleton\n    self.pad_distances = pad_distances\n    self.t = t\n    self.remaining_vertices = None\n    self.vertices = None\n    self.growing_vertices = None\n    self.im_shape = pad_skeleton.shape\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.EdgeIdentification.clear_areas_of_1_or_2_unidentified_pixels","title":"<code>clear_areas_of_1_or_2_unidentified_pixels()</code>","text":"<p>Removes 1 or 2 pixel size non-identified areas from the skeleton.</p> <p>This function checks whether small non-identified areas (1 or 2 pixels) can be removed without breaking the skeleton structure. It performs a series of operations to ensure only safe removals are made, logging errors if the final skeleton is not fully connected or if some unidentified pixels remain.</p> Source code in <code>src/cellects/image_analysis/network_functions.py</code> <pre><code>def clear_areas_of_1_or_2_unidentified_pixels(self):\n    \"\"\"Removes 1 or 2 pixel size non-identified areas from the skeleton.\n\n    This function checks whether small non-identified areas (1 or 2 pixels)\n    can be removed without breaking the skeleton structure. It performs\n    a series of operations to ensure only safe removals are made, logging\n    errors if the final skeleton is not fully connected or if some unidentified pixels remain.\n    \"\"\"\n    # Check whether the 1 or 2 pixel size non-identified areas can be removed without breaking the skel\n    one_pix = np.nonzero(self.unidentified_stats[:, 4 ] &lt;= 2)[0] # == 1)[0]\n    cutting_removal = []\n    for pix_i in one_pix: #pix_i=one_pix[0]\n        skel_copy = self.pad_skeleton.copy()\n        y1, y2, x1, x2 = self.unidentified_stats[pix_i, 1], self.unidentified_stats[pix_i, 1] + self.unidentified_stats[pix_i, 3], self.unidentified_stats[pix_i, 0], self.unidentified_stats[pix_i, 0] + self.unidentified_stats[pix_i, 2]\n        skel_copy[y1:y2, x1:x2][self.unidentified_shapes[y1:y2, x1:x2] == pix_i] = 0\n        nb1, sh1 = cv2.connectedComponents(skel_copy.astype(np.uint8), connectivity=8)\n        if nb1 &gt; 2:\n            cutting_removal.append(pix_i)\n        else:\n            self.pad_skeleton[y1:y2, x1:x2][self.unidentified_shapes[y1:y2, x1:x2] == pix_i] = 0\n    if len(cutting_removal) &gt; 0:\n        logging.error(f\"t={self.t}, These pixels break the skeleton when removed: {cutting_removal}\")\n    if (self.identified &gt; 0).sum() != self.pad_skeleton.sum():\n        logging.error(f\"t={self.t}, Proportion of identified pixels in the skeleton: {(self.identified &gt; 0).sum() / self.pad_skeleton.sum()}\")\n    self.pad_distances *= self.pad_skeleton\n    del self.identified\n    del self.unidentified_stats\n    del self.unidentified_shapes\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.EdgeIdentification.clear_edge_duplicates","title":"<code>clear_edge_duplicates()</code>","text":"<p>Remove duplicate edges by checking vertices and coordinates.</p> <p>This method identifies and removes duplicate edges based on their vertex labels and pixel coordinates. It scans through the edge attributes, compares them, and removes duplicates if they are found.</p> Source code in <code>src/cellects/image_analysis/network_functions.py</code> <pre><code>def clear_edge_duplicates(self):\n    \"\"\"\n    Remove duplicate edges by checking vertices and coordinates.\n\n    This method identifies and removes duplicate edges based on their vertex labels\n    and pixel coordinates. It scans through the edge attributes, compares them,\n    and removes duplicates if they are found.\n    \"\"\"\n    edges_to_remove = []\n    duplicates = find_duplicates_coord(np.vstack((self.edges_labels[:, 1:], self.edges_labels[:, :0:-1])))\n    duplicates = np.logical_or(duplicates[:len(duplicates)//2], duplicates[len(duplicates)//2:])\n    for v in self.edges_labels[duplicates, 1:]: #v = self.edges_labels[duplicates, 1:][4]\n        edge_lab1 = self.edges_labels[np.all(self.edges_labels[:, 1:] == v, axis=1), 0]\n        edge_lab2 = self.edges_labels[np.all(self.edges_labels[:, 1:] == v[::-1], axis=1), 0]\n        edge_labs = np.unique(np.concatenate((edge_lab1, edge_lab2)))\n        for edge_i in range(0, len(edge_labs) - 1):  #  edge_i = 0\n            edge_i_coord = self.edge_pix_coord[self.edge_pix_coord[:, 2] == edge_labs[edge_i], :2]\n            for edge_j in range(edge_i + 1, len(edge_labs)):  #  edge_j = 1\n                edge_j_coord = self.edge_pix_coord[self.edge_pix_coord[:, 2] == edge_labs[edge_j], :2]\n                if np.array_equal(edge_i_coord, edge_j_coord):\n                    edges_to_remove.append(edge_labs[edge_j])\n    edges_to_remove = np.unique(edges_to_remove)\n    for edge in edges_to_remove:\n        edge_bool = self.edges_labels[:, 0] != edge\n        self.edges_labels = self.edges_labels[edge_bool, :]\n        self.edge_lengths = self.edge_lengths[edge_bool]\n        self.edge_pix_coord = self.edge_pix_coord[self.edge_pix_coord[:, 2] != edge, :]\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.EdgeIdentification.clear_vertices_connecting_2_edges","title":"<code>clear_vertices_connecting_2_edges()</code>","text":"<p>Remove vertices connecting exactly two edges and update edge-related attributes.</p> <p>This method identifies vertices that are connected to exactly 2 edges, renames edges, updates edge lengths and vertex coordinates accordingly. It also removes the corresponding vertices from non-tip vertices list.</p> Source code in <code>src/cellects/image_analysis/network_functions.py</code> <pre><code>def clear_vertices_connecting_2_edges(self):\n    \"\"\"\n    Remove vertices connecting exactly two edges and update edge-related attributes.\n\n    This method identifies vertices that are connected to exactly 2 edges,\n    renames edges, updates edge lengths and vertex coordinates accordingly.\n    It also removes the corresponding vertices from non-tip vertices list.\n    \"\"\"\n    v_labels, v_counts = np.unique(self.edges_labels[:, 1:], return_counts=True)\n    vertices2 = v_labels[v_counts == 2]\n    for vertex2 in vertices2:  # vertex2 = vertices2[0]\n        edge_indices = np.nonzero(self.edges_labels[:, 1:] == vertex2)[0]\n        edge_names = [self.edges_labels[edge_indices[0], 0], self.edges_labels[edge_indices[1], 0]]\n        v_names = np.concatenate((self.edges_labels[edge_indices[0], 1:], self.edges_labels[edge_indices[1], 1:]))\n        v_names = v_names[v_names != vertex2]\n        if len(v_names) &gt; 0: # Otherwise it's a vertex between a normal edge and a loop\n            kept_edge = int(self.edge_lengths[edge_indices[1]] &gt;= self.edge_lengths[edge_indices[0]])\n            # Rename the removed edge by the kept edge name in pix_coord:\n            self.edge_pix_coord[self.edge_pix_coord[:, 2] == edge_names[1 - kept_edge], 2] = edge_names[kept_edge]\n            # Add the removed edge length to the kept edge length (minus 2, corresponding to the removed vertex)\n            self.edge_lengths[self.edges_labels[:, 0] == edge_names[kept_edge]] += self.edge_lengths[self.edges_labels[:, 0] == edge_names[1 - kept_edge]] - 1\n            # Remove the corresponding edge length from the list\n            self.edge_lengths = self.edge_lengths[self.edges_labels[:, 0] != edge_names[1 - kept_edge]]\n            # Rename the vertex of the kept edge in edges_labels\n            self.edges_labels[self.edges_labels[:, 0] == edge_names[kept_edge], 1:] = v_names[1 - kept_edge], v_names[kept_edge]\n            # Remove the removed edge from the edges_labels array\n            self.edges_labels = self.edges_labels[self.edges_labels[:, 0] != edge_names[1 - kept_edge], :]\n            # vY, vX = np.nonzero(self.numbered_vertices == vertex2)\n            # v_idx = np.nonzero(np.all(self.non_tip_vertices == [vY[0], vX[0]], axis=1))\n            vY, vX = self.vertex_index_map[vertex2]\n            v_idx = np.nonzero(np.all(self.non_tip_vertices == [vY, vX], axis=1))\n            self.non_tip_vertices = np.delete(self.non_tip_vertices, v_idx, axis=0)\n    # Sometimes, clearing vertices connecting 2 edges can create edge duplicates, so:\n    self.clear_edge_duplicates()\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.EdgeIdentification.get_tipped_edges","title":"<code>get_tipped_edges()</code>","text":"<p>get_tipped_edges : method to extract skeleton edges connecting branching points and tips.</p> <p>Makes sure that there is only one connected component constituting the skeleton of the network and identifies all edges that are connected to a tip.</p> <p>Attributes:</p> Name Type Description <code>pad_skeleton</code> <code>ndarray of bool, modified</code> <p>Boolean mask representing the pruned skeleton after isolating the largest connected component.</p> <code>vertices_branching_tips</code> <code>ndarray of int, shape (N, 2)</code> <p>Coordinates of branching points that connect to tips in the skeleton structure.</p> <code>edge_lengths</code> <code>ndarray of float, shape (M,)</code> <p>Lengths of edges connecting non-tip vertices to identified tip locations.</p> <code>edge_pix_coord</code> <code>list of array of int</code> <p>Pixel coordinates for each edge path between connected skeleton elements.</p> Source code in <code>src/cellects/image_analysis/network_functions.py</code> <pre><code>def get_tipped_edges(self):\n    \"\"\"\n    get_tipped_edges : method to extract skeleton edges connecting branching points and tips.\n\n    Makes sure that there is only one connected component constituting the skeleton of the network and\n    identifies all edges that are connected to a tip.\n\n    Attributes\n    ----------\n    pad_skeleton : ndarray of bool, modified\n        Boolean mask representing the pruned skeleton after isolating the largest connected component.\n    vertices_branching_tips : ndarray of int, shape (N, 2)\n        Coordinates of branching points that connect to tips in the skeleton structure.\n    edge_lengths : ndarray of float, shape (M,)\n        Lengths of edges connecting non-tip vertices to identified tip locations.\n    edge_pix_coord : list of array of int\n        Pixel coordinates for each edge path between connected skeleton elements.\n\n    \"\"\"\n    self.pad_skeleton = keep_one_connected_component(self.pad_skeleton)\n    self.vertices_branching_tips, self.edge_lengths, self.edge_pix_coord = _find_closest_vertices(self.pad_skeleton,\n                                                                                    self.non_tip_vertices,\n                                                                                    self.tips_coord[:, :2])\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.EdgeIdentification.get_vertices_and_tips_coord","title":"<code>get_vertices_and_tips_coord()</code>","text":"<p>Process skeleton data to extract non-tip vertices and tip coordinates.</p> <p>This method processes the skeleton stored in <code>self.pad_skeleton</code> by first extracting all vertices and tips. It then separates these into branch points (non-tip vertices) and specific tip coordinates using internal processing.</p> <p>Attributes:</p> Name Type Description <code>self.non_tip_vertices</code> <code>array - like</code> <p>Coordinates of non-tip (branch) vertices.</p> <code>self.tips_coord</code> <code>array - like</code> <p>Coordinates of identified tips in the skeleton.</p> Source code in <code>src/cellects/image_analysis/network_functions.py</code> <pre><code>def get_vertices_and_tips_coord(self):\n    \"\"\"Process skeleton data to extract non-tip vertices and tip coordinates.\n\n    This method processes the skeleton stored in `self.pad_skeleton` by first\n    extracting all vertices and tips. It then separates these into branch points\n    (non-tip vertices) and specific tip coordinates using internal processing.\n\n    Attributes\n    ----------\n    self.non_tip_vertices : array-like\n        Coordinates of non-tip (branch) vertices.\n    self.tips_coord : array-like\n        Coordinates of identified tips in the skeleton.\n    \"\"\"\n    pad_vertices, pad_tips = get_vertices_and_tips_from_skeleton(self.pad_skeleton)\n    self.non_tip_vertices, self.tips_coord = get_branches_and_tips_coord(pad_vertices, pad_tips)\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.EdgeIdentification.label_edges_connected_with_vertex_clusters","title":"<code>label_edges_connected_with_vertex_clusters()</code>","text":"<p>Identify edges connected to touching vertices by processing vertex clusters.</p> <p>This function processes the skeleton to identify edges connecting vertices that are part of touching clusters. It creates a cropped version of the skeleton by removing already detected edges and their tips, then iterates through vertex clusters to explore and identify nearby edges.</p> Source code in <code>src/cellects/image_analysis/network_functions.py</code> <pre><code>def label_edges_connected_with_vertex_clusters(self):\n    \"\"\"\n    Identify edges connected to touching vertices by processing vertex clusters.\n\n    This function processes the skeleton to identify edges connecting vertices\n    that are part of touching clusters. It creates a cropped version of the skeleton\n    by removing already detected edges and their tips, then iterates through vertex\n    clusters to explore and identify nearby edges.\n    \"\"\"\n    # I.1. Identify edges connected to touching vertices:\n    # First, create another version of these arrays, where we remove every already detected edge and their tips\n    cropped_skeleton = self.pad_skeleton.copy()\n    cropped_skeleton[self.edge_pix_coord[:, 0], self.edge_pix_coord[:, 1]] = 0\n    cropped_skeleton[self.tips_coord[:, 0], self.tips_coord[:, 1]] = 0\n\n    # non_tip_vertices does not need to be updated yet, because it only contains verified branching vertices\n    cropped_non_tip_vertices = self.non_tip_vertices.copy()\n\n    self.new_level_vertices = None\n    # The problem with vertex_to_vertex_connexion is that since they are not separated by zeros,\n    # they always atract each other instead of exploring other paths.\n    # To fix this, we loop over each vertex group to\n    # 1. Add one edge per inter-vertex connexion inside the group\n    # 2. Remove all except one, and loop as many time as necessary.\n    # Inside that second loop, we explore and identify every edge nearby.\n    # Find every vertex_to_vertex_connexion\n    v_cluster_nb, self.v_cluster_lab, self.v_cluster_stats, vgc = cv2.connectedComponentsWithStats(\n        (self.numbered_vertices &gt; 0).astype(np.uint8), connectivity=8)\n    if v_cluster_nb &gt; 0:\n        max_v_nb = np.max(self.v_cluster_stats[1:, 4])\n        cropped_skeleton_list = []\n        starting_vertices_list = []\n        for v_nb in range(2, max_v_nb + 1):\n            labels = np.nonzero(self.v_cluster_stats[:, 4] == v_nb)[0]\n            coord_list = []\n            for lab in labels:  # lab=labels[0]\n                coord_list.append(np.nonzero(self.v_cluster_lab == lab))\n            for iter in range(v_nb):\n                for lab_ in range(labels.shape[0]): # lab=labels[0]\n                    cs = cropped_skeleton.copy()\n                    sv = []\n                    v_c = coord_list[lab_]\n                    # Save the current coordinate in the starting vertices array of this iteration\n                    sv.append([v_c[0][iter], v_c[1][iter]])\n                    # Remove one vertex coordinate to keep it from cs\n                    v_y, v_x = np.delete(v_c[0], iter), np.delete(v_c[1], iter)\n                    cs[v_y, v_x] = 0\n                    cropped_skeleton_list.append(cs)\n                    starting_vertices_list.append(np.array(sv))\n        for cropped_skeleton, starting_vertices in zip(cropped_skeleton_list, starting_vertices_list):\n            _, _ = self._identify_edges_connecting_a_vertex_list(cropped_skeleton, cropped_non_tip_vertices, starting_vertices)\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.EdgeIdentification.label_edges_connecting_vertex_clusters","title":"<code>label_edges_connecting_vertex_clusters()</code>","text":"<p>Label edges connecting vertex clusters.</p> <p>This method identifies the connections between connected vertices within vertex clusters and labels these edges. It uses the previously found connected vertices, creates an image of the connections, and then identifies and labels the edges between these touching vertices.</p> Source code in <code>src/cellects/image_analysis/network_functions.py</code> <pre><code>def label_edges_connecting_vertex_clusters(self):\n    \"\"\"\n    Label edges connecting vertex clusters.\n\n    This method identifies the connections between connected vertices within\n    vertex clusters and labels these edges. It uses the previously found connected\n    vertices, creates an image of the connections, and then identifies\n    and labels the edges between these touching vertices.\n    \"\"\"\n    # I.2. Identify the connexions between connected vertices:\n    all_connected_vertices = np.nonzero(self.v_cluster_stats[:, 4] &gt; 1)[0][1:]\n    all_con_v_im = np.zeros_like(self.pad_skeleton)\n    for v_group in all_connected_vertices:\n        all_con_v_im[self.v_cluster_lab == v_group] = 1\n    cropped_skeleton = all_con_v_im\n    self.vertex_clusters_coord = np.transpose(np.array(np.nonzero(cropped_skeleton)))\n    _, _ = self._identify_edges_connecting_a_vertex_list(cropped_skeleton, self.vertex_clusters_coord, self.vertex_clusters_coord)\n    # self.edges_labels\n    del self.v_cluster_stats\n    del self.v_cluster_lab\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.EdgeIdentification.label_edges_from_known_vertices_iteratively","title":"<code>label_edges_from_known_vertices_iteratively()</code>","text":"<p>Label edges iteratively from known vertices.</p> <p>This method labels edges in an iterative process starting from known vertices. It handles the removal of detected edges and updates the skeleton accordingly, to avoid detecting edges twice.</p> Source code in <code>src/cellects/image_analysis/network_functions.py</code> <pre><code>def label_edges_from_known_vertices_iteratively(self):\n    \"\"\"\n    Label edges iteratively from known vertices.\n\n    This method labels edges in an iterative process starting from\n    known vertices. It handles the removal of detected edges and\n    updates the skeleton accordingly, to avoid detecting edges twice.\n    \"\"\"\n    # II/ Identify all remaining edges\n    if self.new_level_vertices is not None:\n        starting_vertices_coord = np.vstack((self.new_level_vertices[:, :2], self.vertices_branching_tips))\n        starting_vertices_coord = np.unique(starting_vertices_coord, axis=0)\n    else:\n        # We start from the vertices connecting tips\n        starting_vertices_coord = self.vertices_branching_tips.copy()\n    # Remove the detected edges from cropped_skeleton:\n    cropped_skeleton = self.pad_skeleton.copy()\n    cropped_skeleton[self.edge_pix_coord[:, 0], self.edge_pix_coord[:, 1]] = 0\n    cropped_skeleton[self.tips_coord[:, 0], self.tips_coord[:, 1]] = 0\n    cropped_skeleton[self.vertex_clusters_coord[:, 0], self.vertex_clusters_coord[:, 1]] = 0\n\n    # Reinitialize cropped_non_tip_vertices to browse all vertices except tips and groups\n    cropped_non_tip_vertices = self.non_tip_vertices.copy()\n    cropped_non_tip_vertices = remove_coordinates(cropped_non_tip_vertices, self.vertex_clusters_coord)\n    del self.vertex_clusters_coord\n    remaining_v = cropped_non_tip_vertices.shape[0] + 1\n    while remaining_v &gt; cropped_non_tip_vertices.shape[0]:\n        remaining_v = cropped_non_tip_vertices.shape[0]\n        cropped_skeleton, cropped_non_tip_vertices = self._identify_edges_connecting_a_vertex_list(cropped_skeleton, cropped_non_tip_vertices, starting_vertices_coord)\n        if self.new_level_vertices is not None:\n            starting_vertices_coord = np.unique(self.new_level_vertices[:, :2], axis=0)\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.EdgeIdentification.label_edges_looping_on_1_vertex","title":"<code>label_edges_looping_on_1_vertex()</code>","text":"<p>Identify and handle edges that form loops around a single vertex. This method processes the skeleton image to find looping edges and updates the edge data structure accordingly.</p> Source code in <code>src/cellects/image_analysis/network_functions.py</code> <pre><code>def label_edges_looping_on_1_vertex(self):\n    \"\"\"\n    Identify and handle edges that form loops around a single vertex.\n    This method processes the skeleton image to find looping edges and updates\n    the edge data structure accordingly.\n    \"\"\"\n    self.identified = np.zeros_like(self.numbered_vertices)\n    self.identified[self.edge_pix_coord[:, 0], self.edge_pix_coord[:, 1]] = 1\n    self.identified[self.non_tip_vertices[:, 0], self.non_tip_vertices[:, 1]] = 1\n    self.identified[self.tips_coord[:, 0], self.tips_coord[:, 1]] = 1\n    unidentified = (1 - self.identified) * self.pad_skeleton\n\n    # Find out the remaining non-identified pixels\n    nb, self.unidentified_shapes, self.unidentified_stats, ce = cv2.connectedComponentsWithStats(unidentified.astype(np.uint8))\n    # Handle the cases where edges are loops over only one vertex\n    looping_edges = np.nonzero(self.unidentified_stats[:, 4 ] &gt; 2)[0][1:]\n    for loop_i in looping_edges: # loop_i = looping_edges[0] loop_i=11 #  zoom_on_nonzero(unique_vertices_im, return_coord=False)\n        edge_i = (self.unidentified_shapes == loop_i).astype(np.uint8)\n        dil_edge_i = cv2.dilate(edge_i, square_33)\n        unique_vertices_im = self.numbered_vertices.copy()\n        unique_vertices_im[self.tips_coord[:, 0], self.tips_coord[:, 1]] = 0\n        unique_vertices_im = dil_edge_i * unique_vertices_im\n        unique_vertices = np.unique(unique_vertices_im)\n        unique_vertices = unique_vertices[unique_vertices &gt; 0]\n        v_nb = len(unique_vertices)\n        new_edge_lengths = edge_i.sum()\n        new_edge_pix_coord = np.transpose(np.vstack((np.nonzero(edge_i))))\n        new_edge_pix_coord = np.hstack((new_edge_pix_coord, np.repeat(1, new_edge_pix_coord.shape[0])[:, None]))\n        if v_nb == 1:\n            start, end = unique_vertices[0], unique_vertices[0]\n            self._update_edge_data(start, end, new_edge_lengths, new_edge_pix_coord)\n        elif v_nb == 2:\n            # The edge loops around a group of connected vertices\n            start, end = unique_vertices[0], unique_vertices[1]\n            self._update_edge_data(start, end, new_edge_lengths, new_edge_pix_coord)\n            # conn_v_nb, conn_v = cv2.connectedComponents((unique_vertices_im &gt; 0).astype(np.uint8))\n            # if len(unique_vertices) == 2 and conn_v_nb == 2:\n        elif v_nb &gt; 2: # The question is: How to choose two vertices so that they link all missing pixels?\n            # 1. Find every edge pixel connected to these vertices\n            vertex_connected_pixels = np.nonzero(cv2.dilate((unique_vertices_im &gt; 0).astype(np.uint8), square_33) * edge_i)\n            # 2. Find the most distant pair of these\n            pix1, pix2 = get_min_or_max_euclidean_pair(vertex_connected_pixels, \"max\")\n            # 3. The two best vertices are the two nearest to these two most distant edge pixels\n            dist_to_pix1 = np.zeros(v_nb, np.float64)\n            dist_to_pix2 = np.zeros(v_nb, np.float64)\n            for _i, v_i in enumerate(unique_vertices):\n                v_coord = self.vertex_index_map[v_i]\n                dist_to_pix1[_i] = eudist(pix1, v_coord)\n                dist_to_pix2[_i] = eudist(pix2, v_coord)\n            start, end = unique_vertices[np.argmin(dist_to_pix1)], unique_vertices[np.argmin(dist_to_pix2)]\n            self._update_edge_data(start, end, new_edge_lengths, new_edge_pix_coord)\n        else:\n            logging.error(f\"t={self.t}, One long edge is not identified: i={loop_i} of length={edge_i.sum()} close to {len(unique_vertices)} vertices.\")\n    self.identified[self.edge_pix_coord[:, 0], self.edge_pix_coord[:, 1]] = 1\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.EdgeIdentification.label_tipped_edges_and_their_vertices","title":"<code>label_tipped_edges_and_their_vertices()</code>","text":"<p>Label edges connecting tip vertices to branching vertices and assign unique labels to all relevant vertices.</p> <p>Processes vertex coordinates by stacking tips, vertices branching from tips, and remaining non-tip vertices. Assigns unique sequential identifiers to these vertices in a new array. Constructs an array of edge-label information, where each row contains the edge label (starting at 1), corresponding tip label, and connected vertex label.</p> <p>Attributes:</p> Name Type Description <code>tip_number</code> <code>int</code> <p>The number of tip coordinates available in <code>tips_coord</code>.</p> <code>ordered_v_coord</code> <code>ndarray of float</code> <p>Stack of unique vertex coordinates ordered by: tips first, vertices branching tips second, non-tip vertices third.</p> <code>numbered_vertices</code> <code>ndarray of uint32</code> <p>2D array where each coordinate position is labeled with a sequential integer (starting at 1) based on the order in <code>ordered_v_coord</code>.</p> <code>edges_labels</code> <code>ndarray of uint32</code> <p>Array of shape (n_edges, 3). Each row contains: - Edge label (sequential from 1 to n_edges) - Label of the tip vertex for that edge. - Label of the vertex branching the tip.</p> <code>vertices_branching_tips</code> <code>ndarray of float</code> <p>Unique coordinates of vertices directly connected to tips after removing duplicates.</p> Source code in <code>src/cellects/image_analysis/network_functions.py</code> <pre><code>def label_tipped_edges_and_their_vertices(self):\n    \"\"\"Label edges connecting tip vertices to branching vertices and assign unique labels to all relevant vertices.\n\n    Processes vertex coordinates by stacking tips, vertices branching from tips, and remaining non-tip vertices.\n    Assigns unique sequential identifiers to these vertices in a new array. Constructs an array of edge-label information,\n    where each row contains the edge label (starting at 1), corresponding tip label, and connected vertex label.\n\n    Attributes\n    ----------\n    tip_number : int\n        The number of tip coordinates available in `tips_coord`.\n\n    ordered_v_coord : ndarray of float\n        Stack of unique vertex coordinates ordered by: tips first, vertices branching tips second, non-tip vertices third.\n\n    numbered_vertices : ndarray of uint32\n        2D array where each coordinate position is labeled with a sequential integer (starting at 1) based on the order in `ordered_v_coord`.\n\n    edges_labels : ndarray of uint32\n        Array of shape (n_edges, 3). Each row contains:\n        - Edge label (sequential from 1 to n_edges)\n        - Label of the tip vertex for that edge.\n        - Label of the vertex branching the tip.\n\n    vertices_branching_tips : ndarray of float\n        Unique coordinates of vertices directly connected to tips after removing duplicates.\n    \"\"\"\n    self.tip_number = self.tips_coord.shape[0]\n\n    # Stack vertex coordinates in that order: 1. Tips, 2. Vertices branching tips, 3. All remaining vertices\n    ordered_v_coord = np.vstack((self.tips_coord[:, :2], self.vertices_branching_tips[:, :2], self.non_tip_vertices))\n    ordered_v_coord = np.unique(ordered_v_coord, axis=0)\n\n    # Create arrays to store edges and vertices labels\n    self.numbered_vertices = np.zeros(self.im_shape, dtype=np.uint32)\n    self.numbered_vertices[ordered_v_coord[:, 0], ordered_v_coord[:, 1]] = np.arange(1, ordered_v_coord.shape[0] + 1)\n    self.vertices = None\n    self.vertex_index_map = {}\n    for idx, (y, x) in enumerate(ordered_v_coord):\n        self.vertex_index_map[idx + 1] = tuple((np.uint32(y), np.uint32(x)))\n\n    # Name edges from 1 to the number of edges connecting tips and set the vertices labels from all tips to their connected vertices:\n    self.edges_labels = np.zeros((self.tip_number, 3), dtype=np.uint32)\n    # edge label:\n    self.edges_labels[:, 0] = np.arange(self.tip_number) + 1\n    # tip label:\n    self.edges_labels[:, 1] = self.numbered_vertices[self.tips_coord[:, 0], self.tips_coord[:, 1]]\n    # vertex branching tip label:\n    self.edges_labels[:, 2] = self.numbered_vertices[self.vertices_branching_tips[:, 0], self.vertices_branching_tips[:, 1]]\n\n    # Remove duplicates in vertices_branching_tips\n    self.vertices_branching_tips = np.unique(self.vertices_branching_tips[:, :2], axis=0)\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.EdgeIdentification.make_edge_table","title":"<code>make_edge_table(greyscale, compute_BC=False)</code>","text":"<p>Generate edge table with length and average intensity information.</p> <p>This method processes the vertex coordinates, calculates lengths between vertices for each edge, and computes average width and intensity along the edges. Additionally, it computes edge betweenness centrality for each vertex pair.</p> <p>Parameters:</p> Name Type Description Default <code>greyscale</code> <code>ndarray of uint8</code> <p>Grayscale image.</p> required Source code in <code>src/cellects/image_analysis/network_functions.py</code> <pre><code>def make_edge_table(self, greyscale: NDArray[np.uint8], compute_BC: bool=False):\n    \"\"\"\n    Generate edge table with length and average intensity information.\n\n    This method processes the vertex coordinates, calculates lengths\n    between vertices for each edge, and computes average width and intensity\n    along the edges. Additionally, it computes edge betweenness centrality\n    for each vertex pair.\n\n    Parameters\n    ----------\n    greyscale : ndarray of uint8\n        Grayscale image.\n    \"\"\"\n    if self.vertices is None:\n        self._remove_padding()\n    self.edge_table = np.zeros((self.edges_labels.shape[0], 7), float) # edge_id, vertex1, vertex2, length, average_width, int, BC\n    self.edge_table[:, :3] = self.edges_labels[:, :]\n    self.edge_table[:, 3] = self.edge_lengths\n    for idx, edge_lab in enumerate(self.edges_labels[:, 0]):\n        edge_coord = self.edge_pix_coord[self.edge_pix_coord[:, 2] == edge_lab, :]\n        pix_widths = self.distances[edge_coord[:, 0], edge_coord[:, 1]]\n        v_id = self.edges_labels[self.edges_labels[:, 0] == edge_lab, 1:][0]\n        v1_coord = self.vertex_table[self.vertex_table[:, 2] == v_id[0], :2][0]#\n        v2_coord = self.vertex_table[self.vertex_table[:, 2] == v_id[1], :2][0]#\n        v1_width, v2_width = self.distances[v1_coord[0], v1_coord[1]], self.distances[v2_coord[0], v2_coord[1]]\n\n        if not np.isnan(v1_width):\n            pix_widths = np.append(pix_widths, v1_width)\n        if not np.isnan(v2_width):\n            pix_widths = np.append(pix_widths, v2_width)\n        if pix_widths.size &gt; 0:\n            self.edge_table[idx, 4] = pix_widths.mean()\n        else:\n            self.edge_table[idx, 4] = np.nan\n        pix_ints = greyscale[edge_coord[:, 0], edge_coord[:, 1]]\n        v1_int, v2_int = greyscale[v1_coord[0], v1_coord[1]], greyscale[v2_coord[0], v2_coord[1]]\n        pix_ints = np.append(pix_ints, (v1_int, v2_int))\n        self.edge_table[idx, 5] = pix_ints.mean()\n\n    if compute_BC:\n        G = nx.from_edgelist(self.edges_labels[:, 1:])\n        e_BC = nx.edge_betweenness_centrality(G, seed=0)\n        self.BC_net = np.zeros_like(self.distances)\n        for v, k in e_BC.items(): # v=(81, 80)\n            v1_coord = self.vertex_table[self.vertex_table[:, 2] == v[0], :2]\n            v2_coord = self.vertex_table[self.vertex_table[:, 2] == v[1], :2]\n            full_coord = np.concatenate((v1_coord, v2_coord))\n            edge_lab1 = self.edges_labels[np.all(self.edges_labels[:, 1:] == v[::-1], axis=1), 0]\n            edge_lab2 = self.edges_labels[np.all(self.edges_labels[:, 1:] == v, axis=1), 0]\n            edge_lab = np.unique(np.concatenate((edge_lab1, edge_lab2)))\n            if len(edge_lab) == 1:\n                edge_coord = self.edge_pix_coord[self.edge_pix_coord[:, 2] == edge_lab, :2]\n                full_coord = np.concatenate((full_coord, edge_coord))\n                self.BC_net[full_coord[:, 0], full_coord[:, 1]] = k\n                self.edge_table[self.edge_table[:, 0] == edge_lab, 6] = k\n            elif len(edge_lab) &gt; 1:\n                edge_coord0 = self.edge_pix_coord[self.edge_pix_coord[:, 2] == edge_lab[0], :2]\n                for edge_i in range(len(edge_lab)): #  edge_i=1\n                    edge_coord = self.edge_pix_coord[self.edge_pix_coord[:, 2] == edge_lab[edge_i], :2]\n                    self.edge_table[self.edge_table[:, 0] == edge_lab[edge_i], 6] = k\n                    full_coord = np.concatenate((full_coord, edge_coord))\n                    self.BC_net[full_coord[:, 0], full_coord[:, 1]] = k\n                    if edge_i &gt; 0 and np.array_equal(edge_coord0, edge_coord):\n                        logging.error(f\"There still is two identical edges: {edge_lab} of len: {len(edge_coord)} linking vertices {v}\")\n                        break\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.EdgeIdentification.make_vertex_table","title":"<code>make_vertex_table(origin_contours=None, growing_areas=None)</code>","text":"<p>Generate a table for the vertices.</p> <p>This method constructs and returns a 2D NumPy array holding information about all vertices. Each row corresponds to one vertex identified either by its coordinates in <code>self.tips_coord</code> or <code>self.non_tip_vertices</code>. The array includes additional information about each vertex, including whether they are food vertices, growing areas, and connected components.</p> <p>Parameters:</p> Name Type Description Default <code>origin_contours</code> <code>ndarray of uint8</code> <p>Binary map to identify food vertices. Default is <code>None</code>.</p> <code>None</code> <code>growing_areas</code> <code>ndarray</code> <p>Binary map to identify growing regions. Default is <code>None</code>.</p> <code>None</code> Notes <pre><code>The method updates the instance attribute `self.vertex_table` with\nthe generated vertex information.\n</code></pre> Source code in <code>src/cellects/image_analysis/network_functions.py</code> <pre><code>def make_vertex_table(self, origin_contours: NDArray[np.uint8]=None, growing_areas: NDArray=None):\n    \"\"\"\n    Generate a table for the vertices.\n\n    This method constructs and returns a 2D NumPy array holding information\n    about all vertices. Each row corresponds to one vertex identified either\n    by its coordinates in `self.tips_coord` or `self.non_tip_vertices`. The\n    array includes additional information about each vertex, including whether\n    they are food vertices, growing areas, and connected components.\n\n    Parameters\n    ----------\n    origin_contours : ndarray of uint8, optional\n        Binary map to identify food vertices. Default is `None`.\n    growing_areas : ndarray, optional\n        Binary map to identify growing regions. Default is `None`.\n\n    Notes\n    -----\n        The method updates the instance attribute `self.vertex_table` with\n        the generated vertex information.\n    \"\"\"\n    if self.vertices is None:\n        self._remove_padding()\n    self.vertex_table = np.zeros((self.tips_coord.shape[0] + self.non_tip_vertices.shape[0], 6), dtype=self.vertices.dtype)\n    self.vertex_table[:self.tips_coord.shape[0], :2] = self.tips_coord\n    self.vertex_table[self.tips_coord.shape[0]:, :2] = self.non_tip_vertices\n    self.vertex_table[:self.tips_coord.shape[0], 2] = self.vertices[self.tips_coord[:, 0], self.tips_coord[:, 1]]\n    self.vertex_table[self.tips_coord.shape[0]:, 2] = self.vertices[self.non_tip_vertices[:, 0], self.non_tip_vertices[:, 1]]\n    self.vertex_table[:self.tips_coord.shape[0], 3] = 1\n    if origin_contours is not None:\n        food_vertices = self.vertices[origin_contours &gt; 0]\n        food_vertices = food_vertices[food_vertices &gt; 0]\n        self.vertex_table[np.isin(self.vertex_table[:, 2], food_vertices), 4] = 1\n\n    if growing_areas is not None and growing_areas.shape[1] &gt; 0:\n        # growing = np.unique(self.vertices * growing_areas)[1:]\n        growing = np.unique(self.vertices[growing_areas[0], growing_areas[1]])\n        growing = growing[growing &gt; 0]\n        if len(growing) &gt; 0:\n            growing = np.isin(self.vertex_table[:, 2], growing)\n            self.vertex_table[growing, 4] = 2\n\n    nb, sh, stats, cent = cv2.connectedComponentsWithStats((self.vertices &gt; 0).astype(np.uint8))\n    for i, v_i in enumerate(np.nonzero(stats[:, 4] &gt; 1)[0][1:]):\n        v_labs = self.vertices[sh == v_i]\n        for v_lab in v_labs: # v_lab = v_labs[0]\n            self.vertex_table[self.vertex_table[:, 2] == v_lab, 5] = 1\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.EdgeIdentification.remove_tipped_edge_smaller_than_branch_width","title":"<code>remove_tipped_edge_smaller_than_branch_width()</code>","text":"<p>Remove very short edges from the skeleton.</p> <p>This method focuses on edges connecting tips. When too short, they are considered are noise and removed from the skeleton and distances matrices. These edges are considered too short when their length is smaller than the width of the nearest network branch (an information included in pad_distances). This method also updates internal data structures (skeleton, edge coordinates, vertex/tip positions) accordingly through pixel-wise analysis and connectivity checks.</p> Source code in <code>src/cellects/image_analysis/network_functions.py</code> <pre><code>def remove_tipped_edge_smaller_than_branch_width(self):\n    \"\"\"Remove very short edges from the skeleton.\n\n    This method focuses on edges connecting tips. When too short, they are considered are noise and\n    removed from the skeleton and distances matrices. These edges are considered too short when their length is\n    smaller than the width of the nearest network branch (an information included in pad_distances).\n    This method also updates internal data structures (skeleton, edge coordinates, vertex/tip positions)\n    accordingly through pixel-wise analysis and connectivity checks.\n    \"\"\"\n    # Identify edges that are smaller than the width of the branch it is attached to\n    tipped_edges_to_remove = np.zeros(self.edge_lengths.shape[0], dtype=bool)\n    # connecting_vertices_to_remove = np.zeros(self.vertices_branching_tips.shape[0], dtype=bool)\n    branches_to_remove = np.zeros(self.non_tip_vertices.shape[0], dtype=bool)\n    new_edge_pix_coord = []\n    remaining_tipped_edges_nb = 0\n    for i in range(len(self.edge_lengths)): # i = 3142 #1096 # 974 # 222\n        Y, X = self.vertices_branching_tips[i, 0], self.vertices_branching_tips[i, 1]\n        edge_bool = self.edge_pix_coord[:, 2] == i + 1\n        eY, eX = self.edge_pix_coord[edge_bool, 0], self.edge_pix_coord[edge_bool, 1]\n        if np.nanmax(self.pad_distances[(Y - 1): (Y + 2), (X - 1): (X + 2)]) &gt;= self.edge_lengths[i]:\n            tipped_edges_to_remove[i] = True\n            # Remove the edge\n            self.pad_skeleton[eY, eX] = 0\n            # Remove the tip\n            self.pad_skeleton[self.tips_coord[i, 0], self.tips_coord[i, 1]] = 0\n\n            # Remove the coordinates corresponding to that edge\n            self.edge_pix_coord = np.delete(self.edge_pix_coord, edge_bool, 0)\n\n            # check whether the connecting vertex remains a vertex of not\n            pad_sub_skeleton = np.pad(self.pad_skeleton[(Y - 2): (Y + 3), (X - 2): (X + 3)], [(1,), (1,)],\n                                      mode='constant')\n            sub_vertices, sub_tips = get_vertices_and_tips_from_skeleton(pad_sub_skeleton)\n            # If the vertex does not connect at least 3 edges anymore, remove its vertex label\n            if sub_vertices[3, 3] == 0:\n                vertex_to_remove = np.nonzero(np.logical_and(self.non_tip_vertices[:, 0] == Y, self.non_tip_vertices[:, 1] == X))[0]\n                branches_to_remove[vertex_to_remove] = True\n            # If that pixel became a tip connected to another vertex remove it from the skeleton\n            if sub_tips[3, 3]:\n                if sub_vertices[2:5, 2:5].sum() &gt; 1:\n                    self.pad_skeleton[Y, X] = 0\n                    self.edge_pix_coord = np.delete(self.edge_pix_coord, np.all(self.edge_pix_coord[:, :2] == [Y, X], axis=1), 0)\n                    vertex_to_remove = np.nonzero(np.logical_and(self.non_tip_vertices[:, 0] == Y, self.non_tip_vertices[:, 1] == X))[0]\n                    branches_to_remove[vertex_to_remove] = True\n        else:\n            remaining_tipped_edges_nb += 1\n            new_edge_pix_coord.append(np.stack((eY, eX, np.repeat(remaining_tipped_edges_nb, len(eY))), axis=1))\n\n    # Check that excedent connected components are 1 pixel size, if so:\n    # It means that they were neighbors to removed tips and not necessary for the skeleton\n    nb, sh = cv2.connectedComponents(self.pad_skeleton)\n    if nb &gt; 2:\n        logging.error(\"Removing small tipped edges split the skeleton\")\n        # for i in range(2, nb):\n        #     excedent = sh == i\n        #     if (excedent).sum() == 1:\n        #         self.pad_skeleton[excedent] = 0\n\n    # Remove in distances the pixels removed in skeleton:\n    self.pad_distances *= self.pad_skeleton\n\n    # update edge_pix_coord\n    if len(new_edge_pix_coord) &gt; 0:\n        self.edge_pix_coord = np.vstack(new_edge_pix_coord)\n\n    # # Remove tips connected to very small edges\n    # self.tips_coord = np.delete(self.tips_coord, tipped_edges_to_remove, 0)\n    # # Add corresponding edge names\n    # self.tips_coord = np.hstack((self.tips_coord, np.arange(1, len(self.tips_coord) + 1)[:, None]))\n\n    # # Within all branching (non-tip) vertices, keep those that did not lose their vertex status because of the edge removal\n    # self.non_tip_vertices = np.delete(self.non_tip_vertices, branches_to_remove, 0)\n\n    # # Get the branching vertices who kept their typped edge\n    # self.vertices_branching_tips = np.delete(self.vertices_branching_tips, tipped_edges_to_remove, 0)\n\n    # Within all branching (non-tip) vertices, keep those that do not connect a tipped edge.\n    # v_branching_tips_in_branching_v = find_common_coord(self.non_tip_vertices, self.vertices_branching_tips[:, :2])\n    # self.remaining_vertices = np.delete(self.non_tip_vertices, v_branching_tips_in_branching_v, 0)\n    # ordered_v_coord = np.vstack((self.tips_coord[:, :2], self.vertices_branching_tips[:, :2], self.remaining_vertices))\n\n    # tips = self.tips_coord\n    # branching_any_edge = self.non_tip_vertices\n    # branching_typped_edges = self.vertices_branching_tips\n    # branching_no_typped_edges = self.remaining_vertices\n\n    self.get_vertices_and_tips_coord()\n    self.get_tipped_edges()\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.EdgeIdentification.run_edge_identification","title":"<code>run_edge_identification()</code>","text":"<p>Run the edge identification process.</p> <p>This method orchestrates a series of steps to identify and label edges within the graph structure. Each step handles a specific aspect of edge identification, ultimately leading to a clearer and more refined edge network.</p> <p>Steps involved: 1. Get vertices and tips coordinates. 2. Identify tipped edges. 3. Remove tipped edges smaller than branch width. 4. Label tipped edges and their vertices. 5. Label edges connected with vertex clusters. 6. Label edges connecting vertex clusters. 7. Label edges from known vertices iteratively. 8. Label edges looping on 1 vertex. 9. Clear areas with 1 or 2 unidentified pixels. 10. Clear edge duplicates. 11. Clear vertices connecting 2 edges.</p> Source code in <code>src/cellects/image_analysis/network_functions.py</code> <pre><code>def run_edge_identification(self):\n    \"\"\"\n    Run the edge identification process.\n\n    This method orchestrates a series of steps to identify and label edges\n    within the graph structure. Each step handles a specific aspect of edge\n    identification, ultimately leading to a clearer and more refined edge network.\n\n    Steps involved:\n    1. Get vertices and tips coordinates.\n    2. Identify tipped edges.\n    3. Remove tipped edges smaller than branch width.\n    4. Label tipped edges and their vertices.\n    5. Label edges connected with vertex clusters.\n    6. Label edges connecting vertex clusters.\n    7. Label edges from known vertices iteratively.\n    8. Label edges looping on 1 vertex.\n    9. Clear areas with 1 or 2 unidentified pixels.\n    10. Clear edge duplicates.\n    11. Clear vertices connecting 2 edges.\n    \"\"\"\n    self.get_vertices_and_tips_coord()\n    self.get_tipped_edges()\n    self.remove_tipped_edge_smaller_than_branch_width()\n    self.label_tipped_edges_and_their_vertices()\n    self.check_vertex_existence()\n    self.label_edges_connected_with_vertex_clusters()\n    self.label_edges_connecting_vertex_clusters()\n    self.label_edges_from_known_vertices_iteratively()\n    self.label_edges_looping_on_1_vertex()\n    self.clear_areas_of_1_or_2_unidentified_pixels()\n    self.clear_edge_duplicates()\n    self.clear_vertices_connecting_2_edges()\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.NetworkDetection","title":"<code>NetworkDetection</code>","text":"<p>NetworkDetection</p> <p>Class for detecting vessels in images using Frangi and Sato filters with various parameter sets. It applies different thresholding methods, calculates quality metrics, and selects the best detection method.</p> Source code in <code>src/cellects/image_analysis/network_functions.py</code> <pre><code>class  NetworkDetection:\n    \"\"\"\n    NetworkDetection\n\n    Class for detecting vessels in images using Frangi and Sato filters with various parameter sets.\n    It applies different thresholding methods, calculates quality metrics, and selects the best detection method.\n    \"\"\"\n    def __init__(self, greyscale_image: NDArray[np.uint8], possibly_filled_pixels: NDArray[np.uint8]=None, add_rolling_window: bool=False, origin_to_add: NDArray[np.uint8]=None, best_result: dict=None):\n        \"\"\"\n        Initialize the object with given parameters.\n\n        Parameters\n        ----------\n        greyscale_image : NDArray[np.uint8]\n            The input greyscale image.\n        possibly_filled_pixels : NDArray[np.uint8], optional\n            Image containing possibly filled pixels. Defaults to None.\n        add_rolling_window : bool, optional\n            Flag to add rolling window. Defaults to False.\n        origin_to_add : NDArray[np.uint8], optional\n            Origin to add. Defaults to None.\n        best_result : dict, optional\n            Best result dictionary. Defaults to None.\n        \"\"\"\n        self.greyscale_image = greyscale_image\n        if possibly_filled_pixels is None:\n            self.possibly_filled_pixels = np.ones(self.greyscale_image.shape, dtype=np.uint8)\n        else:\n            self.possibly_filled_pixels = possibly_filled_pixels\n        self.best_result = best_result\n        self.add_rolling_window = add_rolling_window\n        self.origin_to_add = origin_to_add\n        self.frangi_beta = 1.\n        self.frangi_gamma = 1.\n        self.black_ridges = True\n\n    def apply_frangi_variations(self) -&gt; list:\n        \"\"\"\n        Applies various Frangi filter variations with different sigma values and thresholding methods.\n\n        This method applies the Frangi vesselness filter with multiple sets of sigma values\n        to detect vessels at different scales. It applies both Otsu thresholding and rolling window\n        segmentation to the filtered results and calculates binary quality indices.\n\n        Returns\n        -------\n        results : list of dict\n            A list containing dictionaries with the method name, binary result, quality index,\n            filtered image, filter type, rolling window flag, and sigma values used.\n        \"\"\"\n        results = []\n\n        # Parameter variations for Frangi filter\n        frangi_sigmas = {\n            's_fine_vessels': [0.75],\n            'fine_vessels': [0.5, 1.0],  # Very fine capillaries, thin fibers\n            'small_vessels': [1.0, 2.0],  # Small vessels, fine structures\n            'multi_scale_medium': [1.0, 2.0, 3.0],  # Standard multi-scale\n            'ultra_fine': [0.3, 0.5, 0.8],  # Ultra-fine structures\n            'comprehensive': [0.5, 1.0, 2.0, 4.0],  # Multi-scale\n            'retinal_vessels': [1.0, 2.0, 4.0, 8.0],  # Optimized for retinal imaging\n            'microscopy': [0.5, 1.0, 1.5, 2.5],  # Microscopy applications\n            'broad_spectrum': [0.5, 1.5, 3.0, 6.0, 10.0]\n        }\n\n        for i, (key, sigmas) in enumerate(frangi_sigmas.items()):\n            # Apply Frangi filter\n            frangi_result = frangi(self.greyscale_image, sigmas=sigmas, beta=self.frangi_beta, gamma=self.frangi_gamma, black_ridges=self.black_ridges)\n\n            # Apply both thresholding methods\n            # Method 1: Otsu thresholding\n            thresh_otsu = threshold_otsu(frangi_result)\n            binary_otsu = frangi_result &gt; thresh_otsu\n            quality_otsu = binary_quality_index(self.possibly_filled_pixels * binary_otsu)\n\n            # Method 2: Rolling window thresholding\n\n            # Store results\n            results.append({\n                'method': f'f_{sigmas}_thresh',\n                'binary': binary_otsu,\n                'quality': quality_otsu,\n                'filtered': frangi_result,\n                'filter': f'Frangi',\n                'rolling_window': False,\n                'sigmas': sigmas\n            })\n            # Method 2: Rolling window thresholding\n            if self.add_rolling_window:\n                binary_rolling = rolling_window_segmentation(frangi_result, self.possibly_filled_pixels, patch_size=(10, 10))\n                quality_rolling = binary_quality_index(binary_rolling)\n                results.append({\n                    'method': f'f_{sigmas}_roll',\n                    'binary': binary_rolling,\n                    'quality': quality_rolling,\n                    'filtered': frangi_result,\n                    'filter': f'Frangi',\n                    'rolling_window': True,\n                    'sigmas': sigmas\n                })\n\n        return results\n\n\n    def apply_sato_variations(self) -&gt; list:\n        \"\"\"\n        Apply various Sato filter variations to an image and store the results.\n\n        This function applies different parameter sets for the Sato vesselness\n        filter to an image, applies two thresholding methods (Otsu and rolling window),\n        and stores the results. The function supports optional rolling window\n        segmentation based on a configuration flag.\n\n        Returns\n        -------\n        list of dict\n            A list containing dictionaries with the results for each filter variation.\n            Each dictionary includes method name, binary image, quality index,\n            filtered result, filter type, rolling window flag, and sigma values.\n        \"\"\"\n        results = []\n\n        # Parameter variations for Frangi filter\n        sato_sigmas = {\n            'super_small_tubes': [0.01, 0.05, 0.1, 0.15],  #\n            'small_tubes': [0.1, 0.2, 0.4, 0.8],  #\n            's_thick_ridges': [0.25, 0.75],  # Thick ridges/tubes\n            'small_multi_scale': [0.1, 0.2, 0.4, 0.8, 1.6],  #\n            'fine_ridges': [0.8, 1.5],  # Fine ridge detection\n            'medium_ridges': [1.5, 3.0],  # Medium ridge structures\n            'multi_scale_fine': [0.8, 1.5, 2.5],  # Multi-scale fine detection\n            'multi_scale_standard': [1.0, 2.5, 5.0],  # Standard multi-scale\n            'edge_enhanced': [0.5, 1.0, 2.0],  # Edge-enhanced detection\n            'noise_robust': [1.5, 2.5, 4.0],  # Robust to noise\n            'fingerprint': [1.0, 1.5, 2.0, 3.0],  # Fingerprint ridge detection\n            'geological': [2.0, 5.0, 10.0, 15.0]  # Geological structures\n        }\n\n        for i, (key, sigmas) in enumerate(sato_sigmas.items()):\n            # Apply sato filter\n            sato_result = sato(self.greyscale_image, sigmas=sigmas, black_ridges=self.black_ridges, mode='reflect')\n\n            # Apply both thresholding methods\n            # Method 1: Otsu thresholding\n            thresh_otsu = threshold_otsu(sato_result)\n            binary_otsu = sato_result &gt; thresh_otsu\n            quality_otsu = binary_quality_index(self.possibly_filled_pixels * binary_otsu)\n\n\n            # Store results\n            results.append({\n                'method': f's_{sigmas}_thresh',\n                'binary': binary_otsu,\n                'quality': quality_otsu,\n                'filtered': sato_result,\n                'filter': f'Sato',\n                'rolling_window': False,\n                'sigmas': sigmas\n            })\n\n            # Method 2: Rolling window thresholding\n            if self.add_rolling_window:\n                binary_rolling = rolling_window_segmentation(sato_result, self.possibly_filled_pixels, patch_size=(10, 10))\n                quality_rolling = binary_quality_index(binary_rolling)\n\n                results.append({\n                    'method': f's_{sigmas}_roll',\n                    'binary': binary_rolling,\n                    'quality': quality_rolling,\n                    'filtered': sato_result,\n                    'filter': f'Sato',\n                    'rolling_window': True,\n                    'sigmas': sigmas\n                })\n\n        return results\n\n    def get_best_network_detection_method(self):\n        \"\"\"\n        Get the best network detection method based on quality metrics.\n\n        This function applies Frangi and Sato variations, combines their results,\n        calculates quality metrics for each result, and selects the best method.\n\n        Attributes\n        ----------\n        all_results : list of dicts\n            Combined results from Frangi and Sato variations.\n        quality_metrics : ndarray of float64\n            Quality metrics for each detection result.\n        best_idx : int\n            Index of the best detection method based on quality metrics.\n        best_result : dict\n            The best detection result from all possible methods.\n        incomplete_network : ndarray of bool\n            Binary representation of the best detection result.\n\n        Examples\n        ----------\n        &gt;&gt;&gt; possibly_filled_pixels = np.zeros((9, 9), dtype=np.uint8)\n        &gt;&gt;&gt; possibly_filled_pixels[3:6, 3:6] = 1\n        &gt;&gt;&gt; possibly_filled_pixels[1:6, 3] = 1\n        &gt;&gt;&gt; possibly_filled_pixels[6:-1, 5] = 1\n        &gt;&gt;&gt; possibly_filled_pixels[4, 1:-1] = 1\n        &gt;&gt;&gt; greyscale_image = possibly_filled_pixels.copy()\n        &gt;&gt;&gt; greyscale_image[greyscale_image &gt; 0] = np.random.randint(170, 255, possibly_filled_pixels.sum())\n        &gt;&gt;&gt; greyscale_image[greyscale_image == 0] = np.random.randint(0, 120, possibly_filled_pixels.size - possibly_filled_pixels.sum())\n        &gt;&gt;&gt; add_rolling_window=False\n        &gt;&gt;&gt; origin_to_add = np.zeros((9, 9), dtype=np.uint8)\n        &gt;&gt;&gt; origin_to_add[3:6, 3:6] = 1\n        &gt;&gt;&gt; NetDet = NetworkDetection(greyscale_image, possibly_filled_pixels, add_rolling_window, origin_to_add)\n        &gt;&gt;&gt; NetDet.get_best_network_detection_method()\n        &gt;&gt;&gt; print(NetDet.best_result['method'])\n        &gt;&gt;&gt; print(NetDet.best_result['binary'])\n        &gt;&gt;&gt; print(NetDet.best_result['quality'])\n        &gt;&gt;&gt; print(NetDet.best_result['filtered'])\n        &gt;&gt;&gt; print(NetDet.best_result['filter'])\n        &gt;&gt;&gt; print(NetDet.best_result['rolling_window'])\n        &gt;&gt;&gt; print(NetDet.best_result['sigmas'])\n        bgr_image = np.random.randint(0, 256, (100, 100, 3), dtype=np.uint8)\n        \"\"\"\n        frangi_res = self.apply_frangi_variations()\n        sato_res = self.apply_sato_variations()\n        self.all_results = frangi_res + sato_res\n        self.quality_metrics = np.array([result['quality'] for result in self.all_results])\n        self.best_idx = np.argmax(self.quality_metrics)\n        self.best_result = self.all_results[self.best_idx]\n        self.incomplete_network = self.best_result['binary'] * self.possibly_filled_pixels\n\n\n    def detect_network(self):\n        \"\"\"\n        Process and detect network features in the greyscale image.\n\n        This method applies a frangi or sato filter based on the best result and\n        performs segmentation using either rolling window or Otsu's thresholding.\n        The final network detection result is stored in `self.incomplete_network`.\n        \"\"\"\n        if self.best_result['filter'] == 'Frangi':\n            filtered_result = frangi(self.greyscale_image, sigmas=self.best_result['sigmas'], beta=self.frangi_beta, gamma=self.frangi_gamma, black_ridges=self.black_ridges)\n        else:\n            filtered_result = sato(self.greyscale_image, sigmas=self.best_result['sigmas'], black_ridges=self.black_ridges, mode='reflect')\n\n        if self.best_result['rolling_window']:\n            binary_image = rolling_window_segmentation(filtered_result, self.possibly_filled_pixels, patch_size=(10, 10))\n        else:\n            thresh_otsu = threshold_otsu(filtered_result)\n            binary_image = filtered_result &gt; thresh_otsu\n        self.incomplete_network = binary_image * self.possibly_filled_pixels\n\n    def change_greyscale(self, img: NDArray[np.uint8], first_dict: dict):\n        \"\"\"\n        Change the image to greyscale using color space combinations.\n\n        This function converts an input image to greyscale by generating\n        and applying a combination of color spaces specified in the dictionary.\n        The resulting greyscale image is stored as an attribute of the instance.\n\n        Parameters\n        ----------\n        img : ndarray of uint8\n            The input image to be converted to greyscale.\n        \"\"\"\n        self.greyscale_image, g2, all_c_spaces, first_pc_vector  = generate_color_space_combination(img, list(first_dict.keys()), first_dict)\n\n    def detect_pseudopods(self, lighter_background: bool, pseudopod_min_width: int=5, pseudopod_min_size: int=50, only_one_connected_component: bool=True):\n        \"\"\"\n        Detect pseudopods in a binary image.\n\n        Identify and process regions that resemble pseudopods based on width, size,\n        and connectivity criteria. This function is used to detect and label areas\n        that are indicative of pseudopod-like structures within a binary image.\n\n        Parameters\n        ----------\n        lighter_background : bool\n            Boolean flag to indicate if the background should be considered lighter.\n        pseudopod_min_width : int, optional\n            Minimum width for pseudopods to be considered valid. Default is 5.\n        pseudopod_min_size : int, optional\n            Minimum size for pseudopods to be considered valid. Default is 50.\n        only_one_connected_component : bool, optional\n            Flag to ensure only one connected component is kept. Default is True.\n\n        Returns\n        -------\n        None\n\n        Notes\n        -----\n        This function modifies internal attributes of the object, specifically setting `self.pseudopods` to an array indicating pseudopod regions.\n\n        Examples\n        --------\n        &gt;&gt;&gt; result = detect_pseudopods(True, 5, 50)\n        &gt;&gt;&gt; print(self.pseudopods)\n        array([[0, 1, ..., 0],\n               [0, 0, ..., 0],\n               ...,\n               [0, 1, ..., 0]], dtype=uint8)\n\n        \"\"\"\n\n        closed_im = close_holes(self.possibly_filled_pixels)\n        dist_trans = distance_transform_edt(closed_im)\n        dist_trans = dist_trans.max() - dist_trans\n        # Add dilatation of bracket of distances from medial_axis to the multiplication\n        if lighter_background:\n            grey = self.greyscale_image.max() - self.greyscale_image\n        else:\n            grey = self.greyscale_image\n        if self.origin_to_add is not None:\n            dist_trans_ori = distance_transform_edt(1 - self.origin_to_add)\n            scored_im = dist_trans * dist_trans_ori * grey\n        else:\n            scored_im = (dist_trans**2) * grey\n        scored_im = bracket_to_uint8_image_contrast(scored_im)\n        thresh = threshold_otsu(scored_im)\n        thresh = find_threshold_given_mask(scored_im, self.possibly_filled_pixels, min_threshold=thresh)\n        high_int_in_periphery = (scored_im &gt; thresh).astype(np.uint8) * self.possibly_filled_pixels\n\n        _, pseudopod_widths = morphology.medial_axis(high_int_in_periphery, return_distance=True, rng=0)\n        bin_im = pseudopod_widths &gt;= pseudopod_min_width\n        dil_bin_im = cv2.dilate(bin_im.astype(np.uint8), kernel=create_ellipse(7, 7).astype(np.uint8), iterations=1)\n        bin_im = high_int_in_periphery * dil_bin_im\n        nb, shapes, stats, centro = cv2.connectedComponentsWithStats(bin_im)\n        true_pseudopods = np.nonzero(stats[:, 4] &gt; pseudopod_min_size)[0][1:]\n        true_pseudopods = np.isin(shapes, true_pseudopods)\n\n        # Make sure that the tubes connecting two pseudopods belong to pseudopods if removing pseudopods cuts the network\n        complete_network = np.logical_or(true_pseudopods, self.incomplete_network).astype(np.uint8)\n        if only_one_connected_component:\n            complete_network = keep_one_connected_component(complete_network)\n            without_pseudopods = complete_network.copy()\n            without_pseudopods[true_pseudopods] = 0\n            only_connected_network = keep_one_connected_component(without_pseudopods)\n            self.pseudopods = (1 - only_connected_network) * complete_network  * self.possibly_filled_pixels\n        else:\n            self.pseudopods = true_pseudopods.astype(np.uint8)\n\n    def merge_network_with_pseudopods(self):\n        \"\"\"\n        Merge the incomplete network with pseudopods.\n\n        This method combines the incomplete network and pseudopods to form\n        the complete network. The incomplete network is updated by subtracting\n        areas where pseudopods are present.\n        \"\"\"\n        self.complete_network = np.logical_or(self.incomplete_network, self.pseudopods).astype(np.uint8)\n        self.incomplete_network *= (1 - self.pseudopods)\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.NetworkDetection.__init__","title":"<code>__init__(greyscale_image, possibly_filled_pixels=None, add_rolling_window=False, origin_to_add=None, best_result=None)</code>","text":"<p>Initialize the object with given parameters.</p> <p>Parameters:</p> Name Type Description Default <code>greyscale_image</code> <code>NDArray[uint8]</code> <p>The input greyscale image.</p> required <code>possibly_filled_pixels</code> <code>NDArray[uint8]</code> <p>Image containing possibly filled pixels. Defaults to None.</p> <code>None</code> <code>add_rolling_window</code> <code>bool</code> <p>Flag to add rolling window. Defaults to False.</p> <code>False</code> <code>origin_to_add</code> <code>NDArray[uint8]</code> <p>Origin to add. Defaults to None.</p> <code>None</code> <code>best_result</code> <code>dict</code> <p>Best result dictionary. Defaults to None.</p> <code>None</code> Source code in <code>src/cellects/image_analysis/network_functions.py</code> <pre><code>def __init__(self, greyscale_image: NDArray[np.uint8], possibly_filled_pixels: NDArray[np.uint8]=None, add_rolling_window: bool=False, origin_to_add: NDArray[np.uint8]=None, best_result: dict=None):\n    \"\"\"\n    Initialize the object with given parameters.\n\n    Parameters\n    ----------\n    greyscale_image : NDArray[np.uint8]\n        The input greyscale image.\n    possibly_filled_pixels : NDArray[np.uint8], optional\n        Image containing possibly filled pixels. Defaults to None.\n    add_rolling_window : bool, optional\n        Flag to add rolling window. Defaults to False.\n    origin_to_add : NDArray[np.uint8], optional\n        Origin to add. Defaults to None.\n    best_result : dict, optional\n        Best result dictionary. Defaults to None.\n    \"\"\"\n    self.greyscale_image = greyscale_image\n    if possibly_filled_pixels is None:\n        self.possibly_filled_pixels = np.ones(self.greyscale_image.shape, dtype=np.uint8)\n    else:\n        self.possibly_filled_pixels = possibly_filled_pixels\n    self.best_result = best_result\n    self.add_rolling_window = add_rolling_window\n    self.origin_to_add = origin_to_add\n    self.frangi_beta = 1.\n    self.frangi_gamma = 1.\n    self.black_ridges = True\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.NetworkDetection.apply_frangi_variations","title":"<code>apply_frangi_variations()</code>","text":"<p>Applies various Frangi filter variations with different sigma values and thresholding methods.</p> <p>This method applies the Frangi vesselness filter with multiple sets of sigma values to detect vessels at different scales. It applies both Otsu thresholding and rolling window segmentation to the filtered results and calculates binary quality indices.</p> <p>Returns:</p> Name Type Description <code>results</code> <code>list of dict</code> <p>A list containing dictionaries with the method name, binary result, quality index, filtered image, filter type, rolling window flag, and sigma values used.</p> Source code in <code>src/cellects/image_analysis/network_functions.py</code> <pre><code>def apply_frangi_variations(self) -&gt; list:\n    \"\"\"\n    Applies various Frangi filter variations with different sigma values and thresholding methods.\n\n    This method applies the Frangi vesselness filter with multiple sets of sigma values\n    to detect vessels at different scales. It applies both Otsu thresholding and rolling window\n    segmentation to the filtered results and calculates binary quality indices.\n\n    Returns\n    -------\n    results : list of dict\n        A list containing dictionaries with the method name, binary result, quality index,\n        filtered image, filter type, rolling window flag, and sigma values used.\n    \"\"\"\n    results = []\n\n    # Parameter variations for Frangi filter\n    frangi_sigmas = {\n        's_fine_vessels': [0.75],\n        'fine_vessels': [0.5, 1.0],  # Very fine capillaries, thin fibers\n        'small_vessels': [1.0, 2.0],  # Small vessels, fine structures\n        'multi_scale_medium': [1.0, 2.0, 3.0],  # Standard multi-scale\n        'ultra_fine': [0.3, 0.5, 0.8],  # Ultra-fine structures\n        'comprehensive': [0.5, 1.0, 2.0, 4.0],  # Multi-scale\n        'retinal_vessels': [1.0, 2.0, 4.0, 8.0],  # Optimized for retinal imaging\n        'microscopy': [0.5, 1.0, 1.5, 2.5],  # Microscopy applications\n        'broad_spectrum': [0.5, 1.5, 3.0, 6.0, 10.0]\n    }\n\n    for i, (key, sigmas) in enumerate(frangi_sigmas.items()):\n        # Apply Frangi filter\n        frangi_result = frangi(self.greyscale_image, sigmas=sigmas, beta=self.frangi_beta, gamma=self.frangi_gamma, black_ridges=self.black_ridges)\n\n        # Apply both thresholding methods\n        # Method 1: Otsu thresholding\n        thresh_otsu = threshold_otsu(frangi_result)\n        binary_otsu = frangi_result &gt; thresh_otsu\n        quality_otsu = binary_quality_index(self.possibly_filled_pixels * binary_otsu)\n\n        # Method 2: Rolling window thresholding\n\n        # Store results\n        results.append({\n            'method': f'f_{sigmas}_thresh',\n            'binary': binary_otsu,\n            'quality': quality_otsu,\n            'filtered': frangi_result,\n            'filter': f'Frangi',\n            'rolling_window': False,\n            'sigmas': sigmas\n        })\n        # Method 2: Rolling window thresholding\n        if self.add_rolling_window:\n            binary_rolling = rolling_window_segmentation(frangi_result, self.possibly_filled_pixels, patch_size=(10, 10))\n            quality_rolling = binary_quality_index(binary_rolling)\n            results.append({\n                'method': f'f_{sigmas}_roll',\n                'binary': binary_rolling,\n                'quality': quality_rolling,\n                'filtered': frangi_result,\n                'filter': f'Frangi',\n                'rolling_window': True,\n                'sigmas': sigmas\n            })\n\n    return results\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.NetworkDetection.apply_sato_variations","title":"<code>apply_sato_variations()</code>","text":"<p>Apply various Sato filter variations to an image and store the results.</p> <p>This function applies different parameter sets for the Sato vesselness filter to an image, applies two thresholding methods (Otsu and rolling window), and stores the results. The function supports optional rolling window segmentation based on a configuration flag.</p> <p>Returns:</p> Type Description <code>list of dict</code> <p>A list containing dictionaries with the results for each filter variation. Each dictionary includes method name, binary image, quality index, filtered result, filter type, rolling window flag, and sigma values.</p> Source code in <code>src/cellects/image_analysis/network_functions.py</code> <pre><code>def apply_sato_variations(self) -&gt; list:\n    \"\"\"\n    Apply various Sato filter variations to an image and store the results.\n\n    This function applies different parameter sets for the Sato vesselness\n    filter to an image, applies two thresholding methods (Otsu and rolling window),\n    and stores the results. The function supports optional rolling window\n    segmentation based on a configuration flag.\n\n    Returns\n    -------\n    list of dict\n        A list containing dictionaries with the results for each filter variation.\n        Each dictionary includes method name, binary image, quality index,\n        filtered result, filter type, rolling window flag, and sigma values.\n    \"\"\"\n    results = []\n\n    # Parameter variations for Frangi filter\n    sato_sigmas = {\n        'super_small_tubes': [0.01, 0.05, 0.1, 0.15],  #\n        'small_tubes': [0.1, 0.2, 0.4, 0.8],  #\n        's_thick_ridges': [0.25, 0.75],  # Thick ridges/tubes\n        'small_multi_scale': [0.1, 0.2, 0.4, 0.8, 1.6],  #\n        'fine_ridges': [0.8, 1.5],  # Fine ridge detection\n        'medium_ridges': [1.5, 3.0],  # Medium ridge structures\n        'multi_scale_fine': [0.8, 1.5, 2.5],  # Multi-scale fine detection\n        'multi_scale_standard': [1.0, 2.5, 5.0],  # Standard multi-scale\n        'edge_enhanced': [0.5, 1.0, 2.0],  # Edge-enhanced detection\n        'noise_robust': [1.5, 2.5, 4.0],  # Robust to noise\n        'fingerprint': [1.0, 1.5, 2.0, 3.0],  # Fingerprint ridge detection\n        'geological': [2.0, 5.0, 10.0, 15.0]  # Geological structures\n    }\n\n    for i, (key, sigmas) in enumerate(sato_sigmas.items()):\n        # Apply sato filter\n        sato_result = sato(self.greyscale_image, sigmas=sigmas, black_ridges=self.black_ridges, mode='reflect')\n\n        # Apply both thresholding methods\n        # Method 1: Otsu thresholding\n        thresh_otsu = threshold_otsu(sato_result)\n        binary_otsu = sato_result &gt; thresh_otsu\n        quality_otsu = binary_quality_index(self.possibly_filled_pixels * binary_otsu)\n\n\n        # Store results\n        results.append({\n            'method': f's_{sigmas}_thresh',\n            'binary': binary_otsu,\n            'quality': quality_otsu,\n            'filtered': sato_result,\n            'filter': f'Sato',\n            'rolling_window': False,\n            'sigmas': sigmas\n        })\n\n        # Method 2: Rolling window thresholding\n        if self.add_rolling_window:\n            binary_rolling = rolling_window_segmentation(sato_result, self.possibly_filled_pixels, patch_size=(10, 10))\n            quality_rolling = binary_quality_index(binary_rolling)\n\n            results.append({\n                'method': f's_{sigmas}_roll',\n                'binary': binary_rolling,\n                'quality': quality_rolling,\n                'filtered': sato_result,\n                'filter': f'Sato',\n                'rolling_window': True,\n                'sigmas': sigmas\n            })\n\n    return results\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.NetworkDetection.change_greyscale","title":"<code>change_greyscale(img, first_dict)</code>","text":"<p>Change the image to greyscale using color space combinations.</p> <p>This function converts an input image to greyscale by generating and applying a combination of color spaces specified in the dictionary. The resulting greyscale image is stored as an attribute of the instance.</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>ndarray of uint8</code> <p>The input image to be converted to greyscale.</p> required Source code in <code>src/cellects/image_analysis/network_functions.py</code> <pre><code>def change_greyscale(self, img: NDArray[np.uint8], first_dict: dict):\n    \"\"\"\n    Change the image to greyscale using color space combinations.\n\n    This function converts an input image to greyscale by generating\n    and applying a combination of color spaces specified in the dictionary.\n    The resulting greyscale image is stored as an attribute of the instance.\n\n    Parameters\n    ----------\n    img : ndarray of uint8\n        The input image to be converted to greyscale.\n    \"\"\"\n    self.greyscale_image, g2, all_c_spaces, first_pc_vector  = generate_color_space_combination(img, list(first_dict.keys()), first_dict)\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.NetworkDetection.detect_network","title":"<code>detect_network()</code>","text":"<p>Process and detect network features in the greyscale image.</p> <p>This method applies a frangi or sato filter based on the best result and performs segmentation using either rolling window or Otsu's thresholding. The final network detection result is stored in <code>self.incomplete_network</code>.</p> Source code in <code>src/cellects/image_analysis/network_functions.py</code> <pre><code>def detect_network(self):\n    \"\"\"\n    Process and detect network features in the greyscale image.\n\n    This method applies a frangi or sato filter based on the best result and\n    performs segmentation using either rolling window or Otsu's thresholding.\n    The final network detection result is stored in `self.incomplete_network`.\n    \"\"\"\n    if self.best_result['filter'] == 'Frangi':\n        filtered_result = frangi(self.greyscale_image, sigmas=self.best_result['sigmas'], beta=self.frangi_beta, gamma=self.frangi_gamma, black_ridges=self.black_ridges)\n    else:\n        filtered_result = sato(self.greyscale_image, sigmas=self.best_result['sigmas'], black_ridges=self.black_ridges, mode='reflect')\n\n    if self.best_result['rolling_window']:\n        binary_image = rolling_window_segmentation(filtered_result, self.possibly_filled_pixels, patch_size=(10, 10))\n    else:\n        thresh_otsu = threshold_otsu(filtered_result)\n        binary_image = filtered_result &gt; thresh_otsu\n    self.incomplete_network = binary_image * self.possibly_filled_pixels\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.NetworkDetection.detect_pseudopods","title":"<code>detect_pseudopods(lighter_background, pseudopod_min_width=5, pseudopod_min_size=50, only_one_connected_component=True)</code>","text":"<p>Detect pseudopods in a binary image.</p> <p>Identify and process regions that resemble pseudopods based on width, size, and connectivity criteria. This function is used to detect and label areas that are indicative of pseudopod-like structures within a binary image.</p> <p>Parameters:</p> Name Type Description Default <code>lighter_background</code> <code>bool</code> <p>Boolean flag to indicate if the background should be considered lighter.</p> required <code>pseudopod_min_width</code> <code>int</code> <p>Minimum width for pseudopods to be considered valid. Default is 5.</p> <code>5</code> <code>pseudopod_min_size</code> <code>int</code> <p>Minimum size for pseudopods to be considered valid. Default is 50.</p> <code>50</code> <code>only_one_connected_component</code> <code>bool</code> <p>Flag to ensure only one connected component is kept. Default is True.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> Notes <p>This function modifies internal attributes of the object, specifically setting <code>self.pseudopods</code> to an array indicating pseudopod regions.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; result = detect_pseudopods(True, 5, 50)\n&gt;&gt;&gt; print(self.pseudopods)\narray([[0, 1, ..., 0],\n       [0, 0, ..., 0],\n       ...,\n       [0, 1, ..., 0]], dtype=uint8)\n</code></pre> Source code in <code>src/cellects/image_analysis/network_functions.py</code> <pre><code>def detect_pseudopods(self, lighter_background: bool, pseudopod_min_width: int=5, pseudopod_min_size: int=50, only_one_connected_component: bool=True):\n    \"\"\"\n    Detect pseudopods in a binary image.\n\n    Identify and process regions that resemble pseudopods based on width, size,\n    and connectivity criteria. This function is used to detect and label areas\n    that are indicative of pseudopod-like structures within a binary image.\n\n    Parameters\n    ----------\n    lighter_background : bool\n        Boolean flag to indicate if the background should be considered lighter.\n    pseudopod_min_width : int, optional\n        Minimum width for pseudopods to be considered valid. Default is 5.\n    pseudopod_min_size : int, optional\n        Minimum size for pseudopods to be considered valid. Default is 50.\n    only_one_connected_component : bool, optional\n        Flag to ensure only one connected component is kept. Default is True.\n\n    Returns\n    -------\n    None\n\n    Notes\n    -----\n    This function modifies internal attributes of the object, specifically setting `self.pseudopods` to an array indicating pseudopod regions.\n\n    Examples\n    --------\n    &gt;&gt;&gt; result = detect_pseudopods(True, 5, 50)\n    &gt;&gt;&gt; print(self.pseudopods)\n    array([[0, 1, ..., 0],\n           [0, 0, ..., 0],\n           ...,\n           [0, 1, ..., 0]], dtype=uint8)\n\n    \"\"\"\n\n    closed_im = close_holes(self.possibly_filled_pixels)\n    dist_trans = distance_transform_edt(closed_im)\n    dist_trans = dist_trans.max() - dist_trans\n    # Add dilatation of bracket of distances from medial_axis to the multiplication\n    if lighter_background:\n        grey = self.greyscale_image.max() - self.greyscale_image\n    else:\n        grey = self.greyscale_image\n    if self.origin_to_add is not None:\n        dist_trans_ori = distance_transform_edt(1 - self.origin_to_add)\n        scored_im = dist_trans * dist_trans_ori * grey\n    else:\n        scored_im = (dist_trans**2) * grey\n    scored_im = bracket_to_uint8_image_contrast(scored_im)\n    thresh = threshold_otsu(scored_im)\n    thresh = find_threshold_given_mask(scored_im, self.possibly_filled_pixels, min_threshold=thresh)\n    high_int_in_periphery = (scored_im &gt; thresh).astype(np.uint8) * self.possibly_filled_pixels\n\n    _, pseudopod_widths = morphology.medial_axis(high_int_in_periphery, return_distance=True, rng=0)\n    bin_im = pseudopod_widths &gt;= pseudopod_min_width\n    dil_bin_im = cv2.dilate(bin_im.astype(np.uint8), kernel=create_ellipse(7, 7).astype(np.uint8), iterations=1)\n    bin_im = high_int_in_periphery * dil_bin_im\n    nb, shapes, stats, centro = cv2.connectedComponentsWithStats(bin_im)\n    true_pseudopods = np.nonzero(stats[:, 4] &gt; pseudopod_min_size)[0][1:]\n    true_pseudopods = np.isin(shapes, true_pseudopods)\n\n    # Make sure that the tubes connecting two pseudopods belong to pseudopods if removing pseudopods cuts the network\n    complete_network = np.logical_or(true_pseudopods, self.incomplete_network).astype(np.uint8)\n    if only_one_connected_component:\n        complete_network = keep_one_connected_component(complete_network)\n        without_pseudopods = complete_network.copy()\n        without_pseudopods[true_pseudopods] = 0\n        only_connected_network = keep_one_connected_component(without_pseudopods)\n        self.pseudopods = (1 - only_connected_network) * complete_network  * self.possibly_filled_pixels\n    else:\n        self.pseudopods = true_pseudopods.astype(np.uint8)\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.NetworkDetection.get_best_network_detection_method","title":"<code>get_best_network_detection_method()</code>","text":"<p>Get the best network detection method based on quality metrics.</p> <p>This function applies Frangi and Sato variations, combines their results, calculates quality metrics for each result, and selects the best method.</p> <p>Attributes:</p> Name Type Description <code>all_results</code> <code>list of dicts</code> <p>Combined results from Frangi and Sato variations.</p> <code>quality_metrics</code> <code>ndarray of float64</code> <p>Quality metrics for each detection result.</p> <code>best_idx</code> <code>int</code> <p>Index of the best detection method based on quality metrics.</p> <code>best_result</code> <code>dict</code> <p>The best detection result from all possible methods.</p> <code>incomplete_network</code> <code>ndarray of bool</code> <p>Binary representation of the best detection result.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; possibly_filled_pixels = np.zeros((9, 9), dtype=np.uint8)\n&gt;&gt;&gt; possibly_filled_pixels[3:6, 3:6] = 1\n&gt;&gt;&gt; possibly_filled_pixels[1:6, 3] = 1\n&gt;&gt;&gt; possibly_filled_pixels[6:-1, 5] = 1\n&gt;&gt;&gt; possibly_filled_pixels[4, 1:-1] = 1\n&gt;&gt;&gt; greyscale_image = possibly_filled_pixels.copy()\n&gt;&gt;&gt; greyscale_image[greyscale_image &gt; 0] = np.random.randint(170, 255, possibly_filled_pixels.sum())\n&gt;&gt;&gt; greyscale_image[greyscale_image == 0] = np.random.randint(0, 120, possibly_filled_pixels.size - possibly_filled_pixels.sum())\n&gt;&gt;&gt; add_rolling_window=False\n&gt;&gt;&gt; origin_to_add = np.zeros((9, 9), dtype=np.uint8)\n&gt;&gt;&gt; origin_to_add[3:6, 3:6] = 1\n&gt;&gt;&gt; NetDet = NetworkDetection(greyscale_image, possibly_filled_pixels, add_rolling_window, origin_to_add)\n&gt;&gt;&gt; NetDet.get_best_network_detection_method()\n&gt;&gt;&gt; print(NetDet.best_result['method'])\n&gt;&gt;&gt; print(NetDet.best_result['binary'])\n&gt;&gt;&gt; print(NetDet.best_result['quality'])\n&gt;&gt;&gt; print(NetDet.best_result['filtered'])\n&gt;&gt;&gt; print(NetDet.best_result['filter'])\n&gt;&gt;&gt; print(NetDet.best_result['rolling_window'])\n&gt;&gt;&gt; print(NetDet.best_result['sigmas'])\nbgr_image = np.random.randint(0, 256, (100, 100, 3), dtype=np.uint8)\n</code></pre> Source code in <code>src/cellects/image_analysis/network_functions.py</code> <pre><code>def get_best_network_detection_method(self):\n    \"\"\"\n    Get the best network detection method based on quality metrics.\n\n    This function applies Frangi and Sato variations, combines their results,\n    calculates quality metrics for each result, and selects the best method.\n\n    Attributes\n    ----------\n    all_results : list of dicts\n        Combined results from Frangi and Sato variations.\n    quality_metrics : ndarray of float64\n        Quality metrics for each detection result.\n    best_idx : int\n        Index of the best detection method based on quality metrics.\n    best_result : dict\n        The best detection result from all possible methods.\n    incomplete_network : ndarray of bool\n        Binary representation of the best detection result.\n\n    Examples\n    ----------\n    &gt;&gt;&gt; possibly_filled_pixels = np.zeros((9, 9), dtype=np.uint8)\n    &gt;&gt;&gt; possibly_filled_pixels[3:6, 3:6] = 1\n    &gt;&gt;&gt; possibly_filled_pixels[1:6, 3] = 1\n    &gt;&gt;&gt; possibly_filled_pixels[6:-1, 5] = 1\n    &gt;&gt;&gt; possibly_filled_pixels[4, 1:-1] = 1\n    &gt;&gt;&gt; greyscale_image = possibly_filled_pixels.copy()\n    &gt;&gt;&gt; greyscale_image[greyscale_image &gt; 0] = np.random.randint(170, 255, possibly_filled_pixels.sum())\n    &gt;&gt;&gt; greyscale_image[greyscale_image == 0] = np.random.randint(0, 120, possibly_filled_pixels.size - possibly_filled_pixels.sum())\n    &gt;&gt;&gt; add_rolling_window=False\n    &gt;&gt;&gt; origin_to_add = np.zeros((9, 9), dtype=np.uint8)\n    &gt;&gt;&gt; origin_to_add[3:6, 3:6] = 1\n    &gt;&gt;&gt; NetDet = NetworkDetection(greyscale_image, possibly_filled_pixels, add_rolling_window, origin_to_add)\n    &gt;&gt;&gt; NetDet.get_best_network_detection_method()\n    &gt;&gt;&gt; print(NetDet.best_result['method'])\n    &gt;&gt;&gt; print(NetDet.best_result['binary'])\n    &gt;&gt;&gt; print(NetDet.best_result['quality'])\n    &gt;&gt;&gt; print(NetDet.best_result['filtered'])\n    &gt;&gt;&gt; print(NetDet.best_result['filter'])\n    &gt;&gt;&gt; print(NetDet.best_result['rolling_window'])\n    &gt;&gt;&gt; print(NetDet.best_result['sigmas'])\n    bgr_image = np.random.randint(0, 256, (100, 100, 3), dtype=np.uint8)\n    \"\"\"\n    frangi_res = self.apply_frangi_variations()\n    sato_res = self.apply_sato_variations()\n    self.all_results = frangi_res + sato_res\n    self.quality_metrics = np.array([result['quality'] for result in self.all_results])\n    self.best_idx = np.argmax(self.quality_metrics)\n    self.best_result = self.all_results[self.best_idx]\n    self.incomplete_network = self.best_result['binary'] * self.possibly_filled_pixels\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.NetworkDetection.merge_network_with_pseudopods","title":"<code>merge_network_with_pseudopods()</code>","text":"<p>Merge the incomplete network with pseudopods.</p> <p>This method combines the incomplete network and pseudopods to form the complete network. The incomplete network is updated by subtracting areas where pseudopods are present.</p> Source code in <code>src/cellects/image_analysis/network_functions.py</code> <pre><code>def merge_network_with_pseudopods(self):\n    \"\"\"\n    Merge the incomplete network with pseudopods.\n\n    This method combines the incomplete network and pseudopods to form\n    the complete network. The incomplete network is updated by subtracting\n    areas where pseudopods are present.\n    \"\"\"\n    self.complete_network = np.logical_or(self.incomplete_network, self.pseudopods).astype(np.uint8)\n    self.incomplete_network *= (1 - self.pseudopods)\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.PickleRick","title":"<code>PickleRick</code>","text":"<p>A class to handle safe file reading and writing operations using pickle.</p> <p>This class ensures that files are not being accessed concurrently by creating a lock file (PickleRickX.pkl) to signal that the file is open. It includes methods to check for the lock file, write data safely, and read data safely.</p> Source code in <code>src/cellects/utils/load_display_save.py</code> <pre><code>class PickleRick:\n    \"\"\"\n    A class to handle safe file reading and writing operations using pickle.\n\n    This class ensures that files are not being accessed concurrently by\n    creating a lock file (PickleRickX.pkl) to signal that the file is open.\n    It includes methods to check for the lock file, write data safely,\n    and read data safely.\n    \"\"\"\n    def __init__(self, pickle_rick_number=\"\"):\n        \"\"\"\n        Initialize a new instance of the class.\n\n        This constructor sets up initial attributes for tracking Rick's state, including\n        a boolean flag for waiting for Pickle Rick, a counter, the provided pickle Rick number,\n        and the time when the first check was performed.\n\n        Parameters\n        ----------\n        pickle_rick_number : str, optional\n            The number associated with Pickle Rick. Defaults to an empty string.\n        \"\"\"\n        self.wait_for_pickle_rick: bool = False\n        self.counter = 0\n        self.pickle_rick_number = pickle_rick_number\n        self.first_check_time = default_timer()\n\n    def _check_that_file_is_not_open(self):\n        \"\"\"\n        Check if a specific pickle file exists and handle it accordingly.\n\n        This function checks whether a file named `PickleRick{self.pickle_rick_number}.pkl`\n        exists. If the file has not been modified for more than 2 seconds, it is removed.\n        The function then updates an attribute to indicate whether the file exists.\n\n        Parameters\n        ----------\n        self : PickleRickObject\n            The instance of the class containing this method.\n\n        Returns\n        -------\n        None\n            This function does not return any value.\n            It updates the `self.wait_for_pickle_rick` attribute.\n\n        Notes\n        -----\n        This function removes the pickle file if it has not been modified for more than 2 seconds.\n        The `self.wait_for_pickle_rick` attribute is updated based on the existence of the file.\n        \"\"\"\n        if os.path.isfile(f\"PickleRick{self.pickle_rick_number}.pkl\"):\n            if default_timer() - self.first_check_time &gt; 2:\n                os.remove(f\"PickleRick{self.pickle_rick_number}.pkl\")\n            # logging.error((f\"Cannot read/write, Trying again... tip: unlock by deleting the file named PickleRick{self.pickle_rick_number}.pkl\"))\n        self.wait_for_pickle_rick = os.path.isfile(f\"PickleRick{self.pickle_rick_number}.pkl\")\n\n    def _write_pickle_rick(self):\n        \"\"\"\n        Write pickle data to a file for Pickle Rick.\n\n        Parameters\n        ----------\n        self : object\n            The instance of the class that this method belongs to.\n            This typically contains attributes and methods relevant to managing\n            pickle operations for Pickle Rick.\n\n        Raises\n        ------\n        Exception\n            General exception raised if there is any issue with writing the file.\n            The error details are logged.\n\n        Notes\n        -----\n        This function creates a file named `PickleRick{self.pickle_rick_number}.pkl`\n        with a dictionary indicating readiness for Pickle Rick.\n\n        Examples\n        --------\n        &gt;&gt;&gt; obj = PickleRick()  # Assuming `YourClassInstance` is the class containing this method\n        &gt;&gt;&gt; obj.pickle_rick_number = 1  # Set an example value for the attribute\n        &gt;&gt;&gt; obj._write_pickle_rick()     # Call the method to create and write to file\n        \"\"\"\n        try:\n            with open(f\"PickleRick{self.pickle_rick_number}.pkl\", 'wb') as file_to_write:\n                pickle.dump({'wait_for_pickle_rick': True}, file_to_write)\n        except Exception as exc:\n            logging.error(f\"Don't know how but Pickle Rick failed... Error is: {exc}\")\n\n    def _delete_pickle_rick(self):\n        \"\"\"\n\n        Delete a specific Pickle Rick file.\n\n        Deletes the pickle file associated with the current instance's\n        `pickle_rick_number`.\n\n        Raises\n        ------\n        FileNotFoundError\n            If the file with name `PickleRick{self.pickle_rick_number}.pkl` does not exist.\n        \"\"\"\n        if os.path.isfile(f\"PickleRick{self.pickle_rick_number}.pkl\"):\n            os.remove(f\"PickleRick{self.pickle_rick_number}.pkl\")\n\n    def write_file(self, file_content, file_name):\n        \"\"\"\n        Write content to a file with error handling and retry logic.\n\n        This function attempts to write the provided content into a file.\n        If it fails, it retries up to 100 times with some additional checks\n        and delays. Note that the content is serialized using pickle.\n\n        Parameters\n        ----------\n        file_content : Any\n            The data to be written into the file. This will be pickled.\n        file_name : str\n            The name of the file where data should be written.\n\n        Returns\n        -------\n        None\n\n        Raises\n        ------\n        Exception\n            If the file cannot be written after 100 attempts, an error is logged.\n\n        Notes\n        -----\n        This function uses pickle to serialize the data, which can introduce security risks\n        if untrusted content is being written. It performs some internal state checks,\n        such as verifying that the target file isn't open and whether it should delete\n        some internal state, represented by `_delete_pickle_rick`.\n\n        The function implements a retry mechanism with a backoff strategy that can include\n        random delays, though the example code does not specify these details explicitly.\n\n        Examples\n        --------\n        &gt;&gt;&gt; result = PickleRick().write_file({'key': 'value'}, 'test.pkl')\n        Success to write file\n        \"\"\"\n        self.counter += 1\n        if self.counter &lt; 100:\n            if self.counter &gt; 95:\n                self._delete_pickle_rick()\n            # time.sleep(np.random.choice(np.arange(1, os.cpu_count(), 0.5)))\n            self._check_that_file_is_not_open()\n            if self.wait_for_pickle_rick:\n                time.sleep(2)\n                self.write_file(file_content, file_name)\n            else:\n                self._write_pickle_rick()\n                try:\n                    with open(file_name, 'wb') as file_to_write:\n                        pickle.dump(file_content, file_to_write, protocol=0)\n                    self._delete_pickle_rick()\n                    logging.info(f\"Success to write file\")\n                except Exception as exc:\n                    logging.error(f\"The Pickle error on the file {file_name} is: {exc}\")\n                    self._delete_pickle_rick()\n                    self.write_file(file_content, file_name)\n        else:\n            logging.error(f\"Failed to write {file_name}\")\n\n    def read_file(self, file_name):\n        \"\"\"\n        Reads the contents of a file using pickle and returns it.\n\n        Parameters\n        ----------\n        file_name : str\n            The name of the file to be read.\n\n        Returns\n        -------\n        Union[Any, None]\n            The content of the file if successfully read; otherwise, `None`.\n\n        Raises\n        ------\n        Exception\n            If there is an error reading the file.\n\n        Notes\n        -----\n        This function attempts to read a file multiple times if it fails.\n        If the number of attempts exceeds 1000, it logs an error and returns `None`.\n\n        Examples\n        --------\n        &gt;&gt;&gt; PickleRick().read_file(\"example.pkl\")\n        \"\"\"\n        self.counter += 1\n        if self.counter &lt; 1000:\n            if self.counter &gt; 950:\n                self._delete_pickle_rick()\n            self._check_that_file_is_not_open()\n            if self.wait_for_pickle_rick:\n                time.sleep(2)\n                self.read_file(file_name)\n            else:\n                self._write_pickle_rick()\n                try:\n                    with open(file_name, 'rb') as fileopen:\n                        file_content = pickle.load(fileopen)\n                except Exception as exc:\n                    logging.error(f\"The Pickle error on the file {file_name} is: {exc}\")\n                    file_content = None\n                self._delete_pickle_rick()\n                if file_content is None:\n                    self.read_file(file_name)\n                else:\n                    logging.info(f\"Success to read file\")\n                return file_content\n        else:\n            logging.error(f\"Failed to read {file_name}\")\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.PickleRick.__init__","title":"<code>__init__(pickle_rick_number='')</code>","text":"<p>Initialize a new instance of the class.</p> <p>This constructor sets up initial attributes for tracking Rick's state, including a boolean flag for waiting for Pickle Rick, a counter, the provided pickle Rick number, and the time when the first check was performed.</p> <p>Parameters:</p> Name Type Description Default <code>pickle_rick_number</code> <code>str</code> <p>The number associated with Pickle Rick. Defaults to an empty string.</p> <code>''</code> Source code in <code>src/cellects/utils/load_display_save.py</code> <pre><code>def __init__(self, pickle_rick_number=\"\"):\n    \"\"\"\n    Initialize a new instance of the class.\n\n    This constructor sets up initial attributes for tracking Rick's state, including\n    a boolean flag for waiting for Pickle Rick, a counter, the provided pickle Rick number,\n    and the time when the first check was performed.\n\n    Parameters\n    ----------\n    pickle_rick_number : str, optional\n        The number associated with Pickle Rick. Defaults to an empty string.\n    \"\"\"\n    self.wait_for_pickle_rick: bool = False\n    self.counter = 0\n    self.pickle_rick_number = pickle_rick_number\n    self.first_check_time = default_timer()\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.PickleRick.read_file","title":"<code>read_file(file_name)</code>","text":"<p>Reads the contents of a file using pickle and returns it.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>The name of the file to be read.</p> required <p>Returns:</p> Type Description <code>Union[Any, None]</code> <p>The content of the file if successfully read; otherwise, <code>None</code>.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If there is an error reading the file.</p> Notes <p>This function attempts to read a file multiple times if it fails. If the number of attempts exceeds 1000, it logs an error and returns <code>None</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; PickleRick().read_file(\"example.pkl\")\n</code></pre> Source code in <code>src/cellects/utils/load_display_save.py</code> <pre><code>def read_file(self, file_name):\n    \"\"\"\n    Reads the contents of a file using pickle and returns it.\n\n    Parameters\n    ----------\n    file_name : str\n        The name of the file to be read.\n\n    Returns\n    -------\n    Union[Any, None]\n        The content of the file if successfully read; otherwise, `None`.\n\n    Raises\n    ------\n    Exception\n        If there is an error reading the file.\n\n    Notes\n    -----\n    This function attempts to read a file multiple times if it fails.\n    If the number of attempts exceeds 1000, it logs an error and returns `None`.\n\n    Examples\n    --------\n    &gt;&gt;&gt; PickleRick().read_file(\"example.pkl\")\n    \"\"\"\n    self.counter += 1\n    if self.counter &lt; 1000:\n        if self.counter &gt; 950:\n            self._delete_pickle_rick()\n        self._check_that_file_is_not_open()\n        if self.wait_for_pickle_rick:\n            time.sleep(2)\n            self.read_file(file_name)\n        else:\n            self._write_pickle_rick()\n            try:\n                with open(file_name, 'rb') as fileopen:\n                    file_content = pickle.load(fileopen)\n            except Exception as exc:\n                logging.error(f\"The Pickle error on the file {file_name} is: {exc}\")\n                file_content = None\n            self._delete_pickle_rick()\n            if file_content is None:\n                self.read_file(file_name)\n            else:\n                logging.info(f\"Success to read file\")\n            return file_content\n    else:\n        logging.error(f\"Failed to read {file_name}\")\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.PickleRick.write_file","title":"<code>write_file(file_content, file_name)</code>","text":"<p>Write content to a file with error handling and retry logic.</p> <p>This function attempts to write the provided content into a file. If it fails, it retries up to 100 times with some additional checks and delays. Note that the content is serialized using pickle.</p> <p>Parameters:</p> Name Type Description Default <code>file_content</code> <code>Any</code> <p>The data to be written into the file. This will be pickled.</p> required <code>file_name</code> <code>str</code> <p>The name of the file where data should be written.</p> required <p>Returns:</p> Type Description <code>None</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If the file cannot be written after 100 attempts, an error is logged.</p> Notes <p>This function uses pickle to serialize the data, which can introduce security risks if untrusted content is being written. It performs some internal state checks, such as verifying that the target file isn't open and whether it should delete some internal state, represented by <code>_delete_pickle_rick</code>.</p> <p>The function implements a retry mechanism with a backoff strategy that can include random delays, though the example code does not specify these details explicitly.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; result = PickleRick().write_file({'key': 'value'}, 'test.pkl')\nSuccess to write file\n</code></pre> Source code in <code>src/cellects/utils/load_display_save.py</code> <pre><code>def write_file(self, file_content, file_name):\n    \"\"\"\n    Write content to a file with error handling and retry logic.\n\n    This function attempts to write the provided content into a file.\n    If it fails, it retries up to 100 times with some additional checks\n    and delays. Note that the content is serialized using pickle.\n\n    Parameters\n    ----------\n    file_content : Any\n        The data to be written into the file. This will be pickled.\n    file_name : str\n        The name of the file where data should be written.\n\n    Returns\n    -------\n    None\n\n    Raises\n    ------\n    Exception\n        If the file cannot be written after 100 attempts, an error is logged.\n\n    Notes\n    -----\n    This function uses pickle to serialize the data, which can introduce security risks\n    if untrusted content is being written. It performs some internal state checks,\n    such as verifying that the target file isn't open and whether it should delete\n    some internal state, represented by `_delete_pickle_rick`.\n\n    The function implements a retry mechanism with a backoff strategy that can include\n    random delays, though the example code does not specify these details explicitly.\n\n    Examples\n    --------\n    &gt;&gt;&gt; result = PickleRick().write_file({'key': 'value'}, 'test.pkl')\n    Success to write file\n    \"\"\"\n    self.counter += 1\n    if self.counter &lt; 100:\n        if self.counter &gt; 95:\n            self._delete_pickle_rick()\n        # time.sleep(np.random.choice(np.arange(1, os.cpu_count(), 0.5)))\n        self._check_that_file_is_not_open()\n        if self.wait_for_pickle_rick:\n            time.sleep(2)\n            self.write_file(file_content, file_name)\n        else:\n            self._write_pickle_rick()\n            try:\n                with open(file_name, 'wb') as file_to_write:\n                    pickle.dump(file_content, file_to_write, protocol=0)\n                self._delete_pickle_rick()\n                logging.info(f\"Success to write file\")\n            except Exception as exc:\n                logging.error(f\"The Pickle error on the file {file_name} is: {exc}\")\n                self._delete_pickle_rick()\n                self.write_file(file_content, file_name)\n    else:\n        logging.error(f\"Failed to write {file_name}\")\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.ad_pad","title":"<code>ad_pad(arr)</code>","text":"<p>Pad the input array with a single layer of zeros around its edges.</p> <p>Parameters:</p> Name Type Description Default <code>arr</code> <code>ndarray</code> <p>The input array to pad. Must be at least 2-dimensional.</p> required <p>Returns:</p> Name Type Description <code>padded_arr</code> <code>ndarray</code> <p>The output array with a single 0-padded layer around its edges.</p> Notes <p>This function uses NumPy's <code>pad</code> with mode='constant' to add a single layer of zeros around the edges of the input array.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; arr = np.array([[1, 2], [3, 4]])\n&gt;&gt;&gt; ad_pad(arr)\narray([[0, 0, 0, 0],\n   [0, 1, 2, 0],\n   [0, 3, 4, 0],\n   [0, 0, 0, 0]])\n</code></pre> Source code in <code>src/cellects/image_analysis/network_functions.py</code> <pre><code>def ad_pad(arr: NDArray) -&gt; NDArray:\n    \"\"\"\n    Pad the input array with a single layer of zeros around its edges.\n\n    Parameters\n    ----------\n    arr : ndarray\n        The input array to pad. Must be at least 2-dimensional.\n\n    Returns\n    -------\n    padded_arr : ndarray\n        The output array with a single 0-padded layer around its edges.\n\n    Notes\n    -----\n    This function uses NumPy's `pad` with mode='constant' to add a single layer\n    of zeros around the edges of the input array.\n\n    Examples\n    --------\n    &gt;&gt;&gt; arr = np.array([[1, 2], [3, 4]])\n    &gt;&gt;&gt; ad_pad(arr)\n    array([[0, 0, 0, 0],\n       [0, 1, 2, 0],\n       [0, 3, 4, 0],\n       [0, 0, 0, 0]])\n    \"\"\"\n    return np.pad(arr, [(1, ), (1, )], mode='constant')\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.add_padding","title":"<code>add_padding(array_list)</code>","text":"<p>Add padding to each 2D array in a list.</p> <p>Parameters:</p> Name Type Description Default <code>array_list</code> <code>list of ndarrays</code> <p>List of 2D NumPy arrays to be processed.</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>list of ndarrays</code> <p>List of 2D NumPy arrays with the padding removed.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; array_list = [np.array([[1, 2], [3, 4]])]\n&gt;&gt;&gt; padded_list = add_padding(array_list)\n&gt;&gt;&gt; print(padded_list[0])\n[[0 0 0]\n [0 1 2 0]\n [0 3 4 0]\n [0 0 0]]\n</code></pre> Source code in <code>src/cellects/image_analysis/network_functions.py</code> <pre><code>def add_padding(array_list: list) -&gt; list:\n    \"\"\"\n    Add padding to each 2D array in a list.\n\n    Parameters\n    ----------\n    array_list : list of ndarrays\n        List of 2D NumPy arrays to be processed.\n\n    Returns\n    -------\n    out : list of ndarrays\n        List of 2D NumPy arrays with the padding removed.\n\n    Examples\n    --------\n    &gt;&gt;&gt; array_list = [np.array([[1, 2], [3, 4]])]\n    &gt;&gt;&gt; padded_list = add_padding(array_list)\n    &gt;&gt;&gt; print(padded_list[0])\n    [[0 0 0]\n     [0 1 2 0]\n     [0 3 4 0]\n     [0 0 0]]\n    \"\"\"\n    new_array_list = []\n    for arr in array_list:\n        new_array_list.append(ad_pad(arr))\n    return new_array_list\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.bracket_to_uint8_image_contrast","title":"<code>bracket_to_uint8_image_contrast(image)</code>","text":"<p>Convert an image with bracket contrast values to uint8 type.</p> <p>This function normalizes an input image by scaling the minimum and maximum values of the image to the range [0, 255] and then converts it to uint8 data type.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>Input image as a numpy array with floating-point values.</p> required <p>Returns:</p> Type Description <code>ndarray of uint8</code> <p>Output image converted to uint8 type after normalization.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; image = np.random.randint(0, 255, (10, 10), dtype=np.uint8)\n&gt;&gt;&gt; res = bracket_to_uint8_image_contrast(image)\n&gt;&gt;&gt; print(res)\n</code></pre> <pre><code>&gt;&gt;&gt; image = np.zeros((10, 10), dtype=np.uint8)\n&gt;&gt;&gt; res = bracket_to_uint8_image_contrast(image)\n&gt;&gt;&gt; print(res)\n</code></pre> Source code in <code>src/cellects/utils/formulas.py</code> <pre><code>@njit()\ndef bracket_to_uint8_image_contrast(image: NDArray):\n    \"\"\"\n    Convert an image with bracket contrast values to uint8 type.\n\n    This function normalizes an input image by scaling the minimum and maximum\n    values of the image to the range [0, 255] and then converts it to uint8\n    data type.\n\n    Parameters\n    ----------\n    image : ndarray\n        Input image as a numpy array with floating-point values.\n\n    Returns\n    -------\n    ndarray of uint8\n        Output image converted to uint8 type after normalization.\n\n    Examples\n    --------\n    &gt;&gt;&gt; image = np.random.randint(0, 255, (10, 10), dtype=np.uint8)\n    &gt;&gt;&gt; res = bracket_to_uint8_image_contrast(image)\n    &gt;&gt;&gt; print(res)\n\n    &gt;&gt;&gt; image = np.zeros((10, 10), dtype=np.uint8)\n    &gt;&gt;&gt; res = bracket_to_uint8_image_contrast(image)\n    &gt;&gt;&gt; print(res)\n    \"\"\"\n    image -= image.min()\n    if image.max() == 0:\n        return np.zeros_like(image, dtype=np.uint8)\n    else:\n        return to_uint8(255 * (image / np.max(image)))\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.combine_color_spaces","title":"<code>combine_color_spaces(c_space_dict, all_c_spaces, subtract_background=None)</code>","text":"<p>Combine color spaces from a dictionary and generate an analyzable image.</p> <p>This function processes multiple color spaces defined in <code>c_space_dict</code>, combines them according to given coefficients, and produces a normalized image that can be converted to uint8. Optionally subtracts background from the resultant image.</p> <p>Parameters:</p> Name Type Description Default <code>c_space_dict</code> <code>dict</code> <p>Dictionary containing color spaces and their respective coefficients.</p> required <code>all_c_spaces</code> <code>Dict</code> <p>Dictionary of all available color spaces in the image.</p> required <code>subtract_background</code> <code>NDArray</code> <p>Background image to subtract from the resultant image. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>out</code> <code>NDArray</code> <p>Processed and normalized image in float64 format, ready for uint8 conversion.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; c_space_dict = Dict()\n&gt;&gt;&gt; c_space_dict['hsv'] = np.array((0, 1, 1))\n&gt;&gt;&gt; all_c_spaces = Dict()\n&gt;&gt;&gt; all_c_spaces['bgr'] = np.random.rand(5, 5, 3)\n&gt;&gt;&gt; all_c_spaces['hsv'] = np.random.rand(5, 5, 3)\n&gt;&gt;&gt; background = np.zeros((5, 5))\n&gt;&gt;&gt; result = combine_color_spaces(c_space_dict, all_c_spaces)\n&gt;&gt;&gt; print(result.shape)\n(5, 5)\n</code></pre> Source code in <code>src/cellects/image_analysis/image_segmentation.py</code> <pre><code>@njit()\ndef combine_color_spaces(c_space_dict: Dict, all_c_spaces: Dict, subtract_background: NDArray=None) -&gt; NDArray:\n    \"\"\"\n    Combine color spaces from a dictionary and generate an analyzable image.\n\n    This function processes multiple color spaces defined in `c_space_dict`, combines\n    them according to given coefficients, and produces a normalized image that can be\n    converted to uint8. Optionally subtracts background from the resultant image.\n\n    Parameters\n    ----------\n    c_space_dict : dict\n        Dictionary containing color spaces and their respective coefficients.\n    all_c_spaces : Dict\n        Dictionary of all available color spaces in the image.\n    subtract_background : NDArray, optional\n        Background image to subtract from the resultant image. Defaults to None.\n\n    Returns\n    -------\n    out : NDArray\n        Processed and normalized image in float64 format, ready for uint8 conversion.\n\n    Examples\n    --------\n    &gt;&gt;&gt; c_space_dict = Dict()\n    &gt;&gt;&gt; c_space_dict['hsv'] = np.array((0, 1, 1))\n    &gt;&gt;&gt; all_c_spaces = Dict()\n    &gt;&gt;&gt; all_c_spaces['bgr'] = np.random.rand(5, 5, 3)\n    &gt;&gt;&gt; all_c_spaces['hsv'] = np.random.rand(5, 5, 3)\n    &gt;&gt;&gt; background = np.zeros((5, 5))\n    &gt;&gt;&gt; result = combine_color_spaces(c_space_dict, all_c_spaces)\n    &gt;&gt;&gt; print(result.shape)\n    (5, 5)\n    \"\"\"\n    image = np.zeros((all_c_spaces['bgr'].shape[0], all_c_spaces['bgr'].shape[1]), dtype=np.float64)\n    for space, channels in c_space_dict.items():\n        image += c_space_dict[space][0] * all_c_spaces[space][:, :, 0] + c_space_dict[space][1] * \\\n                 all_c_spaces[space][:, :, 1] + c_space_dict[space][2] * all_c_spaces[space][:, :, 2]\n    if subtract_background is not None:\n        # add (resp. subtract) the most negative (resp. smallest) value to the whole matrix to get a min = 0\n        image -= np.min(image)\n        # Make analysable this image by bracketing its values between 0 and 255 and converting it to uint8\n        max_im = np.max(image)\n        if max_im != 0:\n            image = 255 * (image / np.max(image))\n        if image.sum() &gt; subtract_background.sum():\n            image -= subtract_background\n        else:\n            image = subtract_background - image\n    # add (resp. subtract) the most negative (resp. smallest) value to the whole matrix to get a min = 0\n    image -= np.min(image)\n    # Make analysable this image by bracketing its values between 0 and 255\n    max_im = np.max(image)\n    if max_im != 0:\n        image = 255 * (image / max_im)\n    return image\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.create_empty_videos","title":"<code>create_empty_videos(image_list, cr, lose_accuracy_to_save_memory, already_greyscale, csc_dict)</code>","text":"<p>Create empty video arrays based on input parameters.</p> <p>Parameters:</p> Name Type Description Default <code>image_list</code> <code>list</code> <p>List of images.</p> required <code>cr</code> <code>list</code> <p>Crop region defined by [x_start, y_start, x_end, y_end].</p> required <code>lose_accuracy_to_save_memory</code> <code>bool</code> <p>Boolean flag to determine if memory should be saved by using uint8 data type.</p> required <code>already_greyscale</code> <code>bool</code> <p>Boolean flag indicating if the images are already in greyscale format.</p> required <code>csc_dict</code> <code>dict</code> <p>Dictionary containing color space conversion settings, including 'logical' key.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing three elements:     - <code>visu</code>: NumPy array with shape (len(image_list), cr[1] - cr[0] + 1, cr[3] - cr[2] + 1, 3) and dtype uint8 for RGB images.     - <code>converted_video</code>: NumPy array with shape (len(image_list), cr[1] - cr[0] + 1, cr[3] - cr[2] + 1) and dtype uint8 or float according to <code>lose_accuracy_to_save_memory</code>.     - <code>converted_video2</code>: NumPy array with shape same as <code>converted_video</code> and dtype uint8 or float according to <code>lose_accuracy_to_save_memory</code>.</p> Notes <p>Performance considerations:     - If <code>lose_accuracy_to_save_memory</code> is True, the function uses np.uint8 for memory efficiency.     - If <code>already_greyscale</code> is False, additional arrays are created to store RGB data.</p> Source code in <code>src/cellects/utils/load_display_save.py</code> <pre><code>def create_empty_videos(image_list: list, cr: list, lose_accuracy_to_save_memory: bool,\n                        already_greyscale: bool, csc_dict: dict):\n    \"\"\"\n\n    Create empty video arrays based on input parameters.\n\n    Parameters\n    ----------\n    image_list : list\n        List of images.\n    cr : list\n        Crop region defined by [x_start, y_start, x_end, y_end].\n    lose_accuracy_to_save_memory : bool\n        Boolean flag to determine if memory should be saved by using uint8 data type.\n    already_greyscale : bool\n        Boolean flag indicating if the images are already in greyscale format.\n    csc_dict : dict\n        Dictionary containing color space conversion settings, including 'logical' key.\n\n    Returns\n    -------\n    tuple\n        A tuple containing three elements:\n            - `visu`: NumPy array with shape (len(image_list), cr[1] - cr[0] + 1, cr[3] - cr[2] + 1, 3) and dtype uint8 for RGB images.\n            - `converted_video`: NumPy array with shape (len(image_list), cr[1] - cr[0] + 1, cr[3] - cr[2] + 1) and dtype uint8 or float according to `lose_accuracy_to_save_memory`.\n            - `converted_video2`: NumPy array with shape same as `converted_video` and dtype uint8 or float according to `lose_accuracy_to_save_memory`.\n\n    Notes\n    -----\n    Performance considerations:\n        - If `lose_accuracy_to_save_memory` is True, the function uses np.uint8 for memory efficiency.\n        - If `already_greyscale` is False, additional arrays are created to store RGB data.\n    \"\"\"\n    visu, converted_video, converted_video2 = None, None, None\n    dims = len(image_list), cr[1] - cr[0], cr[3] - cr[2]\n    if lose_accuracy_to_save_memory:\n        converted_video = np.zeros(dims, dtype=np.uint8)\n    else:\n        converted_video = np.zeros(dims, dtype=float)\n    if not already_greyscale:\n        visu = np.zeros((dims[0], dims[1], dims[2], 3), dtype=np.uint8)\n        if csc_dict['logical'] != 'None':\n            if lose_accuracy_to_save_memory:\n                converted_video2 = np.zeros(dims, dtype=np.uint8)\n            else:\n                converted_video2 = np.zeros(dims, dtype=float)\n    return visu, converted_video, converted_video2\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.detect_first_move","title":"<code>detect_first_move(size_dynamics, growth_threshold)</code>","text":"<p>Detects the first move in a time series where the value exceeds the initial value by a given threshold.</p> <p>Parameters:</p> Name Type Description Default <code>size_dynamics</code> <code>ndarray</code> <p>The time series data of dynamics.</p> required <code>growth_threshold</code> <p>The threshold value for detecting the first move.</p> required <p>Returns:</p> Type Description <code>int or NA</code> <p>The index of the first move where the condition is met. Returns <code>pandas.NA</code> if no such index exists.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; size_dynamics = np.array([10, 12, 15, 18])\n&gt;&gt;&gt; growth_threshold = 5\n&gt;&gt;&gt; detect_first_move(size_dynamics, growth_threshold)\n2\n</code></pre> Source code in <code>src/cellects/utils/formulas.py</code> <pre><code>def detect_first_move(size_dynamics: NDArray, growth_threshold)-&gt; int:\n    \"\"\"\n    Detects the first move in a time series where the value exceeds the initial value by a given threshold.\n\n    Parameters\n    ----------\n    size_dynamics : numpy.ndarray\n        The time series data of dynamics.\n    growth_threshold: int or float\n        The threshold value for detecting the first move.\n\n    Returns\n    -------\n    int or pandas.NA\n        The index of the first move where the condition is met.\n        Returns `pandas.NA` if no such index exists.\n\n    Examples\n    --------\n    &gt;&gt;&gt; size_dynamics = np.array([10, 12, 15, 18])\n    &gt;&gt;&gt; growth_threshold = 5\n    &gt;&gt;&gt; detect_first_move(size_dynamics, growth_threshold)\n    2\n    \"\"\"\n    first_move = pd.NA\n    thresh_reached = np.nonzero(size_dynamics &gt;= (size_dynamics[0] + growth_threshold))[0]\n    if len(thresh_reached) &gt; 0:\n        first_move = thresh_reached[0]\n    return first_move\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.detect_network_dynamics","title":"<code>detect_network_dynamics(converted_video, binary, arena_label=1, starting_time=0, visu=None, origin=None, smooth_segmentation_over_time=True, detect_pseudopods=True, save_coord_network=True, show_seg=False)</code>","text":"<p>Detects and tracks dynamic features (e.g., pseudopods) in a biological network over time from video data.</p> <p>Analyzes spatiotemporal dynamics of a network structure using binary masks and grayscale video data. Processes each frame to detect network components, optionally identifies pseudopods, applies temporal smoothing, and generates visualization overlays. Saves coordinate data for detected networks if enabled.</p> <p>Parameters:</p> Name Type Description Default <code>converted_video</code> <code>NDArray</code> <p>Input video data array with shape (time x y x z) representing grayscale intensities.</p> required <code>binary</code> <code>NDArray[uint8]</code> <p>Binary mask array with shape (time x y x z) indicating filled regions in each frame.</p> required <code>arena_label</code> <code>int</code> <p>Unique identifier for the current processing arena/session to name saved output files.</p> <code>1</code> <code>starting_time</code> <code>int</code> <p>Zero-based index of the first frame to begin network detection and analysis from.</p> <code>0</code> <code>visu</code> <code>NDArray</code> <p>Visualization video array (time x y x z) with RGB channels for overlay rendering.</p> <code>None</code> <code>origin</code> <code>NDArray[uint8]</code> <p>Binary mask defining a central region of interest to exclude from network detection.</p> <code>None</code> <code>smooth_segmentation_over_time</code> <code>(bool, optional(default=True))</code> <p>Flag indicating whether to apply temporal smoothing using adjacent frame data.</p> <code>True</code> <code>detect_pseudopods</code> <code>(bool, optional(default=True))</code> <p>Determines if pseudopod regions should be detected and merged with the network.</p> <code>True</code> <code>save_coord_network</code> <code>(bool, optional(default=True))</code> <p>Controls saving of detected network/pseudopod coordinates as NumPy arrays.</p> <code>True</code> <code>show_seg</code> <code>(bool, optional(default=False))</code> <p>Enables real-time visualization display during processing.</p> <code>False</code> <p>Returns:</p> Type Description <code>NDArray[uint8]</code> <p>3D array containing detected network structures with shape (time x y x z). Uses: - <code>0</code> for background, - <code>1</code> for regular network components, - <code>2</code> for pseudopod regions when detect_pseudopods is True.</p> Notes <ul> <li>Memory-intensive operations on large arrays may require system resources.</li> <li>Temporal smoothing effectiveness depends on network dynamics consistency between frames.</li> <li>Pseudopod detection requires sufficient contrast with the background in grayscale images.</li> </ul> Source code in <code>src/cellects/image_analysis/network_functions.py</code> <pre><code>def detect_network_dynamics(converted_video: NDArray, binary: NDArray[np.uint8], arena_label: int=1,\n                            starting_time: int=0, visu: NDArray=None, origin: NDArray[np.uint8]=None,\n                            smooth_segmentation_over_time: bool = True, detect_pseudopods: bool = True,\n                            save_coord_network: bool = True, show_seg: bool = False):\n    \"\"\"\n    Detects and tracks dynamic features (e.g., pseudopods) in a biological network over time from video data.\n\n    Analyzes spatiotemporal dynamics of a network structure using binary masks and grayscale video data. Processes each frame to detect network components, optionally identifies pseudopods, applies temporal smoothing, and generates visualization overlays. Saves coordinate data for detected networks if enabled.\n\n    Parameters\n    ----------\n    converted_video : NDArray\n        Input video data array with shape (time x y x z) representing grayscale intensities.\n    binary : NDArray[np.uint8]\n        Binary mask array with shape (time x y x z) indicating filled regions in each frame.\n    arena_label : int\n        Unique identifier for the current processing arena/session to name saved output files.\n    starting_time : int\n        Zero-based index of the first frame to begin network detection and analysis from.\n    visu : NDArray\n        Visualization video array (time x y x z) with RGB channels for overlay rendering.\n    origin : NDArray[np.uint8]\n        Binary mask defining a central region of interest to exclude from network detection.\n    smooth_segmentation_over_time : bool, optional (default=True)\n        Flag indicating whether to apply temporal smoothing using adjacent frame data.\n    detect_pseudopods : bool, optional (default=True)\n        Determines if pseudopod regions should be detected and merged with the network.\n    save_coord_network : bool, optional (default=True)\n        Controls saving of detected network/pseudopod coordinates as NumPy arrays.\n    show_seg : bool, optional (default=False)\n        Enables real-time visualization display during processing.\n\n    Returns\n    -------\n    NDArray[np.uint8]\n        3D array containing detected network structures with shape (time x y x z). Uses:\n        - `0` for background,\n        - `1` for regular network components,\n        - `2` for pseudopod regions when detect_pseudopods is True.\n\n    Notes\n    -----\n    - Memory-intensive operations on large arrays may require system resources.\n    - Temporal smoothing effectiveness depends on network dynamics consistency between frames.\n    - Pseudopod detection requires sufficient contrast with the background in grayscale images.\n    \"\"\"\n    logging.info(f\"Arena n\u00b0{arena_label}. Starting network detection.\")\n    # converted_video = self.converted_video; binary=self.binary; arena_label=1; starting_time=0; visu=self.visu; origin=None; smooth_segmentation_over_time=True; detect_pseudopods=True;save_coord_network=True; show_seg=False\n    dims = binary.shape\n    pseudopod_min_size = 50\n    if detect_pseudopods:\n        pseudopod_vid = np.zeros_like(binary, dtype=bool)\n    potential_network = np.zeros_like(binary, dtype=bool)\n    network_dynamics = np.zeros_like(binary, dtype=np.uint8)\n    do_convert = True\n    if visu is None:\n        do_convert = False\n        visu = np.stack((converted_video, converted_video, converted_video), axis=3)\n        greyscale = converted_video[-1, ...]\n    else:\n        greyscale = visu[-1, ...].mean(axis=-1)\n    NetDet = NetworkDetection(greyscale, possibly_filled_pixels=binary[-1, ...],\n                              origin_to_add=origin)\n    NetDet.get_best_network_detection_method()\n    if do_convert:\n        NetDet.greyscale_image = converted_video[-1, ...]\n    lighter_background = NetDet.greyscale_image[binary[-1, ...] &gt; 0].mean() &lt; NetDet.greyscale_image[\n        binary[-1, ...] == 0].mean()\n\n    for t in np.arange(starting_time, dims[0]):  # 20):#\n        if do_convert:\n            greyscale = visu[t, ...].mean(axis=-1)\n        else:\n            greyscale = converted_video[t, ...]\n        NetDet_fast = NetworkDetection(greyscale, possibly_filled_pixels=binary[t, ...],\n                                       origin_to_add=origin, best_result=NetDet.best_result)\n        NetDet_fast.detect_network()\n        NetDet_fast.greyscale_image = converted_video[t, ...]\n        if detect_pseudopods:\n            NetDet_fast.detect_pseudopods(lighter_background, pseudopod_min_size=pseudopod_min_size)\n            NetDet_fast.merge_network_with_pseudopods()\n            pseudopod_vid[t, ...] = NetDet_fast.pseudopods\n        potential_network[t, ...] = NetDet_fast.complete_network\n    if dims[0] == 1:\n        network_dynamics = potential_network\n    else:\n        for t in np.arange(starting_time, dims[0]):  # 20):#\n            if smooth_segmentation_over_time:\n                if 2 &lt;= t &lt;= (dims[0] - 2):\n                    computed_network = potential_network[(t - 2):(t + 3), :, :].sum(axis=0)\n                    computed_network[computed_network == 1] = 0\n                    computed_network[computed_network &gt; 1] = 1\n                else:\n                    if t &lt; 2:\n                        computed_network = potential_network[:2, :, :].sum(axis=0)\n                    else:\n                        computed_network = potential_network[-2:, :, :].sum(axis=0)\n                    computed_network[computed_network &gt; 0] = 1\n            else:\n                computed_network = computed_network[t, :, :].copy()\n\n            if origin is not None:\n                computed_network = computed_network * (1 - origin)\n                origin_contours = get_contours(origin)\n                complete_network = np.logical_or(origin_contours, computed_network).astype(np.uint8)\n            else:\n                complete_network = computed_network\n            complete_network = keep_one_connected_component(complete_network)\n\n            if detect_pseudopods:\n                # Make sure that removing pseudopods do not cut the network:\n                without_pseudopods = complete_network * (1 - pseudopod_vid[t])\n                only_connected_network = keep_one_connected_component(without_pseudopods)\n                # # Option A: To add these cutting regions to the pseudopods do:\n                pseudopods = (1 - only_connected_network) * complete_network\n                pseudopod_vid[t] = pseudopods\n            network_dynamics[t] = complete_network\n\n            imtoshow = visu[t, ...]\n            eroded_binary = cv2.erode(network_dynamics[t, ...], cross_33)\n            net_coord = np.nonzero(network_dynamics[t, ...] - eroded_binary)\n            imtoshow[net_coord[0], net_coord[1], :] = (34, 34, 158)\n            if show_seg:\n                cv2.imshow(\"\", cv2.resize(imtoshow, (1000, 1000)))\n                cv2.waitKey(1)\n            else:\n                visu[t, ...] = imtoshow\n            if show_seg:\n                cv2.destroyAllWindows()\n\n    network_coord = smallest_memory_array(np.nonzero(network_dynamics), \"uint\")\n    pseudopod_coord = None\n    if detect_pseudopods:\n        network_dynamics[pseudopod_vid &gt; 0] = 2\n        pseudopod_coord = smallest_memory_array(np.nonzero(pseudopod_vid), \"uint\")\n        if save_coord_network:\n            np.save(f\"coord_pseudopods{arena_label}_t{dims[0]}_y{dims[1]}_x{dims[2]}.npy\", pseudopod_coord)\n    if save_coord_network:\n        np.save(f\"coord_network{arena_label}_t{dims[0]}_y{dims[1]}_x{dims[2]}.npy\", network_coord)\n    return network_coord, pseudopod_coord\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.display_boxes","title":"<code>display_boxes(binary_image, box_diameter, show=True)</code>","text":"<p>Display grid lines on a binary image at specified box diameter intervals.</p> <p>This function displays the given binary image with vertical and horizontal grid lines drawn at regular intervals defined by <code>box_diameter</code>. The function returns the total number of grid lines drawn.</p> <p>Parameters:</p> Name Type Description Default <code>binary_image</code> <code>ndarray</code> <p>Binary image on which to draw the grid lines.</p> required <code>box_diameter</code> <code>int</code> <p>Diameter of each box in pixels.</p> required <p>Returns:</p> Name Type Description <code>line_nb</code> <code>int</code> <p>Number of grid lines drawn, both vertical and horizontal.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; binary_image = np.random.randint(0, 2, (100, 100), dtype=np.uint8)\n&gt;&gt;&gt; display_boxes(binary_image, box_diameter=25)\n</code></pre> Source code in <code>src/cellects/utils/load_display_save.py</code> <pre><code>def display_boxes(binary_image: NDArray, box_diameter: int, show: bool = True):\n    \"\"\"\n    Display grid lines on a binary image at specified box diameter intervals.\n\n    This function displays the given binary image with vertical and horizontal\n    grid lines drawn at regular intervals defined by `box_diameter`. The function\n    returns the total number of grid lines drawn.\n\n    Parameters\n    ----------\n    binary_image : ndarray\n        Binary image on which to draw the grid lines.\n    box_diameter : int\n        Diameter of each box in pixels.\n\n    Returns\n    -------\n    line_nb : int\n        Number of grid lines drawn, both vertical and horizontal.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; binary_image = np.random.randint(0, 2, (100, 100), dtype=np.uint8)\n    &gt;&gt;&gt; display_boxes(binary_image, box_diameter=25)\n    \"\"\"\n    plt.imshow(binary_image, cmap='gray', extent=(0, binary_image.shape[1], 0, binary_image.shape[0]))\n    height, width = binary_image.shape\n    line_nb = 0\n    for x in range(0, width + 1, box_diameter):\n        line_nb += 1\n        plt.axvline(x=x, color='white', linewidth=1)\n    for y in range(0, height + 1, box_diameter):\n        line_nb += 1\n        plt.axhline(y=y, color='white', linewidth=1)\n\n    if show:\n        plt.show()\n\n    return line_nb\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.display_network_methods","title":"<code>display_network_methods(network_detection, save_path=None)</code>","text":"<p>Display segmentation results from a network detection object.</p> Extended Description <p>Plots the binary segmentation results for various methods stored in <code>network_detection.all_results</code>. Highlights the best result based on quality metrics and allows for saving the figure to a file.</p> <p>Parameters:</p> Name Type Description Default <code>network_detection</code> <code>object</code> <p>An object containing segmentation results and quality metrics.</p> required <code>save_path</code> <code>str</code> <p>Path to save the figure. If <code>None</code>, the plot is displayed.</p> <code>None</code> Source code in <code>src/cellects/utils/load_display_save.py</code> <pre><code>def display_network_methods(network_detection: object, save_path: str=None):\n    \"\"\"\n\n    Display segmentation results from a network detection object.\n\n    Extended Description\n    --------------------\n\n    Plots the binary segmentation results for various methods stored in ``network_detection.all_results``.\n    Highlights the best result based on quality metrics and allows for saving the figure to a file.\n\n    Parameters\n    ----------\n    network_detection : object\n        An object containing segmentation results and quality metrics.\n    save_path : str, optional\n        Path to save the figure. If ``None``, the plot is displayed.\n\n    \"\"\"\n    row_nb = 6\n    fig, axes = plt.subplots(int(np.ceil(len(network_detection.all_results) / row_nb)), row_nb, figsize=(100, 100))\n    fig.suptitle(f'Segmentation Comparison: Frangi + Sato Variations', fontsize=16)\n\n    # Plot all results\n    for idx, result in enumerate(network_detection.all_results):\n        row = idx // row_nb\n        col = idx % row_nb\n\n        ax = axes[row, col]\n\n        # Display binary segmentation result\n        ax.imshow(result['binary'], cmap='gray')\n\n        # Create title with filter info and quality score\n        title = f\"{result['method']}: {str(np.round(network_detection.quality_metrics[idx], 0))}\"\n\n        # Highlight the best result\n        if idx == network_detection.best_idx:\n            ax.set_title(title, fontsize=8, color='red', fontweight='bold')\n            ax.add_patch(plt.Rectangle((0, 0), result['binary'].shape[1] - 1,\n                                       result['binary'].shape[0] - 1,\n                                       fill=False, edgecolor='red', linewidth=3))\n        else:\n            ax.set_title(title, fontsize=8)\n\n        ax.axis('off')\n    plt.tight_layout()\n\n    if save_path is not None:\n        plt.savefig(save_path, bbox_inches='tight', pad_inches=0., transparent=True, dpi=500)\n        plt.close()\n    else:\n        plt.show()\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.eudist","title":"<code>eudist(v1, v2)</code>","text":"<p>Calculate the Euclidean distance between two points in n-dimensional space.</p> <p>Parameters:</p> Name Type Description Default <code>v1</code> <code>iterable of float</code> <p>The coordinates of the first point.</p> required <code>v2</code> <code>iterable of float</code> <p>The coordinates of the second point.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The Euclidean distance between <code>v1</code> and <code>v2</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>v1</code> and <code>v2</code> do not have the same length.</p> Notes <p>The Euclidean distance is calculated using the standard formula: \u221a((x2 \u2212 x1)^2 + (y2 \u2212 y1)^2 + ...).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; v1 = [1.0, 2.0]\n&gt;&gt;&gt; v2 = [4.0, 6.0]\n&gt;&gt;&gt; eudist(v1, v2)\n5.0\n</code></pre> <pre><code>&gt;&gt;&gt; v1 = [1.0, 2.0, 3.0]\n&gt;&gt;&gt; v2 = [4.0, 6.0, 8.0]\n&gt;&gt;&gt; eudist(v1, v2)\n7.0710678118654755\n</code></pre> Source code in <code>src/cellects/utils/formulas.py</code> <pre><code>def eudist(v1, v2) -&gt; float:\n    \"\"\"\n    Calculate the Euclidean distance between two points in n-dimensional space.\n\n    Parameters\n    ----------\n    v1 : iterable of float\n        The coordinates of the first point.\n    v2 : iterable of float\n        The coordinates of the second point.\n\n    Returns\n    -------\n    float\n        The Euclidean distance between `v1` and `v2`.\n\n    Raises\n    ------\n    ValueError\n        If `v1` and `v2` do not have the same length.\n\n    Notes\n    -----\n    The Euclidean distance is calculated using the standard formula:\n    \u221a((x2 \u2212 x1)^2 + (y2 \u2212 y1)^2 + ...).\n\n    Examples\n    --------\n    &gt;&gt;&gt; v1 = [1.0, 2.0]\n    &gt;&gt;&gt; v2 = [4.0, 6.0]\n    &gt;&gt;&gt; eudist(v1, v2)\n    5.0\n\n    &gt;&gt;&gt; v1 = [1.0, 2.0, 3.0]\n    &gt;&gt;&gt; v2 = [4.0, 6.0, 8.0]\n    &gt;&gt;&gt; eudist(v1, v2)\n    7.0710678118654755\n    \"\"\"\n    dist = [(a - b)**2 for a, b in zip(v1, v2)]\n    dist = np.sqrt(np.sum(dist))\n    return dist\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.extract_graph_dynamics","title":"<code>extract_graph_dynamics(converted_video, coord_network, arena_label, starting_time=0, origin=None, coord_pseudopods=None)</code>","text":"<p>Extracts dynamic graph data from video frames based on network dynamics.</p> <p>This function processes time-series binary network structures to extract evolving vertices and edges over time. It computes spatial relationships between networks and an origin point through image processing steps including contour detection, padding for alignment, skeleton extraction, and morphological analysis. Vertex and edge attributes like position, connectivity, width, intensity, and betweenness are compiled into tables saved as CSV files.</p> <p>Parameters:</p> Name Type Description Default <code>converted_video</code> <code>NDArray</code> <p>3D video data array (t x y x) containing pixel intensities used for calculating edge intensity attributes during table generation.</p> required <code>coord_network</code> <code>NDArray[uint8]</code> <p>3D binary network mask array (t x y x) representing connectivity structures across time points.</p> required <code>arena_label</code> <code>int</code> <p>Unique identifier to prefix output filenames corresponding to specific experimental arenas.</p> required <code>starting_time</code> <code>(int, optional(default=0))</code> <p>Time index within <code>coord_network</code> to begin processing from (exclusive of origin initialization).</p> <code>0</code> <code>origin</code> <code>(NDArray[uint8], optional(default=None))</code> <p>Binary mask identifying the region of interest's central origin for spatial reference during network comparison.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <code>Saves two CSV files in working directory:</code> <code>1. `vertex_table{arena_label}_t{T}_y{Y}_x{X}.csv` - Vertex table with time, coordinates, and connectivity information</code> <code>2. `edge_table{arena_label}_t{T}_y{Y}_x{X}.csv` - Edge table containing attributes like length, width, intensity, and betweenness</code> Notes <p>Output CSVs use NumPy arrays converted to pandas DataFrames with columns: - Vertex table includes timestamps (t), coordinates (y,x), and connectivity flags. - Edge table contains betweenness centrality calculated during skeleton processing. Origin contours are spatially aligned through padding operations to maintain coordinate consistency across time points.</p> Source code in <code>src/cellects/image_analysis/network_functions.py</code> <pre><code>def extract_graph_dynamics(converted_video: NDArray, coord_network: NDArray, arena_label: int,\n                            starting_time: int=0, origin: NDArray[np.uint8]=None, coord_pseudopods: NDArray=None):\n    \"\"\"\n    Extracts dynamic graph data from video frames based on network dynamics.\n\n    This function processes time-series binary network structures to extract evolving vertices and edges over time. It computes spatial relationships between networks and an origin point through image processing steps including contour detection, padding for alignment, skeleton extraction, and morphological analysis. Vertex and edge attributes like position, connectivity, width, intensity, and betweenness are compiled into tables saved as CSV files.\n\n    Parameters\n    ----------\n    converted_video : NDArray\n        3D video data array (t x y x) containing pixel intensities used for calculating edge intensity attributes during table generation.\n    coord_network : NDArray[np.uint8]\n        3D binary network mask array (t x y x) representing connectivity structures across time points.\n    arena_label : int\n        Unique identifier to prefix output filenames corresponding to specific experimental arenas.\n    starting_time : int, optional (default=0)\n        Time index within `coord_network` to begin processing from (exclusive of origin initialization).\n    origin : NDArray[np.uint8], optional (default=None)\n        Binary mask identifying the region of interest's central origin for spatial reference during network comparison.\n\n    Returns\n    -------\n    None\n    Saves two CSV files in working directory:\n    1. `vertex_table{arena_label}_t{T}_y{Y}_x{X}.csv` - Vertex table with time, coordinates, and connectivity information\n    2. `edge_table{arena_label}_t{T}_y{Y}_x{X}.csv` - Edge table containing attributes like length, width, intensity, and betweenness\n\n    Notes\n    ---\n    Output CSVs use NumPy arrays converted to pandas DataFrames with columns:\n    - Vertex table includes timestamps (t), coordinates (y,x), and connectivity flags.\n    - Edge table contains betweenness centrality calculated during skeleton processing.\n    Origin contours are spatially aligned through padding operations to maintain coordinate consistency across time points.\n    \"\"\"\n    logging.info(f\"Arena n\u00b0{arena_label}. Starting graph extraction.\")\n    # converted_video = self.converted_video; coord_network=self.coord_network; arena_label=1; starting_time=0; origin=self.origin\n    dims = converted_video.shape[:3]\n    if origin is not None:\n        _, _, _, origin_centroid = cv2.connectedComponentsWithStats(origin)\n        origin_centroid = np.round((origin_centroid[1, 1], origin_centroid[1, 0])).astype(np.int64)\n        pad_origin_centroid = origin_centroid + 1\n        origin_contours = get_contours(origin)\n        pad_origin = add_padding([origin])[0]\n    else:\n        pad_origin_centroid = None\n        pad_origin = None\n        origin_contours = None\n    vertex_table = None\n    for t in np.arange(starting_time, dims[0]): # t=320   Y, X = 729, 554\n        computed_network = np.zeros((dims[1], dims[2]), dtype=np.uint8)\n        net_t = coord_network[1:, coord_network[0, :] == t]\n        computed_network[net_t[0], net_t[1]] = 1\n        if origin is not None:\n            computed_network = computed_network * (1 - origin)\n            computed_network = np.logical_or(origin_contours, computed_network).astype(np.uint8)\n        else:\n            computed_network = computed_network.astype(np.uint8)\n        if computed_network.any():\n            computed_network = keep_one_connected_component(computed_network)\n            pad_network = add_padding([computed_network])[0]\n            pad_skeleton, pad_distances, pad_origin_contours = get_skeleton_and_widths(pad_network, pad_origin,\n                                                                                           pad_origin_centroid)\n            edge_id = EdgeIdentification(pad_skeleton, pad_distances, t)\n            edge_id.run_edge_identification()\n            if pad_origin_contours is not None:\n                origin_contours = remove_padding([pad_origin_contours])[0]\n            growing_areas = None\n            if coord_pseudopods is not None:\n                growing_areas = coord_pseudopods[1:, coord_pseudopods[0, :] == t]\n            edge_id.make_vertex_table(origin_contours, growing_areas)\n            edge_id.make_edge_table(converted_video[t, ...])\n            edge_id.vertex_table = np.hstack((np.repeat(t, edge_id.vertex_table.shape[0])[:, None], edge_id.vertex_table))\n            edge_id.edge_table = np.hstack((np.repeat(t, edge_id.edge_table.shape[0])[:, None], edge_id.edge_table))\n            if vertex_table is None:\n                vertex_table = edge_id.vertex_table.copy()\n                edge_table = edge_id.edge_table.copy()\n            else:\n                vertex_table = np.vstack((vertex_table, edge_id.vertex_table))\n                edge_table = np.vstack((edge_table, edge_id.edge_table))\n\n    vertex_table = pd.DataFrame(vertex_table, columns=[\"t\", \"y\", \"x\", \"vertex_id\", \"is_tip\", \"origin\",\n                                                       \"vertex_connected\"])\n    edge_table = pd.DataFrame(edge_table,\n                              columns=[\"t\", \"edge_id\", \"vertex1\", \"vertex2\", \"length\", \"average_width\", \"intensity\",\n                                       \"betweenness_centrality\"])\n    vertex_table.to_csv(\n        f\"vertex_table{arena_label}_t{dims[0]}_y{dims[1]}_x{dims[2]}.csv\")\n    edge_table.to_csv(\n        f\"edge_table{arena_label}_t{dims[0]}_y{dims[1]}_x{dims[2]}.csv\")\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.extract_time","title":"<code>extract_time(image_list, pathway='', raw_images=False)</code>","text":"<p>Extract timestamps from a list of images.</p> <p>This function extracts the DateTimeOriginal or datetime values from the EXIF data of a list of image files, and computes the total time in seconds.</p> <p>Parameters:</p> Name Type Description Default <code>image_list</code> <code>list of str</code> <p>List of image file names.</p> required <code>pathway</code> <code>str</code> <p>Path to the directory containing the images. Default is an empty string.</p> <code>''</code> <code>raw_images</code> <code>bool</code> <p>If True, use the exifread library. Otherwise, use the exif library. Default is False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>time</code> <code>ndarray of int64</code> <p>Array containing the total time in seconds for each image.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pathway = Path(__name__).resolve().parents[0] / \"data\" / \"single_experiment\"\n&gt;&gt;&gt; image_list = ['image1.tif', 'image2.tif']\n&gt;&gt;&gt; time = extract_time(image_list, pathway)\n&gt;&gt;&gt; print(time)\narray([0, 0])\n</code></pre> Source code in <code>src/cellects/utils/load_display_save.py</code> <pre><code>def extract_time(image_list: list, pathway=\"\", raw_images:bool=False):\n    \"\"\"\n    Extract timestamps from a list of images.\n\n    This function extracts the DateTimeOriginal or datetime values from\n    the EXIF data of a list of image files, and computes the total time in seconds.\n\n    Parameters\n    ----------\n    image_list : list of str\n        List of image file names.\n    pathway : str, optional\n        Path to the directory containing the images. Default is an empty string.\n    raw_images : bool, optional\n        If True, use the exifread library. Otherwise, use the exif library.\n        Default is False.\n\n    Returns\n    -------\n    time : ndarray of int64\n        Array containing the total time in seconds for each image.\n\n    Examples\n    --------\n    &gt;&gt;&gt; pathway = Path(__name__).resolve().parents[0] / \"data\" / \"single_experiment\"\n    &gt;&gt;&gt; image_list = ['image1.tif', 'image2.tif']\n    &gt;&gt;&gt; time = extract_time(image_list, pathway)\n    &gt;&gt;&gt; print(time)\n    array([0, 0])\n\n    \"\"\"\n    if isinstance(pathway, str):\n        pathway = Path(pathway)\n    nb = len(image_list)\n    timings = np.zeros((nb, 6), dtype=np.int64)\n    if raw_images:\n        for i in np.arange(nb):\n            with open(pathway / image_list, 'rb') as image_file:\n                my_image = exifread.process_file(image_file, details=False, stop_tag='DateTimeOriginal')\n                datetime = my_image[\"EXIF DateTimeOriginal\"]\n            datetime = datetime.values[:10] + ':' + datetime.values[11:]\n            timings[i, :] = datetime.split(':')\n    else:\n        for i in np.arange(nb):\n            with open(pathway / image_list[i], 'rb') as image_file:\n                my_image = Image(image_file)\n                if my_image.has_exif:\n                    datetime = my_image.datetime\n                    datetime = datetime[:10] + ':' + datetime[11:]\n                    timings[i, :] = datetime.split(':')\n\n    if np.all(timings[:, 0] == timings[0, 0]):\n        if np.all(timings[:, 1] == timings[0, 1]):\n            if np.all(timings[:, 2] == timings[0, 2]):\n                time = timings[:, 3] * 3600 + timings[:, 4] * 60 + timings[:, 5]\n            else:\n                time = timings[:, 2] * 86400 + timings[:, 3] * 3600 + timings[:, 4] * 60 + timings[:, 5]\n        else:\n            days_per_month = (31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31)\n            for j in np.arange(nb):\n                month_number = timings[j, 1]#int(timings[j, 1])\n                timings[j, 1] = days_per_month[month_number] * month_number\n            time = (timings[:, 1] + timings[:, 2]) * 86400 + timings[:, 3] * 3600 + timings[:, 4] * 60 + timings[:, 5]\n        #time = int(time)\n    else:\n        time = np.repeat(0, nb)#arange(1, nb * 60, 60)#\"Do not experiment the 31th of december!!!\"\n    if time.sum() == 0:\n        time = np.repeat(0, nb)#arange(1, nb * 60, 60)\n    return time\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.find_common_coord","title":"<code>find_common_coord(array1, array2)</code>","text":"<p>Find common coordinates between two arrays.</p> <p>This function compares the given 2D <code>array1</code> and <code>array2</code> to determine if there are any common coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>array1</code> <code>ndarray of int</code> <p>A 2D numpy ndarray.</p> required <code>array2</code> <code>ndarray of int</code> <p>Another 2D numpy ndarray.</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>ndarray of bool</code> <p>A boolean numpy ndarray where True indicates common coordinates.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; array1 = np.array([[1, 2], [3, 4]])\n&gt;&gt;&gt; array2 = np.array([[5, 6], [1, 2]])\n&gt;&gt;&gt; result = find_common_coord(array1, array2)\n&gt;&gt;&gt; print(result)\narray([ True, False])\n</code></pre> Source code in <code>src/cellects/utils/formulas.py</code> <pre><code>def find_common_coord(array1: NDArray[int], array2: NDArray[int]) -&gt; NDArray[bool]:\n    \"\"\"Find common coordinates between two arrays.\n\n    This function compares the given 2D `array1` and `array2`\n    to determine if there are any common coordinates.\n\n    Parameters\n    ----------\n    array1 : ndarray of int\n        A 2D numpy ndarray.\n    array2 : ndarray of int\n        Another 2D numpy ndarray.\n\n    Returns\n    -------\n    out : ndarray of bool\n        A boolean numpy ndarray where True indicates common\n        coordinates.\n\n    Examples\n    --------\n    &gt;&gt;&gt; array1 = np.array([[1, 2], [3, 4]])\n    &gt;&gt;&gt; array2 = np.array([[5, 6], [1, 2]])\n    &gt;&gt;&gt; result = find_common_coord(array1, array2)\n    &gt;&gt;&gt; print(result)\n    array([ True, False])\"\"\"\n    return (array1[:, None, :] == array2[None, :, :]).all(-1).any(-1)\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.find_duplicates_coord","title":"<code>find_duplicates_coord(array1)</code>","text":"<p>Find duplicate rows in a 2D array and return their coordinate indices.</p> <p>Given a 2D NumPy array, this function identifies rows that are duplicated (i.e., appear more than once) and returns a boolean array indicating their positions.</p> <p>Parameters:</p> Name Type Description Default <code>array1</code> <code>ndarray of int</code> <p>Input 2D array of shape (n_rows, n_columns) from which to find duplicate rows.</p> required <p>Returns:</p> Name Type Description <code>duplicates</code> <code>ndarray of bool</code> <p>Boolean array of shape (n_rows,), where <code>True</code> indicates that the corresponding row in <code>array1</code> is a duplicate.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; array1 = np.array([[1, 2], [3, 4], [1, 2], [5, 6]])\n&gt;&gt;&gt; find_duplicates_coord(array1)\narray([ True, False,  True, False])\n</code></pre> Source code in <code>src/cellects/utils/formulas.py</code> <pre><code>def find_duplicates_coord(array1: NDArray[int]) -&gt; NDArray[bool]:\n    \"\"\"\n    Find duplicate rows in a 2D array and return their coordinate indices.\n\n    Given a 2D NumPy array, this function identifies rows that are duplicated (i.e., appear more than once) and returns a boolean array indicating their positions.\n\n    Parameters\n    ----------\n    array1 : ndarray of int\n        Input 2D array of shape (n_rows, n_columns) from which to find duplicate rows.\n\n    Returns\n    -------\n    duplicates : ndarray of bool\n        Boolean array of shape (n_rows,), where `True` indicates that the corresponding row in `array1` is a duplicate.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; array1 = np.array([[1, 2], [3, 4], [1, 2], [5, 6]])\n    &gt;&gt;&gt; find_duplicates_coord(array1)\n    array([ True, False,  True, False])\"\"\"\n    unique_rows, inverse_indices = np.unique(array1, axis=0, return_inverse=True)\n    counts = np.bincount(inverse_indices)\n    # A row is duplicate if its count &gt; 1\n    return counts[inverse_indices] &gt; 1\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.get_branches_and_tips_coord","title":"<code>get_branches_and_tips_coord(pad_vertices, pad_tips)</code>","text":"<p>Extracts the coordinates of branches and tips from vertices and tips binary images.</p> <p>This function calculates branch coordinates by subtracting tips from vertices. Then it finds and outputs the non-zero indices of branches and tips separatly.</p> <p>Parameters:</p> Name Type Description Default <code>pad_vertices</code> <code>ndarray</code> <p>Array containing the vertices to be padded.</p> required <code>pad_tips</code> <code>ndarray</code> <p>Array containing the tips of the padding.</p> required <p>Returns:</p> Name Type Description <code>branch_v_coord</code> <code>ndarray</code> <p>Coordinates of branches derived from subtracting tips from vertices.</p> <code>tips_coord</code> <code>ndarray</code> <p>Coordinates of the tips.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; branch_v, tip_c = get_branches_and_tips_coord(pad_vertices, pad_tips)\n&gt;&gt;&gt; branch_v\n&gt;&gt;&gt; tip_c\n</code></pre> Source code in <code>src/cellects/image_analysis/network_functions.py</code> <pre><code>def get_branches_and_tips_coord(pad_vertices: NDArray[np.uint8], pad_tips: NDArray[np.uint8]) -&gt; Tuple[NDArray, NDArray]:\n    \"\"\"\n    Extracts the coordinates of branches and tips from vertices and tips binary images.\n\n    This function calculates branch coordinates by subtracting\n    tips from vertices. Then it finds and outputs the non-zero indices of branches and tips separatly.\n\n    Parameters\n    ----------\n    pad_vertices : ndarray\n        Array containing the vertices to be padded.\n    pad_tips : ndarray\n        Array containing the tips of the padding.\n\n    Returns\n    -------\n    branch_v_coord : ndarray\n        Coordinates of branches derived from subtracting tips from vertices.\n    tips_coord : ndarray\n        Coordinates of the tips.\n\n    Examples\n    --------\n    &gt;&gt;&gt; branch_v, tip_c = get_branches_and_tips_coord(pad_vertices, pad_tips)\n    &gt;&gt;&gt; branch_v\n    &gt;&gt;&gt; tip_c\n    \"\"\"\n    pad_branches = pad_vertices - pad_tips\n    branch_v_coord = np.transpose(np.array(np.nonzero(pad_branches)))\n    tips_coord = np.transpose(np.array(np.nonzero(pad_tips)))\n    return branch_v_coord, tips_coord\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.get_color_spaces","title":"<code>get_color_spaces(bgr_image, space_names='')</code>","text":"<p>Convert a BGR image into various color spaces.</p> <p>Converts the input BGR image to specified color spaces and returns them as a dictionary. If no space names are provided, converts to all default color spaces (LAB, HSV, LUV, HLS, YUV). If 'logical' is in the space names, it will be removed before conversion.</p> <p>Parameters:</p> Name Type Description Default <code>bgr_image</code> <code>ndarray of uint8</code> <p>Input image in BGR color space.</p> required <code>space_names</code> <code>list of str</code> <p>List of color spaces to convert the image to. Defaults to none.</p> <code>''</code> <p>Returns:</p> Name Type Description <code>out</code> <code>dict</code> <p>Dictionary with keys as color space names and values as the converted images.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; bgr_image = np.zeros((5, 5, 3), dtype=np.uint8)\n&gt;&gt;&gt; c_spaces = get_color_spaces(bgr_image, ['lab', 'hsv'])\n&gt;&gt;&gt; print(list(c_spaces.keys()))\n['bgr', 'lab', 'hsv']\n</code></pre> Source code in <code>src/cellects/image_analysis/image_segmentation.py</code> <pre><code>def get_color_spaces(bgr_image: NDArray[np.uint8], space_names: list=\"\") -&gt; Dict:\n    \"\"\"\n    Convert a BGR image into various color spaces.\n\n    Converts the input BGR image to specified color spaces and returns them\n    as a dictionary. If no space names are provided, converts to all default\n    color spaces (LAB, HSV, LUV, HLS, YUV). If 'logical' is in the space names,\n    it will be removed before conversion.\n\n    Parameters\n    ----------\n    bgr_image : ndarray of uint8\n        Input image in BGR color space.\n    space_names : list of str, optional\n        List of color spaces to convert the image to. Defaults to none.\n\n    Returns\n    -------\n    out : dict\n        Dictionary with keys as color space names and values as the converted images.\n\n    Examples\n    --------\n    &gt;&gt;&gt; bgr_image = np.zeros((5, 5, 3), dtype=np.uint8)\n    &gt;&gt;&gt; c_spaces = get_color_spaces(bgr_image, ['lab', 'hsv'])\n    &gt;&gt;&gt; print(list(c_spaces.keys()))\n    ['bgr', 'lab', 'hsv']\n    \"\"\"\n    if 'logical' in space_names:\n        space_names.pop(np.nonzero(np.array(space_names, dtype=str) == 'logical')[0][0])\n    c_spaces = Dict()\n    c_spaces['bgr'] = bgr_image.astype(np.float64)\n    if len(space_names) == 0:\n        c_spaces['lab'] = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2LAB).astype(np.float64)\n        c_spaces['hsv'] = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2HSV).astype(np.float64)\n        c_spaces['luv'] = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2LUV).astype(np.float64)\n        c_spaces['hls'] = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2HLS).astype(np.float64)\n        c_spaces['yuv'] = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2YUV).astype(np.float64)\n    else:\n        if np.isin('lab', space_names):\n            c_spaces['lab'] = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2LAB).astype(np.float64)\n        if np.isin('hsv', space_names):\n            c_spaces['hsv'] = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2HSV).astype(np.float64)\n        if np.isin('luv', space_names):\n            c_spaces['luv'] = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2LUV).astype(np.float64)\n        if np.isin('hls', space_names):\n            c_spaces['hls'] = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2HLS).astype(np.float64)\n        if np.isin('yuv', space_names):\n            c_spaces['yuv'] = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2YUV).astype(np.float64)\n    return c_spaces\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.get_contour_width_from_im_shape","title":"<code>get_contour_width_from_im_shape(im_shape)</code>","text":"<p>Calculate the contour width based on image shape.</p> <p>Parameters:</p> Name Type Description Default <code>im_shape</code> <code>tuple of int, two items</code> <p>The dimensions of the image.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The calculated contour width.</p> Source code in <code>src/cellects/utils/formulas.py</code> <pre><code>def get_contour_width_from_im_shape(im_shape: Tuple) -&gt; int:\n    \"\"\"\n    Calculate the contour width based on image shape.\n\n    Parameters\n    ----------\n    im_shape : tuple of int, two items\n        The dimensions of the image.\n\n    Returns\n    -------\n    int\n        The calculated contour width.\n    \"\"\"\n    return np.max((np.round(np.log10(np.max(im_shape)) - 2).astype(int), 2))\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.get_h5_keys","title":"<code>get_h5_keys(file_name)</code>","text":"<p>Retrieve all keys from a given HDF5 file.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>The path to the HDF5 file from which keys are to be retrieved.</p> required <p>Returns:</p> Type Description <code>list of str</code> <p>A list containing all the keys present in the specified HDF5 file.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the specified HDF5 file does not exist.</p> Source code in <code>src/cellects/utils/load_display_save.py</code> <pre><code>def get_h5_keys(file_name):\n    \"\"\"\n    Retrieve all keys from a given HDF5 file.\n\n    Parameters\n    ----------\n    file_name : str\n        The path to the HDF5 file from which keys are to be retrieved.\n\n    Returns\n    -------\n    list of str\n        A list containing all the keys present in the specified HDF5 file.\n\n    Raises\n    ------\n    FileNotFoundError\n        If the specified HDF5 file does not exist.\n    \"\"\"\n    try:\n        with h5py.File(file_name, 'r') as h5f:\n            all_keys = list(h5f.keys())\n            return all_keys\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"The file '{file_name}' does not exist.\")\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.get_inertia_axes","title":"<code>get_inertia_axes(mo)</code>","text":"<p>Calculate the inertia axes of a moment object.</p> <p>This function computes the barycenters, central moments, and the lengths of the major and minor axes, as well as their orientation.</p> <p>Parameters:</p> Name Type Description Default <code>mo</code> <code>dict</code> <p>Dictionary containing moments, which should include keys: 'm00', 'm10', 'm01', 'm20', and 'm11'.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing:     - cx : float         The x-coordinate of the barycenter.     - cy : float         The y-coordinate of the barycenter.     - major_axis_len : float         The length of the major axis.     - minor_axis_len : float         The length of the minor axis.     - axes_orientation : float         The orientation of the axes in radians.</p> Notes <p>This function uses Numba's @njit decorator for performance. The moments in the input dictionary should be computed from the same image region.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; mo = {'m00': 1.0, 'm10': 2.0, 'm01': 3.0, 'm20': 4.0, 'm11': 5.0}\n&gt;&gt;&gt; get_inertia_axes(mo)\n(2.0, 3.0, 9.165151389911677, 0.8421875803239, 0.7853981633974483)\n</code></pre> Source code in <code>src/cellects/utils/formulas.py</code> <pre><code>@njit()\ndef get_inertia_axes(mo: dict) -&gt; Tuple[float, float, float, float, float]:\n    \"\"\"\n    Calculate the inertia axes of a moment object.\n\n    This function computes the barycenters, central moments,\n    and the lengths of the major and minor axes, as well as\n    their orientation.\n\n    Parameters\n    ----------\n    mo : dict\n        Dictionary containing moments, which should include keys:\n        'm00', 'm10', 'm01', 'm20', and 'm11'.\n\n    Returns\n    -------\n    tuple\n        A tuple containing:\n            - cx : float\n                The x-coordinate of the barycenter.\n            - cy : float\n                The y-coordinate of the barycenter.\n            - major_axis_len : float\n                The length of the major axis.\n            - minor_axis_len : float\n                The length of the minor axis.\n            - axes_orientation : float\n                The orientation of the axes in radians.\n\n    Notes\n    -----\n    This function uses Numba's @njit decorator for performance.\n    The moments in the input dictionary should be computed from\n    the same image region.\n\n    Examples\n    --------\n    &gt;&gt;&gt; mo = {'m00': 1.0, 'm10': 2.0, 'm01': 3.0, 'm20': 4.0, 'm11': 5.0}\n    &gt;&gt;&gt; get_inertia_axes(mo)\n    (2.0, 3.0, 9.165151389911677, 0.8421875803239, 0.7853981633974483)\n\n    \"\"\"\n    #L. Rocha, L. Velho and P.C.P. Calvalho (2002)\n    #http://sibgrapi.sid.inpe.br/col/sid.inpe.br/banon/2002/10.23.11.34/doc/35.pdf\n    # http://raphael.candelier.fr/?blog=Image%20Moments\n\n    # Calculate barycenters\n    cx = mo[\"m10\"] / mo[\"m00\"]\n    cy = mo[\"m01\"] / mo[\"m00\"]\n    # Calculate central moments\n    c20 = (mo[\"m20\"] / mo[\"m00\"]) - np.square(cx)\n    c02 = (mo[\"m02\"] / mo[\"m00\"]) - np.square(cy)\n    c11 = (mo[\"m11\"] / mo[\"m00\"]) - (cx * cy)\n    # Calculate major and minor axi lengths OK\n    major_axis_len = np.sqrt(6 * (c20 + c02 + np.sqrt(np.square(2 * c11) + np.square(c20 - c02))))\n    minor_axis_len = np.sqrt(6 * (c20 + c02 - np.sqrt(np.square(2 * c11) + np.square(c20 - c02))))\n    if (c20 - c02) != 0:\n        axes_orientation = (0.5 * np.arctan((2 * c11) / (c20 - c02))) + ((c20 &lt; c02) * (np.pi /2))\n    else:\n        axes_orientation = 0.\n    return cx, cy, major_axis_len, minor_axis_len, axes_orientation\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.get_inner_vertices","title":"<code>get_inner_vertices(pad_skeleton, potential_tips, cnv4, cnv8)</code>","text":"<p>Get inner vertices from skeleton image.</p> <p>This function identifies and returns the inner vertices of a skeletonized image. It processes potential tips to determine which pixels should be considered as vertices based on their neighbor count and connectivity.</p> <p>Parameters:</p> Name Type Description Default <code>pad_skeleton</code> <code>ndarray of uint8</code> <p>The padded skeleton image.</p> required <code>potential_tips</code> <code>ndarray of uint8</code> <p>Potential tip points in the skeleton. Defaults to pad_tips.</p> required <code>cnv4</code> <code>object</code> <p>Object for handling 4-connections.</p> required <code>cnv8</code> <code>object</code> <p>Object for handling 8-connections.</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>tuple of ndarray of uint8, ndarray of uint8</code> <p>A tuple containing the final vertices matrix and the updated potential tips.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pad_vertices, potential_tips = get_inner_vertices(pad_skeleton, potential_tips)\n&gt;&gt;&gt; print(pad_vertices)\n</code></pre> Source code in <code>src/cellects/image_analysis/network_functions.py</code> <pre><code>def get_inner_vertices(pad_skeleton: NDArray[np.uint8], potential_tips: NDArray[np.uint8], cnv4: object, cnv8: object) -&gt; Tuple[NDArray[np.uint8], NDArray[np.uint8]]: # potential_tips=pad_tips\n    \"\"\"\n    Get inner vertices from skeleton image.\n\n    This function identifies and returns the inner vertices of a skeletonized image.\n    It processes potential tips to determine which pixels should be considered as\n    vertices based on their neighbor count and connectivity.\n\n    Parameters\n    ----------\n    pad_skeleton : ndarray of uint8\n        The padded skeleton image.\n    potential_tips : ndarray of uint8, optional\n        Potential tip points in the skeleton. Defaults to pad_tips.\n    cnv4 : object\n        Object for handling 4-connections.\n    cnv8 : object\n        Object for handling 8-connections.\n\n    Returns\n    -------\n    out : tuple of ndarray of uint8, ndarray of uint8\n        A tuple containing the final vertices matrix and the updated potential tips.\n\n    Examples\n    --------\n    &gt;&gt;&gt; pad_vertices, potential_tips = get_inner_vertices(pad_skeleton, potential_tips)\n    &gt;&gt;&gt; print(pad_vertices)\n    \"\"\"\n\n    # Initiate the vertices final matrix as a copy of the potential_tips\n    pad_vertices = deepcopy(potential_tips)\n    for neighbor_nb in [8, 7, 6, 5, 4]:\n        # All pixels having neighbor_nb neighbor are potential vertices\n        potential_vertices = np.zeros(potential_tips.shape, dtype=np.uint8)\n\n        potential_vertices[cnv8.equal_neighbor_nb == neighbor_nb] = 1\n        # remove the false intersections that are a neighbor of a previously detected intersection\n        # Dilate vertices to make sure that no neighbors of the current potential vertices are already vertices.\n        dilated_previous_intersections = cv2.dilate(pad_vertices, cross_33, iterations=1)\n        potential_vertices *= (1 - dilated_previous_intersections)\n        pad_vertices[np.nonzero(potential_vertices)] = 1\n\n    # Having 3 neighbors is ambiguous\n    with_3_neighbors = cnv8.equal_neighbor_nb == 3\n    if np.any(with_3_neighbors):\n        # We compare 8-connections with 4-connections\n        # We loop over all 3 connected\n        coord_3 = np.nonzero(with_3_neighbors)\n        for y3, x3 in zip(coord_3[0], coord_3[1]): # y3, x3 = 3,7\n            # If, in the neighborhood of the 3, there is at least a 2 (in 8) that is 0 (in 4), and not a termination: the 3 is a node\n            has_2_8neigh = cnv8.equal_neighbor_nb[(y3 - 1):(y3 + 2), (x3 - 1):(x3 + 2)] &gt; 0  # 1\n            has_2_8neigh_without_focal = has_2_8neigh.copy()\n            has_2_8neigh_without_focal[1, 1] = 0\n            node_but_not_term = pad_vertices[(y3 - 1):(y3 + 2), (x3 - 1):(x3 + 2)] * (1 - potential_tips[(y3 - 1):(y3 + 2), (x3 - 1):(x3 + 2)])\n            all_are_node_but_not_term = np.array_equal(has_2_8neigh_without_focal, node_but_not_term)\n            if np.any(has_2_8neigh * (1 - all_are_node_but_not_term)):\n                # At least 3 of the 8neigh are not connected:\n                has_2_8neigh_without_focal = np.pad(has_2_8neigh_without_focal, [(1,), (1,)], mode='constant')\n                cnv_8con = CompareNeighborsWithValue(has_2_8neigh_without_focal, 4)\n                cnv_8con.is_equal(1, and_itself=True)\n                disconnected_nb = has_2_8neigh_without_focal.sum() - (cnv_8con.equal_neighbor_nb &gt; 0).sum()\n                if disconnected_nb &gt; 2:\n                    pad_vertices[y3, x3] = 1\n    # Now there may be too many vertices:\n    # - Those that are 4-connected:\n    nb, sh, st, ce = cv2.connectedComponentsWithStats(pad_vertices, connectivity=4)\n    problematic_vertices = np.nonzero(st[:, 4] &gt; 1)[0][1:]\n    for prob_v in problematic_vertices:\n        vertices_group = sh == prob_v\n        # If there is a tip in the group, do\n        if np.any(potential_tips[vertices_group]):\n            # Change the most connected one from tip to vertex\n            curr_neighbor_nb = cnv8.equal_neighbor_nb * vertices_group\n            wrong_tip = np.nonzero(curr_neighbor_nb == curr_neighbor_nb.max())\n            potential_tips[wrong_tip] = 0\n        else:\n            #  otherwise do:\n            # Find the most 8-connected one, if its 4-connected neighbors have no more 8-connexions than 4-connexions + 1, they can be removed\n            # Otherwise,\n            # Find the most 4-connected one, and remove its 4 connected neighbors having only 1 or other 8-connexion\n\n            c = zoom_on_nonzero(vertices_group)\n            # 1. Find the most 8-connected one:\n            sub_v_grp = vertices_group[c[0]:c[1], c[2]:c[3]]\n            c8 = cnv8.equal_neighbor_nb[c[0]:c[1], c[2]:c[3]]\n            vertices_group_8 = c8 * sub_v_grp\n            max_8_con = vertices_group_8.max()\n            most_8_con = np.nonzero(vertices_group_8 == max_8_con)\n            # c4[(most_8_con[0][0] - 1):(most_8_con[0][0] + 2), (most_8_con[1][0] - 1):(most_8_con[1][0] + 2)]\n            if len(most_8_con[0]) == 1:\n                skel_copy = pad_skeleton[c[0]:c[1], c[2]:c[3]].copy()\n                skel_copy[most_8_con] = 0\n                sub_cnv8 = CompareNeighborsWithValue(skel_copy, 8)\n                sub_cnv8.is_equal(1, and_itself=False)\n                sub_cnv4 = CompareNeighborsWithValue(skel_copy, 4)\n                sub_cnv4.is_equal(1, and_itself=False)\n                v_to_remove = sub_v_grp * (sub_cnv8.equal_neighbor_nb &lt;= sub_cnv4.equal_neighbor_nb + 1)\n            else:\n                c4 = cnv4.equal_neighbor_nb[c[0]:c[1], c[2]:c[3]]\n                # 1. # Find the most 4-connected one:\n                vertices_group_4 = c4 * sub_v_grp\n                max_con = vertices_group_4.max()\n                most_con = np.nonzero(vertices_group_4 == max_con)\n                if len(most_con[0]) &lt; sub_v_grp.sum():\n                    # 2. Check its 4-connected neighbors and remove those having only 1 other 8-connexion\n                    skel_copy = pad_skeleton[c[0]:c[1], c[2]:c[3]].copy()\n                    skel_copy[most_con] = 0\n                    skel_copy[most_con[0] - 1, most_con[1]] = 0\n                    skel_copy[most_con[0] + 1, most_con[1]] = 0\n                    skel_copy[most_con[0], most_con[1] - 1] = 0\n                    skel_copy[most_con[0], most_con[1] + 1] = 0\n                    sub_cnv8 = CompareNeighborsWithValue(skel_copy, 8)\n                    sub_cnv8.is_equal(1, and_itself=False)\n                    # There are:\n                    v_to_remove = ((vertices_group_4 &gt; 0) * sub_cnv8.equal_neighbor_nb) == 1\n                else:\n                    v_to_remove = np.zeros(sub_v_grp.shape, dtype=bool)\n            pad_vertices[c[0]:c[1], c[2]:c[3]][v_to_remove] = 0\n\n    # Other vertices to remove:\n    # - Those that are forming a cross with 0 at the center while the skeleton contains 1\n    cnv4_false = CompareNeighborsWithValue(pad_vertices, 4)\n    cnv4_false.is_equal(1, and_itself=False)\n    cross_vertices = cnv4_false.equal_neighbor_nb == 4\n    wrong_cross_vertices = cross_vertices * pad_skeleton\n    if wrong_cross_vertices.any():\n        pad_vertices[np.nonzero(wrong_cross_vertices)] = 1\n        cross_fix = cv2.dilate(wrong_cross_vertices, kernel=cross_33, iterations=1)\n        # Remove the 4-connected vertices that have no more than 4 8-connected neighbors\n        # i.e. the three on the side of the surrounded 0 and only one on edge on the other side\n        cross_fix = ((cnv8.equal_neighbor_nb * cross_fix) == 4) * (1 - wrong_cross_vertices)\n        pad_vertices *= (1 - cross_fix)\n    return pad_vertices, potential_tips\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.get_kurtosis","title":"<code>get_kurtosis(mo, binary_image, cx, cy, sx, sy)</code>","text":"<p>Calculate the kurtosis of a binary image.</p> <p>The function calculates the fourth moment (kurtosis) of the given binary image around the specified center coordinates with an option to specify the size of the square window.</p> <p>Parameters:</p> Name Type Description Default <code>mo</code> <code>dict</code> <p>Dictionary containing moments of binary image.</p> required <code>binary_image</code> <code>ndarray</code> <p>A 2D numpy ndarray representing a binary image.</p> required <code>cx</code> <code>int or float</code> <p>The x-coordinate of the center point of the square window.</p> required <code>cy</code> <code>int or float</code> <p>The y-coordinate of the center point of the square window.</p> required <code>sx</code> <code>int or float</code> <p>The x-length of the square window (width).</p> required <code>sy</code> <code>int or float</code> <p>The y-length of the square window (height).</p> required <p>Returns:</p> Type Description <code>float</code> <p>The kurtosis value calculated from the moments.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; mo = np.array([[0, 1], [2, 3]])\n&gt;&gt;&gt; binary_image = np.array([[1, 0], [0, 1]])\n&gt;&gt;&gt; cx = 2\n&gt;&gt;&gt; cy = 3\n&gt;&gt;&gt; sx = 5\n&gt;&gt;&gt; sy = 6\n&gt;&gt;&gt; result = get_kurtosis(mo, binary_image, cx, cy, sx, sy)\n&gt;&gt;&gt; print(result)\nexpected output\n</code></pre> Source code in <code>src/cellects/utils/formulas.py</code> <pre><code>def get_kurtosis(mo: dict, binary_image: NDArray, cx: float, cy: float, sx: float, sy: float) -&gt; Tuple[float, float]:\n    \"\"\"\n    Calculate the kurtosis of a binary image.\n\n    The function calculates the fourth moment (kurtosis) of the given\n    binary image around the specified center coordinates with an option\n    to specify the size of the square window.\n\n    Parameters\n    ----------\n    mo : dict\n        Dictionary containing moments of binary image.\n    binary_image : np.ndarray\n        A 2D numpy ndarray representing a binary image.\n    cx : int or float\n        The x-coordinate of the center point of the square window.\n    cy : int or float\n        The y-coordinate of the center point of the square window.\n    sx : int or float\n        The x-length of the square window (width).\n    sy : int or float\n        The y-length of the square window (height).\n\n    Returns\n    -------\n    float\n        The kurtosis value calculated from the moments.\n\n    Examples\n    --------\n    &gt;&gt;&gt; mo = np.array([[0, 1], [2, 3]])\n    &gt;&gt;&gt; binary_image = np.array([[1, 0], [0, 1]])\n    &gt;&gt;&gt; cx = 2\n    &gt;&gt;&gt; cy = 3\n    &gt;&gt;&gt; sx = 5\n    &gt;&gt;&gt; sy = 6\n    &gt;&gt;&gt; result = get_kurtosis(mo, binary_image, cx, cy, sx, sy)\n    &gt;&gt;&gt; print(result)\n    expected output\n    \"\"\"\n    x4, y4 = get_power_dists(binary_image, cx, cy, 4)\n    X4, Y4 = np.meshgrid(x4, y4)\n    m4x, m4y = get_var(mo, binary_image, X4, Y4)\n    return get_skewness_kurtosis(m4x, m4y, sx, sy, 4)\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.get_mpl_colormap","title":"<code>get_mpl_colormap(cmap_name)</code>","text":"<p>Returns a linear color range array for the given matplotlib colormap.</p> <p>Parameters:</p> Name Type Description Default <code>cmap_name</code> <code>str</code> <p>The name of the colormap to get.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>A 256x1x3 array of bytes representing the linear color range.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; result = get_mpl_colormap('viridis')\n&gt;&gt;&gt; print(result.shape)\n(256, 1, 3)\n</code></pre> Source code in <code>src/cellects/utils/load_display_save.py</code> <pre><code>def get_mpl_colormap(cmap_name: str):\n    \"\"\"\n    Returns a linear color range array for the given matplotlib colormap.\n\n    Parameters\n    ----------\n    cmap_name : str\n        The name of the colormap to get.\n\n    Returns\n    -------\n    numpy.ndarray\n        A 256x1x3 array of bytes representing the linear color range.\n\n    Examples\n    --------\n    &gt;&gt;&gt; result = get_mpl_colormap('viridis')\n    &gt;&gt;&gt; print(result.shape)\n    (256, 1, 3)\n\n    \"\"\"\n    cmap = plt.get_cmap(cmap_name)\n\n    # Initialize the matplotlib color map\n    sm = plt.cm.ScalarMappable(cmap=cmap)\n\n    # Obtain linear color range\n    color_range = sm.to_rgba(np.linspace(0, 1, 256), bytes=True)[:, 2::-1]\n\n    return color_range.reshape(256, 1, 3)\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.get_neighbor_comparisons","title":"<code>get_neighbor_comparisons(pad_skeleton)</code>","text":"<p>Get neighbor comparisons for a padded skeleton.</p> <p>This function creates two <code>CompareNeighborsWithValue</code> objects with different neighborhood sizes (4 and 8) and checks if the neighbors are equal to 1. It returns both comparison objects.</p> <p>Parameters:</p> Name Type Description Default <code>pad_skeleton</code> <code>ndarray of uint8</code> <p>The input padded skeleton array.</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>tuple of CompareNeighborsWithValue, CompareNeighborsWithValue</code> <p>Two comparison objects for 4 and 8 neighbors.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; cnv4, cnv8 = get_neighbor_comparisons(pad_skeleton)\n</code></pre> Source code in <code>src/cellects/image_analysis/network_functions.py</code> <pre><code>def get_neighbor_comparisons(pad_skeleton: NDArray[np.uint8]) -&gt; Tuple[object, object]:\n    \"\"\"\n    Get neighbor comparisons for a padded skeleton.\n\n    This function creates two `CompareNeighborsWithValue` objects with different\n    neighborhood sizes (4 and 8) and checks if the neighbors are equal to 1. It\n    returns both comparison objects.\n\n    Parameters\n    ----------\n    pad_skeleton : ndarray of uint8\n        The input padded skeleton array.\n\n    Returns\n    -------\n    out : tuple of CompareNeighborsWithValue, CompareNeighborsWithValue\n        Two comparison objects for 4 and 8 neighbors.\n\n    Examples\n    --------\n    &gt;&gt;&gt; cnv4, cnv8 = get_neighbor_comparisons(pad_skeleton)\n    \"\"\"\n    cnv4 = CompareNeighborsWithValue(pad_skeleton, 4)\n    cnv4.is_equal(1, and_itself=True)\n    cnv8 = CompareNeighborsWithValue(pad_skeleton, 8)\n    cnv8.is_equal(1, and_itself=True)\n    return cnv4, cnv8\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.get_newly_explored_area","title":"<code>get_newly_explored_area(binary_vid)</code>","text":"<p>Get newly explored area in a binary video.</p> <p>Calculate the number of new pixels that have become active (==1) from the previous frame in a binary video representation.</p> <p>Parameters:</p> Name Type Description Default <code>binary_vid</code> <code>ndarray</code> <p>The current frame of the binary video.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>An array containing the number of new active pixels for each row.</p> Notes <p>This function uses Numba's @njit decorator for performance.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; binary_vid=np.zeros((4, 5, 5), dtype=np.uint8)\n&gt;&gt;&gt; binary_vid[:2, 3, 3] = 1\n&gt;&gt;&gt; binary_vid[1, 4, 3] = 1\n&gt;&gt;&gt; binary_vid[2, 3, 4] = 1\n&gt;&gt;&gt; binary_vid[3, 2, 3] = 1\n&gt;&gt;&gt; get_newly_explored_area(binary_vid)\narray([0, 1, 1, 1])\n</code></pre> <pre><code>&gt;&gt;&gt; binary_vid=np.zeros((5, 5), dtype=np.uint8)[None, :, :]\n&gt;&gt;&gt; get_newly_explored_area(binary_vid)\narray([0])\n</code></pre> Source code in <code>src/cellects/utils/formulas.py</code> <pre><code>@njit()\ndef get_newly_explored_area(binary_vid: NDArray[np.uint8]) -&gt; NDArray:\n    \"\"\"\n    Get newly explored area in a binary video.\n\n    Calculate the number of new pixels that have become active (==1) from\n    the previous frame in a binary video representation.\n\n    Parameters\n    ----------\n    binary_vid : np.ndarray\n        The current frame of the binary video.\n\n    Returns\n    -------\n    np.ndarray\n        An array containing the number of new active pixels for each row.\n\n    Notes\n    -----\n    This function uses Numba's @njit decorator for performance.\n\n    Examples\n    --------\n    &gt;&gt;&gt; binary_vid=np.zeros((4, 5, 5), dtype=np.uint8)\n    &gt;&gt;&gt; binary_vid[:2, 3, 3] = 1\n    &gt;&gt;&gt; binary_vid[1, 4, 3] = 1\n    &gt;&gt;&gt; binary_vid[2, 3, 4] = 1\n    &gt;&gt;&gt; binary_vid[3, 2, 3] = 1\n    &gt;&gt;&gt; get_newly_explored_area(binary_vid)\n    array([0, 1, 1, 1])\n\n    &gt;&gt;&gt; binary_vid=np.zeros((5, 5), dtype=np.uint8)[None, :, :]\n    &gt;&gt;&gt; get_newly_explored_area(binary_vid)\n    array([0])\n    \"\"\"\n    return ((binary_vid - binary_vid[0, ...]) == 1).reshape(binary_vid.shape[0], - 1).sum(1)\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.get_power_dists","title":"<code>get_power_dists(binary_image, cx, cy, n)</code>","text":"<p>Calculate the power distributions based on the given center coordinates and exponent.</p> <p>This function computes the <code>n</code>th powers of x and y distances from a given center point <code>(cx, cy)</code> for each pixel in the binary image.</p> <p>Parameters:</p> Name Type Description Default <code>binary_image</code> <code>ndarray</code> <p>A 2D array (binary image) where the power distributions are calculated.</p> required <code>cx</code> <code>float</code> <p>The x-coordinate of the center point.</p> required <code>cy</code> <code>float</code> <p>The y-coordinate of the center point.</p> required <code>n</code> <code>int</code> <p>The exponent for power distribution calculation.</p> required <p>Returns:</p> Type Description <code>tuple[ndarray, ndarray]</code> <p>A tuple containing two arrays: - The first array contains the <code>n</code>th power of x distances from the center. - The second array contains the <code>n</code>th power of y distances from the center.</p> Notes <p>This function uses Numba's <code>@njit</code> decorator for performance optimization. Ensure that <code>binary_image</code> is a NumPy ndarray to avoid type issues.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; binary_image = np.zeros((10, 10))\n&gt;&gt;&gt; xn, yn = get_power_dists(binary_image, 5.0, 5.0, 2)\n&gt;&gt;&gt; print(xn.shape), print(yn.shape)\n(10,) (10,)\n</code></pre> Source code in <code>src/cellects/utils/formulas.py</code> <pre><code>@njit()\ndef get_power_dists(binary_image: np.ndarray, cx: float, cy: float, n: int):\n    \"\"\"\n    Calculate the power distributions based on the given center coordinates and exponent.\n\n    This function computes the `n`th powers of x and y distances from\n    a given center point `(cx, cy)` for each pixel in the binary image.\n\n    Parameters\n    ----------\n    binary_image : np.ndarray\n        A 2D array (binary image) where the power distributions are calculated.\n    cx : float\n        The x-coordinate of the center point.\n    cy : float\n        The y-coordinate of the center point.\n    n : int\n        The exponent for power distribution calculation.\n\n    Returns\n    -------\n    tuple[np.ndarray, np.ndarray]\n        A tuple containing two arrays:\n        - The first array contains the `n`th power of x distances from the center.\n        - The second array contains the `n`th power of y distances from the center.\n\n    Notes\n    -----\n    This function uses Numba's `@njit` decorator for performance optimization.\n    Ensure that `binary_image` is a NumPy ndarray to avoid type issues.\n\n    Examples\n    --------\n    &gt;&gt;&gt; binary_image = np.zeros((10, 10))\n    &gt;&gt;&gt; xn, yn = get_power_dists(binary_image, 5.0, 5.0, 2)\n    &gt;&gt;&gt; print(xn.shape), print(yn.shape)\n    (10,) (10,)\n    \"\"\"\n    xn = (np.arange(binary_image.shape[1]) - cx) ** n\n    yn = (np.arange(binary_image.shape[0]) - cy) ** n\n    return xn, yn\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.get_skeleton_and_widths","title":"<code>get_skeleton_and_widths(pad_network, pad_origin=None, pad_origin_centroid=None)</code>","text":"<p>Get skeleton and widths from a network.</p> <p>This function computes the morphological skeleton of a network and calculates the distances to the closest zero pixel for each non-zero pixel using medial_axis. If pad_origin is provided, it adds a central contour. Finally, the function removes small loops and keeps only one connected component.</p> <p>Parameters:</p> Name Type Description Default <code>pad_network</code> <code>ndarray of uint8</code> <p>The binary pad network image.</p> required <code>pad_origin</code> <code>ndarray of uint8</code> <p>An array indicating the origin for adding central contour.</p> <code>None</code> <code>pad_origin_centroid</code> <code>ndarray</code> <p>The centroid of the pad origin. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>out</code> <code>tuple(ndarray of uint8, ndarray of uint8, ndarray of uint8)</code> <p>A tuple containing: - pad_skeleton: The skeletonized image. - pad_distances: The distances to the closest zero pixel. - pad_origin_contours: The contours of the central origin, or None if not   used.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pad_network = np.array([[0, 1], [1, 0]])\n&gt;&gt;&gt; skeleton, distances, contours = get_skeleton_and_widths(pad_network)\n&gt;&gt;&gt; print(skeleton)\n</code></pre> Source code in <code>src/cellects/image_analysis/network_functions.py</code> <pre><code>def get_skeleton_and_widths(pad_network: NDArray[np.uint8], pad_origin: NDArray[np.uint8]=None, pad_origin_centroid: NDArray[np.int64]=None) -&gt; Tuple[NDArray[np.uint8], NDArray[np.float64], NDArray[np.uint8]]:\n    \"\"\"\n    Get skeleton and widths from a network.\n\n    This function computes the morphological skeleton of a network and calculates\n    the distances to the closest zero pixel for each non-zero pixel using medial_axis.\n    If pad_origin is provided, it adds a central contour. Finally, the function\n    removes small loops and keeps only one connected component.\n\n    Parameters\n    ----------\n    pad_network : ndarray of uint8\n        The binary pad network image.\n    pad_origin : ndarray of uint8, optional\n        An array indicating the origin for adding central contour.\n    pad_origin_centroid : ndarray, optional\n        The centroid of the pad origin. Defaults to None.\n\n    Returns\n    -------\n    out : tuple(ndarray of uint8, ndarray of uint8, ndarray of uint8)\n        A tuple containing:\n        - pad_skeleton: The skeletonized image.\n        - pad_distances: The distances to the closest zero pixel.\n        - pad_origin_contours: The contours of the central origin, or None if not\n          used.\n\n    Examples\n    --------\n    &gt;&gt;&gt; pad_network = np.array([[0, 1], [1, 0]])\n    &gt;&gt;&gt; skeleton, distances, contours = get_skeleton_and_widths(pad_network)\n    &gt;&gt;&gt; print(skeleton)\n    \"\"\"\n    pad_skeleton, pad_distances = morphology.medial_axis(pad_network, return_distance=True, rng=0)\n    pad_skeleton = pad_skeleton.astype(np.uint8)\n    if pad_origin is not None:\n        pad_skeleton, pad_distances, pad_origin_contours = _add_central_contour(pad_skeleton, pad_distances, pad_origin, pad_network, pad_origin_centroid)\n    else:\n        pad_origin_contours = None\n    pad_skeleton, pad_distances = remove_small_loops(pad_skeleton, pad_distances)\n    pad_skeleton = keep_one_connected_component(pad_skeleton)\n    pad_distances *= pad_skeleton\n    return pad_skeleton, pad_distances, pad_origin_contours\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.get_skewness","title":"<code>get_skewness(mo, binary_image, cx, cy, sx, sy)</code>","text":"<p>Calculate skewness of the given moment.</p> <p>This function computes the skewness based on the third moments and the central moments of a binary image.</p> <p>Parameters:</p> Name Type Description Default <code>mo</code> <code>dict</code> <p>Dictionary containing moments of binary image.</p> required <code>binary_image</code> <code>ndarray</code> <p>Binary image as a 2D numpy array.</p> required <code>cx</code> <code>float</code> <p>Description of parameter <code>cx</code>.</p> required <code>cy</code> <code>float</code> <p>Description of parameter <code>cy</code>.</p> required <code>sx</code> <code>float</code> <p>Description of parameter <code>sx</code>.</p> required <code>sy</code> <code>float</code> <p>Description of parameter <code>sy</code>.</p> required <p>Returns:</p> Type Description <code>Tuple[float, float]</code> <p>Tuple containing skewness values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; result = get_skewness(mo=example_mo, binary_image=binary_img,\n... cx=0.5, cy=0.5, sx=1.0, sy=1.0)\n&gt;&gt;&gt; print(result)\n(skewness_x, skewness_y)  # Example output\n</code></pre> Source code in <code>src/cellects/utils/formulas.py</code> <pre><code>def get_skewness(mo: dict, binary_image: NDArray, cx: float, cy: float, sx: float, sy: float) -&gt; Tuple[float, float]:\n    \"\"\"Calculate skewness of the given moment.\n\n    This function computes the skewness based on the third moments\n    and the central moments of a binary image.\n\n    Parameters\n    ----------\n    mo : dict\n        Dictionary containing moments of binary image.\n    binary_image : ndarray\n        Binary image as a 2D numpy array.\n    cx : float\n        Description of parameter `cx`.\n    cy : float\n        Description of parameter `cy`.\n    sx : float\n        Description of parameter `sx`.\n    sy : float\n        Description of parameter `sy`.\n\n    Returns\n    -------\n    Tuple[float, float]\n        Tuple containing skewness values.\n\n    Examples\n    --------\n    &gt;&gt;&gt; result = get_skewness(mo=example_mo, binary_image=binary_img,\n    ... cx=0.5, cy=0.5, sx=1.0, sy=1.0)\n    &gt;&gt;&gt; print(result)\n    (skewness_x, skewness_y)  # Example output\n    \"\"\"\n    x3, y3 = get_power_dists(binary_image, cx, cy, 3)\n    X3, Y3 = np.meshgrid(x3, y3)\n    m3x, m3y = get_var(mo, binary_image, X3, Y3)\n    return get_skewness_kurtosis(m3x, m3y, sx, sy, 3)\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.get_skewness_kurtosis","title":"<code>get_skewness_kurtosis(mnx, mny, sx, sy, n)</code>","text":"<p>Calculates skewness and kurtosis of a distribution.</p> <p>This function computes the skewness and kurtosis from given statistical moments, standard deviations, and order of moments.</p> <p>Parameters:</p> Name Type Description Default <code>mnx</code> <code>float</code> <p>The third moment about the mean for x.</p> required <code>mny</code> <code>float</code> <p>The fourth moment about the mean for y.</p> required <code>sx</code> <code>float</code> <p>The standard deviation of x.</p> required <code>sy</code> <code>float</code> <p>The standard deviation of y.</p> required <code>n</code> <code>int</code> <p>Order of the moment (3 for skewness, 4 for kurtosis).</p> required <p>Returns:</p> Name Type Description <code>skewness</code> <code>float</code> <p>The computed skewness.</p> <code>kurtosis</code> <code>float</code> <p>The computed kurtosis.</p> Notes <p>This function uses Numba's <code>@njit</code> decorator for performance. Ensure that the values of <code>mnx</code>, <code>mny</code>, <code>sx</code>, and <code>sy</code> are non-zero to avoid division by zero. If <code>n = 3</code>, the function calculates skewness. If <code>n = 4</code>, it calculates kurtosis.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; skewness, kurtosis = get_skewness_kurtosis(1.5, 2.0, 0.5, 0.75, 3)\n&gt;&gt;&gt; print(\"Skewness:\", skewness)\nSkewness: 8.0\n&gt;&gt;&gt; print(\"Kurtosis:\", kurtosis)\nKurtosis: nan\n</code></pre> Source code in <code>src/cellects/utils/formulas.py</code> <pre><code>@njit()\ndef get_skewness_kurtosis(mnx: float, mny: float, sx: float, sy: float, n: int) -&gt; Tuple[float, float]:\n    \"\"\"\n    Calculates skewness and kurtosis of a distribution.\n\n    This function computes the skewness and kurtosis from given statistical\n    moments, standard deviations, and order of moments.\n\n    Parameters\n    ----------\n    mnx : float\n        The third moment about the mean for x.\n    mny : float\n        The fourth moment about the mean for y.\n    sx : float\n        The standard deviation of x.\n    sy : float\n        The standard deviation of y.\n    n : int\n        Order of the moment (3 for skewness, 4 for kurtosis).\n\n    Returns\n    -------\n    skewness : float\n        The computed skewness.\n    kurtosis : float\n        The computed kurtosis.\n\n    Notes\n    -----\n    This function uses Numba's `@njit` decorator for performance.\n    Ensure that the values of `mnx`, `mny`, `sx`, and `sy` are non-zero to avoid division by zero.\n    If `n = 3`, the function calculates skewness. If `n = 4`, it calculates kurtosis.\n\n    Examples\n    --------\n    &gt;&gt;&gt; skewness, kurtosis = get_skewness_kurtosis(1.5, 2.0, 0.5, 0.75, 3)\n    &gt;&gt;&gt; print(\"Skewness:\", skewness)\n    Skewness: 8.0\n    &gt;&gt;&gt; print(\"Kurtosis:\", kurtosis)\n    Kurtosis: nan\n\n    \"\"\"\n    if sx == 0:\n        fx = 0\n    else:\n        fx = mnx / sx ** n\n\n    if sy == 0:\n        fy = 0\n    else:\n        fy = mny / sy ** n\n\n    return fx, fy\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.get_standard_deviations","title":"<code>get_standard_deviations(mo, binary_image, cx, cy)</code>","text":"<p>Return spatial standard deviations for a given moment and binary image.</p> <p>This function computes the square root of variances along <code>x</code> (horizontal) and <code>y</code> (vertical) axes for the given binary image and moment.</p> <p>Parameters:</p> Name Type Description Default <code>mo</code> <code>dict</code> <p>Dictionary containing moments of binary image.</p> required <code>binary_image</code> <code>ndarray of bool or int8</code> <p>The binary input image where the moments are computed.</p> required <code>cx</code> <code>float64</code> <p>X-coordinate of center of mass (horizontal position).</p> required <code>cy</code> <code>float64</code> <p>Y-coordinate of center of mass (vertical position).</p> required <p>Returns:</p> Type Description <code>tuple[ndarray of float64, ndarray of float64]</code> <p>Tuple containing the standard deviations along the x and y axes.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>binary_image</code> is not a binary image or has an invalid datatype.</p> Notes <p>This function uses the <code>get_power_dists</code> and <code>get_var</code> functions to compute the distributed variances, which are then transformed into standard deviations.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; binary_image = np.array([[0, 1], [1, 0]], dtype=np.int8)\n&gt;&gt;&gt; mo = np.array([[2.0], [3.0]])\n&gt;&gt;&gt; cx, cy = 1.5, 1.5\n&gt;&gt;&gt; stdx, stdy = get_standard_deviations(mo, binary_image, cx, cy)\n&gt;&gt;&gt; print(stdx)\n[1.1]\n&gt;&gt;&gt; print(stdy)\n[0.8366600265...]\n</code></pre> Source code in <code>src/cellects/utils/formulas.py</code> <pre><code>def get_standard_deviations(mo: dict, binary_image: NDArray, cx: float, cy: float) -&gt; Tuple[float, float]:\n    \"\"\"\n    Return spatial standard deviations for a given moment and binary image.\n\n    This function computes the square root of variances along `x` (horizontal)\n    and `y` (vertical) axes for the given binary image and moment.\n\n    Parameters\n    ----------\n    mo : dict\n        Dictionary containing moments of binary image.\n    binary_image : ndarray of bool or int8\n        The binary input image where the moments are computed.\n    cx : float64\n        X-coordinate of center of mass (horizontal position).\n    cy : float64\n        Y-coordinate of center of mass (vertical position).\n\n    Returns\n    -------\n    tuple[ndarray of float64, ndarray of float64]\n        Tuple containing the standard deviations along the x and y axes.\n\n    Raises\n    ------\n    ValueError\n        If `binary_image` is not a binary image or has an invalid datatype.\n\n    Notes\n    -----\n    This function uses the `get_power_dists` and `get_var` functions to compute\n    the distributed variances, which are then transformed into standard deviations.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; binary_image = np.array([[0, 1], [1, 0]], dtype=np.int8)\n    &gt;&gt;&gt; mo = np.array([[2.0], [3.0]])\n    &gt;&gt;&gt; cx, cy = 1.5, 1.5\n    &gt;&gt;&gt; stdx, stdy = get_standard_deviations(mo, binary_image, cx, cy)\n    &gt;&gt;&gt; print(stdx)\n    [1.1]\n    &gt;&gt;&gt; print(stdy)\n    [0.8366600265...]\n    \"\"\"\n    x2, y2 = get_power_dists(binary_image, cx, cy, 2)\n    X2, Y2 = np.meshgrid(x2, y2)\n    vx, vy = get_var(mo, binary_image, X2, Y2)\n    return np.sqrt(vx), np.sqrt(vy)\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.get_terminations_and_their_connected_nodes","title":"<code>get_terminations_and_their_connected_nodes(pad_skeleton, cnv4, cnv8)</code>","text":"<p>Get terminations in a skeleton and their connected nodes.</p> <p>This function identifies termination points in a padded skeleton array based on pixel connectivity, marking them and their connected nodes.</p> <p>Parameters:</p> Name Type Description Default <code>pad_skeleton</code> <code>ndarray of uint8</code> <p>The padded skeleton array where terminations are to be identified.</p> required <code>cnv4</code> <code>object</code> <p>Convolution object with 4-connectivity for neighbor comparison.</p> required <code>cnv8</code> <code>object</code> <p>Convolution object with 8-connectivity for neighbor comparison.</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>ndarray of uint8</code> <p>Array containing marked terminations and their connected nodes.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; result = get_terminations_and_their_connected_nodes(pad_skeleton, cnv4, cnv8)\n&gt;&gt;&gt; print(result)\n</code></pre> Source code in <code>src/cellects/image_analysis/network_functions.py</code> <pre><code>def get_terminations_and_their_connected_nodes(pad_skeleton: NDArray[np.uint8], cnv4: object, cnv8: object) -&gt; NDArray[np.uint8]:\n    \"\"\"\n    Get terminations in a skeleton and their connected nodes.\n\n    This function identifies termination points in a padded skeleton array\n    based on pixel connectivity, marking them and their connected nodes.\n\n    Parameters\n    ----------\n    pad_skeleton : ndarray of uint8\n        The padded skeleton array where terminations are to be identified.\n    cnv4 : object\n        Convolution object with 4-connectivity for neighbor comparison.\n    cnv8 : object\n        Convolution object with 8-connectivity for neighbor comparison.\n\n    Returns\n    -------\n    out : ndarray of uint8\n        Array containing marked terminations and their connected nodes.\n\n    Examples\n    --------\n    &gt;&gt;&gt; result = get_terminations_and_their_connected_nodes(pad_skeleton, cnv4, cnv8)\n    &gt;&gt;&gt; print(result)\n    \"\"\"\n    # All pixels having only one neighbor, and containing the value 1, are terminations for sure\n    potential_tips = np.zeros(pad_skeleton.shape, dtype=np.uint8)\n    potential_tips[cnv8.equal_neighbor_nb == 1] = 1\n    # Add more terminations using 4-connectivity\n    # If a pixel is 1 (in 4) and all its neighbors are neighbors (in 4), it is a termination\n\n    coord1_4 = cnv4.equal_neighbor_nb == 1\n    if np.any(coord1_4):\n        coord1_4 = np.nonzero(coord1_4)\n        for y1, x1 in zip(coord1_4[0], coord1_4[1]): # y1, x1 = 3,5\n            # If, in the neighborhood of the 1 (in 4), all (in 8) its neighbors are 4-connected together, and none of them are terminations, the 1 is a termination\n            is_4neigh = cnv4.equal_neighbor_nb[(y1 - 1):(y1 + 2), (x1 - 1):(x1 + 2)] != 0\n            all_4_connected = pad_skeleton[(y1 - 1):(y1 + 2), (x1 - 1):(x1 + 2)] == is_4neigh\n            is_not_term = 1 - potential_tips[y1, x1]\n            if np.all(all_4_connected * is_not_term):\n                is_4neigh[1, 1] = 0\n                is_4neigh = np.pad(is_4neigh, [(1,), (1,)], mode='constant')\n                cnv_4con = CompareNeighborsWithValue(is_4neigh, 4)\n                cnv_4con.is_equal(1, and_itself=True)\n                all_connected = (is_4neigh.sum() - (cnv_4con.equal_neighbor_nb &gt; 0).sum()) == 0\n                # If they are connected, it can be a termination\n                if all_connected:\n                    # If its closest neighbor is above 3 (in 8), this one is also a node\n                    is_closest_above_3 = cnv8.equal_neighbor_nb[(y1 - 1):(y1 + 2), (x1 - 1):(x1 + 2)] * cross_33 &gt; 3\n                    if np.any(is_closest_above_3):\n                        Y, X = np.nonzero(is_closest_above_3)\n                        Y += y1 - 1\n                        X += x1 - 1\n                        potential_tips[Y, X] = 1\n                    potential_tips[y1, x1] = 1\n    return potential_tips\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.get_var","title":"<code>get_var(mo, binary_image, Xn, Yn)</code>","text":"<p>Compute the center of mass in 2D space.</p> <p>This function calculates the weighted average position (centroid) of a binary image using given pixel coordinates and moments.</p> <p>Parameters:</p> Name Type Description Default <code>mo</code> <code>dict</code> <p>Dictionary containing moments of binary image.</p> required <code>binary_image</code> <code>ndarray</code> <p>2D binary image where non-zero pixels are considered.</p> required <code>Xn</code> <code>ndarray</code> <p>Array of x-coordinates for each pixel in <code>binary_image</code>.</p> required <code>Yn</code> <code>ndarray</code> <p>Array of y-coordinates for each pixel in <code>binary_image</code>.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple of two floats <code>(vx, vy)</code> representing the centroid coordinates.</p> <p>Raises:</p> Type Description <code>ZeroDivisionError</code> <p>If <code>mo['m00']</code> is zero, indicating no valid pixels in the image. The function raises a <code>ZeroDivisionError</code>.</p> Notes <p>Performance considerations: This function uses Numba's <code>@njit</code> decorator for performance.</p> Source code in <code>src/cellects/utils/formulas.py</code> <pre><code>@njit()\ndef get_var(mo: dict, binary_image: NDArray, Xn: NDArray, Yn: NDArray) -&gt; Tuple[float, float]:\n    \"\"\"\n    Compute the center of mass in 2D space.\n\n    This function calculates the weighted average position (centroid) of\n    a binary image using given pixel coordinates and moments.\n\n    Parameters\n    ----------\n    mo : dict\n        Dictionary containing moments of binary image.\n    binary_image : ndarray\n        2D binary image where non-zero pixels are considered.\n    Xn : ndarray\n        Array of x-coordinates for each pixel in `binary_image`.\n    Yn : ndarray\n        Array of y-coordinates for each pixel in `binary_image`.\n\n    Returns\n    -------\n    tuple\n        A tuple of two floats `(vx, vy)` representing the centroid coordinates.\n\n    Raises\n    ------\n    ZeroDivisionError\n        If `mo['m00']` is zero, indicating no valid pixels in the image.\n        The function raises a `ZeroDivisionError`.\n\n    Notes\n    -----\n    Performance considerations: This function uses Numba's `@njit` decorator for performance.\n    \"\"\"\n    if mo['m00'] == 0:\n        vx, vy = 0., 0.\n    else:\n        vx = np.sum(binary_image * Xn) / mo[\"m00\"]\n        vy = np.sum(binary_image * Yn) / mo[\"m00\"]\n    return vx, vy\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.get_vertices_and_tips_from_skeleton","title":"<code>get_vertices_and_tips_from_skeleton(pad_skeleton)</code>","text":"<p>Get vertices and tips from a padded skeleton.</p> <p>This function identifies the vertices and tips of a skeletonized image. Tips are endpoints of the skeleton while vertices include tips and points where three or more edges meet.</p> <p>Parameters:</p> Name Type Description Default <code>pad_skeleton</code> <code>ndarray of uint8</code> <p>Input skeleton image that has been padded.</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>tuple (ndarray of uint8, ndarray of uint8)</code> <p>Tuple containing arrays of vertex points and tip points.</p> Source code in <code>src/cellects/image_analysis/network_functions.py</code> <pre><code>def get_vertices_and_tips_from_skeleton(pad_skeleton: NDArray[np.uint8]) -&gt; Tuple[NDArray[np.uint8], NDArray[np.uint8]]:\n    \"\"\"\n    Get vertices and tips from a padded skeleton.\n\n    This function identifies the vertices and tips of a skeletonized image.\n    Tips are endpoints of the skeleton while vertices include tips and points where three or more edges meet.\n\n    Parameters\n    ----------\n    pad_skeleton : ndarray of uint8\n        Input skeleton image that has been padded.\n\n    Returns\n    -------\n    out : tuple (ndarray of uint8, ndarray of uint8)\n        Tuple containing arrays of vertex points and tip points.\n    \"\"\"\n    cnv4, cnv8 = get_neighbor_comparisons(pad_skeleton)\n    potential_tips = get_terminations_and_their_connected_nodes(pad_skeleton, cnv4, cnv8)\n    pad_vertices, pad_tips = get_inner_vertices(pad_skeleton, potential_tips, cnv4, cnv8)\n    return pad_vertices, pad_tips\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.is_raw_image","title":"<code>is_raw_image(image_path)</code>","text":"<p>Determine if the image path corresponds to a raw image.</p> <p>Parameters:</p> Name Type Description Default <code>image_path</code> <code>str</code> <p>The file path of the image.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the image is considered raw, False otherwise.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; result = is_raw_image(\"image.jpg\")\n&gt;&gt;&gt; print(result)\nFalse\n</code></pre> Source code in <code>src/cellects/utils/load_display_save.py</code> <pre><code>def is_raw_image(image_path) -&gt; bool:\n    \"\"\"\n    Determine if the image path corresponds to a raw image.\n\n    Parameters\n    ----------\n    image_path : str\n        The file path of the image.\n\n    Returns\n    -------\n    bool\n        True if the image is considered raw, False otherwise.\n\n    Examples\n    --------\n    &gt;&gt;&gt; result = is_raw_image(\"image.jpg\")\n    &gt;&gt;&gt; print(result)\n    False\n    \"\"\"\n    ext = image_path.split(\".\")[-1]\n    if np.isin(ext, opencv_accepted_formats):\n        raw_image = False\n    else:\n        raw_image = True\n    return raw_image\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.linear_model","title":"<code>linear_model(x, a, b)</code>","text":"<p>Perform a linear transformation on input data using slope and intercept.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array_like</code> <p>Input data.</p> required <code>a</code> <code>float</code> <p>Slope coefficient.</p> required <code>b</code> <code>float</code> <p>Intercept.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Resulting value from linear transformation: <code>a</code> * <code>x</code> + <code>b</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; result = linear_model(5, 2.0, 1.5)\n&gt;&gt;&gt; print(result)\n11.5\n</code></pre> Notes <p>This function uses Numba's @njit decorator for performance.</p> Source code in <code>src/cellects/utils/formulas.py</code> <pre><code>@njit()\ndef linear_model(x: NDArray, a: float, b: float) -&gt; float:\n    \"\"\"\n    Perform a linear transformation on input data using slope and intercept.\n\n    Parameters\n    ----------\n    x : array_like\n        Input data.\n    a : float\n        Slope coefficient.\n    b : float\n        Intercept.\n\n    Returns\n    -------\n    float\n        Resulting value from linear transformation: `a` * `x` + `b`.\n\n    Examples\n    --------\n    &gt;&gt;&gt; result = linear_model(5, 2.0, 1.5)\n    &gt;&gt;&gt; print(result)  # doctest: +SKIP\n    11.5\n\n    Notes\n    -----\n    This function uses Numba's @njit decorator for performance.\n    \"\"\"\n    return a * x + b\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.movie","title":"<code>movie(video, increase_contrast=True)</code>","text":"Summary <p>Processes a video to display each frame with optional contrast increase and resizing.</p> <p>Parameters:</p> Name Type Description Default <code>video</code> <code>ndarray</code> <p>The input video represented as a 3D NumPy array.</p> required <code>increase_contrast</code> <code>bool</code> <p>Flag to increase the contrast of each frame (default is True).</p> <code>True</code> <p>Other Parameters:</p> Name Type Description <code>keyboard</code> <code>int</code> <p>Key to wait for during the display of each frame.</p> <code>increase_contrast</code> <code>bool</code> <p>Whether to increase contrast for the displayed frames.</p> <p>Returns:</p> Type Description <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>video</code> is not a 3D NumPy array.</p> Notes <p>This function uses OpenCV's <code>imshow</code> to display each frame. Ensure that the required OpenCV dependencies are met.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; movie(video)\nProcesses and displays a video with default settings.\n&gt;&gt;&gt; movie(video, keyboard=0)\nProcesses and displays a video waiting for the SPACE key between frames.\n&gt;&gt;&gt; movie(video, increase_contrast=False)\nProcesses and displays a video without increasing contrast.\n</code></pre> Source code in <code>src/cellects/utils/load_display_save.py</code> <pre><code>def movie(video, increase_contrast: bool=True):\n    \"\"\"\n    Summary\n    -------\n    Processes a video to display each frame with optional contrast increase and resizing.\n\n    Parameters\n    ----------\n    video : numpy.ndarray\n        The input video represented as a 3D NumPy array.\n    increase_contrast : bool, optional\n        Flag to increase the contrast of each frame (default is True).\n\n    Other Parameters\n    ----------------\n    keyboard : int, optional\n        Key to wait for during the display of each frame.\n    increase_contrast : bool, optional\n        Whether to increase contrast for the displayed frames.\n\n    Returns\n    -------\n    None\n\n    Raises\n    ------\n    ValueError\n        If `video` is not a 3D NumPy array.\n\n    Notes\n    -----\n    This function uses OpenCV's `imshow` to display each frame. Ensure that the required\n    OpenCV dependencies are met.\n\n    Examples\n    --------\n    &gt;&gt;&gt; movie(video)\n    Processes and displays a video with default settings.\n    &gt;&gt;&gt; movie(video, keyboard=0)\n    Processes and displays a video waiting for the SPACE key between frames.\n    &gt;&gt;&gt; movie(video, increase_contrast=False)\n    Processes and displays a video without increasing contrast.\n\n    \"\"\"\n    for i in np.arange(video.shape[0]):\n        image = video[i, :, :]\n        if np.any(image):\n            if increase_contrast:\n                image = bracket_to_uint8_image_contrast(image)\n            final_img = cv2.resize(image, (500, 500))\n            cv2.imshow('Motion analysis', final_img)\n            if cv2.waitKey(25) &amp; 0xFF == ord('q'):\n                break\n    cv2.destroyAllWindows()\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.moving_average","title":"<code>moving_average(vector, step)</code>","text":"<p>Calculate the moving average of a given vector with specified step size.</p> <p>Computes the moving average of input <code>vector</code> using specified <code>step</code> size. NaN values are treated as zeros in the calculation to allow for continuous averaging.</p> <p>Parameters:</p> Name Type Description Default <code>vector</code> <code>ndarray</code> <p>Input vector for which to calculate the moving average.</p> required <code>step</code> <code>int</code> <p>Size of the window for computing the moving average.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Vector containing the moving averages of the input vector.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>step</code> is less than 1.</p> <code>ValueError</code> <p>If the input vector has no valid (non-NaN) elements.</p> Notes <ul> <li>The function considers NaN values as zeros during the averaging process.</li> <li>If <code>step</code> is greater than or equal to the length of the vector, a warning will be raised.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; vector = np.array([1.0, 2.0, np.nan, 4.0, 5.0])\n&gt;&gt;&gt; step = 3\n&gt;&gt;&gt; result = moving_average(vector, step)\n&gt;&gt;&gt; print(result)\n[1.5 2.33333333 3.66666667 4.         nan]\n</code></pre> Source code in <code>src/cellects/utils/formulas.py</code> <pre><code>def moving_average(vector: NDArray, step: int) -&gt; NDArray[float]:\n    \"\"\"\n    Calculate the moving average of a given vector with specified step size.\n\n    Computes the moving average of input `vector` using specified `step`\n    size. NaN values are treated as zeros in the calculation to allow\n    for continuous averaging.\n\n    Parameters\n    ----------\n    vector : ndarray\n        Input vector for which to calculate the moving average.\n    step : int\n        Size of the window for computing the moving average.\n\n    Returns\n    -------\n    numpy.ndarray\n        Vector containing the moving averages of the input vector.\n\n    Raises\n    ------\n    ValueError\n        If `step` is less than 1.\n    ValueError\n        If the input vector has no valid (non-NaN) elements.\n\n    Notes\n    -----\n    - The function considers NaN values as zeros during the averaging process.\n    - If `step` is greater than or equal to the length of the vector, a warning will be raised.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; vector = np.array([1.0, 2.0, np.nan, 4.0, 5.0])\n    &gt;&gt;&gt; step = 3\n    &gt;&gt;&gt; result = moving_average(vector, step)\n    &gt;&gt;&gt; print(result)\n    [1.5 2.33333333 3.66666667 4.         nan]\n    \"\"\"\n    substep = np.array((- int(np.floor((step - 1) / 2)), int(np.ceil((step - 1) / 2))))\n    sums = np.zeros(vector.shape)\n    n_okays = deepcopy(sums)\n    true_numbers = np.logical_not(np.isnan(vector))\n    vector[np.logical_not(true_numbers)] = 0\n    for step_i in np.arange(substep[1] + 1):\n        sums[step_i: (sums.size - step_i)] = sums[step_i: (sums.size - step_i)] + vector[(2 * step_i):]\n        n_okays[step_i: (sums.size - step_i)] = n_okays[step_i: (sums.size - step_i)] + true_numbers[(2 * step_i):]\n        if np.logical_and(step_i &gt; 0, step_i &lt; np.absolute(substep[0])):\n            sums[step_i: (sums.size - step_i)] = sums[step_i: (sums.size - step_i)] + vector[:(sums.size - (2 * step_i)):]\n            n_okays[step_i: (sums.size - step_i)] = n_okays[step_i: (sums.size - step_i)] + true_numbers[:(\n                        true_numbers.size - (2 * step_i))]\n    vector = sums / n_okays\n    return vector\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.njit","title":"<code>njit(*args, **kwargs)</code>","text":"<p>numba.njit decorator that can be disabled. Useful for testing.</p> Source code in <code>src/cellects/utils/decorators.py</code> <pre><code>def njit(*args, **kwargs):\n    \"\"\" numba.njit decorator that can be disabled. Useful for testing.\n    \"\"\"\n    if USE_NUMBA:\n        return _real_njit(*args, **kwargs)\n    # test mode: return identity decorator\n    def deco(func):\n        return func\n    return deco\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.read_and_rotate","title":"<code>read_and_rotate(image_name, prev_img=None, raw_images=False, is_landscape=True, crop_coord=None)</code>","text":"<p>Read and rotate an image based on specified parameters.</p> <p>This function reads an image from the given file name, optionally rotates it by 90 degrees clockwise or counterclockwise based on its dimensions and the <code>is_landscape</code> flag, and applies cropping if specified. It also compares rotated images against a previous image to choose the best rotation.</p> <p>Parameters:</p> Name Type Description Default <code>image_name</code> <code>str</code> <p>Name of the image file to read.</p> required <code>prev_img</code> <code>ndarray</code> <p>Previous image for comparison. Default is <code>None</code>.</p> <code>None</code> <code>raw_images</code> <code>bool</code> <p>Flag to read raw images. Default is <code>False</code>.</p> <code>False</code> <code>is_landscape</code> <code>bool</code> <p>Flag to determine if the image should be considered in landscape mode. Default is <code>True</code>.</p> <code>True</code> <code>crop_coord</code> <code>ndarray</code> <p>Coordinates for cropping the image. Default is <code>None</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Rotated and optionally cropped image.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the specified image file does not exist.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pathway = Path(__name__).resolve().parents[0] / \"data\" / \"single_experiment\"\n&gt;&gt;&gt; image_name = 'image1.tif'\n&gt;&gt;&gt; image = read_and_rotate(pathway /image_name)\n&gt;&gt;&gt; print(image.shape)\n(245, 300, 3)\n</code></pre> Source code in <code>src/cellects/utils/load_display_save.py</code> <pre><code>def read_and_rotate(image_name, prev_img: NDArray=None, raw_images: bool=False, is_landscape: bool=True, crop_coord: NDArray=None) -&gt; NDArray:\n    \"\"\"\n    Read and rotate an image based on specified parameters.\n\n    This function reads an image from the given file name, optionally rotates\n    it by 90 degrees clockwise or counterclockwise based on its dimensions and\n    the `is_landscape` flag, and applies cropping if specified. It also compares\n    rotated images against a previous image to choose the best rotation.\n\n    Parameters\n    ----------\n    image_name : str\n        Name of the image file to read.\n    prev_img : ndarray, optional\n        Previous image for comparison. Default is `None`.\n    raw_images : bool, optional\n        Flag to read raw images. Default is `False`.\n    is_landscape : bool, optional\n        Flag to determine if the image should be considered in landscape mode.\n        Default is `True`.\n    crop_coord : ndarray, optional\n        Coordinates for cropping the image. Default is `None`.\n\n    Returns\n    -------\n    ndarray\n        Rotated and optionally cropped image.\n\n    Raises\n    ------\n    FileNotFoundError\n        If the specified image file does not exist.\n\n    Examples\n    ------\n    &gt;&gt;&gt; pathway = Path(__name__).resolve().parents[0] / \"data\" / \"single_experiment\"\n    &gt;&gt;&gt; image_name = 'image1.tif'\n    &gt;&gt;&gt; image = read_and_rotate(pathway /image_name)\n    &gt;&gt;&gt; print(image.shape)\n    (245, 300, 3)\n    \"\"\"\n    if not os.path.exists(image_name):\n        raise FileNotFoundError(image_name)\n    img = readim(image_name, raw_images)\n    if (img.shape[0] &gt; img.shape[1] and is_landscape) or (img.shape[0] &lt; img.shape[1] and not is_landscape):\n        clockwise = cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)\n        if crop_coord is not None:\n            clockwise = clockwise[crop_coord[0]:crop_coord[1], crop_coord[2]:crop_coord[3], ...]\n        if prev_img is not None:\n            prev_img = np.int16(prev_img)\n            clock_diff = sum_of_abs_differences(prev_img, np.int16(clockwise))\n            counter_clockwise = cv2.rotate(img, cv2.ROTATE_90_COUNTERCLOCKWISE)\n            if crop_coord is not None:\n                counter_clockwise = counter_clockwise[crop_coord[0]:crop_coord[1], crop_coord[2]:crop_coord[3], ...]\n            counter_clock_diff = sum_of_abs_differences(prev_img, np.int16(counter_clockwise))\n            if clock_diff &gt; counter_clock_diff:\n                img = counter_clockwise\n            else:\n                img = clockwise\n        else:\n            img = clockwise\n    else:\n        if crop_coord is not None:\n            img = img[crop_coord[0]:crop_coord[1], crop_coord[2]:crop_coord[3], ...]\n    return img\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.read_h5_array","title":"<code>read_h5_array(file_name, key='data')</code>","text":"<p>Read data array from an HDF5 file.</p> <p>This function reads a specific dataset from an HDF5 file using the provided key.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>The path to the HDF5 file.</p> required <code>key</code> <code>str</code> <p>The dataset name within the HDF5 file.</p> <code>'data'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The data array from the specified dataset in the HDF5 file.</p> Source code in <code>src/cellects/utils/load_display_save.py</code> <pre><code>def read_h5_array(file_name, key: str=\"data\"):\n    \"\"\"\n    Read data array from an HDF5 file.\n\n    This function reads a specific dataset from an HDF5 file using the provided key.\n\n    Parameters\n    ----------\n    file_name : str\n        The path to the HDF5 file.\n    key : str, optional, default: 'data'\n        The dataset name within the HDF5 file.\n\n    Returns\n    -------\n    ndarray\n        The data array from the specified dataset in the HDF5 file.\n    \"\"\"\n    try:\n        with h5py.File(file_name, 'r') as h5f:\n            if key in h5f:\n                data = h5f[key][:]\n                return data\n            else:\n                raise KeyError(f\"Dataset '{key}' not found in file '{file_name}'.\")\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"The file '{file_name}' does not exist.\")\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.read_one_arena","title":"<code>read_one_arena(arena_label, already_greyscale, csc_dict, videos_already_in_ram=None, true_frame_width=None, vid_name=None, background=None, background2=None)</code>","text":"<p>Read a single arena's video data, potentially converting it from color to greyscale.</p> <p>Parameters:</p> Name Type Description Default <code>arena_label</code> <code>int</code> <p>The label of the arena.</p> required <code>already_greyscale</code> <code>bool</code> <p>Whether the video is already in greyscale format.</p> required <code>csc_dict</code> <code>dict</code> <p>Dictionary containing color space conversion settings.</p> required <code>videos_already_in_ram</code> <code>ndarray</code> <p>Pre-loaded video frames in memory. Default is None.</p> <code>None</code> <code>true_frame_width</code> <code>int</code> <p>The true width of the video frames. Default is None.</p> <code>None</code> <code>vid_name</code> <code>str</code> <p>Name of the video file. Default is None.</p> <code>None</code> <code>background</code> <code>ndarray</code> <p>Background image for subtractions. Default is None.</p> <code>None</code> <code>background2</code> <code>ndarray</code> <p>Second background image for subtractions. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing:     - visu: np.ndarray or None, the visual frame.     - converted_video: np.ndarray or None, the video data converted as needed.     - converted_video2: np.ndarray or None, additional video data if necessary.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the specified video file does not exist.</p> <code>ValueError</code> <p>If the video data shape is invalid.</p> Notes <p>This function assumes that <code>video2numpy</code> is a helper function available in the scope. For optimal performance, ensure all video data fits in RAM.</p> Source code in <code>src/cellects/utils/load_display_save.py</code> <pre><code>def read_one_arena(arena_label, already_greyscale:bool, csc_dict: dict, videos_already_in_ram=None,\n                   true_frame_width=None, vid_name: str=None, background: NDArray=None, background2: NDArray=None):\n    \"\"\"\n    Read a single arena's video data, potentially converting it from color to greyscale.\n\n    Parameters\n    ----------\n    arena_label : int\n        The label of the arena.\n    already_greyscale : bool\n        Whether the video is already in greyscale format.\n    csc_dict : dict\n        Dictionary containing color space conversion settings.\n    videos_already_in_ram : np.ndarray, optional\n        Pre-loaded video frames in memory. Default is None.\n    true_frame_width : int, optional\n        The true width of the video frames. Default is None.\n    vid_name : str, optional\n        Name of the video file. Default is None.\n    background : np.ndarray, optional\n        Background image for subtractions. Default is None.\n    background2 : np.ndarray, optional\n        Second background image for subtractions. Default is None.\n\n    Returns\n    -------\n    tuple\n        A tuple containing:\n            - visu: np.ndarray or None, the visual frame.\n            - converted_video: np.ndarray or None, the video data converted as needed.\n            - converted_video2: np.ndarray or None, additional video data if necessary.\n\n    Raises\n    ------\n    FileNotFoundError\n        If the specified video file does not exist.\n    ValueError\n        If the video data shape is invalid.\n\n    Notes\n    -----\n    This function assumes that `video2numpy` is a helper function available in the scope.\n    For optimal performance, ensure all video data fits in RAM.\n    \"\"\"\n    visu, converted_video, converted_video2 = None, None, None\n    logging.info(f\"Arena n\u00b0{arena_label}. Load images and videos\")\n    if videos_already_in_ram is not None:\n        if already_greyscale:\n            converted_video = videos_already_in_ram\n        else:\n            if csc_dict['logical'] == 'None':\n                visu, converted_video = videos_already_in_ram\n            else:\n                visu, converted_video, converted_video2 = videos_already_in_ram\n    else:\n        if vid_name is not None:\n            if already_greyscale:\n                converted_video = video2numpy(vid_name, None, background, background2, true_frame_width)\n                if len(converted_video.shape) == 4:\n                    converted_video = converted_video[:, :, :, 0]\n            else:\n                visu = video2numpy(vid_name, None, background, background2, true_frame_width)\n        else:\n            vid_name = f\"ind_{arena_label}.npy\"\n            if os.path.isfile(vid_name):\n                if already_greyscale:\n                    converted_video = video2numpy(vid_name, None, background, background2, true_frame_width)\n                    if len(converted_video.shape) == 4:\n                        converted_video = converted_video[:, :, :, 0]\n                else:\n                    visu = video2numpy(vid_name, None, background, background2, true_frame_width)\n    return visu, converted_video, converted_video2\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.readim","title":"<code>readim(image_path, raw_image=False)</code>","text":"<p>Read an image from a file and optionally process it.</p> <p>Parameters:</p> Name Type Description Default <code>image_path</code> <code>str</code> <p>Path to the image file.</p> required <code>raw_image</code> <code>bool</code> <p>If True, logs an error message indicating that the raw image format cannot be processed. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The decoded image represented as a NumPy array of shape (height, width, channels).</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If <code>raw_image</code> is set to True, logs an error indicating that the raw image format cannot be processed.</p> Notes <p>Although <code>raw_image</code> is set to False by default, currently it does not perform any raw image processing.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; cv2.imread(\"example.jpg\")\narray([[[255, 0, 0],\n        [255, 0, 0]],\n</code></pre> <pre><code>   [[  0, 255, 0],\n    [  0, 255, 0]],\n\n   [[  0,   0, 255],\n    [  0,   0, 255]]], dtype=np.uint8)\n</code></pre> Source code in <code>src/cellects/utils/load_display_save.py</code> <pre><code>def readim(image_path, raw_image: bool=False):\n    \"\"\"\n    Read an image from a file and optionally process it.\n\n    Parameters\n    ----------\n    image_path : str\n        Path to the image file.\n    raw_image : bool, optional\n        If True, logs an error message indicating that the raw image format cannot be processed. Default is False.\n\n    Returns\n    -------\n    ndarray\n        The decoded image represented as a NumPy array of shape (height, width, channels).\n\n    Raises\n    ------\n    RuntimeError\n        If `raw_image` is set to True, logs an error indicating that the raw image format cannot be processed.\n\n    Notes\n    -----\n    Although `raw_image` is set to False by default, currently it does not perform any raw image processing.\n\n    Examples\n    --------\n    &gt;&gt;&gt; cv2.imread(\"example.jpg\")\n    array([[[255, 0, 0],\n            [255, 0, 0]],\n\n           [[  0, 255, 0],\n            [  0, 255, 0]],\n\n           [[  0,   0, 255],\n            [  0,   0, 255]]], dtype=np.uint8)\n    \"\"\"\n    if raw_image:\n        logging.error(\"Cannot read this image format. If the rawpy package can, ask for a version of Cellects using it.\")\n        # import rawpy\n        # raw = rawpy.imread(image_path)\n        # raw = raw.postprocess()\n        # return cv2.cvtColor(raw, COLOR_RGB2BGR)\n        return cv2.imread(image_path)\n    else:\n        return cv2.imread(image_path)\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.remove_h5_key","title":"<code>remove_h5_key(file_name, key='data')</code>","text":"<p>Remove a specified key from an HDF5 file.</p> <p>This function opens an HDF5 file in append mode and deletes the specified key if it exists. It handles exceptions related to file not found and other runtime errors.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>The path to the HDF5 file from which the key should be removed.</p> required <code>key</code> <code>str</code> <p>The name of the dataset or group to delete from the HDF5 file. Default is \"data\".</p> <code>'data'</code> <p>Returns:</p> Type Description <code>None</code> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the specified file does not exist.</p> <code>RuntimeError</code> <p>If any other error occurs during file operations.</p> Notes <p>This function modifies the HDF5 file in place. Ensure you have a backup if necessary.</p> Source code in <code>src/cellects/utils/load_display_save.py</code> <pre><code>def remove_h5_key(file_name, key: str=\"data\"):\n    \"\"\"\n    Remove a specified key from an HDF5 file.\n\n    This function opens an HDF5 file in append mode and deletes the specified\n    key if it exists. It handles exceptions related to file not found\n    and other runtime errors.\n\n    Parameters\n    ----------\n    file_name : str\n        The path to the HDF5 file from which the key should be removed.\n    key : str, optional\n        The name of the dataset or group to delete from the HDF5 file.\n        Default is \"data\".\n\n    Returns\n    -------\n    None\n\n    Raises\n    ------\n    FileNotFoundError\n        If the specified file does not exist.\n    RuntimeError\n        If any other error occurs during file operations.\n\n    Notes\n    -----\n    This function modifies the HDF5 file in place. Ensure you have a backup if necessary.\n    \"\"\"\n    try:\n        with h5py.File(file_name, 'a') as h5f:  # Open in append mode to modify the file\n            if key in h5f:\n                del h5f[key]\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"The file '{file_name}' does not exist.\")\n    except Exception as e:\n        raise RuntimeError(f\"An error occurred: {e}\")\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.remove_padding","title":"<code>remove_padding(array_list)</code>","text":"<p>Remove padding from a list of 2D arrays.</p> <p>Parameters:</p> Name Type Description Default <code>array_list</code> <code>list of ndarrays</code> <p>List of 2D NumPy arrays to be processed.</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>list of ndarrays</code> <p>List of 2D NumPy arrays with the padding removed.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; arr1 = np.array([[0, 0, 0], [0, 1, 0], [0, 0, 0]])\n&gt;&gt;&gt; arr2 = np.array([[1, 1, 1], [1, 0, 1], [1, 1, 1]])\n&gt;&gt;&gt; remove_padding([arr1, arr2])\n[array([[1]]), array([[0]])]\n</code></pre> Source code in <code>src/cellects/image_analysis/network_functions.py</code> <pre><code>def remove_padding(array_list: list) -&gt; list:\n    \"\"\"\n    Remove padding from a list of 2D arrays.\n\n    Parameters\n    ----------\n    array_list : list of ndarrays\n        List of 2D NumPy arrays to be processed.\n\n    Returns\n    -------\n    out : list of ndarrays\n        List of 2D NumPy arrays with the padding removed.\n\n    Examples\n    --------\n    &gt;&gt;&gt; arr1 = np.array([[0, 0, 0], [0, 1, 0], [0, 0, 0]])\n    &gt;&gt;&gt; arr2 = np.array([[1, 1, 1], [1, 0, 1], [1, 1, 1]])\n    &gt;&gt;&gt; remove_padding([arr1, arr2])\n    [array([[1]]), array([[0]])]\n    \"\"\"\n    new_array_list = []\n    for arr in array_list:\n        new_array_list.append(un_pad(arr))\n    return new_array_list\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.remove_small_loops","title":"<code>remove_small_loops(pad_skeleton, pad_distances=None)</code>","text":"<p>Remove small loops from a skeletonized image.</p> <p>This function identifies and removes small loops in a skeletonized image, returning the modified skeleton. If distance information is provided, it updates that as well.</p> <p>Parameters:</p> Name Type Description Default <code>pad_skeleton</code> <code>ndarray of uint8</code> <p>The skeletonized image with potential small loops.</p> required <code>pad_distances</code> <code>ndarray of float64</code> <p>The distance map corresponding to the skeleton image. Default is <code>None</code>.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>out</code> <code>ndarray of uint8 or tuple(ndarray of uint8, ndarray of float64)</code> <p>If <code>pad_distances</code> is None, returns the modified skeleton. Otherwise, returns a tuple of the modified skeleton and updated distances.</p> Source code in <code>src/cellects/image_analysis/network_functions.py</code> <pre><code>def remove_small_loops(pad_skeleton: NDArray[np.uint8], pad_distances: NDArray[np.float64]=None):\n    \"\"\"\n    Remove small loops from a skeletonized image.\n\n    This function identifies and removes small loops in a skeletonized image, returning the modified skeleton.\n    If distance information is provided, it updates that as well.\n\n    Parameters\n    ----------\n    pad_skeleton : ndarray of uint8\n        The skeletonized image with potential small loops.\n    pad_distances : ndarray of float64, optional\n        The distance map corresponding to the skeleton image. Default is `None`.\n\n    Returns\n    -------\n    out : ndarray of uint8 or tuple(ndarray of uint8, ndarray of float64)\n        If `pad_distances` is None, returns the modified skeleton. Otherwise,\n        returns a tuple of the modified skeleton and updated distances.\n    \"\"\"\n    cnv4, cnv8 = get_neighbor_comparisons(pad_skeleton)\n    # potential_tips = get_terminations_and_their_connected_nodes(pad_skeleton, cnv4, cnv8)\n\n    cnv_diag_0 = CompareNeighborsWithValue(pad_skeleton, 0)\n    cnv_diag_0.is_equal(0, and_itself=True)\n\n    cnv4_false = CompareNeighborsWithValue(pad_skeleton, 4)\n    cnv4_false.is_equal(1, and_itself=False)\n\n    loop_centers = np.logical_and((cnv4_false.equal_neighbor_nb == 4), cnv_diag_0.equal_neighbor_nb &gt; 2).astype(np.uint8)\n\n    surrounding = cv2.dilate(loop_centers, kernel=square_33)\n    surrounding -= loop_centers\n    surrounding = surrounding * cnv8.equal_neighbor_nb\n\n    # Every 2 can be replaced by 0 if the loop center becomes 1\n    filled_loops = pad_skeleton.copy()\n    filled_loops[surrounding == 2] = 0\n    filled_loops += loop_centers\n\n    new_pad_skeleton = morphology.skeletonize(filled_loops, method='lee')\n\n    # Put the new pixels in pad_distances\n    new_pixels = new_pad_skeleton * (1 - pad_skeleton)\n    pad_skeleton = new_pad_skeleton.astype(np.uint8)\n    if pad_distances is None:\n        return pad_skeleton\n    else:\n        pad_distances[np.nonzero(new_pixels)] = np.nan # 2. # Put nearest value instead?\n        pad_distances *= pad_skeleton\n        # for yi, xi in zip(npY, npX): # yi, xi = npY[0], npX[0]\n        #     distances[yi, xi] = 2.\n        return pad_skeleton, pad_distances\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.save_fig","title":"<code>save_fig(img, full_path, cmap=None)</code>","text":"<p>Save an image figure to a file with specified options.</p> <p>This function creates a matplotlib figure from the given image, optionally applies a colormap, displays it briefly, saves the figure to disk at high resolution, and closes the figure.</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>array_like(M, N, 3)</code> <p>Input image to be saved as a figure. Expected to be in RGB format.</p> required <code>full_path</code> <code>str</code> <p>The complete file path where the figure will be saved. Must include extension (e.g., '.png', '.jpg').</p> required <code>cmap</code> <code>str or None</code> <p>Colormap to be applied if the image should be displayed with a specific color map. If <code>None</code>, no colormap is applied.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>This function does not return any value. It saves the figure to disk at the specified location.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the directory in <code>full_path</code> does not exist.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; img = np.random.rand(100, 100, 3) * 255\n&gt;&gt;&gt; save_fig(img, 'test.png')\nCreates and saves a figure from the random image to 'test.png'.\n</code></pre> <pre><code>&gt;&gt;&gt; save_fig(img, 'colored_test.png', cmap='viridis')\nCreates and saves a figure from the random image with 'viridis' colormap\nto 'colored_test.png'.\n</code></pre> Source code in <code>src/cellects/utils/load_display_save.py</code> <pre><code>def save_fig(img: NDArray, full_path, cmap=None):\n    \"\"\"\n    Save an image figure to a file with specified options.\n\n    This function creates a matplotlib figure from the given image,\n    optionally applies a colormap, displays it briefly, saves the\n    figure to disk at high resolution, and closes the figure.\n\n    Parameters\n    ----------\n    img : array_like (M, N, 3)\n        Input image to be saved as a figure. Expected to be in RGB format.\n    full_path : str\n        The complete file path where the figure will be saved. Must include\n        extension (e.g., '.png', '.jpg').\n    cmap : str or None, optional\n        Colormap to be applied if the image should be displayed with a specific\n        color map. If `None`, no colormap is applied.\n\n    Returns\n    -------\n    None\n\n        This function does not return any value. It saves the figure to disk\n        at the specified location.\n\n    Raises\n    ------\n    FileNotFoundError\n        If the directory in `full_path` does not exist.\n\n    Examples\n    --------\n    &gt;&gt;&gt; img = np.random.rand(100, 100, 3) * 255\n    &gt;&gt;&gt; save_fig(img, 'test.png')\n    Creates and saves a figure from the random image to 'test.png'.\n\n    &gt;&gt;&gt; save_fig(img, 'colored_test.png', cmap='viridis')\n    Creates and saves a figure from the random image with 'viridis' colormap\n    to 'colored_test.png'.\n    \"\"\"\n    sizes = img.shape[0] / 100,  img.shape[1] / 100\n    fig = plt.figure(figsize=(sizes[0], sizes[1]))\n    ax = fig.gca()\n    if cmap is None:\n        ax.imshow(img, interpolation=\"none\")\n    else:\n        ax.imshow(img, cmap=cmap, interpolation=\"none\")\n    plt.axis('off')\n    if np.min(img.shape) &gt; 50:\n        fig.tight_layout()\n\n    fig.savefig(full_path, bbox_inches='tight', pad_inches=0., transparent=True, dpi=500)\n    plt.close(fig)\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.scale_coordinates","title":"<code>scale_coordinates(coord, scale, dims)</code>","text":"<p>Scale coordinates based on given scale factors and dimensions.</p> <p>Parameters:</p> Name Type Description Default <code>coord</code> <code>ndarray</code> <p>A 2x2 array of coordinates to be scaled.</p> required <code>scale</code> <code>tuple of float</code> <p>Scaling factors for the x and y coordinates, respectively.</p> required <code>dims</code> <code>tuple of int</code> <p>Maximum dimensions (height, width) for the scaled coordinates.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Scaled and rounded coordinates.</p> <code>int</code> <p>Minimum y-coordinate.</p> <code>int</code> <p>Maximum y-coordinate.</p> <code>int</code> <p>Minimum x-coordinate.</p> <code>int</code> <p>Maximum x-coordinate.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; coord = np.array(((47, 38), (59, 37)))\n&gt;&gt;&gt; scale = (0.92, 0.87)\n&gt;&gt;&gt; dims = (245, 300, 3)\n&gt;&gt;&gt; scaled_coord, min_y, max_y, min_x, max_x = scale_coordinates(coord, scale, dims)\n&gt;&gt;&gt; scaled_coord\narray([[43, 33],\n       [54, 32]])\n&gt;&gt;&gt; min_y, max_y\n(np.int64(43), np.int64(54))\n&gt;&gt;&gt; min_x, max_x\n(np.int64(32), np.int64(33))\n</code></pre> Notes <p>This function assumes that the input coordinates are in a specific format and will fail if not. The scaling factors should be positive.</p> Source code in <code>src/cellects/utils/formulas.py</code> <pre><code>def scale_coordinates(coord: NDArray, scale: Tuple, dims: Tuple) -&gt; Tuple[NDArray[np.int64], np.int64, np.int64, np.int64, np.int64]:\n    \"\"\"\n    Scale coordinates based on given scale factors and dimensions.\n\n    Parameters\n    ----------\n    coord : numpy.ndarray\n        A 2x2 array of coordinates to be scaled.\n    scale : tuple of float\n        Scaling factors for the x and y coordinates, respectively.\n    dims : tuple of int\n        Maximum dimensions (height, width) for the scaled coordinates.\n\n    Returns\n    -------\n    numpy.ndarray\n        Scaled and rounded coordinates.\n    int\n        Minimum y-coordinate.\n    int\n        Maximum y-coordinate.\n    int\n        Minimum x-coordinate.\n    int\n        Maximum x-coordinate.\n\n    Examples\n    --------\n    &gt;&gt;&gt; coord = np.array(((47, 38), (59, 37)))\n    &gt;&gt;&gt; scale = (0.92, 0.87)\n    &gt;&gt;&gt; dims = (245, 300, 3)\n    &gt;&gt;&gt; scaled_coord, min_y, max_y, min_x, max_x = scale_coordinates(coord, scale, dims)\n    &gt;&gt;&gt; scaled_coord\n    array([[43, 33],\n           [54, 32]])\n    &gt;&gt;&gt; min_y, max_y\n    (np.int64(43), np.int64(54))\n    &gt;&gt;&gt; min_x, max_x\n    (np.int64(32), np.int64(33))\n\n    Notes\n    -----\n    This function assumes that the input coordinates are in a specific format\n    and will fail if not. The scaling factors should be positive.\n    \"\"\"\n    coord = np.array(((np.round(coord[0][0] * scale[0]), np.round(coord[0][1] * scale[1])),\n                    (np.round(coord[1][0] * scale[0]), np.round(coord[1][1] * scale[1]))), dtype=np.int64)\n    min_y = np.max((0, np.min(coord[:, 0])))\n    max_y = np.min((dims[0], np.max(coord[:, 0])))\n    min_x = np.max((0, np.min(coord[:, 1])))\n    max_x = np.min((dims[1], np.max(coord[:, 1])))\n    return coord, min_y, max_y, min_x, max_x\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.show","title":"<code>show(img, interactive=True, cmap=None, show=True)</code>","text":"<p>Display an image using Matplotlib with optional interactivity and colormap.</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>ndarray</code> <p>The image data to be displayed.</p> required <code>interactive</code> <code>bool</code> <p>If <code>True</code>, turn on interactive mode. Default is <code>True</code>.</p> <code>True</code> <code>cmap</code> <code>str or Colormap</code> <p>The colormap to be used. If <code>None</code>, the default colormap will be used.</p> <code>None</code> <p>Other Parameters:</p> Name Type Description <code>interactive</code> <code>bool</code> <p>If <code>True</code>, turn on interactive mode. Default is <code>True</code>.</p> <code>cmap</code> <code>str or Colormap</code> <p>The colormap to be used. If <code>None</code>, the default colormap will be used.</p> <p>Returns:</p> Name Type Description <code>fig</code> <code>Figure</code> <p>The Matplotlib figure object containing the displayed image.</p> <code>ax</code> <code>AxesSubplot</code> <p>The axes on which the image is plotted.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>cmap</code> is not a recognized colormap name or object.</p> Notes <p>If interactive mode is enabled, the user can manipulate the figure window interactively.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; img = np.random.rand(100, 50)\n&gt;&gt;&gt; fig, ax = show(img)\n&gt;&gt;&gt; print(fig)\n&lt;Figure size ... with ... Axes&gt;\n</code></pre> <pre><code>&gt;&gt;&gt; fig, ax = show(img, interactive=False)\n&gt;&gt;&gt; print(fig)\n&lt;Figure size ... with ... Axes&gt;\n</code></pre> <pre><code>&gt;&gt;&gt; fig, ax = show(img, cmap='gray')\n&gt;&gt;&gt; print(fig)\n&lt;Figure size ... with .... Axes&gt;\n</code></pre> Source code in <code>src/cellects/utils/load_display_save.py</code> <pre><code>def show(img, interactive: bool=True, cmap=None, show: bool=True):\n    \"\"\"\n    Display an image using Matplotlib with optional interactivity and colormap.\n\n    Parameters\n    ----------\n    img : ndarray\n        The image data to be displayed.\n    interactive : bool, optional\n        If ``True``, turn on interactive mode. Default is ``True``.\n    cmap : str or Colormap, optional\n        The colormap to be used. If ``None``, the default colormap will\n        be used.\n\n    Other Parameters\n    ----------------\n    interactive : bool, optional\n        If ``True``, turn on interactive mode. Default is ``True``.\n    cmap : str or Colormap, optional\n        The colormap to be used. If ``None``, the default colormap will\n        be used.\n\n    Returns\n    -------\n    fig : Figure\n        The Matplotlib figure object containing the displayed image.\n    ax : AxesSubplot\n        The axes on which the image is plotted.\n\n    Raises\n    ------\n    ValueError\n        If `cmap` is not a recognized colormap name or object.\n\n    Notes\n    -----\n    If interactive mode is enabled, the user can manipulate the figure\n    window interactively.\n\n    Examples\n    --------\n    &gt;&gt;&gt; img = np.random.rand(100, 50)\n    &gt;&gt;&gt; fig, ax = show(img)\n    &gt;&gt;&gt; print(fig) # doctest: +SKIP\n    &lt;Figure size ... with ... Axes&gt;\n\n    &gt;&gt;&gt; fig, ax = show(img, interactive=False)\n    &gt;&gt;&gt; print(fig) # doctest: +SKIP\n    &lt;Figure size ... with ... Axes&gt;\n\n    &gt;&gt;&gt; fig, ax = show(img, cmap='gray')\n    &gt;&gt;&gt; print(fig) # doctest: +SKIP\n    &lt;Figure size ... with .... Axes&gt;\n    \"\"\"\n    if interactive:\n        plt.ion()\n    else:\n        plt.ioff()\n    sizes = img.shape[0] / 100,  img.shape[1] / 100\n    fig = plt.figure(figsize=(sizes[1], sizes[0]))\n    ax = fig.gca()\n    if cmap is None:\n        ax.imshow(img, interpolation=\"none\", extent=(0, sizes[1], 0, sizes[0]))\n    else:\n        ax.imshow(img, cmap=cmap, interpolation=\"none\", extent=(0, sizes[1], 0, sizes[0]))\n\n    if show:\n        fig.tight_layout()\n        fig.show()\n\n    return fig, ax\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.split_dict","title":"<code>split_dict(c_space_dict)</code>","text":"<p>Split a dictionary into two dictionaries based on specific criteria and return their keys.</p> <p>Split the input dictionary <code>c_space_dict</code> into two dictionaries: one for items not ending with '2' and another where the key is truncated by removing its last character if it does end with '2'. Additionally, return the keys that have been processed.</p> <p>Parameters:</p> Name Type Description Default <code>c_space_dict</code> <code>dict</code> <p>The dictionary to be split. Expected keys are strings and values can be any type.</p> required <p>Returns:</p> Name Type Description <code>first_dict</code> <code>dict</code> <p>Dictionary containing items from <code>c_space_dict</code> whose keys do not end with '2'.</p> <code>second_dict</code> <code>dict</code> <p>Dictionary containing items from <code>c_space_dict</code> whose keys end with '2', where the key is truncated by removing its last character.</p> <code>c_spaces</code> <code>list</code> <p>List of keys from <code>c_space_dict</code> that have been processed.</p> <p>Raises:</p> Type Description <code>None</code> Notes <p>No critical information to share.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; c_space_dict = {'key1': 10, 'key2': 20, 'logical': 30}\n&gt;&gt;&gt; first_dict, second_dict, c_spaces = split_dict(c_space_dict)\n&gt;&gt;&gt; print(first_dict)\n{'key1': 10}\n&gt;&gt;&gt; print(second_dict)\n{'key': 20}\n&gt;&gt;&gt; print(c_spaces)\n['key1', 'key']\n</code></pre> Source code in <code>src/cellects/utils/utilitarian.py</code> <pre><code>def split_dict(c_space_dict: dict) -&gt; Tuple[Dict, Dict, list]:\n    \"\"\"\n\n    Split a dictionary into two dictionaries based on specific criteria and return their keys.\n\n    Split the input dictionary `c_space_dict` into two dictionaries: one for items not\n    ending with '2' and another where the key is truncated by removing its last\n    character if it does end with '2'. Additionally, return the keys that have been\n    processed.\n\n    Parameters\n    ----------\n    c_space_dict : dict\n        The dictionary to be split. Expected keys are strings and values can be any type.\n\n    Returns\n    -------\n    first_dict : dict\n        Dictionary containing items from `c_space_dict` whose keys do not end with '2'.\n    second_dict : dict\n        Dictionary containing items from `c_space_dict` whose keys end with '2',\n        where the key is truncated by removing its last character.\n    c_spaces : list\n        List of keys from `c_space_dict` that have been processed.\n\n    Raises\n    ------\n    None\n\n    Notes\n    -----\n    No critical information to share.\n\n    Examples\n    --------\n    &gt;&gt;&gt; c_space_dict = {'key1': 10, 'key2': 20, 'logical': 30}\n    &gt;&gt;&gt; first_dict, second_dict, c_spaces = split_dict(c_space_dict)\n    &gt;&gt;&gt; print(first_dict)\n    {'key1': 10}\n    &gt;&gt;&gt; print(second_dict)\n    {'key': 20}\n    &gt;&gt;&gt; print(c_spaces)\n    ['key1', 'key']\n\n    \"\"\"\n    first_dict = Dict()\n    second_dict = Dict()\n    c_spaces = []\n    for k, v in c_space_dict.items():\n        if k == 'PCA' or k != 'logical' and np.absolute(v).sum() &gt; 0:\n            if k[-1] != '2':\n                first_dict[k] = v\n                c_spaces.append(k)\n            else:\n                second_dict[k[:-1]] = v\n                c_spaces.append(k[:-1])\n    return first_dict, second_dict, c_spaces\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.sum_of_abs_differences","title":"<code>sum_of_abs_differences(array1, array2)</code>","text":"<p>Compute the sum of absolute differences between two arrays.</p> <p>Parameters:</p> Name Type Description Default <code>array1</code> <code>NDArray</code> <p>The first input array.</p> required <code>array2</code> <code>NDArray</code> <p>The second input array.</p> required <p>Returns:</p> Type Description <code>int</code> <p>Sum of absolute differences between elements of <code>array1</code> and <code>array2</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; arr1 = np.array([1.2, 2.5, -3.7])\n&gt;&gt;&gt; arr2 = np.array([12, 25, -37])\n&gt;&gt;&gt; result = sum_of_abs_differences(arr1, arr2)\n&gt;&gt;&gt; print(result)\n66.6\n</code></pre> Source code in <code>src/cellects/utils/formulas.py</code> <pre><code>@njit()\ndef sum_of_abs_differences(array1: NDArray, array2: NDArray):\n    \"\"\"\n    Compute the sum of absolute differences between two arrays.\n\n    Parameters\n    ----------\n    array1 : NDArray\n        The first input array.\n    array2 : NDArray\n        The second input array.\n\n    Returns\n    -------\n    int\n        Sum of absolute differences between elements of `array1` and `array2`.\n\n    Examples\n    --------\n    &gt;&gt;&gt; arr1 = np.array([1.2, 2.5, -3.7])\n    &gt;&gt;&gt; arr2 = np.array([12, 25, -37])\n    &gt;&gt;&gt; result = sum_of_abs_differences(arr1, arr2)\n    &gt;&gt;&gt; print(result)\n    66.6\n    \"\"\"\n    return np.sum(np.absolute(array1 - array2))\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.to_uint8","title":"<code>to_uint8(an_array)</code>","text":"<p>Convert an array to unsigned 8-bit integers.</p> <p>Parameters:</p> Name Type Description Default <code>an_array</code> <code>ndarray</code> <p>Input array to be converted. It can be of any numeric dtype.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>The input array rounded to the nearest integer and then cast to unsigned 8-bit integers.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>an_array</code> is not a ndarray.</p> Notes <p>This function uses Numba's <code>@njit</code> decorator for performance optimization.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; result = to_uint8(np.array([1.2, 2.5, -3.7]))\n&gt;&gt;&gt; print(result)\n[1 3 0]\n</code></pre> Source code in <code>src/cellects/utils/formulas.py</code> <pre><code>@njit()\ndef to_uint8(an_array: NDArray):\n    \"\"\"\n    Convert an array to unsigned 8-bit integers.\n\n    Parameters\n    ----------\n    an_array : ndarray\n        Input array to be converted. It can be of any numeric dtype.\n\n    Returns\n    -------\n    ndarray\n        The input array rounded to the nearest integer and then cast to\n        unsigned 8-bit integers.\n\n    Raises\n    ------\n    TypeError\n        If `an_array` is not a ndarray.\n\n    Notes\n    -----\n    This function uses Numba's `@njit` decorator for performance optimization.\n\n    Examples\n    --------\n    &gt;&gt;&gt; result = to_uint8(np.array([1.2, 2.5, -3.7]))\n    &gt;&gt;&gt; print(result)\n    [1 3 0]\n    \"\"\"\n    out = np.empty_like(an_array)\n    return np.round(an_array, 0, out).astype(np.uint8)\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.translate_dict","title":"<code>translate_dict(old_dict)</code>","text":"<p>Translate a dictionary to a typed dictionary and filter out non-string values.</p> <p>Parameters:</p> Name Type Description Default <code>old_dict</code> <code>dict</code> <p>The input dictionary that may contain non-string values</p> required <p>Returns:</p> Name Type Description <code>numba_dict</code> <code>Dict</code> <p>A typed dictionary containing only the items from <code>old_dict</code> where the value is not a string</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; result = translate_dict({'a': 1., 'b': 'string', 'c': 2.0})\n&gt;&gt;&gt; print(result)\n{a: 1.0, c: 2.0}\n</code></pre> Source code in <code>src/cellects/utils/utilitarian.py</code> <pre><code>def translate_dict(old_dict: dict) -&gt; Dict:\n    \"\"\"\n    Translate a dictionary to a typed dictionary and filter out non-string values.\n\n    Parameters\n    ----------\n    old_dict : dict\n        The input dictionary that may contain non-string values\n\n    Returns\n    -------\n    numba_dict : Dict\n        A typed dictionary containing only the items from `old_dict` where the value is not a string\n\n    Examples\n    --------\n    &gt;&gt;&gt; result = translate_dict({'a': 1., 'b': 'string', 'c': 2.0})\n    &gt;&gt;&gt; print(result)\n    {a: 1.0, c: 2.0}\n    \"\"\"\n    numba_dict = Dict()\n    for k, v in old_dict.items():\n        if not isinstance(v, str):\n            numba_dict[k] = v\n    return numba_dict\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.un_pad","title":"<code>un_pad(arr)</code>","text":"<p>Unpads a 2D NumPy array by removing the first and last row/column.</p> Extended Description <p>Reduces the size of a 2D array by removing the outermost rows and columns. Useful for trimming boundaries added during padding operations.</p> <p>Parameters:</p> Name Type Description Default <code>arr</code> <code>ndarray</code> <p>Input 2D array to be unpadded. Shape (n,m) is expected.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Unpadded 2D array with shape (n-2, m-2).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; arr = np.array([[0, 0, 0],\n&gt;&gt;&gt;                 [0, 4, 0],\n&gt;&gt;&gt;                 [0, 0, 0]])\n&gt;&gt;&gt; un_pad(arr)\narray([[4]])\n</code></pre> Source code in <code>src/cellects/image_analysis/network_functions.py</code> <pre><code>def un_pad(arr: NDArray) -&gt; NDArray:\n    \"\"\"\n    Unpads a 2D NumPy array by removing the first and last row/column.\n\n    Extended Description\n    --------------------\n    Reduces the size of a 2D array by removing the outermost rows and columns.\n    Useful for trimming boundaries added during padding operations.\n\n    Parameters\n    ----------\n    arr : ndarray\n        Input 2D array to be unpadded. Shape (n,m) is expected.\n\n    Returns\n    -------\n    ndarray\n        Unpadded 2D array with shape (n-2, m-2).\n\n    Examples\n    --------\n    &gt;&gt;&gt; arr = np.array([[0, 0, 0],\n    &gt;&gt;&gt;                 [0, 4, 0],\n    &gt;&gt;&gt;                 [0, 0, 0]])\n    &gt;&gt;&gt; un_pad(arr)\n    array([[4]])\n    \"\"\"\n    return arr[1:-1, 1:-1]\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.video2numpy","title":"<code>video2numpy(vid_name, conversion_dict=None, background=None, background2=None, true_frame_width=None)</code>","text":"<p>Convert a video file to a NumPy array.</p> <p>Parameters:</p> Name Type Description Default <code>vid_name</code> <code>str</code> <p>The path to the video file. Can be a <code>.mp4</code> or <code>.npy</code>.</p> required <code>conversion_dict</code> <code>dict</code> <p>Dictionary containing color space conversion parameters.</p> <code>None</code> <code>background</code> <code>NDArray</code> <p>Background image for processing.</p> <code>None</code> <code>background2</code> <code>NDArray</code> <p>Second background image for processing.</p> <code>None</code> <code>true_frame_width</code> <code>int</code> <p>True width of the frame. If specified and the current width is double this value, adjusts to true_frame_width.</p> <code>None</code> <p>Returns:</p> Type Description <code>NDArray or tuple of NDArrays</code> <p>If conversion_dict is None, returns the video as a NumPy array. Otherwise, returns a tuple containing the original video and converted video.</p> Notes <p>This function uses OpenCV to read the contents of a <code>.mp4</code> video file.</p> Source code in <code>src/cellects/utils/load_display_save.py</code> <pre><code>def video2numpy(vid_name: str, conversion_dict=None, background: NDArray=None, background2: NDArray=None,\n                true_frame_width: int=None):\n    \"\"\"\n    Convert a video file to a NumPy array.\n\n    Parameters\n    ----------\n    vid_name : str\n        The path to the video file. Can be a `.mp4` or `.npy`.\n    conversion_dict : dict, optional\n        Dictionary containing color space conversion parameters.\n    background : NDArray, optional\n        Background image for processing.\n    background2 : NDArray, optional\n        Second background image for processing.\n    true_frame_width : int, optional\n        True width of the frame. If specified and the current width is double this value,\n        adjusts to true_frame_width.\n\n    Returns\n    -------\n    NDArray or tuple of NDArrays\n        If conversion_dict is None, returns the video as a NumPy array.\n        Otherwise, returns a tuple containing the original video and converted video.\n\n    Notes\n    -----\n    This function uses OpenCV to read the contents of a `.mp4` video file.\n    \"\"\"\n    np_loading = vid_name[-4:] == \".npy\"\n    if np_loading:\n        video = np.load(vid_name)\n        dims = list(video.shape)\n    else:\n        cap = cv2.VideoCapture(vid_name)\n        dims = [int(cap.get(cv2.CAP_PROP_FRAME_COUNT)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)), int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))]\n\n    if true_frame_width is not None:\n        if dims[2] == 2 * true_frame_width:\n            dims[2] = true_frame_width\n\n    if conversion_dict is not None:\n        first_dict, second_dict, c_spaces = split_dict(conversion_dict)\n        converted_video = np.empty(dims[:3], dtype=np.uint8)\n        if conversion_dict['logical'] == 'None':\n            converted_video2 = np.empty(dims[:3], dtype=np.uint8)\n        if np_loading:\n            for counter in np.arange(video.shape[0]):\n                img = video[counter, :, :dims[2], :]\n                greyscale_image, greyscale_image2, all_c_spaces, first_pc_vector = generate_color_space_combination(img, c_spaces,\n                                                                                     first_dict, second_dict, background=background,background2=background2,\n                                                                                     convert_to_uint8=True)\n                converted_video[counter, ...] = greyscale_image\n                if conversion_dict['logical'] == 'None':\n                    converted_video2[counter, ...] = greyscale_image2\n            video = video[:, :, :dims[2], ...]\n\n    if not np_loading:\n        # 2) Create empty arrays to store video analysis data\n        video = np.empty((dims[0], dims[1], dims[2], 3), dtype=np.uint8)\n        # 3) Read and convert the video frame by frame\n        counter = 0\n        while cap.isOpened() and counter &lt; dims[0]:\n            ret, frame = cap.read()\n            frame = frame[:, :dims[2], ...]\n            video[counter, ...] = frame\n            if conversion_dict is not None:\n                greyscale_image, greyscale_image2, all_c_spaces, first_pc_vector = generate_color_space_combination(frame, c_spaces,\n                                                                                     first_dict, second_dict, background=background,background2=background2,\n                                                                                     convert_to_uint8=True)\n                converted_video[counter, ...] = greyscale_image\n                if conversion_dict['logical'] == 'None':\n                    converted_video2[counter, ...] = greyscale_image2\n            counter += 1\n        cap.release()\n\n    if conversion_dict is None:\n        return video\n    else:\n        return video, converted_video\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.vstack_h5_array","title":"<code>vstack_h5_array(file_name, table, key='data')</code>","text":"<p>Stack tables vertically in an HDF5 file.</p> <p>This function either appends the input table to an existing dataset in the specified HDF5 file or creates a new dataset if the key doesn't exist.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>Path to the HDF5 file.</p> required <code>table</code> <code>NDArray[uint8]</code> <p>The table to be stacked vertically with the existing data.</p> required <code>key</code> <code>str</code> <p>Key under which the dataset will be stored. Defaults to 'data'.</p> <code>'data'</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; table = np.array([[1, 2], [3, 4]], dtype=np.uint8)\n&gt;&gt;&gt; vstack_h5_array('example.h5', table)\n</code></pre> Source code in <code>src/cellects/utils/load_display_save.py</code> <pre><code>def vstack_h5_array(file_name, table: NDArray, key: str=\"data\"):\n    \"\"\"\n    Stack tables vertically in an HDF5 file.\n\n    This function either appends the input table to an existing dataset\n    in the specified HDF5 file or creates a new dataset if the key doesn't exist.\n\n    Parameters\n    ----------\n    file_name : str\n        Path to the HDF5 file.\n    table : NDArray[np.uint8]\n        The table to be stacked vertically with the existing data.\n    key : str, optional\n        Key under which the dataset will be stored. Defaults to 'data'.\n\n    Examples\n    --------\n    &gt;&gt;&gt; table = np.array([[1, 2], [3, 4]], dtype=np.uint8)\n    &gt;&gt;&gt; vstack_h5_array('example.h5', table)\n    \"\"\"\n    if os.path.exists(file_name):\n        # Open the file in append mode\n        with h5py.File(file_name, 'a') as h5f:\n            if key in h5f:\n                # Append to the existing dataset\n                existing_data = h5f[key][:]\n                new_data = np.vstack((existing_data, table))\n                del h5f[key]\n                h5f.create_dataset(key, data=new_data)\n            else:\n                # Create a new dataset if the key doesn't exist\n                h5f.create_dataset(key, data=table)\n    else:\n        with h5py.File(file_name, 'w') as h5f:\n            h5f.create_dataset(key, data=table)\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.write_video","title":"<code>write_video(np_array, vid_name, is_color=True, fps=40)</code>","text":"<p>Write video from numpy array.</p> <p>Save a numpy array as a video file. Supports .npy format for saving raw numpy arrays and various video formats (mp4, avi, mkv) using OpenCV. For video formats, automatically selects a suitable codec and handles file extensions.</p> <p>Parameters:</p> Name Type Description Default <code>np_array</code> <code>ndarray of uint8</code> <p>Input array containing video frames.</p> required <code>vid_name</code> <code>str</code> <p>Filename for the output video. Can include extension or not (defaults to .mp4).</p> required <code>is_color</code> <code>bool</code> <p>Whether the video should be written in color. Defaults to True.</p> <code>True</code> <code>fps</code> <code>int</code> <p>Frame rate for the video in frames per second. Defaults to 40.</p> <code>40</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; video_array = np.random.randint(0, 255, size=(10, 100, 100, 3), dtype=np.uint8)\n&gt;&gt;&gt; write_video(video_array, 'output.mp4', True, 30)\nSaves `video_array` as a color video 'output.mp4' with FPS 30.\n&gt;&gt;&gt; video_array = np.random.randint(0, 255, size=(10, 100, 100), dtype=np.uint8)\n&gt;&gt;&gt; write_video(video_array, 'raw_data.npy')\nSaves `video_array` as a raw numpy array file without frame rate.\n</code></pre> Source code in <code>src/cellects/utils/load_display_save.py</code> <pre><code>def write_video(np_array: NDArray[np.uint8], vid_name: str, is_color: bool=True, fps: int=40):\n    \"\"\"\n    Write video from numpy array.\n\n    Save a numpy array as a video file. Supports .npy format for saving raw\n    numpy arrays and various video formats (mp4, avi, mkv) using OpenCV.\n    For video formats, automatically selects a suitable codec and handles\n    file extensions.\n\n    Parameters\n    ----------\n    np_array : ndarray of uint8\n        Input array containing video frames.\n    vid_name : str\n        Filename for the output video. Can include extension or not (defaults to .mp4).\n    is_color : bool, optional\n        Whether the video should be written in color. Defaults to True.\n    fps : int, optional\n        Frame rate for the video in frames per second. Defaults to 40.\n\n    Examples\n    --------\n    &gt;&gt;&gt; video_array = np.random.randint(0, 255, size=(10, 100, 100, 3), dtype=np.uint8)\n    &gt;&gt;&gt; write_video(video_array, 'output.mp4', True, 30)\n    Saves `video_array` as a color video 'output.mp4' with FPS 30.\n    &gt;&gt;&gt; video_array = np.random.randint(0, 255, size=(10, 100, 100), dtype=np.uint8)\n    &gt;&gt;&gt; write_video(video_array, 'raw_data.npy')\n    Saves `video_array` as a raw numpy array file without frame rate.\n    \"\"\"\n    #h265 ou h265 (mp4)\n    # linux: fourcc = 0x00000021 -&gt; don't forget to change it bellow as well\n    if vid_name[-4:] == '.npy':\n        with open(vid_name, 'wb') as file:\n             np.save(file, np_array)\n    else:\n        valid_extensions = ['.mp4', '.avi', '.mkv']\n        vid_ext = vid_name[-4:]\n        if vid_ext not in valid_extensions:\n            vid_name = vid_name[:-4]\n            vid_name += '.mp4'\n            vid_ext = '.mp4'\n        if vid_ext =='.mp4':\n            fourcc = 0x7634706d# VideoWriter_fourcc(*'FMP4') #(*'MP4V') (*'h265') (*'x264') (*'DIVX')\n        else:\n            fourcc = cv2.VideoWriter_fourcc('F', 'F', 'V', '1')  # lossless\n        size = np_array.shape[2], np_array.shape[1]\n        vid = cv2.VideoWriter(vid_name, fourcc, float(fps), tuple(size), is_color)\n        for image_i in np.arange(np_array.shape[0]):\n            image = np_array[image_i, ...]\n            vid.write(image)\n        vid.release()\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.write_video_sets","title":"<code>write_video_sets(img_list, sizes, vid_names, crop_coord, bounding_boxes, bunch_nb, video_nb_per_bunch, remaining, raw_images, is_landscape, use_list_of_vid, in_colors=False, reduce_image_dim=False, pathway='')</code>","text":"<p>Write video sets from a list of images, applying cropping and optional rotation.</p> <p>Parameters:</p> Name Type Description Default <code>img_list</code> <code>list</code> <p>List of image file names.</p> required <code>sizes</code> <code>NDArray</code> <p>Array containing the dimensions of each video frame.</p> required <code>vid_names</code> <code>list</code> <p>List of video file names to be saved.</p> required <code>crop_coord</code> <code>dict or tuple</code> <p>Coordinates for cropping regions of interest in images/videos.</p> required <code>bounding_boxes</code> <code>tuple</code> <p>Bounding box coordinates to extract sub-images from the original images.</p> required <code>bunch_nb</code> <code>int</code> <p>Number of bunches to divide the videos into.</p> required <code>video_nb_per_bunch</code> <code>int</code> <p>Number of videos per bunch.</p> required <code>remaining</code> <code>int</code> <p>Number of videos remaining after the last full bunch.</p> required <code>raw_images</code> <code>bool</code> <p>Whether the images are in raw format.</p> required <code>is_landscape</code> <code>bool</code> <p>If true, rotate the images to landscape orientation before processing.</p> required <code>use_list_of_vid</code> <code>bool</code> <p>Flag indicating if the output should be a list of videos.</p> required <code>in_colors</code> <code>bool</code> <p>If true, process images with color information. Default is False.</p> <code>False</code> <code>reduce_image_dim</code> <code>bool</code> <p>If true, reduce image dimensions. Default is False.</p> <code>False</code> <code>pathway</code> <code>str</code> <p>Path where the videos should be saved. Default is an empty string.</p> <code>''</code> Source code in <code>src/cellects/utils/load_display_save.py</code> <pre><code>def write_video_sets(img_list: list, sizes: NDArray, vid_names: list, crop_coord, bounding_boxes,\n                      bunch_nb: int, video_nb_per_bunch: int, remaining: int,\n                      raw_images: bool, is_landscape: bool, use_list_of_vid: bool,\n                      in_colors: bool=False, reduce_image_dim: bool=False, pathway: str=\"\"):\n    \"\"\"\n    Write video sets from a list of images, applying cropping and optional rotation.\n\n    Parameters\n    ----------\n    img_list : list\n        List of image file names.\n    sizes : NDArray\n        Array containing the dimensions of each video frame.\n    vid_names : list\n        List of video file names to be saved.\n    crop_coord : dict or tuple\n        Coordinates for cropping regions of interest in images/videos.\n    bounding_boxes : tuple\n        Bounding box coordinates to extract sub-images from the original images.\n    bunch_nb : int\n        Number of bunches to divide the videos into.\n    video_nb_per_bunch : int\n        Number of videos per bunch.\n    remaining : int\n        Number of videos remaining after the last full bunch.\n    raw_images : bool\n        Whether the images are in raw format.\n    is_landscape : bool\n        If true, rotate the images to landscape orientation before processing.\n    use_list_of_vid : bool\n        Flag indicating if the output should be a list of videos.\n    in_colors : bool, optional\n        If true, process images with color information. Default is False.\n    reduce_image_dim : bool, optional\n        If true, reduce image dimensions. Default is False.\n    pathway : str, optional\n        Path where the videos should be saved. Default is an empty string.\n    \"\"\"\n    top, bot, left, right = bounding_boxes\n    for bunch in np.arange(bunch_nb):\n        print(f'\\nSaving the bunch n: {bunch + 1} / {bunch_nb} of videos:', end=' ')\n        if bunch == (bunch_nb - 1) and remaining &gt; 0:\n            arenas = np.arange(bunch * video_nb_per_bunch, bunch * video_nb_per_bunch + remaining, dtype=np.uint32)\n        else:\n            arenas = np.arange(bunch * video_nb_per_bunch, (bunch + 1) * video_nb_per_bunch, dtype=np.uint32)\n        if use_list_of_vid:\n            video_bunch = [np.zeros(sizes[i, :], dtype=np.uint8) for i in arenas]\n        else:\n            video_bunch = np.zeros(np.append(sizes[0, :], len(arenas)), dtype=np.uint8)\n        prev_img = None\n        images_done = bunch * len(img_list)\n        for image_i, image_name in enumerate(img_list):\n            img = read_and_rotate(image_name, prev_img, raw_images, is_landscape, crop_coord)\n            prev_img = img.copy()\n            if not in_colors and reduce_image_dim:\n                img = img[:, :, 0]\n\n            for arena_i, arena_name in enumerate(arenas):\n                # arena_i = 0; arena_name = arena[arena_i]\n                sub_img = img[top[arena_name]: (bot[arena_name] + 1), left[arena_name]: (right[arena_name] + 1), ...]\n                if use_list_of_vid:\n                    video_bunch[arena_i][image_i, ...] = sub_img\n                else:\n                    if len(video_bunch.shape) == 5:\n                        video_bunch[image_i, :, :, :, arena_i] = sub_img\n                    else:\n                        video_bunch[image_i, :, :, arena_i] = sub_img\n        for arena_i, arena_name in enumerate(arenas):\n            if use_list_of_vid:\n                 np.save(pathway + vid_names[arena_name], video_bunch[arena_i])\n            else:\n                if len(video_bunch.shape) == 5:\n                     np.save(pathway + vid_names[arena_name], video_bunch[:, :, :, :, arena_i])\n                else:\n                     np.save(pathway + vid_names[arena_name], video_bunch[:, :, :, arena_i])\n</code></pre>"},{"location":"api/cellects/image_analysis/network_functions/#cellects.image_analysis.network_functions.zoom_on_nonzero","title":"<code>zoom_on_nonzero(binary_image, padding=2, return_coord=True)</code>","text":"<p>Crops a binary image around non-zero elements with optional padding and returns either coordinates or cropped region.</p> <p>Parameters:</p> Name Type Description Default <code>binary_image</code> <code>NDArray</code> <p>2D NumPy array containing binary values (0/1)</p> required <code>padding</code> <code>int</code> <p>Amount of zero-padding to add around the minimum bounding box</p> <code>2</code> <code>return_coord</code> <code>bool</code> <p>If True, return slice coordinates instead of cropped image</p> <code>True</code> <p>Returns:</p> Type Description <code>    If `return_coord` is True: [y_min, y_max, x_min, x_max] as 4-element Tuple.</code> <p>If False: 2D binary array representing the cropped region defined by non-zero elements plus padding.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; img = np.zeros((10,10))\n&gt;&gt;&gt; img[3:7,4:6] = 1\n&gt;&gt;&gt; result = zoom_on_nonzero(img)\n&gt;&gt;&gt; print(result)\n[1 8 2 7]\n&gt;&gt;&gt; cropped = zoom_on_nonzero(img, return_coord=False)\n&gt;&gt;&gt; print(cropped.shape)\n(6, 5)\n</code></pre> Notes <ul> <li>Returns empty slice coordinates if input contains no non-zero elements.</li> <li>Coordinate indices are 0-based and compatible with NumPy array slicing syntax.</li> </ul> Source code in <code>src/cellects/utils/load_display_save.py</code> <pre><code>def zoom_on_nonzero(binary_image:NDArray, padding: int = 2, return_coord: bool=True):\n    \"\"\"\n    Crops a binary image around non-zero elements with optional padding and returns either coordinates or cropped region.\n\n    Parameters\n    ----------\n    binary_image : NDArray\n        2D NumPy array containing binary values (0/1)\n    padding : int, default=2\n        Amount of zero-padding to add around the minimum bounding box\n    return_coord : bool, default=True\n        If True, return slice coordinates instead of cropped image\n\n    Returns\n    -------\n        If `return_coord` is True: [y_min, y_max, x_min, x_max] as 4-element Tuple.\n        If False: 2D binary array representing the cropped region defined by non-zero elements plus padding.\n\n    Examples\n    --------\n    &gt;&gt;&gt; img = np.zeros((10,10))\n    &gt;&gt;&gt; img[3:7,4:6] = 1\n    &gt;&gt;&gt; result = zoom_on_nonzero(img)\n    &gt;&gt;&gt; print(result)\n    [1 8 2 7]\n    &gt;&gt;&gt; cropped = zoom_on_nonzero(img, return_coord=False)\n    &gt;&gt;&gt; print(cropped.shape)\n    (6, 5)\n\n    Notes\n    -----\n    - Returns empty slice coordinates if input contains no non-zero elements.\n    - Coordinate indices are 0-based and compatible with NumPy array slicing syntax.\n    \"\"\"\n    y, x = np.nonzero(binary_image)\n    cy_min = np.max((0, y.min() - padding))\n    cy_max = np.min((binary_image.shape[0], y.max() + padding + 1))\n    cx_min = np.max((0, x.min() - padding))\n    cx_max = np.min((binary_image.shape[1], x.max() + padding + 1))\n    if return_coord:\n        return cy_min, cy_max, cx_min, cx_max\n    else:\n        return binary_image[cy_min:cy_max, cx_min:cx_max]\n</code></pre>"},{"location":"api/cellects/image_analysis/one_image_analysis_threads/","title":"<code>cellects.image_analysis.one_image_analysis_threads</code>","text":""},{"location":"api/cellects/image_analysis/one_image_analysis_threads/#cellects.image_analysis.one_image_analysis_threads","title":"<code>cellects.image_analysis.one_image_analysis_threads</code>","text":"<p>Module implementing an image processing pipeline for segmentation and analysis.</p> <p>This module defines functionality to process images using various color space combinations, filters, and segmentation techniques like K-means clustering or Otsu thresholding. It supports logical operations between images and iterative refinement of color space combinations by adding/removing channels based on validation criteria. The pipeline evaluates resulting binary masks for blob statistics and spatial relationships.</p> <p>Classes ProcessImage : Processes an image according to parameterized instructions, performing color space combination, filtering, segmentation, and shape validation.</p> <p>Notes Relies on external functions from cellects.image_analysis module for core operations like color space combination, filtering, PCA extraction, and segmentation.</p>"},{"location":"api/cellects/image_analysis/one_image_analysis_threads/#cellects.image_analysis.one_image_analysis_threads.ProcessImage","title":"<code>ProcessImage</code>","text":"<p>A class for processing an image according to a list of parameters.</p> Source code in <code>src/cellects/image_analysis/one_image_analysis_threads.py</code> <pre><code>class ProcessImage:\n    \"\"\"\n    A class for processing an image according to a list of parameters.\n    \"\"\"\n    def __init__(self, l):\n        \"\"\"\n        Arguments:\n            list : list\n        \"\"\"\n        self.stats = None\n        self.greyscale = None\n        self.start_processing(l)\n\n    def start_processing(self, l: list):\n        \"\"\"\n        Begin processing tasks based on the given process type.\n\n        Parameters\n        ----------\n        l : list\n            A list containing parameters and instructions for processing.\n            Expected structure:\n                - First element: self.parent (object)\n                - Second element: params (dict)\n                - Third element: process type (str) ('one', 'PCA', 'add', 'subtract', or 'logical')\n                - Fourth element (if applicable): additional parameters based on process type.\n\n        Notes\n        -----\n        This function modifies instance attributes directly and performs various operations based on the `process` type.\n        \"\"\"\n        self.parent = l[0]\n        self.fact = pd.DataFrame(np.zeros((1, len(self.parent.factors)), dtype=np.uint32), columns=self.parent.factors)\n        self.params = l[1]\n        process = l[2]\n        self.all_c_spaces = self.parent.all_c_spaces\n        if process == 'one':\n            self.csc_dict = l[3]\n            self.combine_and_segment()\n            self.evaluate_segmentation()\n        elif process == 'PCA':\n            self.pca_and_segment()\n            self.evaluate_segmentation()\n        elif process == 'add':\n            self.try_adding_channels(l[3])\n        elif process == 'subtract':\n            self.try_subtracting_channels()\n        elif process == 'logical':\n            self.operation = l[3]\n            self.apply_logical_operation()\n\n    def combine_and_segment(self, csc_dict: Dict=None):\n        \"\"\"\n        Combines color spaces and segments images into binary masks using either K-means clustering or Otsu's thresholding based on specified parameters.\n\n        Parameters\n        ----------\n        csc_dict : dict or None\n            Optional dictionary mapping color space abbreviations (e.g., 'bgr', 'hsv') to their relative contribution coefficients for combination. If not provided, uses the instance's `self.csc_dict` attribute instead.\n\n        \"\"\"\n        if csc_dict is None:\n            csc_dict = self.csc_dict\n        self.image = combine_color_spaces(csc_dict, self.all_c_spaces)\n        self.apply_filter_and_segment()\n\n    def pca_and_segment(self):\n        \"\"\"\n        Extract the first principal component and perform segmentation.\n\n        This method extracts the first principal component from the 'bgr' color space\n        and performs k-means clustering or Otsu thresholding to segment the image based on the\n        parameters provided.\n        \"\"\"\n        self.image, _, first_pc_vector = extract_first_pc(self.all_c_spaces['bgr'])\n        self.csc_dict = Dict()\n        self.csc_dict['bgr'] = first_pc_vector\n        self.apply_filter_and_segment()\n\n    def apply_filter_and_segment(self):\n        self.greyscale = self.image\n        if self.params['filter_spec'] is not None and self.params['filter_spec'][\"filter1_type\"] != \"\":\n            self.greyscale = apply_filter(self.greyscale, self.params['filter_spec'][\"filter1_type\"], self.params['filter_spec'][\"filter1_param\"])\n\n\n        if self.params['kmeans_clust_nb'] is not None and (self.params['bio_mask'] is not None or self.params['back_mask'] is not None):\n            self.binary_image, _, self.bio_label, _ = kmeans(self.greyscale, None, self.params['kmeans_clust_nb'],\n                                                             self.params['bio_mask'], self.params['back_mask'])\n        else:\n            self.binary_image = otsu_thresholding(self.greyscale)\n        self.validated_shapes = self.binary_image\n\n    def evaluate_segmentation(self):\n        \"\"\"\n        Use the filtering algorithm based on the kind of image to analyse\n        \"\"\"\n        if self.params['is_first_image']:\n            self.eval_first_image()\n        else:\n            self.eval_any_image()\n\n    def eval_first_image(self):\n        \"\"\"\n        First image filtering process for binary images.\n\n        This method processes the first binary image by identifying connected components, computing\n        the total area of the image and its potential size limits. If the number of blobs and their\n        total area fall within acceptable thresholds, it proceeds with additional processing and saving.\n        \"\"\"\n        self.fact['unaltered_blob_nb'], shapes = cv2.connectedComponents(self.binary_image)\n        self.fact['unaltered_blob_nb'] -= 1\n        if 1 &lt;= self.fact['unaltered_blob_nb'].values[0] &lt; 10000:\n            self.fact['total_area'] = np.sum(self.binary_image)\n            inf_lim = np.min((20, np.ceil(self.binary_image.size / 1000)))\n            if inf_lim &lt; self.fact['total_area'].values[0] &lt; self.binary_image.size * 0.9:\n                self.process_first_binary_image()\n                self.save_combination()\n\n    def eval_any_image(self):\n        \"\"\"\n\n        Summarizes the binary image analysis and determines blob characteristics.\n\n        Evaluates a binary image to determine various attributes like surface area,\n        blob number, and their relative positions within specified masks. It also\n        checks for common areas with a reference image and saves the combination if\n        certain conditions are met.\n        \"\"\"\n        surf = self.binary_image.sum()\n        if surf &lt; self.params['total_surface_area']:\n            self.fact['unaltered_blob_nb'], shapes = cv2.connectedComponents(self.binary_image)\n            self.fact['unaltered_blob_nb'] -= 1\n            test: bool = True\n            if self.params['arenas_mask'] is not None:\n                self.fact['out_of_arenas'] = np.sum(self.binary_image * self.params['out_of_arenas_mask'])\n                self.fact['in_arenas'] = np.sum(self.binary_image * self.params['arenas_mask'])\n                test = self.fact['out_of_arenas'].values[0] &lt; self.fact['in_arenas'].values[0]\n            if test:\n                if self.params['con_comp_extent'][0] &lt;= self.fact['unaltered_blob_nb'].values[0] &lt;= self.params['con_comp_extent'][1]:\n                    if self.params['ref_image'] is not None:\n                        self.fact['common_with_ref'] = np.sum(self.params['ref_image'] * self.binary_image)\n                        test = self.fact['common_with_ref'].values[0] &gt; 0\n                    if test:\n                        self.fact['blob_nb'], shapes, self.stats, centroids = cv2.connectedComponentsWithStats(self.binary_image)\n                        self.fact['blob_nb'] -= 1\n                        if np.all(np.sort(self.stats[:, 4])[:-1] &lt; self.params['max_blob_size']):\n                            self.save_combination()\n\n    def apply_logical_operation(self):\n        \"\"\"\n        Apply a logical operation between two saved images.\n\n        This method applies a specified logical operation ('And' or 'Or')\n        between two images stored in the parent's saved_images_list. The result\n        is stored as a binary image and validated, with color space information\n        updated accordingly.\n\n        Notes\n        -----\n        This method modifies the following instance attributes:\n        - `binary_image`\n        - `validated_shapes`\n        - `image`\n        - `csc_dict`\n        \"\"\"\n        im1 = self.parent.saved_images_list[self.operation[0]]\n        im2 = self.parent.saved_images_list[self.operation[1]]\n        if self.operation['logical'] == 'And':\n            self.binary_image = np.logical_and(im1, im2).astype(np.uint8)\n        elif self.operation['logical'] == 'Or':\n            self.binary_image = np.logical_or(im1, im2).astype(np.uint8)\n        self.validated_shapes = self.binary_image\n        self.greyscale = self.parent.converted_images_list[self.operation[0]]\n        csc1 = self.parent.saved_color_space_list[self.operation[0]]\n        csc2 = self.parent.saved_color_space_list[self.operation[1]]\n        self.csc_dict = {}\n        for k, v in csc1.items():\n            self.csc_dict[k] = v\n        for k, v in csc2.items():\n            self.csc_dict[k] = v\n        self.csc_dict['logical'] = self.operation['logical']\n        self.evaluate_segmentation()\n\n    def try_adding_channels(self, i: int):\n        \"\"\"\n        Try adding channels to the current color space combination.\n\n        Extend the functionality of a selected color space combination by attempting\n        to add channels from other combinations, evaluating the results based on\n        the number of shapes and total area.\n\n        Parameters\n        ----------\n        i : int\n            The index of the saved color space and combination features to start with.\n        \"\"\"\n        saved_color_space_list = self.parent.saved_color_space_list\n        combination_features = self.parent.combination_features\n        self.csc_dict = saved_color_space_list[i]\n        previous_shape_number = combination_features.loc[i, 'blob_nb']\n        previous_sum = combination_features.loc[i, 'total_area']\n        for j in self.params['possibilities'][::-1]:\n            csc_dict2 = saved_color_space_list[j]\n            csc_dict = self.csc_dict.copy()\n            keys = list(csc_dict.keys())\n\n            k2 = list(csc_dict2.keys())[0]\n            v2 = csc_dict2[k2]\n            if np.isin(k2, keys) and np.sum(v2 * csc_dict[k2]) != 0:\n                break\n            for factor in [2, 1]:\n                if np.isin(k2, keys):\n                    csc_dict[k2] += v2 * factor\n                else:\n                    csc_dict[k2] = v2 * factor\n                self.combine_and_segment(csc_dict)\n                if self.params['is_first_image']:\n                    self.process_first_binary_image()\n                else:\n                    self.fact['blob_nb'], shapes, self.stats, centroids = cv2.connectedComponentsWithStats(self.validated_shapes)\n                    self.fact['blob_nb'] -= 1\n                self.fact['total_area'] = self.validated_shapes.sum()\n                if self.fact['blob_nb'].values[0] &lt; previous_shape_number  and self.fact['total_area'].values[0] &gt; previous_sum * 0.9:\n                    previous_shape_number = self.fact['blob_nb'].values[0]\n                    previous_sum = self.fact['total_area'].values[0]\n                    self.csc_dict = csc_dict.copy()\n                    self.fact['unaltered_blob_nb'] = combination_features.loc[i, 'unaltered_blob_nb']\n                    self.save_combination()\n\n    def try_subtracting_channels(self):\n        \"\"\"\n        Tries to subtract channels to find the optimal color space combination.\n\n        This method attempts to remove color spaces one by one from the image\n        to find a combination that maintains the majority of areas while reducing\n        the number of color spaces. This process is repeated until no further\n        improvements are possible.\n        \"\"\"\n        potentials = self.parent.all_combined\n        # Try to remove color space one by one\n        i = 0\n        original_length = len(potentials)\n        # The while loop until one col space remains or the removal of one implies a strong enough area change\n        while np.logical_and(len(potentials) &gt; 1, i &lt; original_length - 1):\n            self.combine_and_segment(potentials)\n            if self.params['is_first_image']:\n                self.process_first_binary_image()\n                previous_blob_nb = self.fact['blob_nb'].values[0]\n            else:\n                previous_blob_nb, shapes, self.stats, centroids = cv2.connectedComponentsWithStats(\n                    self.validated_shapes)\n                previous_blob_nb -= 1\n            previous_sum = self.validated_shapes.sum()\n            color_space_to_remove = List()\n            previous_c_space = list(potentials.keys())[-1]\n            for c_space in potentials.keys():\n                try_potentials = potentials.copy()\n                try_potentials.pop(c_space)\n                if i &gt; 0:\n                    try_potentials.pop(previous_c_space)\n                self.combine_and_segment(try_potentials)\n                if self.params['is_first_image']:\n                    self.process_first_binary_image()\n                else:\n                    self.fact['blob_nb'], shapes, self.stats, centroids = cv2.connectedComponentsWithStats(\n                        self.validated_shapes)\n                    self.fact['blob_nb'] -= 1\n                self.fact['total_area'] = self.validated_shapes.sum()\n                if self.fact['blob_nb'].values[0] &lt; previous_blob_nb and self.fact['total_area'].values[0] &gt; previous_sum * 0.9:\n                    previous_blob_nb = self.fact['blob_nb'].values[0]\n                    previous_sum = self.fact['total_area'].values[0]\n                    self.csc_dict = try_potentials.copy()\n                    self.fact['unaltered_blob_nb'] = previous_blob_nb\n                    self.save_combination()\n                    # If removing that color space helps, we remove it from potentials\n                    color_space_to_remove.append(c_space)\n                    if i &gt; 0:\n                        color_space_to_remove.append(previous_c_space)\n                previous_c_space = c_space\n            if len(color_space_to_remove) == 0:\n                break\n            color_space_to_remove = np.unique(color_space_to_remove)\n            for remove_col_space in color_space_to_remove:\n                potentials.pop(remove_col_space)\n            i += 1\n\n    def process_first_binary_image(self):\n        \"\"\"\n        Process the binary image to identify and validate shapes.\n\n        This method processes a binary image to detect connected components,\n        validate their sizes, and handle bio and back masks if specified.\n        It ensures that the number of validated shapes matches the expected\n        sample number or applies additional filtering if necessary.\n\n        \"\"\"\n        shapes_features = shape_selection(self.binary_image, true_shape_number=self.params['blob_nb'],\n                        horizontal_size=self.params['blob_size'], spot_shape=self.params['blob_shape'],\n                        several_blob_per_arena=self.params['several_blob_per_arena'],\n                        bio_mask=self.params['bio_mask'], back_mask=self.params['back_mask'])\n        self.validated_shapes, self.fact['blob_nb'], self.stats, self.centroids = shapes_features\n\n    def save_combination(self):\n        \"\"\"\n        Saves the calculated features and masks for a combination of shapes.\n\n        This method calculates various statistical properties (std) and sums for the\n        validated shapes, and optionally computes sums for bio and back masks if they are\n        specified in the parameters.\n\n        Parameters\n        ----------\n        self : object\n            The instance of the class containing this method. Must have attributes:\n            - ``validated_shapes``: (ndarray) Array of validated shapes.\n            - ``stats``: (ndarray) Statistics array containing width, height, and area.\n            - ``params``: dictionary with parameters including 'bio_mask' and 'back_mask'.\n            - ``fact``: dictionary to store the calculated features.\n            - ``parent``: The parent object containing the method `save_combination_features`.\n        \"\"\"\n        self.fact['total_area'] = self.validated_shapes.sum()\n        self.fact['width_std'] = np.std(self.stats[1:, 2])\n        self.fact['height_std'] = np.std(self.stats[1:, 3])\n        self.fact['area_std'] = np.std(self.stats[1:, 4])\n        if self.params['bio_mask'] is not None:\n            self.fact['bio_sum'] = self.validated_shapes[self.params['bio_mask'][0], self.params['bio_mask'][1]].sum()\n        if self.params['back_mask'] is not None:\n            self.fact['back_sum'] = (1 - self.validated_shapes)[self.params['back_mask'][0], self.params['back_mask'][1]].sum()\n        self.parent.save_combination_features(self)\n</code></pre>"},{"location":"api/cellects/image_analysis/one_image_analysis_threads/#cellects.image_analysis.one_image_analysis_threads.ProcessImage.__init__","title":"<code>__init__(l)</code>","text":"<p>Arguments:     list : list</p> Source code in <code>src/cellects/image_analysis/one_image_analysis_threads.py</code> <pre><code>def __init__(self, l):\n    \"\"\"\n    Arguments:\n        list : list\n    \"\"\"\n    self.stats = None\n    self.greyscale = None\n    self.start_processing(l)\n</code></pre>"},{"location":"api/cellects/image_analysis/one_image_analysis_threads/#cellects.image_analysis.one_image_analysis_threads.ProcessImage.apply_logical_operation","title":"<code>apply_logical_operation()</code>","text":"<p>Apply a logical operation between two saved images.</p> <p>This method applies a specified logical operation ('And' or 'Or') between two images stored in the parent's saved_images_list. The result is stored as a binary image and validated, with color space information updated accordingly.</p> Notes <p>This method modifies the following instance attributes: - <code>binary_image</code> - <code>validated_shapes</code> - <code>image</code> - <code>csc_dict</code></p> Source code in <code>src/cellects/image_analysis/one_image_analysis_threads.py</code> <pre><code>def apply_logical_operation(self):\n    \"\"\"\n    Apply a logical operation between two saved images.\n\n    This method applies a specified logical operation ('And' or 'Or')\n    between two images stored in the parent's saved_images_list. The result\n    is stored as a binary image and validated, with color space information\n    updated accordingly.\n\n    Notes\n    -----\n    This method modifies the following instance attributes:\n    - `binary_image`\n    - `validated_shapes`\n    - `image`\n    - `csc_dict`\n    \"\"\"\n    im1 = self.parent.saved_images_list[self.operation[0]]\n    im2 = self.parent.saved_images_list[self.operation[1]]\n    if self.operation['logical'] == 'And':\n        self.binary_image = np.logical_and(im1, im2).astype(np.uint8)\n    elif self.operation['logical'] == 'Or':\n        self.binary_image = np.logical_or(im1, im2).astype(np.uint8)\n    self.validated_shapes = self.binary_image\n    self.greyscale = self.parent.converted_images_list[self.operation[0]]\n    csc1 = self.parent.saved_color_space_list[self.operation[0]]\n    csc2 = self.parent.saved_color_space_list[self.operation[1]]\n    self.csc_dict = {}\n    for k, v in csc1.items():\n        self.csc_dict[k] = v\n    for k, v in csc2.items():\n        self.csc_dict[k] = v\n    self.csc_dict['logical'] = self.operation['logical']\n    self.evaluate_segmentation()\n</code></pre>"},{"location":"api/cellects/image_analysis/one_image_analysis_threads/#cellects.image_analysis.one_image_analysis_threads.ProcessImage.combine_and_segment","title":"<code>combine_and_segment(csc_dict=None)</code>","text":"<p>Combines color spaces and segments images into binary masks using either K-means clustering or Otsu's thresholding based on specified parameters.</p> <p>Parameters:</p> Name Type Description Default <code>csc_dict</code> <code>dict or None</code> <p>Optional dictionary mapping color space abbreviations (e.g., 'bgr', 'hsv') to their relative contribution coefficients for combination. If not provided, uses the instance's <code>self.csc_dict</code> attribute instead.</p> <code>None</code> Source code in <code>src/cellects/image_analysis/one_image_analysis_threads.py</code> <pre><code>def combine_and_segment(self, csc_dict: Dict=None):\n    \"\"\"\n    Combines color spaces and segments images into binary masks using either K-means clustering or Otsu's thresholding based on specified parameters.\n\n    Parameters\n    ----------\n    csc_dict : dict or None\n        Optional dictionary mapping color space abbreviations (e.g., 'bgr', 'hsv') to their relative contribution coefficients for combination. If not provided, uses the instance's `self.csc_dict` attribute instead.\n\n    \"\"\"\n    if csc_dict is None:\n        csc_dict = self.csc_dict\n    self.image = combine_color_spaces(csc_dict, self.all_c_spaces)\n    self.apply_filter_and_segment()\n</code></pre>"},{"location":"api/cellects/image_analysis/one_image_analysis_threads/#cellects.image_analysis.one_image_analysis_threads.ProcessImage.eval_any_image","title":"<code>eval_any_image()</code>","text":"<p>Summarizes the binary image analysis and determines blob characteristics.</p> <p>Evaluates a binary image to determine various attributes like surface area, blob number, and their relative positions within specified masks. It also checks for common areas with a reference image and saves the combination if certain conditions are met.</p> Source code in <code>src/cellects/image_analysis/one_image_analysis_threads.py</code> <pre><code>def eval_any_image(self):\n    \"\"\"\n\n    Summarizes the binary image analysis and determines blob characteristics.\n\n    Evaluates a binary image to determine various attributes like surface area,\n    blob number, and their relative positions within specified masks. It also\n    checks for common areas with a reference image and saves the combination if\n    certain conditions are met.\n    \"\"\"\n    surf = self.binary_image.sum()\n    if surf &lt; self.params['total_surface_area']:\n        self.fact['unaltered_blob_nb'], shapes = cv2.connectedComponents(self.binary_image)\n        self.fact['unaltered_blob_nb'] -= 1\n        test: bool = True\n        if self.params['arenas_mask'] is not None:\n            self.fact['out_of_arenas'] = np.sum(self.binary_image * self.params['out_of_arenas_mask'])\n            self.fact['in_arenas'] = np.sum(self.binary_image * self.params['arenas_mask'])\n            test = self.fact['out_of_arenas'].values[0] &lt; self.fact['in_arenas'].values[0]\n        if test:\n            if self.params['con_comp_extent'][0] &lt;= self.fact['unaltered_blob_nb'].values[0] &lt;= self.params['con_comp_extent'][1]:\n                if self.params['ref_image'] is not None:\n                    self.fact['common_with_ref'] = np.sum(self.params['ref_image'] * self.binary_image)\n                    test = self.fact['common_with_ref'].values[0] &gt; 0\n                if test:\n                    self.fact['blob_nb'], shapes, self.stats, centroids = cv2.connectedComponentsWithStats(self.binary_image)\n                    self.fact['blob_nb'] -= 1\n                    if np.all(np.sort(self.stats[:, 4])[:-1] &lt; self.params['max_blob_size']):\n                        self.save_combination()\n</code></pre>"},{"location":"api/cellects/image_analysis/one_image_analysis_threads/#cellects.image_analysis.one_image_analysis_threads.ProcessImage.eval_first_image","title":"<code>eval_first_image()</code>","text":"<p>First image filtering process for binary images.</p> <p>This method processes the first binary image by identifying connected components, computing the total area of the image and its potential size limits. If the number of blobs and their total area fall within acceptable thresholds, it proceeds with additional processing and saving.</p> Source code in <code>src/cellects/image_analysis/one_image_analysis_threads.py</code> <pre><code>def eval_first_image(self):\n    \"\"\"\n    First image filtering process for binary images.\n\n    This method processes the first binary image by identifying connected components, computing\n    the total area of the image and its potential size limits. If the number of blobs and their\n    total area fall within acceptable thresholds, it proceeds with additional processing and saving.\n    \"\"\"\n    self.fact['unaltered_blob_nb'], shapes = cv2.connectedComponents(self.binary_image)\n    self.fact['unaltered_blob_nb'] -= 1\n    if 1 &lt;= self.fact['unaltered_blob_nb'].values[0] &lt; 10000:\n        self.fact['total_area'] = np.sum(self.binary_image)\n        inf_lim = np.min((20, np.ceil(self.binary_image.size / 1000)))\n        if inf_lim &lt; self.fact['total_area'].values[0] &lt; self.binary_image.size * 0.9:\n            self.process_first_binary_image()\n            self.save_combination()\n</code></pre>"},{"location":"api/cellects/image_analysis/one_image_analysis_threads/#cellects.image_analysis.one_image_analysis_threads.ProcessImage.evaluate_segmentation","title":"<code>evaluate_segmentation()</code>","text":"<p>Use the filtering algorithm based on the kind of image to analyse</p> Source code in <code>src/cellects/image_analysis/one_image_analysis_threads.py</code> <pre><code>def evaluate_segmentation(self):\n    \"\"\"\n    Use the filtering algorithm based on the kind of image to analyse\n    \"\"\"\n    if self.params['is_first_image']:\n        self.eval_first_image()\n    else:\n        self.eval_any_image()\n</code></pre>"},{"location":"api/cellects/image_analysis/one_image_analysis_threads/#cellects.image_analysis.one_image_analysis_threads.ProcessImage.pca_and_segment","title":"<code>pca_and_segment()</code>","text":"<p>Extract the first principal component and perform segmentation.</p> <p>This method extracts the first principal component from the 'bgr' color space and performs k-means clustering or Otsu thresholding to segment the image based on the parameters provided.</p> Source code in <code>src/cellects/image_analysis/one_image_analysis_threads.py</code> <pre><code>def pca_and_segment(self):\n    \"\"\"\n    Extract the first principal component and perform segmentation.\n\n    This method extracts the first principal component from the 'bgr' color space\n    and performs k-means clustering or Otsu thresholding to segment the image based on the\n    parameters provided.\n    \"\"\"\n    self.image, _, first_pc_vector = extract_first_pc(self.all_c_spaces['bgr'])\n    self.csc_dict = Dict()\n    self.csc_dict['bgr'] = first_pc_vector\n    self.apply_filter_and_segment()\n</code></pre>"},{"location":"api/cellects/image_analysis/one_image_analysis_threads/#cellects.image_analysis.one_image_analysis_threads.ProcessImage.process_first_binary_image","title":"<code>process_first_binary_image()</code>","text":"<p>Process the binary image to identify and validate shapes.</p> <p>This method processes a binary image to detect connected components, validate their sizes, and handle bio and back masks if specified. It ensures that the number of validated shapes matches the expected sample number or applies additional filtering if necessary.</p> Source code in <code>src/cellects/image_analysis/one_image_analysis_threads.py</code> <pre><code>def process_first_binary_image(self):\n    \"\"\"\n    Process the binary image to identify and validate shapes.\n\n    This method processes a binary image to detect connected components,\n    validate their sizes, and handle bio and back masks if specified.\n    It ensures that the number of validated shapes matches the expected\n    sample number or applies additional filtering if necessary.\n\n    \"\"\"\n    shapes_features = shape_selection(self.binary_image, true_shape_number=self.params['blob_nb'],\n                    horizontal_size=self.params['blob_size'], spot_shape=self.params['blob_shape'],\n                    several_blob_per_arena=self.params['several_blob_per_arena'],\n                    bio_mask=self.params['bio_mask'], back_mask=self.params['back_mask'])\n    self.validated_shapes, self.fact['blob_nb'], self.stats, self.centroids = shapes_features\n</code></pre>"},{"location":"api/cellects/image_analysis/one_image_analysis_threads/#cellects.image_analysis.one_image_analysis_threads.ProcessImage.save_combination","title":"<code>save_combination()</code>","text":"<p>Saves the calculated features and masks for a combination of shapes.</p> <p>This method calculates various statistical properties (std) and sums for the validated shapes, and optionally computes sums for bio and back masks if they are specified in the parameters.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>object</code> <p>The instance of the class containing this method. Must have attributes: - <code>validated_shapes</code>: (ndarray) Array of validated shapes. - <code>stats</code>: (ndarray) Statistics array containing width, height, and area. - <code>params</code>: dictionary with parameters including 'bio_mask' and 'back_mask'. - <code>fact</code>: dictionary to store the calculated features. - <code>parent</code>: The parent object containing the method <code>save_combination_features</code>.</p> required Source code in <code>src/cellects/image_analysis/one_image_analysis_threads.py</code> <pre><code>def save_combination(self):\n    \"\"\"\n    Saves the calculated features and masks for a combination of shapes.\n\n    This method calculates various statistical properties (std) and sums for the\n    validated shapes, and optionally computes sums for bio and back masks if they are\n    specified in the parameters.\n\n    Parameters\n    ----------\n    self : object\n        The instance of the class containing this method. Must have attributes:\n        - ``validated_shapes``: (ndarray) Array of validated shapes.\n        - ``stats``: (ndarray) Statistics array containing width, height, and area.\n        - ``params``: dictionary with parameters including 'bio_mask' and 'back_mask'.\n        - ``fact``: dictionary to store the calculated features.\n        - ``parent``: The parent object containing the method `save_combination_features`.\n    \"\"\"\n    self.fact['total_area'] = self.validated_shapes.sum()\n    self.fact['width_std'] = np.std(self.stats[1:, 2])\n    self.fact['height_std'] = np.std(self.stats[1:, 3])\n    self.fact['area_std'] = np.std(self.stats[1:, 4])\n    if self.params['bio_mask'] is not None:\n        self.fact['bio_sum'] = self.validated_shapes[self.params['bio_mask'][0], self.params['bio_mask'][1]].sum()\n    if self.params['back_mask'] is not None:\n        self.fact['back_sum'] = (1 - self.validated_shapes)[self.params['back_mask'][0], self.params['back_mask'][1]].sum()\n    self.parent.save_combination_features(self)\n</code></pre>"},{"location":"api/cellects/image_analysis/one_image_analysis_threads/#cellects.image_analysis.one_image_analysis_threads.ProcessImage.start_processing","title":"<code>start_processing(l)</code>","text":"<p>Begin processing tasks based on the given process type.</p> <p>Parameters:</p> Name Type Description Default <code>l</code> <code>list</code> <p>A list containing parameters and instructions for processing. Expected structure:     - First element: self.parent (object)     - Second element: params (dict)     - Third element: process type (str) ('one', 'PCA', 'add', 'subtract', or 'logical')     - Fourth element (if applicable): additional parameters based on process type.</p> required Notes <p>This function modifies instance attributes directly and performs various operations based on the <code>process</code> type.</p> Source code in <code>src/cellects/image_analysis/one_image_analysis_threads.py</code> <pre><code>def start_processing(self, l: list):\n    \"\"\"\n    Begin processing tasks based on the given process type.\n\n    Parameters\n    ----------\n    l : list\n        A list containing parameters and instructions for processing.\n        Expected structure:\n            - First element: self.parent (object)\n            - Second element: params (dict)\n            - Third element: process type (str) ('one', 'PCA', 'add', 'subtract', or 'logical')\n            - Fourth element (if applicable): additional parameters based on process type.\n\n    Notes\n    -----\n    This function modifies instance attributes directly and performs various operations based on the `process` type.\n    \"\"\"\n    self.parent = l[0]\n    self.fact = pd.DataFrame(np.zeros((1, len(self.parent.factors)), dtype=np.uint32), columns=self.parent.factors)\n    self.params = l[1]\n    process = l[2]\n    self.all_c_spaces = self.parent.all_c_spaces\n    if process == 'one':\n        self.csc_dict = l[3]\n        self.combine_and_segment()\n        self.evaluate_segmentation()\n    elif process == 'PCA':\n        self.pca_and_segment()\n        self.evaluate_segmentation()\n    elif process == 'add':\n        self.try_adding_channels(l[3])\n    elif process == 'subtract':\n        self.try_subtracting_channels()\n    elif process == 'logical':\n        self.operation = l[3]\n        self.apply_logical_operation()\n</code></pre>"},{"location":"api/cellects/image_analysis/one_image_analysis_threads/#cellects.image_analysis.one_image_analysis_threads.ProcessImage.try_adding_channels","title":"<code>try_adding_channels(i)</code>","text":"<p>Try adding channels to the current color space combination.</p> <p>Extend the functionality of a selected color space combination by attempting to add channels from other combinations, evaluating the results based on the number of shapes and total area.</p> <p>Parameters:</p> Name Type Description Default <code>i</code> <code>int</code> <p>The index of the saved color space and combination features to start with.</p> required Source code in <code>src/cellects/image_analysis/one_image_analysis_threads.py</code> <pre><code>def try_adding_channels(self, i: int):\n    \"\"\"\n    Try adding channels to the current color space combination.\n\n    Extend the functionality of a selected color space combination by attempting\n    to add channels from other combinations, evaluating the results based on\n    the number of shapes and total area.\n\n    Parameters\n    ----------\n    i : int\n        The index of the saved color space and combination features to start with.\n    \"\"\"\n    saved_color_space_list = self.parent.saved_color_space_list\n    combination_features = self.parent.combination_features\n    self.csc_dict = saved_color_space_list[i]\n    previous_shape_number = combination_features.loc[i, 'blob_nb']\n    previous_sum = combination_features.loc[i, 'total_area']\n    for j in self.params['possibilities'][::-1]:\n        csc_dict2 = saved_color_space_list[j]\n        csc_dict = self.csc_dict.copy()\n        keys = list(csc_dict.keys())\n\n        k2 = list(csc_dict2.keys())[0]\n        v2 = csc_dict2[k2]\n        if np.isin(k2, keys) and np.sum(v2 * csc_dict[k2]) != 0:\n            break\n        for factor in [2, 1]:\n            if np.isin(k2, keys):\n                csc_dict[k2] += v2 * factor\n            else:\n                csc_dict[k2] = v2 * factor\n            self.combine_and_segment(csc_dict)\n            if self.params['is_first_image']:\n                self.process_first_binary_image()\n            else:\n                self.fact['blob_nb'], shapes, self.stats, centroids = cv2.connectedComponentsWithStats(self.validated_shapes)\n                self.fact['blob_nb'] -= 1\n            self.fact['total_area'] = self.validated_shapes.sum()\n            if self.fact['blob_nb'].values[0] &lt; previous_shape_number  and self.fact['total_area'].values[0] &gt; previous_sum * 0.9:\n                previous_shape_number = self.fact['blob_nb'].values[0]\n                previous_sum = self.fact['total_area'].values[0]\n                self.csc_dict = csc_dict.copy()\n                self.fact['unaltered_blob_nb'] = combination_features.loc[i, 'unaltered_blob_nb']\n                self.save_combination()\n</code></pre>"},{"location":"api/cellects/image_analysis/one_image_analysis_threads/#cellects.image_analysis.one_image_analysis_threads.ProcessImage.try_subtracting_channels","title":"<code>try_subtracting_channels()</code>","text":"<p>Tries to subtract channels to find the optimal color space combination.</p> <p>This method attempts to remove color spaces one by one from the image to find a combination that maintains the majority of areas while reducing the number of color spaces. This process is repeated until no further improvements are possible.</p> Source code in <code>src/cellects/image_analysis/one_image_analysis_threads.py</code> <pre><code>def try_subtracting_channels(self):\n    \"\"\"\n    Tries to subtract channels to find the optimal color space combination.\n\n    This method attempts to remove color spaces one by one from the image\n    to find a combination that maintains the majority of areas while reducing\n    the number of color spaces. This process is repeated until no further\n    improvements are possible.\n    \"\"\"\n    potentials = self.parent.all_combined\n    # Try to remove color space one by one\n    i = 0\n    original_length = len(potentials)\n    # The while loop until one col space remains or the removal of one implies a strong enough area change\n    while np.logical_and(len(potentials) &gt; 1, i &lt; original_length - 1):\n        self.combine_and_segment(potentials)\n        if self.params['is_first_image']:\n            self.process_first_binary_image()\n            previous_blob_nb = self.fact['blob_nb'].values[0]\n        else:\n            previous_blob_nb, shapes, self.stats, centroids = cv2.connectedComponentsWithStats(\n                self.validated_shapes)\n            previous_blob_nb -= 1\n        previous_sum = self.validated_shapes.sum()\n        color_space_to_remove = List()\n        previous_c_space = list(potentials.keys())[-1]\n        for c_space in potentials.keys():\n            try_potentials = potentials.copy()\n            try_potentials.pop(c_space)\n            if i &gt; 0:\n                try_potentials.pop(previous_c_space)\n            self.combine_and_segment(try_potentials)\n            if self.params['is_first_image']:\n                self.process_first_binary_image()\n            else:\n                self.fact['blob_nb'], shapes, self.stats, centroids = cv2.connectedComponentsWithStats(\n                    self.validated_shapes)\n                self.fact['blob_nb'] -= 1\n            self.fact['total_area'] = self.validated_shapes.sum()\n            if self.fact['blob_nb'].values[0] &lt; previous_blob_nb and self.fact['total_area'].values[0] &gt; previous_sum * 0.9:\n                previous_blob_nb = self.fact['blob_nb'].values[0]\n                previous_sum = self.fact['total_area'].values[0]\n                self.csc_dict = try_potentials.copy()\n                self.fact['unaltered_blob_nb'] = previous_blob_nb\n                self.save_combination()\n                # If removing that color space helps, we remove it from potentials\n                color_space_to_remove.append(c_space)\n                if i &gt; 0:\n                    color_space_to_remove.append(previous_c_space)\n            previous_c_space = c_space\n        if len(color_space_to_remove) == 0:\n            break\n        color_space_to_remove = np.unique(color_space_to_remove)\n        for remove_col_space in color_space_to_remove:\n            potentials.pop(remove_col_space)\n        i += 1\n</code></pre>"},{"location":"api/cellects/image_analysis/oscillations_functions/","title":"<code>cellects.image_analysis.oscillations_functions</code>","text":""},{"location":"api/cellects/image_analysis/oscillations_functions/#cellects.image_analysis.oscillations_functions","title":"<code>cellects.image_analysis.oscillations_functions</code>","text":"<p>Analyze oscillating clusters in 2D video data through flux tracking.</p> <p>This module implements a class to track cluster dynamics by analyzing pixel flux changes over time. The core functionality updates cluster identifiers, tracks periods of activity, and archives final data for completed clusters based on morphological analysis and contour boundaries.</p> <p>Classes ClusterFluxStudy : Updates flux information and tracks oscillating clusters in 2D space</p> <p>Functions update_flux : Processes flux changes to update cluster tracking and archive completed clusters</p> <p>Notes Uses cv2.connectedComponentsWithStats and custom distance calculations for boundary analysis Maintains cumulative pixel data for active clusters during time-lapse processing</p>"},{"location":"api/cellects/image_analysis/oscillations_functions/#cellects.image_analysis.oscillations_functions.detect_oscillations_dynamics","title":"<code>detect_oscillations_dynamics(converted_video, binary, arena_label, starting_time, expected_oscillation_period, time_interval, minimal_oscillating_cluster_size, min_ram_free=1.0, lose_accuracy_to_save_memory=False, save_coord_thickening_slimming=True)</code>","text":"<p>Detects oscillatory dynamics in a labeled arena from processed video data</p> <p>Parameters:</p> Name Type Description Default <code>converted_video</code> <code>NDArray</code> <p>Processed intensity values of the input video as 3D/4D array (t,y,x[,c])</p> required <code>binary</code> <code>NDArray[uint8]</code> <p>Binary segmentation mask with 1 for active region and 0 otherwise</p> required <code>arena_label</code> <code>int</code> <p>Label identifier for the specific arena being analyzed in binary mask</p> required <code>starting_time</code> <code>int</code> <p>Timepoint index to start oscillation analysis from (earlier frames are ignored)</p> required <code>expected_oscillation_period</code> <code>int</code> <p>Expected average period of oscillations in seconds</p> required <code>time_interval</code> <code>int</code> <p>Sampling interval between consecutive video frames in seconds</p> required <code>minimal_oscillating_cluster_size</code> <code>int</code> <p>Minimum number of pixels required for a cluster to be considered an oscillation feature</p> required <code>min_ram_free</code> <code>(float, optional(default=1.0))</code> <p>Minimum free RAM in GB that must remain available during processing</p> <code>1.0</code> <code>lose_accuracy_to_save_memory</code> <code>(bool, optional(default=False))</code> <p>If True, uses low-precision calculations to reduce memory usage at the cost of accuracy</p> <code>False</code> <code>save_coord_thickening_slimming</code> <code>(bool, optional(default=True))</code> <p>If True, saves detected cluster coordinates as .npy files</p> <code>True</code> <p>Returns:</p> Type Description <code>NDArray[int8]</code> <p>3D array where each pixel is labeled with 1=influx region, 2=efflux region, or 0=no oscillation</p> Notes <ul> <li>Processes video data by calculating intensity gradients to detect directional oscillations</li> <li>Memory-intensive operations use float16 when available RAM would otherwise be exceeded</li> <li>Saves coordinate arrays if requested, which may consume significant disk space for large datasets</li> </ul> Source code in <code>src/cellects/image_analysis/oscillations_functions.py</code> <pre><code>def detect_oscillations_dynamics(converted_video: NDArray, binary: NDArray[np.uint8], arena_label: int,\n                                 starting_time: int, expected_oscillation_period: int,\n                                 time_interval: int, minimal_oscillating_cluster_size:int,\n                                 min_ram_free: float=1., lose_accuracy_to_save_memory: bool=False,\n                                 save_coord_thickening_slimming: bool=True):\n    \"\"\"\n    Detects oscillatory dynamics in a labeled arena from processed video data\n\n    Parameters\n    ----------\n    converted_video : NDArray\n        Processed intensity values of the input video as 3D/4D array (t,y,x[,c])\n    binary : NDArray[np.uint8]\n        Binary segmentation mask with 1 for active region and 0 otherwise\n    arena_label : int\n        Label identifier for the specific arena being analyzed in binary mask\n    starting_time : int\n        Timepoint index to start oscillation analysis from (earlier frames are ignored)\n    expected_oscillation_period : int\n        Expected average period of oscillations in seconds\n    time_interval : int\n        Sampling interval between consecutive video frames in seconds\n    minimal_oscillating_cluster_size : int\n        Minimum number of pixels required for a cluster to be considered an oscillation feature\n    min_ram_free : float, optional (default=1.0)\n        Minimum free RAM in GB that must remain available during processing\n    lose_accuracy_to_save_memory : bool, optional (default=False)\n        If True, uses low-precision calculations to reduce memory usage at the cost of accuracy\n    save_coord_thickening_slimming : bool, optional (default=True)\n        If True, saves detected cluster coordinates as .npy files\n\n    Returns\n    -------\n    NDArray[np.int8]\n        3D array where each pixel is labeled with 1=influx region, 2=efflux region, or 0=no oscillation\n\n    Notes\n    -----\n    - Processes video data by calculating intensity gradients to detect directional oscillations\n    - Memory-intensive operations use float16 when available RAM would otherwise be exceeded\n    - Saves coordinate arrays if requested, which may consume significant disk space for large datasets\n    \"\"\"\n    logging.info(f\"Arena n\u00b0{arena_label}. Starting oscillation analysis.\")\n    dims = converted_video.shape\n    oscillations_video = None\n    if dims[0] &gt; 1:\n        period_in_frame_nb = int(expected_oscillation_period / time_interval)\n        if period_in_frame_nb &lt; 2:\n            period_in_frame_nb = 2\n        necessary_memory = dims[0] * dims[1] * dims[2] * 64 * 4 * 1.16415e-10\n        available_memory = (virtual_memory().available &gt;&gt; 30) - min_ram_free\n    if len(dims) == 4:\n        converted_video = converted_video[:, :, :, 0]\n        average_intensities = np.mean(converted_video, (1, 2))\n        if lose_accuracy_to_save_memory or (necessary_memory &gt; available_memory):\n            oscillations_video = np.zeros(dims, dtype=np.float16)\n            for cy in np.arange(dims[1]):\n                for cx in np.arange(dims[2]):\n                    oscillations_video[:, cy, cx] = np.round(\n                        np.gradient(converted_video[:, cy, cx, ...] / average_intensities, period_in_frame_nb), 3).astype(np.float16)\n        else:\n            oscillations_video = np.gradient(converted_video / average_intensities[:, None, None], period_in_frame_nb, axis=0)\n        oscillations_video = np.sign(oscillations_video)\n        oscillations_video = oscillations_video.astype(np.int8)\n        oscillations_video[binary == 0] = 0\n\n        for t in np.arange(starting_time, dims[0]):\n            oscillations_image = np.zeros(dims[1:], np.uint8)\n            # Add in or ef if a pixel has at least 4 neighbor in or ef\n            neigh_comp = CompareNeighborsWithValue(oscillations_video[t, :, :], connectivity=8, data_type=np.int8)\n            neigh_comp.is_inf(0, and_itself=False)\n            neigh_comp.is_sup(0, and_itself=False)\n            # Not verified if influx is really influx (resp efflux)\n            influx = neigh_comp.sup_neighbor_nb\n            efflux = neigh_comp.inf_neighbor_nb\n\n            # Only keep pixels having at least 4 positive (resp. negative) neighbors\n            influx[influx &lt;= 4] = 0\n            efflux[efflux &lt;= 4] = 0\n            influx[influx &gt; 4] = 1\n            efflux[efflux &gt; 4] = 1\n            if np.any(influx) or np.any(efflux):\n                influx, in_stats, in_centroids = cc(influx)\n                efflux, ef_stats, ef_centroids = cc(efflux)\n                # Only keep clusters larger than 'minimal_oscillating_cluster_size' pixels (smaller are considered as noise\n                in_smalls = np.nonzero(in_stats[:, 4] &lt; minimal_oscillating_cluster_size)[0]\n                if len(in_smalls) &gt; 0:\n                    influx[np.isin(influx, in_smalls)] = 0\n                ef_smalls = np.nonzero(ef_stats[:, 4] &lt; minimal_oscillating_cluster_size)[0]\n                if len(ef_smalls) &gt; 0:\n                    efflux[np.isin(efflux, ef_smalls)] = 0\n                oscillations_image[influx &gt; 0] = 1\n                oscillations_image[efflux &gt; 0] = 2\n            oscillations_video[t, :, :] = oscillations_image\n        oscillations_video[:starting_time, :, :] = 0\n        if save_coord_thickening_slimming:\n            np.save(\n                f\"coord_thickening{arena_label}_t{dims[0]}_y{dims[1]}_x{dims[2]}.npy\",\n                smallest_memory_array(np.nonzero(oscillations_video == 1), \"uint\"))\n            np.save(\n                f\"coord_slimming{arena_label}_t{dims[0]}_y{dims[1]}_x{dims[2]}.npy\",\n                smallest_memory_array(np.nonzero(oscillations_video == 2), \"uint\"))\n    return oscillations_video\n</code></pre>"},{"location":"api/cellects/image_analysis/progressively_add_distant_shapes/","title":"<code>cellects.image_analysis.progressively_add_distant_shapes</code>","text":""},{"location":"api/cellects/image_analysis/progressively_add_distant_shapes/#cellects.image_analysis.progressively_add_distant_shapes","title":"<code>cellects.image_analysis.progressively_add_distant_shapes</code>","text":"<p>Progressively Add Distant Shapes Module</p> <p>This module contains the <code>ProgressivelyAddDistantShapes</code> class which is designed to analyze and connect shapes in binary images based on their size and distance from a main shape. It can progressively grow bridges between shapes in binary video sequences, with growth speeds that depend on neighboring growth speed.</p> <p>The module provides functionality to: - Check and adjust main shape labels - Consider shapes based on size criteria - Connect shapes that meet distance and size requirements - Expand small shapes toward the main shape - Modify past analysis by progressively filling pixels based on shape growth patterns</p> <p>Classes:     ProgressivelyAddDistantShapes: Main class for analyzing and connecting shapes in binary images.</p> <p>Functions:     make_gravity_field: Creates a gravity field around the main shape.     CompareNeighborsWithValue: Compares neighbor values in an array.     get_radius_distance_against_time: Calculates the relationship between distance and time for shape expansion.</p> <p>This module is particularly useful in image analysis tasks where shapes need to be tracked and connected over time based on spatial relationships.</p>"},{"location":"api/cellects/image_analysis/progressively_add_distant_shapes/#cellects.image_analysis.progressively_add_distant_shapes.ProgressivelyAddDistantShapes","title":"<code>ProgressivelyAddDistantShapes</code>","text":"<p>This class checks new potential shapes sizes and distance to a main shape.</p> <p>If these sizes and distance match requirements, create a bridge between these and the main shape. Then, the <code>modify_past_analysis</code> method progressively grows that bridge in a binary video. Bridge growth speed depends on neighboring growth speed.</p> <p>Attributes:</p> Name Type Description <code>new_order</code> <code>ndarray</code> <p>A binary image of all shapes detected at t.</p> <code>main_shape</code> <code>ndarray</code> <p>A binary image of the main shape (1) at t - 1.</p> <code>stats</code> <code>ndarray</code> <p>Statistics about the connected components found in <code>new_order</code>.</p> <code>max_distance</code> <code>int</code> <p>The maximal distance for a shape from new_potentials to get bridged.</p> <code>gravity_field</code> <code>ndarray</code> <p>The gravity field used for connecting shapes.</p> <p>Parameters:</p> Name Type Description Default <code>new_potentials</code> <code>ndarray</code> <p>A binary image of all shapes detected at t.</p> required <code>previous_shape</code> <code>ndarray</code> <p>A binary image of the main shape (1) at t - 1.</p> required <code>max_distance</code> <code>int</code> <p>The maximal distance for a shape from new_potentials to get bridged.</p> required <p>Methods:</p> Name Description <code>check_main_shape_label</code> <p>Check if the main shape label is correctly set.</p> <code>consider_shapes_sizes</code> <p>Consider shapes sizes and eliminate too small or large ones.</p> <code>connect_shapes</code> <p>Connect shapes that are within the maximal distance and of appropriate size.</p> <code>_expand_smalls_toward_main</code> <p>Expand small shapes toward the main shape.</p> Example <p>new_potentials = np.array([[0, 1, 0], [1, 0, 1], [0, 1, 0]]) previous_shape = np.array([[0, 1, 0], [1, 0, 0], [0, 1, 0]]) max_distance = 2 bridge_shapes = ProgressivelyAddDistantShapes(new_potentials, previous_shape, max_distance) bridge_shapes.consider_shapes_sizes(min_shape_size=2, max_shape_size=10) bridge_shapes.connect_shapes(only_keep_connected_shapes=True, rank_connecting_pixels=False) print(bridge_shapes.expanded_shape) [[0 1 0]  [1 1 1]  [0 1 0]]</p> Source code in <code>src/cellects/image_analysis/progressively_add_distant_shapes.py</code> <pre><code>class ProgressivelyAddDistantShapes:\n    \"\"\"\n    This class checks new potential shapes sizes and distance to a main shape.\n\n    If these sizes and distance match requirements, create a bridge between\n    these and the main shape. Then, the `modify_past_analysis` method progressively grows that bridge\n    in a binary video. Bridge growth speed depends on neighboring growth speed.\n\n    Attributes\n    ----------\n    new_order : numpy.ndarray\n        A binary image of all shapes detected at t.\n    main_shape : numpy.ndarray\n        A binary image of the main shape (1) at t - 1.\n    stats : numpy.ndarray\n        Statistics about the connected components found in `new_order`.\n    max_distance : int\n        The maximal distance for a shape from new_potentials to get bridged.\n    gravity_field : numpy.ndarray\n        The gravity field used for connecting shapes.\n\n    Parameters\n    ----------\n    new_potentials : numpy.ndarray\n        A binary image of all shapes detected at t.\n    previous_shape : numpy.ndarray\n        A binary image of the main shape (1) at t - 1.\n    max_distance : int\n        The maximal distance for a shape from new_potentials to get bridged.\n\n    Methods\n    -------\n    check_main_shape_label(previous_shape)\n        Check if the main shape label is correctly set.\n    consider_shapes_sizes(min_shape_size=None, max_shape_size=None)\n        Consider shapes sizes and eliminate too small or large ones.\n    connect_shapes(only_keep_connected_shapes, rank_connecting_pixels, intensity_valley=None)\n        Connect shapes that are within the maximal distance and of appropriate size.\n    _expand_smalls_toward_main()\n        Expand small shapes toward the main shape.\n\n    Example\n    -------\n    &gt;&gt;&gt; new_potentials = np.array([[0, 1, 0], [1, 0, 1], [0, 1, 0]])\n    &gt;&gt;&gt; previous_shape = np.array([[0, 1, 0], [1, 0, 0], [0, 1, 0]])\n    &gt;&gt;&gt; max_distance = 2\n    &gt;&gt;&gt; bridge_shapes = ProgressivelyAddDistantShapes(new_potentials, previous_shape, max_distance)\n    &gt;&gt;&gt; bridge_shapes.consider_shapes_sizes(min_shape_size=2, max_shape_size=10)\n    &gt;&gt;&gt; bridge_shapes.connect_shapes(only_keep_connected_shapes=True, rank_connecting_pixels=False)\n    &gt;&gt;&gt; print(bridge_shapes.expanded_shape)\n    [[0 1 0]\n     [1 1 1]\n     [0 1 0]]\n    \"\"\"\n    def __init__(self, new_potentials: NDArray[np.uint8], previous_shape: NDArray[np.uint8], max_distance):\n        \"\"\"\n        Find connected components and update order.\n\n        This class processes new potentials and previous shape to find\n        connected components and updates the main shape based on a maximum\n        distance threshold.\n\n        Parameters\n        ----------\n        new_potentials : ndarray of uint8\n            The new potential values to process.\n        previous_shape : ndarray of uint8\n            The previous shape information.\n        max_distance :\n            The maximum distance threshold for processing.\n\n        Attributes\n        ----------\n        new_order : ndarray of uint8\n            The result after applying logical OR on `new_potentials` and\n            `previous_shape`.\n        stats : ndarray of int64\n            Statistics of the connected components.\n        centers : ndarray of float64\n            Centers of the connected components.\n        main_shape : ndarray of uint8\n            The main shape array initialized to zeros.\n        max_distance : int\n            The maximum distance threshold for processing.\n\n        Examples\n        --------\n        &gt;&gt;&gt; new_potentials = np.array([[0, 1, 2], [3, 4, 5]])\n        &gt;&gt;&gt; previous_shape = np.array([[0, 1, 0], [1, 0, 1]])\n        &gt;&gt;&gt; max_distance = 2\n        &gt;&gt;&gt; obj = ClassName(new_potentials, previous_shape, max_distance)\n        &gt;&gt;&gt; print(obj.new_order)\n        [[1 1 2]\n         [1 1 1]]\n        \"\"\"\n        self.new_order = np.logical_or(new_potentials, previous_shape).astype(np.uint8)\n        self.new_order, self.stats, centers = cc(self.new_order)\n        self.main_shape = np.zeros(self.new_order.shape, np.uint8)\n        self.max_distance = max_distance\n        self._check_main_shape_label(previous_shape)\n\n    def _check_main_shape_label(self, previous_shape: NDArray[np.uint8]):\n        \"\"\"\n        Check and update main shape label based on previous shape data when multiple shapes exist in new_order.\n\n        This method ensures consistent labeling of the primary shape (labeled 1) in `new_order` by analyzing overlaps\n        with labels from a prior segmentation step. If multiple candidate labels exist for the main shape, it selects\n        the one with the highest pixel count and swaps its label with '1' in both `new_order` and associated statistics.\n\n        Parameters\n        ----------\n        previous_shape\n            Input array representing previous segmentation labels used to identify the primary shape when\n            `new_order` contains multiple potential candidates (labels &gt; 1).\n\n        Examples\n        --------\n        &gt;&gt;&gt; new_potentials = np.array([[1, 1, 0], [0, 0, 0], [0, 1, 1]])\n        &gt;&gt;&gt; previous_shape = np.array([[0, 0, 0], [0, 0, 0], [0, 1, 1]])\n        &gt;&gt;&gt; max_distance = 2\n        &gt;&gt;&gt; pads = ProgressivelyAddDistantShapes(new_potentials, previous_shape, max_distance)\n        &gt;&gt;&gt; pads.main_shape\n        array([[0, 0, 0],\n               [0, 0, 0],\n               [0, 1, 1]], dtype=np.uint8)\n        \"\"\"\n        if np.any(self.new_order &gt; 1):\n            # If there is at least one pixel of the previous shape that is not among pixels labelled 1,\n            # clarify who's main shape\n            main_shape_label = np.unique(previous_shape * self.new_order)\n            main_shape_label = main_shape_label[main_shape_label != 0]\n\n            # If the main shape is not labelled 1 in main_shape:\n            if not np.isin(1, main_shape_label):\n                # If it is not 1, find which label correspond to the previous shape\n                if len(main_shape_label) &gt; 1:\n                    pixel_sum_per_label = np.zeros(len(main_shape_label), dtype =np.uint64)\n                    # Find out the label corresponding to the largest shape\n                    for li, label in enumerate(main_shape_label):\n                        pixel_sum_per_label[li] = self.new_order[self.new_order == label].sum()\n                    main_shape_label = main_shape_label[np.argmax(pixel_sum_per_label)]\n                # Attribute the correct main shape\n                self.main_shape[self.new_order == main_shape_label] = 1\n                # Exchange the 1 and the main shape label in new_order image\n                not_one_idx = np.nonzero(self.new_order == main_shape_label)\n                one_idx = np.nonzero(self.new_order == 1)\n                self.new_order[not_one_idx[0], not_one_idx[1]] = 1\n                self.new_order[one_idx[0], one_idx[1]] = main_shape_label\n                # Do the same for stats\n                not_one_stats = deepcopy(self.stats[main_shape_label - 1, :])\n                self.stats[main_shape_label - 1, :] = self.stats[1, :]\n                self.stats[1, :] = not_one_stats\n            else:\n            #if np.any(previous_shape * (self.new_order == 1)):\n                # Create an image of the principal shape\n                self.main_shape[self.new_order == 1] = 1\n        else:\n            self.main_shape[np.nonzero(self.new_order)] = 1\n\n    def consider_shapes_sizes(self, min_shape_size: int=None, max_shape_size: int=None):\n        \"\"\"Filter shapes based on minimum and maximum size thresholds.\n\n        This method adjusts `new_order` by excluding indices of shapes that are either\n        smaller than `min_shape_size` or larger than `max_shape_size`. The main shape index\n        (1) is preserved even if it meets the filtering criteria. When no constraints apply,\n        the expanded shape defaults to the main shape.\n\n        Parameters\n        ----------\n        min_shape_size : int, optional\n            Minimum allowed size for shapes (compared against 4th column of `self.stats`).\n        max_shape_size : int, optional\n            Maximum allowed size for shapes (compared against 4th column of `self.stats`).\n\n        Examples\n        --------\n        &gt;&gt;&gt; new_potentials = np.array([[1, 1, 0], [0, 0, 0], [0, 1, 1]])\n        &gt;&gt;&gt; previous_shape = np.array([[0, 0, 0], [0, 0, 0], [0, 1, 1]])\n        &gt;&gt;&gt; max_distance = 2\n        &gt;&gt;&gt; pads = ProgressivelyAddDistantShapes(new_potentials, previous_shape, max_distance)\n        &gt;&gt;&gt; pads.consider_shapes_sizes(min_shape_size=2, max_shape_size=10)\n        &gt;&gt;&gt; pads.new_order\n        array([[2, 2, 0],\n       [0, 0, 0],\n       [0, 1, 1]], dtype=np.uint8)\n        \"\"\"\n        if self.max_distance != 0:\n            # Eliminate too small and too large shapes\n            if min_shape_size is not None or max_shape_size is not None:\n                if min_shape_size is not None:\n                    small_shapes = self.stats[:, 4] &lt; min_shape_size\n                    extreme_shapes = deepcopy(small_shapes)\n                if max_shape_size is not None:\n                    large_shapes = self.stats[:, 4] &gt; max_shape_size\n                    extreme_shapes = deepcopy(large_shapes)\n                if min_shape_size is not None and max_shape_size is not None:\n                    extreme_shapes = np.nonzero(np.logical_or(small_shapes, large_shapes))[0]\n                is_main_in_it = np.isin(extreme_shapes, 1)\n                if np.any(is_main_in_it):\n                    extreme_shapes = np.delete(extreme_shapes, is_main_in_it)\n                for extreme_shape in extreme_shapes:\n                    self.new_order[self.new_order == extreme_shape] = 0\n        else:\n            self.expanded_shape = self.main_shape\n\n    def _find_shape_connection_order(self):\n        # Dilate the main shape, progressively to infer in what order other shapes should be expanded toward it\n        other_shapes = np.zeros(self.main_shape.shape, np.uint8)\n        other_shapes[self.new_order &gt; 1] = 1\n        new_order = deepcopy(self.new_order)\n        dil_main_shape = deepcopy(self.main_shape)\n        order_of_shapes_to_expand = np.empty(0, dtype=np.uint32)\n        nb = 3\n        while nb &gt; 2:\n            dil_main_shape = cv2.dilate(dil_main_shape, rhombus_55)\n            connections = dil_main_shape * new_order\n            new_connections = np.unique(connections)[2:]\n            new_order[np.isin(new_order, new_connections)] = 1\n            order_of_shapes_to_expand = np.append(order_of_shapes_to_expand, new_connections)\n            connections[dil_main_shape &gt; 0] = 1\n            connections[other_shapes &gt; 0] = 1\n            connections[connections &gt; 0] = 1\n            nb, connections = cv2.connectedComponents(connections.astype(np.uint8))\n        if len(order_of_shapes_to_expand) == 0:\n            order_of_shapes_to_expand = np.unique(new_order)[2:]\n        return order_of_shapes_to_expand\n\n    def _expand_smalls_toward_main(self):\n        \"\"\"Expands small shapes toward a main shape using morphological operations and gravity field analysis.\n\n        The method dilates the main shape to determine an order of expansion for connected regions.\n        Each identified region is iteratively expanded until overlapping with the main shape, guided by a gravity field gradient.\n        Results include both the final expanded binary mask and peak values from the gravity field during expansion phases.\n\n        Returns\n        -------\n        numpy.ndarray[numpy.uint8]\n            Binary array where small shapes are fully expanded to connect with the main shape.\n        numpy.ndarray[numpy.uint32]\n            Array containing maximum detected field strengths for each expanded region, in order of connection.\n\n        Examples\n        --------\n        &gt;&gt;&gt; new_potentials = np.array([[1, 1, 0], [0, 0, 0], [0, 1, 1]])\n        &gt;&gt;&gt; previous_shape = np.array([[0, 0, 0], [0, 0, 0], [0, 1, 1]])\n        &gt;&gt;&gt; max_distance = 3\n        &gt;&gt;&gt; pads = ProgressivelyAddDistantShapes(new_potentials, previous_shape, max_distance)\n        &gt;&gt;&gt; pads.consider_shapes_sizes(min_shape_size=2, max_shape_size=10)\n        &gt;&gt;&gt; pads.gravity_field = make_gravity_field(pads.main_shape, max_distance=pads.max_distance, with_erosion=0)\n        &gt;&gt;&gt; expanded_main, max_field_feelings = pads._expand_smalls_toward_main()\n        &gt;&gt;&gt; print(expanded_main)\n        [[1 1 0]\n         [0 1 1]\n         [0 1 1]]\n        \"\"\"\n        simple_disk = cross_33\n        order_of_shapes_to_expand = self._find_shape_connection_order()\n        expanded_main = deepcopy(self.main_shape)\n        max_field_feelings = np.empty(0, dtype=np.uint32)\n        # Loop over each shape to connect, from the nearest to the furthest to the main shape\n        for shape_i in order_of_shapes_to_expand:#  shape_i = order_of_shapes_to_expand[0]\n            current_shape = np.zeros(self.main_shape.shape, np.uint8)\n            current_shape[self.new_order == shape_i] = 1\n            dil = 0\n            # Dilate that shape until it overlaps the main shape\n            while np.logical_and(dil &lt;= self.max_distance, not np.any(current_shape * expanded_main)):\n                dil += 1\n                rings = cv2.dilate(current_shape, simple_disk, iterations=1, borderType=cv2.BORDER_CONSTANT,\n                               borderValue=0)\n\n                rings = self.gravity_field * (rings - current_shape)\n                max_field_feeling = np.max(rings) # np.min(rings[rings&gt;0])\n                max_field_feelings = np.append(max_field_feeling, max_field_feelings)\n                if max_field_feeling &gt; 0:  # If there is no shape within max_distance range, quit the loop\n\n                    if dil == 1:\n                        initial_pixel_number = np.sum(rings == max_field_feeling)\n                    while np.sum(rings == max_field_feeling) &gt; initial_pixel_number:\n                        shrinking_stick = CompareNeighborsWithValue(rings, 8, np.uint32)\n                        shrinking_stick.is_equal(max_field_feeling, True)\n                        rings[shrinking_stick.equal_neighbor_nb &lt; 2] = 0\n                    current_shape[rings == max_field_feeling] = 1\n                else:\n                    break\n\n            expanded_main[current_shape != 0] = 1\n        return expanded_main, max_field_feelings\n\n\n    def connect_shapes(self, only_keep_connected_shapes: bool, rank_connecting_pixels: bool, intensity_valley: NDArray=None):\n        \"\"\"Connects small shapes to a main shape using gravity field expansion and filtering based on distance and intensity conditions.\n\n        Extended Description\n        --------------------\n        When distant shapes of sufficient size are present, this method generates a gravity field around the main shape. It then expands smaller shapes toward the main one according to gradient values. If shapes fall within the gravity field range:\n        - Shapes not connected to the main one (via `only_keep_connected_shapes`) are filtered out.\n        - Connecting pixels between small and main shapes (via `rank_connecting_pixels`) receive distance-based ranking.\n\n        Parameters\n        ----------\n        only_keep_connected_shapes : bool\n            If True, filters expanded shapes to retain only those connected directly to the main shape.\n        rank_connecting_pixels : bool\n            If True, ranks connecting pixel extensions based on distance between small/main shapes.\n        intensity_valley : array-like, optional\n            Optional intensity values defining a valley region for gravity field calculation. Default is None.\n\n        Attributes\n        ----------\n        gravity_field : ndarray or array-like\n            Stores the computed gravity field used to guide shape expansion.\n        expanded_shape : ndarray of dtype uint8\n            Final combined shape after processing; contains main and connected small shapes.\n        Examples\n        --------\n        &gt;&gt;&gt; new_potentials = np.array([[1, 1, 0], [0, 0, 0], [0, 1, 1]])\n        &gt;&gt;&gt; previous_shape = np.array([[0, 0, 0], [0, 0, 0], [0, 1, 1]])\n        &gt;&gt;&gt; max_distance = 3\n        &gt;&gt;&gt; pads = ProgressivelyAddDistantShapes(new_potentials, previous_shape, max_distance)\n        &gt;&gt;&gt; pads.consider_shapes_sizes(min_shape_size=2, max_shape_size=10)\n        &gt;&gt;&gt; pads.gravity_field = make_gravity_field(pads.main_shape, max_distance=pads.max_distance, with_erosion=0)\n        &gt;&gt;&gt; pads.connect_shapes(only_keep_connected_shapes=False, rank_connecting_pixels=True)\n        &gt;&gt;&gt; expanded_main, max_field_feelings = pads._expand_smalls_toward_main()\n        &gt;&gt;&gt; print(expanded_main)\n        [[1 1 0]\n         [0 1 1]\n         [0 1 1]]\n        \"\"\"\n        # If there are distant shapes of the good size, run the following:\n        if self.max_distance != 0 and np.any(self.new_order &gt; 1):\n            # The intensity valley method does not work yet, don't use it\n            if intensity_valley is not None:\n                self.gravity_field = intensity_valley # make sure that the values correspond to the coord\n            else:\n                # 1) faire un champ gravitationnel autour de la forme principale\n                self.gravity_field = rounded_inverted_distance_transform(self.main_shape, max_distance=self.max_distance, with_erosion=1)\n\n                # If there are near enough shapes, run the following\n                # 2) Dilate other shapes toward the main according to the gradient\n            other_shapes, max_field_feelings = self._expand_smalls_toward_main()\n\n\n            # plt.imshow(other_shapes)\n            # If there are shapes within gravity field range\n            if np.any(max_field_feelings &gt; 0):\n                self.expanded_shape = np.zeros(self.main_shape.shape, np.uint8)\n                self.expanded_shape[np.nonzero(self.main_shape + other_shapes)] = 1\n                if only_keep_connected_shapes:\n                    # Make sure that only shapes connected with the main one remain on the final image\n                    expanded_shape = keep_shape_connected_with_ref(self.expanded_shape, self.main_shape)\n                    if expanded_shape is not None:\n                        self.expanded_shape = expanded_shape\n                    if rank_connecting_pixels:\n                        # Rate the extension of small shapes according to the distance between the small and the main shapes\n                        self.distance_ranking_of_connecting_pixels()\n                #self.expanded_shape\n                # plt.imshow(self.expanded_shape)\n            else:\n                self.expanded_shape = self.main_shape\n        # Otherwise, end by putting the main shape as output\n        else:\n            self.expanded_shape = self.main_shape\n\n        # else:\n        #     self.expanded_shape = other_shapes + self.main_shape\n        # self.expanded_shape[self.expanded_shape &gt; 1] = 1\n\n    def distance_ranking_of_connecting_pixels(self):\n        \"\"\"\n        Calculate the distance ranking of connecting pixels.\n\n        This function computes a ranked extension map based on the difference between\n        `main_shape` and `expanded_shape`, modifies it using a gravity field, and then\n        updates the `expanded_shape` with this ranked extension.\n        \"\"\"\n        rated_extension = np.zeros(self.main_shape.shape, np.uint8)\n        rated_extension[(self.main_shape - self.expanded_shape) == 255] = 1\n        rated_extension = rated_extension * self.gravity_field\n        if np.any(rated_extension):\n            rated_extension[np.nonzero(rated_extension)] -= np.min(\n                rated_extension[np.nonzero(rated_extension)]) - 1\n        rated_extension *= self.expanded_shape\n        self.expanded_shape += rated_extension\n\n    def modify_past_analysis(self, binary_video: NDArray[np.uint8], draft_seg: NDArray[np.uint8]) -&gt; NDArray[np.uint8]:\n        \"\"\"\n        Modify past analysis based on binary video and draft segmentation.\n\n        This method modifies the past analysis by updating `binary_video` with\n        information from `draft_seg`, and then iteratively filling pixels based on\n        expansion timings.\n\n        Parameters\n        ----------\n        binary_video : ndarray of uint8\n            Input binary video to be modified.\n        draft_seg : ndarray of uint8\n            Draft segmentation used for expanding the shape.\n\n        Returns\n        -------\n        ndarray of uint8\n            Modified binary video after past analysis.\n        \"\"\"\n        self.binary_video = binary_video\n        self.draft_seg = draft_seg\n        self.expanded_shape[self.expanded_shape == 1] = 0\n        # Find the time at which the shape became connected to the expanded shape\n        # (i.e. the time to start looking for a growth)\n        distance_against_time, time_start, time_end = self.find_expansion_timings()\n\n        # Use that vector to progressively fill pixels at the same speed as shape grows\n        for t in np.arange(len(distance_against_time)):\n            image_garbage = (self.expanded_shape &gt;= distance_against_time[t]).astype(np.uint8)\n            new_order, stats, centers = cc(image_garbage)\n            for comp_i in np.arange(1, stats.shape[0]):\n                past_image = deepcopy(self.binary_video[time_start + t, :, :])\n                with_new_comp = new_order == comp_i\n                past_image[with_new_comp] = 1\n                nb_comp, image_garbage = cv2.connectedComponents(past_image)\n                if nb_comp == 2:\n                    self.binary_video[time_start + t, :, :][with_new_comp] = 1\n        #self.expanded_shape[self.expanded_shape &gt; 0] = 1\n        #self.binary_video[time_end:, :, :] += self.expanded_shape\n        for t in np.arange(time_end, self.binary_video.shape[0]):\n            self.binary_video[t, :, :][np.nonzero(self.expanded_shape)] = 1\n        last_image = self.binary_video[t, :, :] + self.binary_video[t - 1, :, :]\n        last_image[last_image &gt; 0] = 1\n        self.binary_video[-1, :, :] = last_image\n        return self.binary_video\n\n    def find_expansion_timings(self) -&gt; Tuple[NDArray[np.float64], int, int]:\n        \"\"\"\n        Find the expansion timings of a shape in binary video.\n\n        This method calculates the time at which an expanded shape reaches\n        the main shape, as well as the distance and time relationship during\n        expansion.\n\n        Returns\n        -------\n        distance_against_time : ndarray of float64\n            Array representing the distance against time.\n        time_start : int\n            The start time of expansion in frames.\n        time_end : int\n            The end time of expansion in frames.\n\n        Raises\n        ------\n        AttributeError\n            If 'binary_video', 'expanded_shape' or 'main_shape' are not defined.\n        \"\"\"\n        max_t = self.binary_video.shape[0] - 1\n        dilated_one = cv2.dilate(self.expanded_shape, cross_33)\n        # Find the time at which the nearest pixel of the expanded_shape si reached by the main shape\n        closest_pixels = np.zeros(self.main_shape.shape, dtype=np.uint8)\n        closest_pixels[self.expanded_shape == np.max(dilated_one)] = 1\n        expand_start = max_t\n        # Loop until there is no overlap between the dilated added shape and the original shape\n        # Stop one frame before in order to obtain the exact reaching moment.\n        while np.any(self.binary_video[expand_start - 1, :, :] * closest_pixels):\n            expand_start -= 1\n\n        # Find the relationship between distance and time\n        distance_against_time, time_start, time_end = get_radius_distance_against_time(\n            self.draft_seg[expand_start:(max_t + 1), :, :], dilated_one)\n        time_start += expand_start\n        time_end += expand_start\n        return distance_against_time, time_start, time_end\n</code></pre>"},{"location":"api/cellects/image_analysis/progressively_add_distant_shapes/#cellects.image_analysis.progressively_add_distant_shapes.ProgressivelyAddDistantShapes.__init__","title":"<code>__init__(new_potentials, previous_shape, max_distance)</code>","text":"<p>Find connected components and update order.</p> <p>This class processes new potentials and previous shape to find connected components and updates the main shape based on a maximum distance threshold.</p> <p>Parameters:</p> Name Type Description Default <code>new_potentials</code> <code>ndarray of uint8</code> <p>The new potential values to process.</p> required <code>previous_shape</code> <code>ndarray of uint8</code> <p>The previous shape information.</p> required <code>max_distance</code> <p>The maximum distance threshold for processing.</p> required <p>Attributes:</p> Name Type Description <code>new_order</code> <code>ndarray of uint8</code> <p>The result after applying logical OR on <code>new_potentials</code> and <code>previous_shape</code>.</p> <code>stats</code> <code>ndarray of int64</code> <p>Statistics of the connected components.</p> <code>centers</code> <code>ndarray of float64</code> <p>Centers of the connected components.</p> <code>main_shape</code> <code>ndarray of uint8</code> <p>The main shape array initialized to zeros.</p> <code>max_distance</code> <code>int</code> <p>The maximum distance threshold for processing.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; new_potentials = np.array([[0, 1, 2], [3, 4, 5]])\n&gt;&gt;&gt; previous_shape = np.array([[0, 1, 0], [1, 0, 1]])\n&gt;&gt;&gt; max_distance = 2\n&gt;&gt;&gt; obj = ClassName(new_potentials, previous_shape, max_distance)\n&gt;&gt;&gt; print(obj.new_order)\n[[1 1 2]\n [1 1 1]]\n</code></pre> Source code in <code>src/cellects/image_analysis/progressively_add_distant_shapes.py</code> <pre><code>def __init__(self, new_potentials: NDArray[np.uint8], previous_shape: NDArray[np.uint8], max_distance):\n    \"\"\"\n    Find connected components and update order.\n\n    This class processes new potentials and previous shape to find\n    connected components and updates the main shape based on a maximum\n    distance threshold.\n\n    Parameters\n    ----------\n    new_potentials : ndarray of uint8\n        The new potential values to process.\n    previous_shape : ndarray of uint8\n        The previous shape information.\n    max_distance :\n        The maximum distance threshold for processing.\n\n    Attributes\n    ----------\n    new_order : ndarray of uint8\n        The result after applying logical OR on `new_potentials` and\n        `previous_shape`.\n    stats : ndarray of int64\n        Statistics of the connected components.\n    centers : ndarray of float64\n        Centers of the connected components.\n    main_shape : ndarray of uint8\n        The main shape array initialized to zeros.\n    max_distance : int\n        The maximum distance threshold for processing.\n\n    Examples\n    --------\n    &gt;&gt;&gt; new_potentials = np.array([[0, 1, 2], [3, 4, 5]])\n    &gt;&gt;&gt; previous_shape = np.array([[0, 1, 0], [1, 0, 1]])\n    &gt;&gt;&gt; max_distance = 2\n    &gt;&gt;&gt; obj = ClassName(new_potentials, previous_shape, max_distance)\n    &gt;&gt;&gt; print(obj.new_order)\n    [[1 1 2]\n     [1 1 1]]\n    \"\"\"\n    self.new_order = np.logical_or(new_potentials, previous_shape).astype(np.uint8)\n    self.new_order, self.stats, centers = cc(self.new_order)\n    self.main_shape = np.zeros(self.new_order.shape, np.uint8)\n    self.max_distance = max_distance\n    self._check_main_shape_label(previous_shape)\n</code></pre>"},{"location":"api/cellects/image_analysis/progressively_add_distant_shapes/#cellects.image_analysis.progressively_add_distant_shapes.ProgressivelyAddDistantShapes.connect_shapes","title":"<code>connect_shapes(only_keep_connected_shapes, rank_connecting_pixels, intensity_valley=None)</code>","text":"<p>Connects small shapes to a main shape using gravity field expansion and filtering based on distance and intensity conditions.</p> Extended Description <p>When distant shapes of sufficient size are present, this method generates a gravity field around the main shape. It then expands smaller shapes toward the main one according to gradient values. If shapes fall within the gravity field range: - Shapes not connected to the main one (via <code>only_keep_connected_shapes</code>) are filtered out. - Connecting pixels between small and main shapes (via <code>rank_connecting_pixels</code>) receive distance-based ranking.</p> <p>Parameters:</p> Name Type Description Default <code>only_keep_connected_shapes</code> <code>bool</code> <p>If True, filters expanded shapes to retain only those connected directly to the main shape.</p> required <code>rank_connecting_pixels</code> <code>bool</code> <p>If True, ranks connecting pixel extensions based on distance between small/main shapes.</p> required <code>intensity_valley</code> <code>array - like</code> <p>Optional intensity values defining a valley region for gravity field calculation. Default is None.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>gravity_field</code> <code>ndarray or array - like</code> <p>Stores the computed gravity field used to guide shape expansion.</p> <code>expanded_shape</code> <code>ndarray of dtype uint8</code> <p>Final combined shape after processing; contains main and connected small shapes.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; new_potentials = np.array([[1, 1, 0], [0, 0, 0], [0, 1, 1]])\n&gt;&gt;&gt; previous_shape = np.array([[0, 0, 0], [0, 0, 0], [0, 1, 1]])\n&gt;&gt;&gt; max_distance = 3\n&gt;&gt;&gt; pads = ProgressivelyAddDistantShapes(new_potentials, previous_shape, max_distance)\n&gt;&gt;&gt; pads.consider_shapes_sizes(min_shape_size=2, max_shape_size=10)\n&gt;&gt;&gt; pads.gravity_field = make_gravity_field(pads.main_shape, max_distance=pads.max_distance, with_erosion=0)\n&gt;&gt;&gt; pads.connect_shapes(only_keep_connected_shapes=False, rank_connecting_pixels=True)\n&gt;&gt;&gt; expanded_main, max_field_feelings = pads._expand_smalls_toward_main()\n&gt;&gt;&gt; print(expanded_main)\n[[1 1 0]\n [0 1 1]\n [0 1 1]]\n</code></pre> Source code in <code>src/cellects/image_analysis/progressively_add_distant_shapes.py</code> <pre><code>def connect_shapes(self, only_keep_connected_shapes: bool, rank_connecting_pixels: bool, intensity_valley: NDArray=None):\n    \"\"\"Connects small shapes to a main shape using gravity field expansion and filtering based on distance and intensity conditions.\n\n    Extended Description\n    --------------------\n    When distant shapes of sufficient size are present, this method generates a gravity field around the main shape. It then expands smaller shapes toward the main one according to gradient values. If shapes fall within the gravity field range:\n    - Shapes not connected to the main one (via `only_keep_connected_shapes`) are filtered out.\n    - Connecting pixels between small and main shapes (via `rank_connecting_pixels`) receive distance-based ranking.\n\n    Parameters\n    ----------\n    only_keep_connected_shapes : bool\n        If True, filters expanded shapes to retain only those connected directly to the main shape.\n    rank_connecting_pixels : bool\n        If True, ranks connecting pixel extensions based on distance between small/main shapes.\n    intensity_valley : array-like, optional\n        Optional intensity values defining a valley region for gravity field calculation. Default is None.\n\n    Attributes\n    ----------\n    gravity_field : ndarray or array-like\n        Stores the computed gravity field used to guide shape expansion.\n    expanded_shape : ndarray of dtype uint8\n        Final combined shape after processing; contains main and connected small shapes.\n    Examples\n    --------\n    &gt;&gt;&gt; new_potentials = np.array([[1, 1, 0], [0, 0, 0], [0, 1, 1]])\n    &gt;&gt;&gt; previous_shape = np.array([[0, 0, 0], [0, 0, 0], [0, 1, 1]])\n    &gt;&gt;&gt; max_distance = 3\n    &gt;&gt;&gt; pads = ProgressivelyAddDistantShapes(new_potentials, previous_shape, max_distance)\n    &gt;&gt;&gt; pads.consider_shapes_sizes(min_shape_size=2, max_shape_size=10)\n    &gt;&gt;&gt; pads.gravity_field = make_gravity_field(pads.main_shape, max_distance=pads.max_distance, with_erosion=0)\n    &gt;&gt;&gt; pads.connect_shapes(only_keep_connected_shapes=False, rank_connecting_pixels=True)\n    &gt;&gt;&gt; expanded_main, max_field_feelings = pads._expand_smalls_toward_main()\n    &gt;&gt;&gt; print(expanded_main)\n    [[1 1 0]\n     [0 1 1]\n     [0 1 1]]\n    \"\"\"\n    # If there are distant shapes of the good size, run the following:\n    if self.max_distance != 0 and np.any(self.new_order &gt; 1):\n        # The intensity valley method does not work yet, don't use it\n        if intensity_valley is not None:\n            self.gravity_field = intensity_valley # make sure that the values correspond to the coord\n        else:\n            # 1) faire un champ gravitationnel autour de la forme principale\n            self.gravity_field = rounded_inverted_distance_transform(self.main_shape, max_distance=self.max_distance, with_erosion=1)\n\n            # If there are near enough shapes, run the following\n            # 2) Dilate other shapes toward the main according to the gradient\n        other_shapes, max_field_feelings = self._expand_smalls_toward_main()\n\n\n        # plt.imshow(other_shapes)\n        # If there are shapes within gravity field range\n        if np.any(max_field_feelings &gt; 0):\n            self.expanded_shape = np.zeros(self.main_shape.shape, np.uint8)\n            self.expanded_shape[np.nonzero(self.main_shape + other_shapes)] = 1\n            if only_keep_connected_shapes:\n                # Make sure that only shapes connected with the main one remain on the final image\n                expanded_shape = keep_shape_connected_with_ref(self.expanded_shape, self.main_shape)\n                if expanded_shape is not None:\n                    self.expanded_shape = expanded_shape\n                if rank_connecting_pixels:\n                    # Rate the extension of small shapes according to the distance between the small and the main shapes\n                    self.distance_ranking_of_connecting_pixels()\n            #self.expanded_shape\n            # plt.imshow(self.expanded_shape)\n        else:\n            self.expanded_shape = self.main_shape\n    # Otherwise, end by putting the main shape as output\n    else:\n        self.expanded_shape = self.main_shape\n</code></pre>"},{"location":"api/cellects/image_analysis/progressively_add_distant_shapes/#cellects.image_analysis.progressively_add_distant_shapes.ProgressivelyAddDistantShapes.consider_shapes_sizes","title":"<code>consider_shapes_sizes(min_shape_size=None, max_shape_size=None)</code>","text":"<p>Filter shapes based on minimum and maximum size thresholds.</p> <p>This method adjusts <code>new_order</code> by excluding indices of shapes that are either  smaller than <code>min_shape_size</code> or larger than <code>max_shape_size</code>. The main shape index  (1) is preserved even if it meets the filtering criteria. When no constraints apply,  the expanded shape defaults to the main shape.</p> Parameters <p>min_shape_size : int, optional      Minimum allowed size for shapes (compared against 4th column of <code>self.stats</code>).  max_shape_size : int, optional      Maximum allowed size for shapes (compared against 4th column of <code>self.stats</code>).</p> Examples <p>new_potentials = np.array([[1, 1, 0], [0, 0, 0], [0, 1, 1]]) previous_shape = np.array([[0, 0, 0], [0, 0, 0], [0, 1, 1]]) max_distance = 2 pads = ProgressivelyAddDistantShapes(new_potentials, previous_shape, max_distance) pads.consider_shapes_sizes(min_shape_size=2, max_shape_size=10) pads.new_order  array([[2, 2, 0], [0, 0, 0], [0, 1, 1]], dtype=np.uint8)</p> Source code in <code>src/cellects/image_analysis/progressively_add_distant_shapes.py</code> <pre><code>def consider_shapes_sizes(self, min_shape_size: int=None, max_shape_size: int=None):\n    \"\"\"Filter shapes based on minimum and maximum size thresholds.\n\n    This method adjusts `new_order` by excluding indices of shapes that are either\n    smaller than `min_shape_size` or larger than `max_shape_size`. The main shape index\n    (1) is preserved even if it meets the filtering criteria. When no constraints apply,\n    the expanded shape defaults to the main shape.\n\n    Parameters\n    ----------\n    min_shape_size : int, optional\n        Minimum allowed size for shapes (compared against 4th column of `self.stats`).\n    max_shape_size : int, optional\n        Maximum allowed size for shapes (compared against 4th column of `self.stats`).\n\n    Examples\n    --------\n    &gt;&gt;&gt; new_potentials = np.array([[1, 1, 0], [0, 0, 0], [0, 1, 1]])\n    &gt;&gt;&gt; previous_shape = np.array([[0, 0, 0], [0, 0, 0], [0, 1, 1]])\n    &gt;&gt;&gt; max_distance = 2\n    &gt;&gt;&gt; pads = ProgressivelyAddDistantShapes(new_potentials, previous_shape, max_distance)\n    &gt;&gt;&gt; pads.consider_shapes_sizes(min_shape_size=2, max_shape_size=10)\n    &gt;&gt;&gt; pads.new_order\n    array([[2, 2, 0],\n   [0, 0, 0],\n   [0, 1, 1]], dtype=np.uint8)\n    \"\"\"\n    if self.max_distance != 0:\n        # Eliminate too small and too large shapes\n        if min_shape_size is not None or max_shape_size is not None:\n            if min_shape_size is not None:\n                small_shapes = self.stats[:, 4] &lt; min_shape_size\n                extreme_shapes = deepcopy(small_shapes)\n            if max_shape_size is not None:\n                large_shapes = self.stats[:, 4] &gt; max_shape_size\n                extreme_shapes = deepcopy(large_shapes)\n            if min_shape_size is not None and max_shape_size is not None:\n                extreme_shapes = np.nonzero(np.logical_or(small_shapes, large_shapes))[0]\n            is_main_in_it = np.isin(extreme_shapes, 1)\n            if np.any(is_main_in_it):\n                extreme_shapes = np.delete(extreme_shapes, is_main_in_it)\n            for extreme_shape in extreme_shapes:\n                self.new_order[self.new_order == extreme_shape] = 0\n    else:\n        self.expanded_shape = self.main_shape\n</code></pre>"},{"location":"api/cellects/image_analysis/progressively_add_distant_shapes/#cellects.image_analysis.progressively_add_distant_shapes.ProgressivelyAddDistantShapes.distance_ranking_of_connecting_pixels","title":"<code>distance_ranking_of_connecting_pixels()</code>","text":"<p>Calculate the distance ranking of connecting pixels.</p> <p>This function computes a ranked extension map based on the difference between <code>main_shape</code> and <code>expanded_shape</code>, modifies it using a gravity field, and then updates the <code>expanded_shape</code> with this ranked extension.</p> Source code in <code>src/cellects/image_analysis/progressively_add_distant_shapes.py</code> <pre><code>def distance_ranking_of_connecting_pixels(self):\n    \"\"\"\n    Calculate the distance ranking of connecting pixels.\n\n    This function computes a ranked extension map based on the difference between\n    `main_shape` and `expanded_shape`, modifies it using a gravity field, and then\n    updates the `expanded_shape` with this ranked extension.\n    \"\"\"\n    rated_extension = np.zeros(self.main_shape.shape, np.uint8)\n    rated_extension[(self.main_shape - self.expanded_shape) == 255] = 1\n    rated_extension = rated_extension * self.gravity_field\n    if np.any(rated_extension):\n        rated_extension[np.nonzero(rated_extension)] -= np.min(\n            rated_extension[np.nonzero(rated_extension)]) - 1\n    rated_extension *= self.expanded_shape\n    self.expanded_shape += rated_extension\n</code></pre>"},{"location":"api/cellects/image_analysis/progressively_add_distant_shapes/#cellects.image_analysis.progressively_add_distant_shapes.ProgressivelyAddDistantShapes.find_expansion_timings","title":"<code>find_expansion_timings()</code>","text":"<p>Find the expansion timings of a shape in binary video.</p> <p>This method calculates the time at which an expanded shape reaches the main shape, as well as the distance and time relationship during expansion.</p> <p>Returns:</p> Name Type Description <code>distance_against_time</code> <code>ndarray of float64</code> <p>Array representing the distance against time.</p> <code>time_start</code> <code>int</code> <p>The start time of expansion in frames.</p> <code>time_end</code> <code>int</code> <p>The end time of expansion in frames.</p> <p>Raises:</p> Type Description <code>AttributeError</code> <p>If 'binary_video', 'expanded_shape' or 'main_shape' are not defined.</p> Source code in <code>src/cellects/image_analysis/progressively_add_distant_shapes.py</code> <pre><code>def find_expansion_timings(self) -&gt; Tuple[NDArray[np.float64], int, int]:\n    \"\"\"\n    Find the expansion timings of a shape in binary video.\n\n    This method calculates the time at which an expanded shape reaches\n    the main shape, as well as the distance and time relationship during\n    expansion.\n\n    Returns\n    -------\n    distance_against_time : ndarray of float64\n        Array representing the distance against time.\n    time_start : int\n        The start time of expansion in frames.\n    time_end : int\n        The end time of expansion in frames.\n\n    Raises\n    ------\n    AttributeError\n        If 'binary_video', 'expanded_shape' or 'main_shape' are not defined.\n    \"\"\"\n    max_t = self.binary_video.shape[0] - 1\n    dilated_one = cv2.dilate(self.expanded_shape, cross_33)\n    # Find the time at which the nearest pixel of the expanded_shape si reached by the main shape\n    closest_pixels = np.zeros(self.main_shape.shape, dtype=np.uint8)\n    closest_pixels[self.expanded_shape == np.max(dilated_one)] = 1\n    expand_start = max_t\n    # Loop until there is no overlap between the dilated added shape and the original shape\n    # Stop one frame before in order to obtain the exact reaching moment.\n    while np.any(self.binary_video[expand_start - 1, :, :] * closest_pixels):\n        expand_start -= 1\n\n    # Find the relationship between distance and time\n    distance_against_time, time_start, time_end = get_radius_distance_against_time(\n        self.draft_seg[expand_start:(max_t + 1), :, :], dilated_one)\n    time_start += expand_start\n    time_end += expand_start\n    return distance_against_time, time_start, time_end\n</code></pre>"},{"location":"api/cellects/image_analysis/progressively_add_distant_shapes/#cellects.image_analysis.progressively_add_distant_shapes.ProgressivelyAddDistantShapes.modify_past_analysis","title":"<code>modify_past_analysis(binary_video, draft_seg)</code>","text":"<p>Modify past analysis based on binary video and draft segmentation.</p> <p>This method modifies the past analysis by updating <code>binary_video</code> with information from <code>draft_seg</code>, and then iteratively filling pixels based on expansion timings.</p> <p>Parameters:</p> Name Type Description Default <code>binary_video</code> <code>ndarray of uint8</code> <p>Input binary video to be modified.</p> required <code>draft_seg</code> <code>ndarray of uint8</code> <p>Draft segmentation used for expanding the shape.</p> required <p>Returns:</p> Type Description <code>ndarray of uint8</code> <p>Modified binary video after past analysis.</p> Source code in <code>src/cellects/image_analysis/progressively_add_distant_shapes.py</code> <pre><code>def modify_past_analysis(self, binary_video: NDArray[np.uint8], draft_seg: NDArray[np.uint8]) -&gt; NDArray[np.uint8]:\n    \"\"\"\n    Modify past analysis based on binary video and draft segmentation.\n\n    This method modifies the past analysis by updating `binary_video` with\n    information from `draft_seg`, and then iteratively filling pixels based on\n    expansion timings.\n\n    Parameters\n    ----------\n    binary_video : ndarray of uint8\n        Input binary video to be modified.\n    draft_seg : ndarray of uint8\n        Draft segmentation used for expanding the shape.\n\n    Returns\n    -------\n    ndarray of uint8\n        Modified binary video after past analysis.\n    \"\"\"\n    self.binary_video = binary_video\n    self.draft_seg = draft_seg\n    self.expanded_shape[self.expanded_shape == 1] = 0\n    # Find the time at which the shape became connected to the expanded shape\n    # (i.e. the time to start looking for a growth)\n    distance_against_time, time_start, time_end = self.find_expansion_timings()\n\n    # Use that vector to progressively fill pixels at the same speed as shape grows\n    for t in np.arange(len(distance_against_time)):\n        image_garbage = (self.expanded_shape &gt;= distance_against_time[t]).astype(np.uint8)\n        new_order, stats, centers = cc(image_garbage)\n        for comp_i in np.arange(1, stats.shape[0]):\n            past_image = deepcopy(self.binary_video[time_start + t, :, :])\n            with_new_comp = new_order == comp_i\n            past_image[with_new_comp] = 1\n            nb_comp, image_garbage = cv2.connectedComponents(past_image)\n            if nb_comp == 2:\n                self.binary_video[time_start + t, :, :][with_new_comp] = 1\n    #self.expanded_shape[self.expanded_shape &gt; 0] = 1\n    #self.binary_video[time_end:, :, :] += self.expanded_shape\n    for t in np.arange(time_end, self.binary_video.shape[0]):\n        self.binary_video[t, :, :][np.nonzero(self.expanded_shape)] = 1\n    last_image = self.binary_video[t, :, :] + self.binary_video[t - 1, :, :]\n    last_image[last_image &gt; 0] = 1\n    self.binary_video[-1, :, :] = last_image\n    return self.binary_video\n</code></pre>"},{"location":"api/cellects/image_analysis/shape_descriptors/","title":"<code>cellects.image_analysis.shape_descriptors</code>","text":""},{"location":"api/cellects/image_analysis/shape_descriptors/#cellects.image_analysis.shape_descriptors","title":"<code>cellects.image_analysis.shape_descriptors</code>","text":"<p>Module for computing shape descriptors from binary images.</p> <p>This module provides a framework for calculating various geometric and statistical descriptors of shapes in binary images through configurable dictionaries and a core class. Supported metrics include area, perimeter, axis lengths, orientation, and more. Descriptor computation is controlled via category dictionaries (e.g., <code>descriptors_categories</code>) and implemented as methods in the ShapeDescriptors class.</p> <p>Classes:</p> Name Description <code>ShapeDescriptors : Class to compute various descriptors for a binary image</code> Notes <p>Relies on OpenCV and NumPy for image processing operations. Shape descriptors: The following names, lists and computes all the variables describing a shape in a binary image. If you want to allow the software to compute another variable: 1) In the following dicts and list, you need to:         add the variable name and whether to compute it (True/False) by default 2) In the ShapeDescriptors class:         add a method to compute that variable 3) In the init method of the ShapeDescriptors class     attribute a None value to the variable that store it     add a if condition in the for loop to compute that variable when its name appear in the wanted_descriptors_list</p>"},{"location":"api/cellects/image_analysis/shape_descriptors/#cellects.image_analysis.shape_descriptors.ShapeDescriptors","title":"<code>ShapeDescriptors</code>","text":"<pre><code>This class takes :\n- a binary image of 0 and 1 drawing one shape\n- a list of descriptors to calculate from that image\n[\"area\", \"perimeter\", \"circularity\", \"rectangularity\", \"total_hole_area\", \"solidity\", \"convexity\",\n \"eccentricity\", \"euler_number\",\n\n\"standard_deviation_y\", \"standard_deviation_x\", \"skewness_y\", \"skewness_x\", \"kurtosis_y\", \"kurtosis_x\",\n\"major_axis_len\", \"minor_axis_len\", \"axes_orientation\",\n\n\"mo\", \"contours\", \"min_bounding_rectangle\", \"convex_hull\"]\n\nBe careful! mo, contours, min_bounding_rectangle, convex_hull,\nstandard_deviations, skewness and kurtosis are not atomics\n</code></pre> <p>https://www.researchgate.net/publication/27343879_Estimators_for_Orientation_and_Anisotropy_in_Digitized_Images</p> Source code in <code>src/cellects/image_analysis/shape_descriptors.py</code> <pre><code>class ShapeDescriptors:\n    \"\"\"\n        This class takes :\n        - a binary image of 0 and 1 drawing one shape\n        - a list of descriptors to calculate from that image\n        [\"area\", \"perimeter\", \"circularity\", \"rectangularity\", \"total_hole_area\", \"solidity\", \"convexity\",\n         \"eccentricity\", \"euler_number\",\n\n        \"standard_deviation_y\", \"standard_deviation_x\", \"skewness_y\", \"skewness_x\", \"kurtosis_y\", \"kurtosis_x\",\n        \"major_axis_len\", \"minor_axis_len\", \"axes_orientation\",\n\n        \"mo\", \"contours\", \"min_bounding_rectangle\", \"convex_hull\"]\n\n        Be careful! mo, contours, min_bounding_rectangle, convex_hull,\n        standard_deviations, skewness and kurtosis are not atomics\n    https://www.researchgate.net/publication/27343879_Estimators_for_Orientation_and_Anisotropy_in_Digitized_Images\n    \"\"\"\n\n    def __init__(self, binary_image, wanted_descriptors_list):\n        \"\"\"\n        Class to compute various descriptors for a binary image.\n\n        Parameters\n        ----------\n        binary_image : ndarray\n            Binary image used to compute the descriptors.\n        wanted_descriptors_list : list\n            List of strings with the names of the wanted descriptors.\n\n        Attributes\n        ----------\n        binary_image : ndarray\n            The binary image.\n        descriptors : dict\n            Dictionary containing the computed descriptors.\n        mo : float or None, optional\n            Moment of inertia (default is `None`).\n        area : int or None, optional\n            Area of the object (default is `None`).\n        contours : ndarray or None, optional\n            Contours of the object (default is `None`).\n        min_bounding_rectangle : tuple or None, optional\n            Minimum bounding rectangle of the object (default is `None`).\n        convex_hull : ndarray or None, optional\n            Convex hull of the object (default is `None`).\n        major_axis_len : float or None, optional\n            Major axis length of the object (default is `None`).\n        minor_axis_len : float or None, optional\n            Minor axis length of the object (default is `None`).\n        axes_orientation : float or None, optional\n            Orientation of the axes (default is `None`).\n        sx : float or None, optional\n            Standard deviation in x-axis (default is `None`).\n        kx : float or None, optional\n            Kurtosis in x-axis (default is `None`).\n        skx : float or None, optional\n            Skewness in x-axis (default is `None`).\n        perimeter : float or None, optional\n            Perimeter of the object (default is `None`).\n        convexity : float or None, optional\n            Convexity of the object (default is `None`).\n\n        Examples\n        --------\n        &gt;&gt;&gt; binary_image = np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]], dtype=np.uint8)\n        &gt;&gt;&gt; wanted_descriptors_list = [\"area\", \"perimeter\"]\n        &gt;&gt;&gt; SD = ShapeDescriptors(binary_image, wanted_descriptors_list)\n        &gt;&gt;&gt; SD.descriptors\n        {'area': np.uint64(9), 'perimeter': 8.0}\n        \"\"\"\n        # Give a None value to each parameters whose presence is assessed before calculation (less calculus for speed)\n        self.mo = None\n        self.area = None\n        self.contours = None\n        self.min_bounding_rectangle = None\n        self.convex_hull = None\n        self.major_axis_len = None\n        self.minor_axis_len = None\n        self.axes_orientation = None\n        self.sx = None\n        self.kx = None\n        self.skx = None\n        self.perimeter = None\n        self.convexity = None\n\n        self.binary_image = binary_image\n        if self.binary_image.dtype == 'bool':\n            self.binary_image = self.binary_image.astype(np.uint8)\n\n        self.descriptors = {i: np.empty(0, dtype=np.float64) for i in wanted_descriptors_list}\n        self.get_area()\n\n        for name in self.descriptors.keys():\n            if name == \"mo\":\n                self.get_mo()\n                self.descriptors[name] = self.mo\n            elif name == \"area\":\n                self.descriptors[name] = self.area\n            elif name == \"contours\":\n                self.get_contours()\n                self.descriptors[name] = self.contours\n            elif name == \"min_bounding_rectangle\":\n                self.get_min_bounding_rectangle()\n                self.descriptors[name] = self.min_bounding_rectangle\n            elif name == \"major_axis_len\":\n                self.get_major_axis_len()\n                self.descriptors[name] = self.major_axis_len\n            elif name == \"minor_axis_len\":\n                self.get_minor_axis_len()\n                self.descriptors[name] = self.minor_axis_len\n            elif name == \"axes_orientation\":\n                self.get_inertia_axes()\n                self.descriptors[name] = self.axes_orientation\n            elif name == \"standard_deviation_y\":\n                self.get_standard_deviations()\n                self.descriptors[name] = self.sy\n            elif name == \"standard_deviation_x\":\n                self.get_standard_deviations()\n                self.descriptors[name] = self.sx\n            elif name == \"skewness_y\":\n                self.get_skewness()\n                self.descriptors[name] = self.sky\n            elif name == \"skewness_x\":\n                self.get_skewness()\n                self.descriptors[name] = self.skx\n            elif name == \"kurtosis_y\":\n                self.get_kurtosis()\n                self.descriptors[name] = self.ky\n            elif name == \"kurtosis_x\":\n                self.get_kurtosis()\n                self.descriptors[name] = self.kx\n            elif name == \"convex_hull\":\n                self.get_convex_hull()\n                self.descriptors[name] = self.convex_hull\n            elif name == \"perimeter\":\n                self.get_perimeter()\n                self.descriptors[name] = self.perimeter\n            elif name == \"circularity\":\n                self.get_circularity()\n                self.descriptors[name] = self.circularity\n            elif name == \"rectangularity\":\n                self.get_rectangularity()\n                self.descriptors[name] = self.rectangularity\n            elif name == \"total_hole_area\":\n                self.get_total_hole_area()\n                self.descriptors[name] = self.total_hole_area\n            elif name == \"solidity\":\n                self.get_solidity()\n                self.descriptors[name] = self.solidity\n            elif name == \"convexity\":\n                self.get_convexity()\n                self.descriptors[name] = self.convexity\n            elif name == \"eccentricity\":\n                self.get_eccentricity()\n                self.descriptors[name] = self.eccentricity\n            elif name == \"euler_number\":\n                self.get_euler_number()\n                self.descriptors[name] = self.euler_number\n\n    \"\"\"\n        The following methods can be called to compute parameters for descriptors requiring it\n    \"\"\"\n\n    def get_mo(self):\n        \"\"\"\n        Get moments of a binary image.\n\n        Calculate the image moments for a given binary image using OpenCV's\n        `cv2.moments` function and then translate these moments into a formatted\n        dictionary.\n\n        Notes\n        -----\n        This function assumes the binary image has already been processed and is in a\n        suitable format for moment calculation.\n\n        Returns\n        -------\n        None\n\n        Examples\n        --------\n       &gt;&gt;&gt; SD = ShapeDescriptors(np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]], dtype=np.uint8), [\"mo\"])\n        &gt;&gt;&gt; print(SD.mo[\"m00\"])\n        9.0\n        \"\"\"\n        self.mo = translate_dict(cv2.moments(self.binary_image))\n\n    def get_area(self):\n        \"\"\"\n        Calculate the area of a binary image by summing its pixel values.\n\n        This function computes the area covered by white pixels (value 1) in a binary image,\n        which is equivalent to counting the number of 'on' pixels.\n\n        Notes\n        -----\n        Sums values in `self.binary_image` and stores the result in `self.area`.\n\n        Returns\n        -------\n        None\n\n        Examples\n        --------\n        &gt;&gt;&gt; SD = ShapeDescriptors(np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]], dtype=np.uint8), [\"area\"])\n        &gt;&gt;&gt; print(SD.area)\n        9.0\n        \"\"\"\n        self.area = self.binary_image.sum()\n\n    def get_contours(self):\n        \"\"\"\n        Find and process the largest contour in a binary image.\n\n        Retrieves contours from a binary image, calculates the Euler number,\n        and identifies the largest contour based on its length.\n\n        Notes\n        -----\n        This function modifies the internal state of the `self` object to store\n        the largest contour and Euler number.\n\n        Returns\n        -------\n        None\n\n        Examples\n        --------\n        &gt;&gt;&gt; SD = ShapeDescriptors(np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]], dtype=np.uint8), [\"euler_number\"])\n        &gt;&gt;&gt; print(len(SD.contours))\n        8\n        \"\"\"\n        if self.area == 0:\n            self.euler_number = 0.\n            self.contours = np.array([], np.uint8)\n        else:\n            contours, hierarchy = cv2.findContours(self.binary_image, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n            nb, shapes = cv2.connectedComponents(self.binary_image, ltype=cv2.CV_16U)\n            self.euler_number = (nb - 1) - len(contours)\n            self.contours = contours[0]\n            if len(contours) &gt; 1:\n                all_lengths = np.zeros(len(contours))\n                for i, contour in enumerate(contours):\n                    all_lengths[i] = len(contour)\n                self.contours = contours[np.argmax(all_lengths)]\n\n    def get_min_bounding_rectangle(self):\n        \"\"\"\n        Retrieve the minimum bounding rectangle from the contours of an image.\n\n        This method calculates the smallest area rectangle that can enclose\n        the object outlines present in the image, which is useful for\n        object detection and analysis tasks.\n\n        Notes\n        -----\n        - The bounding rectangle is calculated only if contours are available.\n          If not, they will be retrieved first before calculating the rectangle.\n\n        Raises\n        ------\n        RuntimeError\n            If the contours are not available and cannot be retrieved,\n            indicating a problem with the image or preprocessing steps.\n\n        Returns\n        -------\n        None\n\n        Examples\n        --------\n        &gt;&gt;&gt; SD = ShapeDescriptors(np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]], dtype=np.uint8), [\"rectangularity\"])\n        &gt;&gt;&gt; print(len(SD.min_bounding_rectangle))\n        3\n        \"\"\"\n        if self.area == 0:\n            self.min_bounding_rectangle = np.array([], np.uint8)\n        else:\n            if self.contours is None:\n                self.get_contours()\n            if len(self.contours) == 0:\n                self.min_bounding_rectangle = np.array([], np.uint8)\n            else:\n                self.min_bounding_rectangle = cv2.minAreaRect(self.contours)  # ((cx, cy), (width, height), angle)\n\n    def get_inertia_axes(self):\n        \"\"\"\n        Calculate and set the moments of inertia properties of an object.\n\n        This function computes the centroid, major axis length,\n        minor axis length, and axes orientation for an object. It\n        first ensures that the moments of inertia (`mo`) attribute is available,\n        computing them if necessary, before using the `get_inertia_axes` function.\n\n        Returns\n        -------\n        None\n\n            This method sets the following attributes:\n            - `cx` : float\n                The x-coordinate of the centroid.\n            - `cy` : float\n                The y-coordinate of the centroid.\n            - `major_axis_len` : float\n                The length of the major axis.\n            - `minor_axis_len` : float\n                The length of the minor axis.\n            - `axes_orientation` : float\n                The orientation angle of the axes.\n\n        Raises\n        ------\n        ValueError\n            If there is an issue with the moments of inertia computation.\n\n        Notes\n        -----\n        This function modifies in-place the object's attributes related to its geometry.\n\n        Examples\n        --------\n        &gt;&gt;&gt; SD = ShapeDescriptors(np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]], dtype=np.uint8), [\"major_axis_len\"])\n        &gt;&gt;&gt; print(SD.axes_orientation)\n        0.0\n        \"\"\"\n        if self.mo is None:\n            self.get_mo()\n        if self.area == 0:\n            self.cx, self.cy, self.major_axis_len, self.minor_axis_len, self.axes_orientation = 0, 0, 0, 0, 0\n        else:\n            self.cx, self.cy, self.major_axis_len, self.minor_axis_len, self.axes_orientation = get_inertia_axes(self.mo)\n\n    def get_standard_deviations(self):\n        \"\"\"\n        Calculate and store standard deviations along x and y (sx, sy).\n\n        Notes\n        -----\n        Requires centroid and moments; values are stored in `self.sx` and `self.sy`.\n\n        Returns\n        -------\n        None\n\n        Examples\n        --------\n       &gt;&gt;&gt; SD = ShapeDescriptors(np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]], dtype=np.uint8), [\"standard_deviation_x\", \"standard_deviation_y\"])\n        &gt;&gt;&gt; print(SD.sx, SD.sy)\n        0.816496580927726 0.816496580927726\n        \"\"\"\n        if self.sx is None:\n            if self.axes_orientation is None:\n                self.get_inertia_axes()\n            self.sx, self.sy = get_standard_deviations(self.mo, self.binary_image, self.cx, self.cy)\n\n    def get_skewness(self):\n        \"\"\"\n        Calculate and store skewness along x and y (skx, sky).\n\n        This function computes the skewness about the x-axis and y-axis of\n        an image. Skewness is a measure of the asymmetry of the probability\n        distribution of values in an image.\n\n        Notes\n        -----\n        Requires standard deviations; values are stored in `self.skx` and `self.sky`.\n\n        Returns\n        -------\n        None\n\n        Examples\n        --------\n        &gt;&gt;&gt; SD = ShapeDescriptors(np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]], dtype=np.uint8), [\"skewness_x\", \"skewness_y\"])\n        &gt;&gt;&gt; print(SD.skx, SD.sky)\n        0.0 0.0\n        \"\"\"\n        if self.skx is None:\n            if self.sx is None:\n                self.get_standard_deviations()\n\n            self.skx, self.sky = get_skewness(self.mo, self.binary_image, self.cx, self.cy, self.sx, self.sy)\n\n    def get_kurtosis(self):\n        \"\"\"\n        Calculates the kurtosis of the image moments.\n\n        Kurtosis is a statistical measure that describes the shape of\n        a distribution's tails in relation to its overall shape. It is\n        used here in the context of image moments analysis.\n\n        Notes\n        -----\n        This function first checks if the kurtosis values have already been calculated.\n        If not, it calculates them using the `get_kurtosis` function.\n\n        Examples\n        --------\n        &gt;&gt;&gt; SD = ShapeDescriptors(np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]], dtype=np.uint8), [\"kurtosis_x\", \"kurtosis_y\"])\n        &gt;&gt;&gt; print(SD.kx, SD.ky)\n        1.5 1.5\n        \"\"\"\n        if self.kx is None:\n            if self.sx is None:\n                self.get_standard_deviations()\n\n            self.kx, self.ky = get_kurtosis(self.mo, self.binary_image, self.cx, self.cy, self.sx, self.sy)\n\n    def get_convex_hull(self):\n        \"\"\"\n        Compute and store the convex hull of the object's contour.\n\n        Notes\n        -----\n        Stores the result in `self.convex_hull`. Computes contours if needed.\n\n        Returns\n        -------\n        None\n\n        Examples\n        --------\n        &gt;&gt;&gt; SD = ShapeDescriptors(np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]], dtype=np.uint8), [\"solidity\"])\n        &gt;&gt;&gt; print(len(SD.convex_hull))\n        4\n        \"\"\"\n        if self.area == 0:\n            self.convex_hull = np.array([], np.uint8)\n        else:\n            if self.contours is None:\n                self.get_contours()\n            self.convex_hull = cv2.convexHull(self.contours)\n\n    def get_perimeter(self):\n        \"\"\"\n        Compute and store the contour perimeter length.\n\n        Notes\n        -----\n        Computes contours if needed and stores the length in `self.perimeter`.\n\n        Returns\n        -------\n        None\n\n        Examples\n        --------\n        &gt;&gt;&gt; SD = ShapeDescriptors(np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]], dtype=np.uint8), [\"perimeter\"])\n        &gt;&gt;&gt; print(SD.perimeter)\n        8.0\n        \"\"\"\n        if self.area == 0:\n            self.perimeter = 0.\n        else:\n            if self.contours is None:\n                self.get_contours()\n            if len(self.contours) == 0:\n                self.perimeter = 0.\n            else:\n                self.perimeter = cv2.arcLength(self.contours, True)\n\n    def get_circularity(self):\n        \"\"\"\n        Compute and store circularity: 4\u03c0A / P\u00b2.\n\n        Notes\n        -----\n        Uses `self.area` and `self.perimeter`; stores result in `self.circularity`.\n\n        Returns\n        -------\n        None\n\n        Examples\n        --------\n         &gt;&gt;&gt; SD = ShapeDescriptors(np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]], dtype=np.uint8), [\"circularity\"])\n        &gt;&gt;&gt; print(SD.circularity)\n        1.7671458676442586\n        \"\"\"\n        if self.area == 0:\n            self.circularity = 0.\n        else:\n            if self.perimeter is None:\n                self.get_perimeter()\n            if self.perimeter == 0:\n                self.circularity = 0.\n            else:\n                self.circularity = (4 * np.pi * self.binary_image.sum()) / np.square(self.perimeter)\n\n    def get_rectangularity(self):\n        \"\"\"\n        Compute and store rectangularity: area / bounding-rectangle-area.\n\n        Notes\n        -----\n        Uses `self.binary_image` and `self.min_bounding_rectangle`. Computes the MBR if needed.\n\n        Returns\n        -------\n        None\n\n        Examples\n        --------\n        &gt;&gt;&gt; SD = ShapeDescriptors(np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]], dtype=np.uint8), [\"rectangularity\"])\n        &gt;&gt;&gt; print(SD.rectangularity)\n        2.25\n        \"\"\"\n        if self.area == 0:\n            self.rectangularity = 0.\n        else:\n            if self.min_bounding_rectangle is None:\n                self.get_min_bounding_rectangle()\n            bounding_rectangle_area = self.min_bounding_rectangle[1][0] * self.min_bounding_rectangle[1][1]\n            if bounding_rectangle_area == 0:\n                self.rectangularity = 0.\n            else:\n                self.rectangularity = self.binary_image.sum() / bounding_rectangle_area\n\n    def get_total_hole_area(self):\n        \"\"\"\n        Calculate the total area of holes in a binary image.\n\n        This function uses connected component labeling to detect and\n        measure the area of holes in a binary image.\n\n        Returns\n        -------\n        float\n            The total area of all detected holes in the binary image.\n\n        Notes\n        -----\n        This function assumes that the binary image has been pre-processed\n        and that holes are represented as connected components of zero\n        pixels within the foreground\n\n        Examples\n        --------\n        &gt;&gt;&gt; SD = ShapeDescriptors(np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]], dtype=np.uint8), [\"total_hole_area\"])\n        &gt;&gt;&gt; print(SD.total_hole_area)\n        0\n        \"\"\"\n        nb, new_order = cv2.connectedComponents(1 - self.binary_image)\n        if nb &gt; 2:\n            self.total_hole_area = (new_order &gt; 1).sum()\n        else:\n            self.total_hole_area = 0.\n\n    def get_solidity(self):\n        \"\"\"\n        Compute and store solidity: contour area / convex hull area.\n\n        Extended Summary\n        ----------------\n        The solidity is a dimensionless measure that compares the area of a shape to\n        its convex hull. A solidity of 1 means the contour is fully convex, while a\n        value less than 1 indicates concavities.\n\n        Notes\n        -----\n        If the convex hull area is 0 or absent, solidity is set to 0.\n\n        Returns\n        -------\n        None\n\n        Examples\n        --------\n        &gt;&gt;&gt; SD = ShapeDescriptors(np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]], dtype=np.uint8), [\"solidity\"])\n        &gt;&gt;&gt; print(SD.solidity)\n        1.0\n        \"\"\"\n        if self.area == 0:\n            self.solidity = 0.\n        else:\n            if self.convex_hull is None:\n                self.get_convex_hull()\n            if len(self.convex_hull) == 0:\n                self.solidity = 0.\n            else:\n                hull_area = cv2.contourArea(self.convex_hull)\n                if hull_area == 0:\n                    self.solidity = 0.\n                else:\n                    self.solidity = cv2.contourArea(self.contours) / hull_area\n\n    def get_convexity(self):\n        \"\"\"\n        Compute and store convexity: convex hull perimeter / contour perimeter.\n\n        Notes\n        -----\n        Requires `self.perimeter` and `self.convex_hull`.\n\n        Returns\n        -------\n        None\n\n        Examples\n        --------\n        &gt;&gt;&gt; SD = ShapeDescriptors(np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]], dtype=np.uint8), [\"convexity\"])\n        &gt;&gt;&gt; print(SD.convexity)\n        1.0\n        \"\"\"\n        if self.perimeter is None:\n            self.get_perimeter()\n        if self.convex_hull is None:\n            self.get_convex_hull()\n        if self.perimeter == 0 or len(self.convex_hull) == 0:\n            self.convexity = 0.\n        else:\n            self.convexity = cv2.arcLength(self.convex_hull, True) / self.perimeter\n\n    def get_eccentricity(self):\n        \"\"\"\n        Compute and store eccentricity from major and minor axis lengths.\n\n        Notes\n        -----\n        Calls `get_inertia_axes()` if needed and stores result in `self.eccentricity`.\n\n        Returns\n        -------\n        None\n\n        Examples\n        --------\n        &gt;&gt;&gt; SD = ShapeDescriptors(np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]], dtype=np.uint8), [\"eccentricity\"])\n        &gt;&gt;&gt; print(SD.eccentricity)\n        0.0\n        \"\"\"\n        self.get_inertia_axes()\n        if self.major_axis_len == 0:\n            self.eccentricity = 0.\n        else:\n            self.eccentricity = np.sqrt(1 - np.square(self.minor_axis_len / self.major_axis_len))\n\n    def get_euler_number(self):\n        \"\"\"\n        Ensure contours are computed; stores Euler number in `self.euler_number` via `get_contours()`.\n\n        Returns\n        -------\n        None\n\n        Notes\n        -----\n        Euler number is computed in `get_contours()` as `(components - 1) - len(contours)`.\n        \"\"\"\n        if self.contours is None:\n            self.get_contours()\n\n    def get_major_axis_len(self):\n        \"\"\"\n        Ensure the major axis length is computed and stored in `self.major_axis_len`.\n\n        Returns\n        -------\n        None\n\n        Notes\n        -----\n        Triggers `get_inertia_axes()` if needed.\n\n        Examples\n        --------\n        &gt;&gt;&gt; SD = ShapeDescriptors(np.array([[0, 1, 0], [0, 1, 0], [0, 1, 0]], dtype=np.uint8), [\"major_axis_len\"])\n        &gt;&gt;&gt; print(SD.major_axis_len)\n        2.8284271247461907\n        \"\"\"\n        if self.major_axis_len is None:\n            self.get_inertia_axes()\n\n    def get_minor_axis_len(self):\n        \"\"\"\n        Ensure the minor axis length is computed and stored in `self.minor_axis_len`.\n\n        Returns\n        -------\n        None\n\n        Notes\n        -----\n        Triggers `get_inertia_axes()` if needed.\n\n        Examples\n        --------\n        &gt;&gt;&gt; SD = ShapeDescriptors(np.array([[0, 1, 0], [0, 1, 0], [0, 1, 0]], dtype=np.uint8), [\"minor_axis_len\"])\n        &gt;&gt;&gt; print(SD.minor_axis_len)\n        0.0\n        \"\"\"\n        if self.minor_axis_len is None:\n            self.get_inertia_axes()\n\n    def get_axes_orientation(self):\n        \"\"\"\n        Ensure the axes orientation angle is computed and stored in `self.axes_orientation`.\n\n        Returns\n        -------\n        None\n\n        Notes\n        -----\n        Calls `get_inertia_axes()` if orientation is not yet computed.\n\n        Examples\n        --------\n        &gt;&gt;&gt; SD = ShapeDescriptors(np.array([[0, 1, 0], [0, 1, 0], [0, 1, 0]], dtype=np.uint8), [\"axes_orientation\"])\n        &gt;&gt;&gt; print(SD.axes_orientation)\n        1.5707963267948966\n        \"\"\"\n        if self.axes_orientation is None:\n            self.get_inertia_axes()\n</code></pre>"},{"location":"api/cellects/image_analysis/shape_descriptors/#cellects.image_analysis.shape_descriptors.ShapeDescriptors.__init__","title":"<code>__init__(binary_image, wanted_descriptors_list)</code>","text":"<p>Class to compute various descriptors for a binary image.</p> <p>Parameters:</p> Name Type Description Default <code>binary_image</code> <code>ndarray</code> <p>Binary image used to compute the descriptors.</p> required <code>wanted_descriptors_list</code> <code>list</code> <p>List of strings with the names of the wanted descriptors.</p> required <p>Attributes:</p> Name Type Description <code>binary_image</code> <code>ndarray</code> <p>The binary image.</p> <code>descriptors</code> <code>dict</code> <p>Dictionary containing the computed descriptors.</p> <code>mo</code> <code>(float or None, optional)</code> <p>Moment of inertia (default is <code>None</code>).</p> <code>area</code> <code>(int or None, optional)</code> <p>Area of the object (default is <code>None</code>).</p> <code>contours</code> <code>(ndarray or None, optional)</code> <p>Contours of the object (default is <code>None</code>).</p> <code>min_bounding_rectangle</code> <code>(tuple or None, optional)</code> <p>Minimum bounding rectangle of the object (default is <code>None</code>).</p> <code>convex_hull</code> <code>(ndarray or None, optional)</code> <p>Convex hull of the object (default is <code>None</code>).</p> <code>major_axis_len</code> <code>(float or None, optional)</code> <p>Major axis length of the object (default is <code>None</code>).</p> <code>minor_axis_len</code> <code>(float or None, optional)</code> <p>Minor axis length of the object (default is <code>None</code>).</p> <code>axes_orientation</code> <code>(float or None, optional)</code> <p>Orientation of the axes (default is <code>None</code>).</p> <code>sx</code> <code>(float or None, optional)</code> <p>Standard deviation in x-axis (default is <code>None</code>).</p> <code>kx</code> <code>(float or None, optional)</code> <p>Kurtosis in x-axis (default is <code>None</code>).</p> <code>skx</code> <code>(float or None, optional)</code> <p>Skewness in x-axis (default is <code>None</code>).</p> <code>perimeter</code> <code>(float or None, optional)</code> <p>Perimeter of the object (default is <code>None</code>).</p> <code>convexity</code> <code>(float or None, optional)</code> <p>Convexity of the object (default is <code>None</code>).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; binary_image = np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]], dtype=np.uint8)\n&gt;&gt;&gt; wanted_descriptors_list = [\"area\", \"perimeter\"]\n&gt;&gt;&gt; SD = ShapeDescriptors(binary_image, wanted_descriptors_list)\n&gt;&gt;&gt; SD.descriptors\n{'area': np.uint64(9), 'perimeter': 8.0}\n</code></pre> Source code in <code>src/cellects/image_analysis/shape_descriptors.py</code> <pre><code>def __init__(self, binary_image, wanted_descriptors_list):\n    \"\"\"\n    Class to compute various descriptors for a binary image.\n\n    Parameters\n    ----------\n    binary_image : ndarray\n        Binary image used to compute the descriptors.\n    wanted_descriptors_list : list\n        List of strings with the names of the wanted descriptors.\n\n    Attributes\n    ----------\n    binary_image : ndarray\n        The binary image.\n    descriptors : dict\n        Dictionary containing the computed descriptors.\n    mo : float or None, optional\n        Moment of inertia (default is `None`).\n    area : int or None, optional\n        Area of the object (default is `None`).\n    contours : ndarray or None, optional\n        Contours of the object (default is `None`).\n    min_bounding_rectangle : tuple or None, optional\n        Minimum bounding rectangle of the object (default is `None`).\n    convex_hull : ndarray or None, optional\n        Convex hull of the object (default is `None`).\n    major_axis_len : float or None, optional\n        Major axis length of the object (default is `None`).\n    minor_axis_len : float or None, optional\n        Minor axis length of the object (default is `None`).\n    axes_orientation : float or None, optional\n        Orientation of the axes (default is `None`).\n    sx : float or None, optional\n        Standard deviation in x-axis (default is `None`).\n    kx : float or None, optional\n        Kurtosis in x-axis (default is `None`).\n    skx : float or None, optional\n        Skewness in x-axis (default is `None`).\n    perimeter : float or None, optional\n        Perimeter of the object (default is `None`).\n    convexity : float or None, optional\n        Convexity of the object (default is `None`).\n\n    Examples\n    --------\n    &gt;&gt;&gt; binary_image = np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]], dtype=np.uint8)\n    &gt;&gt;&gt; wanted_descriptors_list = [\"area\", \"perimeter\"]\n    &gt;&gt;&gt; SD = ShapeDescriptors(binary_image, wanted_descriptors_list)\n    &gt;&gt;&gt; SD.descriptors\n    {'area': np.uint64(9), 'perimeter': 8.0}\n    \"\"\"\n    # Give a None value to each parameters whose presence is assessed before calculation (less calculus for speed)\n    self.mo = None\n    self.area = None\n    self.contours = None\n    self.min_bounding_rectangle = None\n    self.convex_hull = None\n    self.major_axis_len = None\n    self.minor_axis_len = None\n    self.axes_orientation = None\n    self.sx = None\n    self.kx = None\n    self.skx = None\n    self.perimeter = None\n    self.convexity = None\n\n    self.binary_image = binary_image\n    if self.binary_image.dtype == 'bool':\n        self.binary_image = self.binary_image.astype(np.uint8)\n\n    self.descriptors = {i: np.empty(0, dtype=np.float64) for i in wanted_descriptors_list}\n    self.get_area()\n\n    for name in self.descriptors.keys():\n        if name == \"mo\":\n            self.get_mo()\n            self.descriptors[name] = self.mo\n        elif name == \"area\":\n            self.descriptors[name] = self.area\n        elif name == \"contours\":\n            self.get_contours()\n            self.descriptors[name] = self.contours\n        elif name == \"min_bounding_rectangle\":\n            self.get_min_bounding_rectangle()\n            self.descriptors[name] = self.min_bounding_rectangle\n        elif name == \"major_axis_len\":\n            self.get_major_axis_len()\n            self.descriptors[name] = self.major_axis_len\n        elif name == \"minor_axis_len\":\n            self.get_minor_axis_len()\n            self.descriptors[name] = self.minor_axis_len\n        elif name == \"axes_orientation\":\n            self.get_inertia_axes()\n            self.descriptors[name] = self.axes_orientation\n        elif name == \"standard_deviation_y\":\n            self.get_standard_deviations()\n            self.descriptors[name] = self.sy\n        elif name == \"standard_deviation_x\":\n            self.get_standard_deviations()\n            self.descriptors[name] = self.sx\n        elif name == \"skewness_y\":\n            self.get_skewness()\n            self.descriptors[name] = self.sky\n        elif name == \"skewness_x\":\n            self.get_skewness()\n            self.descriptors[name] = self.skx\n        elif name == \"kurtosis_y\":\n            self.get_kurtosis()\n            self.descriptors[name] = self.ky\n        elif name == \"kurtosis_x\":\n            self.get_kurtosis()\n            self.descriptors[name] = self.kx\n        elif name == \"convex_hull\":\n            self.get_convex_hull()\n            self.descriptors[name] = self.convex_hull\n        elif name == \"perimeter\":\n            self.get_perimeter()\n            self.descriptors[name] = self.perimeter\n        elif name == \"circularity\":\n            self.get_circularity()\n            self.descriptors[name] = self.circularity\n        elif name == \"rectangularity\":\n            self.get_rectangularity()\n            self.descriptors[name] = self.rectangularity\n        elif name == \"total_hole_area\":\n            self.get_total_hole_area()\n            self.descriptors[name] = self.total_hole_area\n        elif name == \"solidity\":\n            self.get_solidity()\n            self.descriptors[name] = self.solidity\n        elif name == \"convexity\":\n            self.get_convexity()\n            self.descriptors[name] = self.convexity\n        elif name == \"eccentricity\":\n            self.get_eccentricity()\n            self.descriptors[name] = self.eccentricity\n        elif name == \"euler_number\":\n            self.get_euler_number()\n            self.descriptors[name] = self.euler_number\n</code></pre>"},{"location":"api/cellects/image_analysis/shape_descriptors/#cellects.image_analysis.shape_descriptors.ShapeDescriptors.get_area","title":"<code>get_area()</code>","text":"<p>Calculate the area of a binary image by summing its pixel values.</p> <p>This function computes the area covered by white pixels (value 1) in a binary image, which is equivalent to counting the number of 'on' pixels.</p> Notes <p>Sums values in <code>self.binary_image</code> and stores the result in <code>self.area</code>.</p> <p>Returns:</p> Type Description <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; SD = ShapeDescriptors(np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]], dtype=np.uint8), [\"area\"])\n&gt;&gt;&gt; print(SD.area)\n9.0\n</code></pre> Source code in <code>src/cellects/image_analysis/shape_descriptors.py</code> <pre><code>def get_area(self):\n    \"\"\"\n    Calculate the area of a binary image by summing its pixel values.\n\n    This function computes the area covered by white pixels (value 1) in a binary image,\n    which is equivalent to counting the number of 'on' pixels.\n\n    Notes\n    -----\n    Sums values in `self.binary_image` and stores the result in `self.area`.\n\n    Returns\n    -------\n    None\n\n    Examples\n    --------\n    &gt;&gt;&gt; SD = ShapeDescriptors(np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]], dtype=np.uint8), [\"area\"])\n    &gt;&gt;&gt; print(SD.area)\n    9.0\n    \"\"\"\n    self.area = self.binary_image.sum()\n</code></pre>"},{"location":"api/cellects/image_analysis/shape_descriptors/#cellects.image_analysis.shape_descriptors.ShapeDescriptors.get_axes_orientation","title":"<code>get_axes_orientation()</code>","text":"<p>Ensure the axes orientation angle is computed and stored in <code>self.axes_orientation</code>.</p> <p>Returns:</p> Type Description <code>None</code> Notes <p>Calls <code>get_inertia_axes()</code> if orientation is not yet computed.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; SD = ShapeDescriptors(np.array([[0, 1, 0], [0, 1, 0], [0, 1, 0]], dtype=np.uint8), [\"axes_orientation\"])\n&gt;&gt;&gt; print(SD.axes_orientation)\n1.5707963267948966\n</code></pre> Source code in <code>src/cellects/image_analysis/shape_descriptors.py</code> <pre><code>def get_axes_orientation(self):\n    \"\"\"\n    Ensure the axes orientation angle is computed and stored in `self.axes_orientation`.\n\n    Returns\n    -------\n    None\n\n    Notes\n    -----\n    Calls `get_inertia_axes()` if orientation is not yet computed.\n\n    Examples\n    --------\n    &gt;&gt;&gt; SD = ShapeDescriptors(np.array([[0, 1, 0], [0, 1, 0], [0, 1, 0]], dtype=np.uint8), [\"axes_orientation\"])\n    &gt;&gt;&gt; print(SD.axes_orientation)\n    1.5707963267948966\n    \"\"\"\n    if self.axes_orientation is None:\n        self.get_inertia_axes()\n</code></pre>"},{"location":"api/cellects/image_analysis/shape_descriptors/#cellects.image_analysis.shape_descriptors.ShapeDescriptors.get_circularity","title":"<code>get_circularity()</code>","text":"<p>Compute and store circularity: 4\u03c0A / P\u00b2.</p> Notes <p>Uses <code>self.area</code> and <code>self.perimeter</code>; stores result in <code>self.circularity</code>.</p> <p>Returns:</p> Type Description <code>None</code> <p>Examples:</p> <p>SD = ShapeDescriptors(np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]], dtype=np.uint8), [\"circularity\"])</p> <pre><code>&gt;&gt;&gt; print(SD.circularity)\n1.7671458676442586\n</code></pre> Source code in <code>src/cellects/image_analysis/shape_descriptors.py</code> <pre><code>def get_circularity(self):\n    \"\"\"\n    Compute and store circularity: 4\u03c0A / P\u00b2.\n\n    Notes\n    -----\n    Uses `self.area` and `self.perimeter`; stores result in `self.circularity`.\n\n    Returns\n    -------\n    None\n\n    Examples\n    --------\n     &gt;&gt;&gt; SD = ShapeDescriptors(np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]], dtype=np.uint8), [\"circularity\"])\n    &gt;&gt;&gt; print(SD.circularity)\n    1.7671458676442586\n    \"\"\"\n    if self.area == 0:\n        self.circularity = 0.\n    else:\n        if self.perimeter is None:\n            self.get_perimeter()\n        if self.perimeter == 0:\n            self.circularity = 0.\n        else:\n            self.circularity = (4 * np.pi * self.binary_image.sum()) / np.square(self.perimeter)\n</code></pre>"},{"location":"api/cellects/image_analysis/shape_descriptors/#cellects.image_analysis.shape_descriptors.ShapeDescriptors.get_contours","title":"<code>get_contours()</code>","text":"<p>Find and process the largest contour in a binary image.</p> <p>Retrieves contours from a binary image, calculates the Euler number, and identifies the largest contour based on its length.</p> Notes <p>This function modifies the internal state of the <code>self</code> object to store the largest contour and Euler number.</p> <p>Returns:</p> Type Description <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; SD = ShapeDescriptors(np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]], dtype=np.uint8), [\"euler_number\"])\n&gt;&gt;&gt; print(len(SD.contours))\n8\n</code></pre> Source code in <code>src/cellects/image_analysis/shape_descriptors.py</code> <pre><code>def get_contours(self):\n    \"\"\"\n    Find and process the largest contour in a binary image.\n\n    Retrieves contours from a binary image, calculates the Euler number,\n    and identifies the largest contour based on its length.\n\n    Notes\n    -----\n    This function modifies the internal state of the `self` object to store\n    the largest contour and Euler number.\n\n    Returns\n    -------\n    None\n\n    Examples\n    --------\n    &gt;&gt;&gt; SD = ShapeDescriptors(np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]], dtype=np.uint8), [\"euler_number\"])\n    &gt;&gt;&gt; print(len(SD.contours))\n    8\n    \"\"\"\n    if self.area == 0:\n        self.euler_number = 0.\n        self.contours = np.array([], np.uint8)\n    else:\n        contours, hierarchy = cv2.findContours(self.binary_image, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n        nb, shapes = cv2.connectedComponents(self.binary_image, ltype=cv2.CV_16U)\n        self.euler_number = (nb - 1) - len(contours)\n        self.contours = contours[0]\n        if len(contours) &gt; 1:\n            all_lengths = np.zeros(len(contours))\n            for i, contour in enumerate(contours):\n                all_lengths[i] = len(contour)\n            self.contours = contours[np.argmax(all_lengths)]\n</code></pre>"},{"location":"api/cellects/image_analysis/shape_descriptors/#cellects.image_analysis.shape_descriptors.ShapeDescriptors.get_convex_hull","title":"<code>get_convex_hull()</code>","text":"<p>Compute and store the convex hull of the object's contour.</p> Notes <p>Stores the result in <code>self.convex_hull</code>. Computes contours if needed.</p> <p>Returns:</p> Type Description <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; SD = ShapeDescriptors(np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]], dtype=np.uint8), [\"solidity\"])\n&gt;&gt;&gt; print(len(SD.convex_hull))\n4\n</code></pre> Source code in <code>src/cellects/image_analysis/shape_descriptors.py</code> <pre><code>def get_convex_hull(self):\n    \"\"\"\n    Compute and store the convex hull of the object's contour.\n\n    Notes\n    -----\n    Stores the result in `self.convex_hull`. Computes contours if needed.\n\n    Returns\n    -------\n    None\n\n    Examples\n    --------\n    &gt;&gt;&gt; SD = ShapeDescriptors(np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]], dtype=np.uint8), [\"solidity\"])\n    &gt;&gt;&gt; print(len(SD.convex_hull))\n    4\n    \"\"\"\n    if self.area == 0:\n        self.convex_hull = np.array([], np.uint8)\n    else:\n        if self.contours is None:\n            self.get_contours()\n        self.convex_hull = cv2.convexHull(self.contours)\n</code></pre>"},{"location":"api/cellects/image_analysis/shape_descriptors/#cellects.image_analysis.shape_descriptors.ShapeDescriptors.get_convexity","title":"<code>get_convexity()</code>","text":"<p>Compute and store convexity: convex hull perimeter / contour perimeter.</p> Notes <p>Requires <code>self.perimeter</code> and <code>self.convex_hull</code>.</p> <p>Returns:</p> Type Description <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; SD = ShapeDescriptors(np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]], dtype=np.uint8), [\"convexity\"])\n&gt;&gt;&gt; print(SD.convexity)\n1.0\n</code></pre> Source code in <code>src/cellects/image_analysis/shape_descriptors.py</code> <pre><code>def get_convexity(self):\n    \"\"\"\n    Compute and store convexity: convex hull perimeter / contour perimeter.\n\n    Notes\n    -----\n    Requires `self.perimeter` and `self.convex_hull`.\n\n    Returns\n    -------\n    None\n\n    Examples\n    --------\n    &gt;&gt;&gt; SD = ShapeDescriptors(np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]], dtype=np.uint8), [\"convexity\"])\n    &gt;&gt;&gt; print(SD.convexity)\n    1.0\n    \"\"\"\n    if self.perimeter is None:\n        self.get_perimeter()\n    if self.convex_hull is None:\n        self.get_convex_hull()\n    if self.perimeter == 0 or len(self.convex_hull) == 0:\n        self.convexity = 0.\n    else:\n        self.convexity = cv2.arcLength(self.convex_hull, True) / self.perimeter\n</code></pre>"},{"location":"api/cellects/image_analysis/shape_descriptors/#cellects.image_analysis.shape_descriptors.ShapeDescriptors.get_eccentricity","title":"<code>get_eccentricity()</code>","text":"<p>Compute and store eccentricity from major and minor axis lengths.</p> Notes <p>Calls <code>get_inertia_axes()</code> if needed and stores result in <code>self.eccentricity</code>.</p> <p>Returns:</p> Type Description <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; SD = ShapeDescriptors(np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]], dtype=np.uint8), [\"eccentricity\"])\n&gt;&gt;&gt; print(SD.eccentricity)\n0.0\n</code></pre> Source code in <code>src/cellects/image_analysis/shape_descriptors.py</code> <pre><code>def get_eccentricity(self):\n    \"\"\"\n    Compute and store eccentricity from major and minor axis lengths.\n\n    Notes\n    -----\n    Calls `get_inertia_axes()` if needed and stores result in `self.eccentricity`.\n\n    Returns\n    -------\n    None\n\n    Examples\n    --------\n    &gt;&gt;&gt; SD = ShapeDescriptors(np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]], dtype=np.uint8), [\"eccentricity\"])\n    &gt;&gt;&gt; print(SD.eccentricity)\n    0.0\n    \"\"\"\n    self.get_inertia_axes()\n    if self.major_axis_len == 0:\n        self.eccentricity = 0.\n    else:\n        self.eccentricity = np.sqrt(1 - np.square(self.minor_axis_len / self.major_axis_len))\n</code></pre>"},{"location":"api/cellects/image_analysis/shape_descriptors/#cellects.image_analysis.shape_descriptors.ShapeDescriptors.get_euler_number","title":"<code>get_euler_number()</code>","text":"<p>Ensure contours are computed; stores Euler number in <code>self.euler_number</code> via <code>get_contours()</code>.</p> <p>Returns:</p> Type Description <code>None</code> Notes <p>Euler number is computed in <code>get_contours()</code> as <code>(components - 1) - len(contours)</code>.</p> Source code in <code>src/cellects/image_analysis/shape_descriptors.py</code> <pre><code>def get_euler_number(self):\n    \"\"\"\n    Ensure contours are computed; stores Euler number in `self.euler_number` via `get_contours()`.\n\n    Returns\n    -------\n    None\n\n    Notes\n    -----\n    Euler number is computed in `get_contours()` as `(components - 1) - len(contours)`.\n    \"\"\"\n    if self.contours is None:\n        self.get_contours()\n</code></pre>"},{"location":"api/cellects/image_analysis/shape_descriptors/#cellects.image_analysis.shape_descriptors.ShapeDescriptors.get_inertia_axes","title":"<code>get_inertia_axes()</code>","text":"<p>Calculate and set the moments of inertia properties of an object.</p> <p>This function computes the centroid, major axis length, minor axis length, and axes orientation for an object. It first ensures that the moments of inertia (<code>mo</code>) attribute is available, computing them if necessary, before using the <code>get_inertia_axes</code> function.</p> <p>Returns:</p> Type Description <code>None</code> <p>This method sets the following attributes: - <code>cx</code> : float     The x-coordinate of the centroid. - <code>cy</code> : float     The y-coordinate of the centroid. - <code>major_axis_len</code> : float     The length of the major axis. - <code>minor_axis_len</code> : float     The length of the minor axis. - <code>axes_orientation</code> : float     The orientation angle of the axes.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If there is an issue with the moments of inertia computation.</p> Notes <p>This function modifies in-place the object's attributes related to its geometry.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; SD = ShapeDescriptors(np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]], dtype=np.uint8), [\"major_axis_len\"])\n&gt;&gt;&gt; print(SD.axes_orientation)\n0.0\n</code></pre> Source code in <code>src/cellects/image_analysis/shape_descriptors.py</code> <pre><code>def get_inertia_axes(self):\n    \"\"\"\n    Calculate and set the moments of inertia properties of an object.\n\n    This function computes the centroid, major axis length,\n    minor axis length, and axes orientation for an object. It\n    first ensures that the moments of inertia (`mo`) attribute is available,\n    computing them if necessary, before using the `get_inertia_axes` function.\n\n    Returns\n    -------\n    None\n\n        This method sets the following attributes:\n        - `cx` : float\n            The x-coordinate of the centroid.\n        - `cy` : float\n            The y-coordinate of the centroid.\n        - `major_axis_len` : float\n            The length of the major axis.\n        - `minor_axis_len` : float\n            The length of the minor axis.\n        - `axes_orientation` : float\n            The orientation angle of the axes.\n\n    Raises\n    ------\n    ValueError\n        If there is an issue with the moments of inertia computation.\n\n    Notes\n    -----\n    This function modifies in-place the object's attributes related to its geometry.\n\n    Examples\n    --------\n    &gt;&gt;&gt; SD = ShapeDescriptors(np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]], dtype=np.uint8), [\"major_axis_len\"])\n    &gt;&gt;&gt; print(SD.axes_orientation)\n    0.0\n    \"\"\"\n    if self.mo is None:\n        self.get_mo()\n    if self.area == 0:\n        self.cx, self.cy, self.major_axis_len, self.minor_axis_len, self.axes_orientation = 0, 0, 0, 0, 0\n    else:\n        self.cx, self.cy, self.major_axis_len, self.minor_axis_len, self.axes_orientation = get_inertia_axes(self.mo)\n</code></pre>"},{"location":"api/cellects/image_analysis/shape_descriptors/#cellects.image_analysis.shape_descriptors.ShapeDescriptors.get_kurtosis","title":"<code>get_kurtosis()</code>","text":"<p>Calculates the kurtosis of the image moments.</p> <p>Kurtosis is a statistical measure that describes the shape of a distribution's tails in relation to its overall shape. It is used here in the context of image moments analysis.</p> Notes <p>This function first checks if the kurtosis values have already been calculated. If not, it calculates them using the <code>get_kurtosis</code> function.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; SD = ShapeDescriptors(np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]], dtype=np.uint8), [\"kurtosis_x\", \"kurtosis_y\"])\n&gt;&gt;&gt; print(SD.kx, SD.ky)\n1.5 1.5\n</code></pre> Source code in <code>src/cellects/image_analysis/shape_descriptors.py</code> <pre><code>def get_kurtosis(self):\n    \"\"\"\n    Calculates the kurtosis of the image moments.\n\n    Kurtosis is a statistical measure that describes the shape of\n    a distribution's tails in relation to its overall shape. It is\n    used here in the context of image moments analysis.\n\n    Notes\n    -----\n    This function first checks if the kurtosis values have already been calculated.\n    If not, it calculates them using the `get_kurtosis` function.\n\n    Examples\n    --------\n    &gt;&gt;&gt; SD = ShapeDescriptors(np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]], dtype=np.uint8), [\"kurtosis_x\", \"kurtosis_y\"])\n    &gt;&gt;&gt; print(SD.kx, SD.ky)\n    1.5 1.5\n    \"\"\"\n    if self.kx is None:\n        if self.sx is None:\n            self.get_standard_deviations()\n\n        self.kx, self.ky = get_kurtosis(self.mo, self.binary_image, self.cx, self.cy, self.sx, self.sy)\n</code></pre>"},{"location":"api/cellects/image_analysis/shape_descriptors/#cellects.image_analysis.shape_descriptors.ShapeDescriptors.get_major_axis_len","title":"<code>get_major_axis_len()</code>","text":"<p>Ensure the major axis length is computed and stored in <code>self.major_axis_len</code>.</p> <p>Returns:</p> Type Description <code>None</code> Notes <p>Triggers <code>get_inertia_axes()</code> if needed.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; SD = ShapeDescriptors(np.array([[0, 1, 0], [0, 1, 0], [0, 1, 0]], dtype=np.uint8), [\"major_axis_len\"])\n&gt;&gt;&gt; print(SD.major_axis_len)\n2.8284271247461907\n</code></pre> Source code in <code>src/cellects/image_analysis/shape_descriptors.py</code> <pre><code>def get_major_axis_len(self):\n    \"\"\"\n    Ensure the major axis length is computed and stored in `self.major_axis_len`.\n\n    Returns\n    -------\n    None\n\n    Notes\n    -----\n    Triggers `get_inertia_axes()` if needed.\n\n    Examples\n    --------\n    &gt;&gt;&gt; SD = ShapeDescriptors(np.array([[0, 1, 0], [0, 1, 0], [0, 1, 0]], dtype=np.uint8), [\"major_axis_len\"])\n    &gt;&gt;&gt; print(SD.major_axis_len)\n    2.8284271247461907\n    \"\"\"\n    if self.major_axis_len is None:\n        self.get_inertia_axes()\n</code></pre>"},{"location":"api/cellects/image_analysis/shape_descriptors/#cellects.image_analysis.shape_descriptors.ShapeDescriptors.get_min_bounding_rectangle","title":"<code>get_min_bounding_rectangle()</code>","text":"<p>Retrieve the minimum bounding rectangle from the contours of an image.</p> <p>This method calculates the smallest area rectangle that can enclose the object outlines present in the image, which is useful for object detection and analysis tasks.</p> Notes <ul> <li>The bounding rectangle is calculated only if contours are available.   If not, they will be retrieved first before calculating the rectangle.</li> </ul> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the contours are not available and cannot be retrieved, indicating a problem with the image or preprocessing steps.</p> <p>Returns:</p> Type Description <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; SD = ShapeDescriptors(np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]], dtype=np.uint8), [\"rectangularity\"])\n&gt;&gt;&gt; print(len(SD.min_bounding_rectangle))\n3\n</code></pre> Source code in <code>src/cellects/image_analysis/shape_descriptors.py</code> <pre><code>def get_min_bounding_rectangle(self):\n    \"\"\"\n    Retrieve the minimum bounding rectangle from the contours of an image.\n\n    This method calculates the smallest area rectangle that can enclose\n    the object outlines present in the image, which is useful for\n    object detection and analysis tasks.\n\n    Notes\n    -----\n    - The bounding rectangle is calculated only if contours are available.\n      If not, they will be retrieved first before calculating the rectangle.\n\n    Raises\n    ------\n    RuntimeError\n        If the contours are not available and cannot be retrieved,\n        indicating a problem with the image or preprocessing steps.\n\n    Returns\n    -------\n    None\n\n    Examples\n    --------\n    &gt;&gt;&gt; SD = ShapeDescriptors(np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]], dtype=np.uint8), [\"rectangularity\"])\n    &gt;&gt;&gt; print(len(SD.min_bounding_rectangle))\n    3\n    \"\"\"\n    if self.area == 0:\n        self.min_bounding_rectangle = np.array([], np.uint8)\n    else:\n        if self.contours is None:\n            self.get_contours()\n        if len(self.contours) == 0:\n            self.min_bounding_rectangle = np.array([], np.uint8)\n        else:\n            self.min_bounding_rectangle = cv2.minAreaRect(self.contours)  # ((cx, cy), (width, height), angle)\n</code></pre>"},{"location":"api/cellects/image_analysis/shape_descriptors/#cellects.image_analysis.shape_descriptors.ShapeDescriptors.get_minor_axis_len","title":"<code>get_minor_axis_len()</code>","text":"<p>Ensure the minor axis length is computed and stored in <code>self.minor_axis_len</code>.</p> <p>Returns:</p> Type Description <code>None</code> Notes <p>Triggers <code>get_inertia_axes()</code> if needed.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; SD = ShapeDescriptors(np.array([[0, 1, 0], [0, 1, 0], [0, 1, 0]], dtype=np.uint8), [\"minor_axis_len\"])\n&gt;&gt;&gt; print(SD.minor_axis_len)\n0.0\n</code></pre> Source code in <code>src/cellects/image_analysis/shape_descriptors.py</code> <pre><code>def get_minor_axis_len(self):\n    \"\"\"\n    Ensure the minor axis length is computed and stored in `self.minor_axis_len`.\n\n    Returns\n    -------\n    None\n\n    Notes\n    -----\n    Triggers `get_inertia_axes()` if needed.\n\n    Examples\n    --------\n    &gt;&gt;&gt; SD = ShapeDescriptors(np.array([[0, 1, 0], [0, 1, 0], [0, 1, 0]], dtype=np.uint8), [\"minor_axis_len\"])\n    &gt;&gt;&gt; print(SD.minor_axis_len)\n    0.0\n    \"\"\"\n    if self.minor_axis_len is None:\n        self.get_inertia_axes()\n</code></pre>"},{"location":"api/cellects/image_analysis/shape_descriptors/#cellects.image_analysis.shape_descriptors.ShapeDescriptors.get_mo","title":"<code>get_mo()</code>","text":"<p>Get moments of a binary image.</p> <p>Calculate the image moments for a given binary image using OpenCV's  <code>cv2.moments</code> function and then translate these moments into a formatted  dictionary.</p> Notes <p>This function assumes the binary image has already been processed and is in a  suitable format for moment calculation.</p> Returns <p>None</p> Examples <p>SD = ShapeDescriptors(np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]], dtype=np.uint8), [\"mo\"]) print(SD.mo[\"m00\"])  9.0</p> Source code in <code>src/cellects/image_analysis/shape_descriptors.py</code> <pre><code>def get_mo(self):\n    \"\"\"\n    Get moments of a binary image.\n\n    Calculate the image moments for a given binary image using OpenCV's\n    `cv2.moments` function and then translate these moments into a formatted\n    dictionary.\n\n    Notes\n    -----\n    This function assumes the binary image has already been processed and is in a\n    suitable format for moment calculation.\n\n    Returns\n    -------\n    None\n\n    Examples\n    --------\n   &gt;&gt;&gt; SD = ShapeDescriptors(np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]], dtype=np.uint8), [\"mo\"])\n    &gt;&gt;&gt; print(SD.mo[\"m00\"])\n    9.0\n    \"\"\"\n    self.mo = translate_dict(cv2.moments(self.binary_image))\n</code></pre>"},{"location":"api/cellects/image_analysis/shape_descriptors/#cellects.image_analysis.shape_descriptors.ShapeDescriptors.get_perimeter","title":"<code>get_perimeter()</code>","text":"<p>Compute and store the contour perimeter length.</p> Notes <p>Computes contours if needed and stores the length in <code>self.perimeter</code>.</p> <p>Returns:</p> Type Description <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; SD = ShapeDescriptors(np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]], dtype=np.uint8), [\"perimeter\"])\n&gt;&gt;&gt; print(SD.perimeter)\n8.0\n</code></pre> Source code in <code>src/cellects/image_analysis/shape_descriptors.py</code> <pre><code>def get_perimeter(self):\n    \"\"\"\n    Compute and store the contour perimeter length.\n\n    Notes\n    -----\n    Computes contours if needed and stores the length in `self.perimeter`.\n\n    Returns\n    -------\n    None\n\n    Examples\n    --------\n    &gt;&gt;&gt; SD = ShapeDescriptors(np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]], dtype=np.uint8), [\"perimeter\"])\n    &gt;&gt;&gt; print(SD.perimeter)\n    8.0\n    \"\"\"\n    if self.area == 0:\n        self.perimeter = 0.\n    else:\n        if self.contours is None:\n            self.get_contours()\n        if len(self.contours) == 0:\n            self.perimeter = 0.\n        else:\n            self.perimeter = cv2.arcLength(self.contours, True)\n</code></pre>"},{"location":"api/cellects/image_analysis/shape_descriptors/#cellects.image_analysis.shape_descriptors.ShapeDescriptors.get_rectangularity","title":"<code>get_rectangularity()</code>","text":"<p>Compute and store rectangularity: area / bounding-rectangle-area.</p> Notes <p>Uses <code>self.binary_image</code> and <code>self.min_bounding_rectangle</code>. Computes the MBR if needed.</p> <p>Returns:</p> Type Description <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; SD = ShapeDescriptors(np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]], dtype=np.uint8), [\"rectangularity\"])\n&gt;&gt;&gt; print(SD.rectangularity)\n2.25\n</code></pre> Source code in <code>src/cellects/image_analysis/shape_descriptors.py</code> <pre><code>def get_rectangularity(self):\n    \"\"\"\n    Compute and store rectangularity: area / bounding-rectangle-area.\n\n    Notes\n    -----\n    Uses `self.binary_image` and `self.min_bounding_rectangle`. Computes the MBR if needed.\n\n    Returns\n    -------\n    None\n\n    Examples\n    --------\n    &gt;&gt;&gt; SD = ShapeDescriptors(np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]], dtype=np.uint8), [\"rectangularity\"])\n    &gt;&gt;&gt; print(SD.rectangularity)\n    2.25\n    \"\"\"\n    if self.area == 0:\n        self.rectangularity = 0.\n    else:\n        if self.min_bounding_rectangle is None:\n            self.get_min_bounding_rectangle()\n        bounding_rectangle_area = self.min_bounding_rectangle[1][0] * self.min_bounding_rectangle[1][1]\n        if bounding_rectangle_area == 0:\n            self.rectangularity = 0.\n        else:\n            self.rectangularity = self.binary_image.sum() / bounding_rectangle_area\n</code></pre>"},{"location":"api/cellects/image_analysis/shape_descriptors/#cellects.image_analysis.shape_descriptors.ShapeDescriptors.get_skewness","title":"<code>get_skewness()</code>","text":"<p>Calculate and store skewness along x and y (skx, sky).</p> <p>This function computes the skewness about the x-axis and y-axis of an image. Skewness is a measure of the asymmetry of the probability distribution of values in an image.</p> Notes <p>Requires standard deviations; values are stored in <code>self.skx</code> and <code>self.sky</code>.</p> <p>Returns:</p> Type Description <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; SD = ShapeDescriptors(np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]], dtype=np.uint8), [\"skewness_x\", \"skewness_y\"])\n&gt;&gt;&gt; print(SD.skx, SD.sky)\n0.0 0.0\n</code></pre> Source code in <code>src/cellects/image_analysis/shape_descriptors.py</code> <pre><code>def get_skewness(self):\n    \"\"\"\n    Calculate and store skewness along x and y (skx, sky).\n\n    This function computes the skewness about the x-axis and y-axis of\n    an image. Skewness is a measure of the asymmetry of the probability\n    distribution of values in an image.\n\n    Notes\n    -----\n    Requires standard deviations; values are stored in `self.skx` and `self.sky`.\n\n    Returns\n    -------\n    None\n\n    Examples\n    --------\n    &gt;&gt;&gt; SD = ShapeDescriptors(np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]], dtype=np.uint8), [\"skewness_x\", \"skewness_y\"])\n    &gt;&gt;&gt; print(SD.skx, SD.sky)\n    0.0 0.0\n    \"\"\"\n    if self.skx is None:\n        if self.sx is None:\n            self.get_standard_deviations()\n\n        self.skx, self.sky = get_skewness(self.mo, self.binary_image, self.cx, self.cy, self.sx, self.sy)\n</code></pre>"},{"location":"api/cellects/image_analysis/shape_descriptors/#cellects.image_analysis.shape_descriptors.ShapeDescriptors.get_solidity","title":"<code>get_solidity()</code>","text":"<p>Compute and store solidity: contour area / convex hull area.</p> Extended Summary <p>The solidity is a dimensionless measure that compares the area of a shape to its convex hull. A solidity of 1 means the contour is fully convex, while a value less than 1 indicates concavities.</p> Notes <p>If the convex hull area is 0 or absent, solidity is set to 0.</p> <p>Returns:</p> Type Description <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; SD = ShapeDescriptors(np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]], dtype=np.uint8), [\"solidity\"])\n&gt;&gt;&gt; print(SD.solidity)\n1.0\n</code></pre> Source code in <code>src/cellects/image_analysis/shape_descriptors.py</code> <pre><code>def get_solidity(self):\n    \"\"\"\n    Compute and store solidity: contour area / convex hull area.\n\n    Extended Summary\n    ----------------\n    The solidity is a dimensionless measure that compares the area of a shape to\n    its convex hull. A solidity of 1 means the contour is fully convex, while a\n    value less than 1 indicates concavities.\n\n    Notes\n    -----\n    If the convex hull area is 0 or absent, solidity is set to 0.\n\n    Returns\n    -------\n    None\n\n    Examples\n    --------\n    &gt;&gt;&gt; SD = ShapeDescriptors(np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]], dtype=np.uint8), [\"solidity\"])\n    &gt;&gt;&gt; print(SD.solidity)\n    1.0\n    \"\"\"\n    if self.area == 0:\n        self.solidity = 0.\n    else:\n        if self.convex_hull is None:\n            self.get_convex_hull()\n        if len(self.convex_hull) == 0:\n            self.solidity = 0.\n        else:\n            hull_area = cv2.contourArea(self.convex_hull)\n            if hull_area == 0:\n                self.solidity = 0.\n            else:\n                self.solidity = cv2.contourArea(self.contours) / hull_area\n</code></pre>"},{"location":"api/cellects/image_analysis/shape_descriptors/#cellects.image_analysis.shape_descriptors.ShapeDescriptors.get_standard_deviations","title":"<code>get_standard_deviations()</code>","text":"<p>Calculate and store standard deviations along x and y (sx, sy).</p> Notes <p>Requires centroid and moments; values are stored in <code>self.sx</code> and <code>self.sy</code>.</p> Returns <p>None</p> Examples <p>SD = ShapeDescriptors(np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]], dtype=np.uint8), [\"standard_deviation_x\", \"standard_deviation_y\"]) print(SD.sx, SD.sy)  0.816496580927726 0.816496580927726</p> Source code in <code>src/cellects/image_analysis/shape_descriptors.py</code> <pre><code>def get_standard_deviations(self):\n    \"\"\"\n    Calculate and store standard deviations along x and y (sx, sy).\n\n    Notes\n    -----\n    Requires centroid and moments; values are stored in `self.sx` and `self.sy`.\n\n    Returns\n    -------\n    None\n\n    Examples\n    --------\n   &gt;&gt;&gt; SD = ShapeDescriptors(np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]], dtype=np.uint8), [\"standard_deviation_x\", \"standard_deviation_y\"])\n    &gt;&gt;&gt; print(SD.sx, SD.sy)\n    0.816496580927726 0.816496580927726\n    \"\"\"\n    if self.sx is None:\n        if self.axes_orientation is None:\n            self.get_inertia_axes()\n        self.sx, self.sy = get_standard_deviations(self.mo, self.binary_image, self.cx, self.cy)\n</code></pre>"},{"location":"api/cellects/image_analysis/shape_descriptors/#cellects.image_analysis.shape_descriptors.ShapeDescriptors.get_total_hole_area","title":"<code>get_total_hole_area()</code>","text":"<p>Calculate the total area of holes in a binary image.</p> <p>This function uses connected component labeling to detect and measure the area of holes in a binary image.</p> <p>Returns:</p> Type Description <code>float</code> <p>The total area of all detected holes in the binary image.</p> Notes <p>This function assumes that the binary image has been pre-processed and that holes are represented as connected components of zero pixels within the foreground</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; SD = ShapeDescriptors(np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]], dtype=np.uint8), [\"total_hole_area\"])\n&gt;&gt;&gt; print(SD.total_hole_area)\n0\n</code></pre> Source code in <code>src/cellects/image_analysis/shape_descriptors.py</code> <pre><code>def get_total_hole_area(self):\n    \"\"\"\n    Calculate the total area of holes in a binary image.\n\n    This function uses connected component labeling to detect and\n    measure the area of holes in a binary image.\n\n    Returns\n    -------\n    float\n        The total area of all detected holes in the binary image.\n\n    Notes\n    -----\n    This function assumes that the binary image has been pre-processed\n    and that holes are represented as connected components of zero\n    pixels within the foreground\n\n    Examples\n    --------\n    &gt;&gt;&gt; SD = ShapeDescriptors(np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]], dtype=np.uint8), [\"total_hole_area\"])\n    &gt;&gt;&gt; print(SD.total_hole_area)\n    0\n    \"\"\"\n    nb, new_order = cv2.connectedComponents(1 - self.binary_image)\n    if nb &gt; 2:\n        self.total_hole_area = (new_order &gt; 1).sum()\n    else:\n        self.total_hole_area = 0.\n</code></pre>"},{"location":"api/cellects/image_analysis/shape_descriptors/#cellects.image_analysis.shape_descriptors.compute_one_descriptor_per_frame","title":"<code>compute_one_descriptor_per_frame(binary_vid, arena_label, timings, descriptors_dict, output_in_mm, pixel_size, do_fading, save_coord_specimen)</code>","text":"<p>Computes descriptors for each frame in a binary video and returns them as a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>binary_vid</code> <code>NDArray[uint8]</code> <p>The binary video data where each frame is a 2D array.</p> required <code>arena_label</code> <code>int</code> <p>Label for the arena in the video.</p> required <code>timings</code> <code>NDArray</code> <p>Array of timestamps corresponding to each frame.</p> required <code>descriptors_dict</code> <code>dict</code> <p>Dictionary containing the descriptors to be computed.</p> required <code>output_in_mm</code> <code>bool</code> <p>Flag indicating if output should be in millimeters. Default is False.</p> required <code>pixel_size</code> <code>float</code> <p>Size of a pixel in the video when <code>output_in_mm</code> is True. Default is None.</p> required <code>do_fading</code> <code>bool</code> <p>Flag indicating if the fading effect should be applied. Default is False.</p> required <code>save_coord_specimen</code> <code>bool</code> <p>Flag indicating if the coordinates of specimens should be saved. Default is False.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame containing the descriptors for each frame in the video.</p> Notes <p>For large inputs, consider pre-allocating memory for efficiency. The <code>save_coord_specimen</code> flag will save coordinate data to a file.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; binary_vid = np.ones((10, 640, 480), dtype=np.uint8)\n&gt;&gt;&gt; timings = np.arange(10)\n&gt;&gt;&gt; descriptors_dict = {'area': True, 'perimeter': True}\n&gt;&gt;&gt; result = compute_one_descriptor_per_frame(binary_vid, 1, timings, descriptors_dict)\n&gt;&gt;&gt; print(result.head())\n   arena  time  area  perimeter\n0      1     0     0          0\n1      1     1     0          0\n2      1     2     0          0\n3      1     3     0          0\n4      1     4     0          0\n</code></pre> <pre><code>&gt;&gt;&gt; binary_vid = np.ones((5, 640, 480), dtype=np.uint8)\n&gt;&gt;&gt; timings = np.arange(5)\n&gt;&gt;&gt; descriptors_dict = {'area': True, 'perimeter': True}\n&gt;&gt;&gt; result = compute_one_descriptor_per_frame(binary_vid, 2, timings,\n...                                            descriptors_dict,\n...                                            output_in_mm=True,\n...                                            pixel_size=0.1)\n&gt;&gt;&gt; print(result.head())\n   arena  time  area  perimeter\n0      2     0    0         0.0\n1      2     1    0         0.0\n2      2     2    0         0.0\n3      2     3    0         0.0\n4      2     4    0         0.0\n</code></pre> Source code in <code>src/cellects/image_analysis/shape_descriptors.py</code> <pre><code>def compute_one_descriptor_per_frame(binary_vid: NDArray[np.uint8], arena_label: int, timings: NDArray,\n                                     descriptors_dict: dict, output_in_mm: bool, pixel_size: float,\n                                     do_fading: bool, save_coord_specimen:bool):\n    \"\"\"\n    Computes descriptors for each frame in a binary video and returns them as a DataFrame.\n\n    Parameters\n    ----------\n    binary_vid : NDArray[np.uint8]\n        The binary video data where each frame is a 2D array.\n    arena_label : int\n        Label for the arena in the video.\n    timings : NDArray\n        Array of timestamps corresponding to each frame.\n    descriptors_dict : dict\n        Dictionary containing the descriptors to be computed.\n    output_in_mm : bool, optional\n        Flag indicating if output should be in millimeters. Default is False.\n    pixel_size : float, optional\n        Size of a pixel in the video when `output_in_mm` is True. Default is None.\n    do_fading : bool, optional\n        Flag indicating if the fading effect should be applied. Default is False.\n    save_coord_specimen : bool, optional\n        Flag indicating if the coordinates of specimens should be saved. Default is False.\n\n    Returns\n    -------\n    pandas.DataFrame\n        DataFrame containing the descriptors for each frame in the video.\n\n    Notes\n    -----\n    For large inputs, consider pre-allocating memory for efficiency.\n    The `save_coord_specimen` flag will save coordinate data to a file.\n\n    Examples\n    --------\n    &gt;&gt;&gt; binary_vid = np.ones((10, 640, 480), dtype=np.uint8)\n    &gt;&gt;&gt; timings = np.arange(10)\n    &gt;&gt;&gt; descriptors_dict = {'area': True, 'perimeter': True}\n    &gt;&gt;&gt; result = compute_one_descriptor_per_frame(binary_vid, 1, timings, descriptors_dict)\n    &gt;&gt;&gt; print(result.head())\n       arena  time  area  perimeter\n    0      1     0     0          0\n    1      1     1     0          0\n    2      1     2     0          0\n    3      1     3     0          0\n    4      1     4     0          0\n\n    &gt;&gt;&gt; binary_vid = np.ones((5, 640, 480), dtype=np.uint8)\n    &gt;&gt;&gt; timings = np.arange(5)\n    &gt;&gt;&gt; descriptors_dict = {'area': True, 'perimeter': True}\n    &gt;&gt;&gt; result = compute_one_descriptor_per_frame(binary_vid, 2, timings,\n    ...                                            descriptors_dict,\n    ...                                            output_in_mm=True,\n    ...                                            pixel_size=0.1)\n    &gt;&gt;&gt; print(result.head())\n       arena  time  area  perimeter\n    0      2     0    0         0.0\n    1      2     1    0         0.0\n    2      2     2    0         0.0\n    3      2     3    0         0.0\n    4      2     4    0         0.0\n    \"\"\"\n    dims = binary_vid.shape\n    all_descriptors, to_compute_from_sd, length_measures, area_measures = initialize_descriptor_computation(descriptors_dict)\n    one_row_per_frame = pd.DataFrame(np.zeros((dims[0], 2 + len(all_descriptors))),\n                                          columns=['arena', 'time'] + all_descriptors)\n    one_row_per_frame['arena'] = [arena_label] * dims[0]\n    one_row_per_frame['time'] = timings\n    for t in np.arange(dims[0]):\n        SD = ShapeDescriptors(binary_vid[t, :, :], to_compute_from_sd)\n        for descriptor in to_compute_from_sd:\n            one_row_per_frame.loc[t, descriptor] = SD.descriptors[descriptor]\n    if save_coord_specimen:\n        np.save(f\"coord_specimen{arena_label}_t{dims[0]}_y{dims[1]}_x{dims[2]}.npy\",\n                smallest_memory_array(np.nonzero(binary_vid), \"uint\"))\n    # Adjust descriptors scale if output_in_mm is specified\n    if do_fading:\n        one_row_per_frame['newly_explored_area'] = get_newly_explored_area(binary_vid)\n    if output_in_mm:\n        one_row_per_frame = scale_descriptors(one_row_per_frame, pixel_size,\n                                                   length_measures, area_measures)\n    return one_row_per_frame\n</code></pre>"},{"location":"api/cellects/image_analysis/shape_descriptors/#cellects.image_analysis.shape_descriptors.initialize_descriptor_computation","title":"<code>initialize_descriptor_computation(descriptors_dict)</code>","text":"<p>Initialize descriptor computation based on available and requested descriptors.</p> <p>Parameters:</p> Name Type Description Default <code>descriptors_dict</code> <code>dict</code> <p>A dictionary where keys are descriptor names and values are booleans indicating whether to compute the corresponding descriptor.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing four lists: - all_descriptors: List of all requested descriptor names. - to_compute_from_sd: Array of descriptor names that need to be computed from the shape descriptors class. - length_measures: Array of descriptor names that are length measures and need to be computed. - area_measures: Array of descriptor names that are area measures and need to be computed.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; descriptors_dict = {'perimeter': True, 'area': False}\n&gt;&gt;&gt; all_descriptors, to_compute_from_sd, length_measures, area_measures = initialize_descriptor_computation(descriptors_dict)\n&gt;&gt;&gt; print(all_descriptors, to_compute_from_sd, length_measures, area_measures)\n['length'] ['length'] ['length'] []\n</code></pre> Source code in <code>src/cellects/image_analysis/shape_descriptors.py</code> <pre><code>def initialize_descriptor_computation(descriptors_dict: dict) -&gt; Tuple[list, list, list, list]:\n    \"\"\"\n\n    Initialize descriptor computation based on available and requested descriptors.\n\n    Parameters\n    ----------\n    descriptors_dict : dict\n        A dictionary where keys are descriptor names and values are booleans indicating whether\n        to compute the corresponding descriptor.\n\n    Returns\n    -------\n    tuple\n        A tuple containing four lists:\n        - all_descriptors: List of all requested descriptor names.\n        - to_compute_from_sd: Array of descriptor names that need to be computed from the shape descriptors class.\n        - length_measures: Array of descriptor names that are length measures and need to be computed.\n        - area_measures: Array of descriptor names that are area measures and need to be computed.\n\n    Examples\n    --------\n    &gt;&gt;&gt; descriptors_dict = {'perimeter': True, 'area': False}\n    &gt;&gt;&gt; all_descriptors, to_compute_from_sd, length_measures, area_measures = initialize_descriptor_computation(descriptors_dict)\n    &gt;&gt;&gt; print(all_descriptors, to_compute_from_sd, length_measures, area_measures)\n    ['length'] ['length'] ['length'] []\n\n    \"\"\"\n    available_descriptors_in_sd = list(from_shape_descriptors_class.keys())\n    all_descriptors = []\n    to_compute_from_sd = []\n    for name, do_compute in descriptors_dict.items():\n        if do_compute:\n            all_descriptors.append(name)\n            if np.isin(name, available_descriptors_in_sd):\n                to_compute_from_sd.append(name)\n    to_compute_from_sd = np.array(to_compute_from_sd)\n    length_measures = to_compute_from_sd[np.isin(to_compute_from_sd, length_descriptors)]\n    area_measures = to_compute_from_sd[np.isin(to_compute_from_sd, area_descriptors)]\n\n    return all_descriptors, to_compute_from_sd, length_measures, area_measures\n</code></pre>"},{"location":"api/cellects/image_analysis/shape_descriptors/#cellects.image_analysis.shape_descriptors.scale_descriptors","title":"<code>scale_descriptors(descriptors_dict, pixel_size, length_measures=None, area_measures=None)</code>","text":"<p>Scale the spatial descriptors in a dictionary based on pixel size.</p> <p>Parameters:</p> Name Type Description Default <code>descriptors_dict</code> <code>dict</code> <p>Dictionary containing spatial descriptors.</p> required <code>pixel_size</code> <code>float</code> <p>Pixel size used for scaling.</p> required <code>length_measures</code> <code>ndarray</code> <p>Array of descriptors that represent lengths. If not provided, they will be initialized.</p> <code>None</code> <code>area_measures</code> <code>ndarray</code> <p>Array of descriptors that represent areas. If not provided, they will be initialized.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary with scaled spatial descriptors.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from numpy import array as ndarray\n&gt;&gt;&gt; descriptors_dict = {'length': ndarray([1, 2]), 'area': ndarray([3, 4])}\n&gt;&gt;&gt; pixel_size = 0.5\n&gt;&gt;&gt; scaled_dict = scale_descriptors(descriptors_dict, pixel_size)\n&gt;&gt;&gt; print(scaled_dict)\n{'length': array([0.5, 1.]), 'area': array([1.58421369, 2.])}\n</code></pre> Source code in <code>src/cellects/image_analysis/shape_descriptors.py</code> <pre><code>def scale_descriptors(descriptors_dict, pixel_size: float, length_measures: NDArray[str]=None, area_measures: NDArray[str]=None):\n    \"\"\"\n    Scale the spatial descriptors in a dictionary based on pixel size.\n\n    Parameters\n    ----------\n    descriptors_dict : dict\n        Dictionary containing spatial descriptors.\n    pixel_size : float\n        Pixel size used for scaling.\n    length_measures : numpy.ndarray, optional\n        Array of descriptors that represent lengths. If not provided,\n        they will be initialized.\n    area_measures : numpy.ndarray, optional\n        Array of descriptors that represent areas. If not provided,\n        they will be initialized.\n\n    Returns\n    -------\n    dict\n        Dictionary with scaled spatial descriptors.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from numpy import array as ndarray\n    &gt;&gt;&gt; descriptors_dict = {'length': ndarray([1, 2]), 'area': ndarray([3, 4])}\n    &gt;&gt;&gt; pixel_size = 0.5\n    &gt;&gt;&gt; scaled_dict = scale_descriptors(descriptors_dict, pixel_size)\n    &gt;&gt;&gt; print(scaled_dict)\n    {'length': array([0.5, 1.]), 'area': array([1.58421369, 2.])}\n    \"\"\"\n    if length_measures is None or area_measures is None:\n        to_compute_from_sd = np.array(list(descriptors_dict.keys()))\n        length_measures = to_compute_from_sd[np.isin(to_compute_from_sd, length_descriptors)]\n        area_measures = to_compute_from_sd[np.isin(to_compute_from_sd, area_descriptors)]\n    for descr in length_measures:\n        descriptors_dict[descr] *= pixel_size\n    for descr in area_measures:\n        descriptors_dict[descr] *= np.sqrt(pixel_size)\n    return descriptors_dict\n</code></pre>"},{"location":"api/cellects/utils/","title":"<code>cellects.utils</code>","text":""},{"location":"api/cellects/utils/#cellects.utils","title":"<code>cellects.utils</code>","text":""},{"location":"api/cellects/utils/decorators/","title":"<code>cellects.utils.decorators</code>","text":""},{"location":"api/cellects/utils/decorators/#cellects.utils.decorators","title":"<code>cellects.utils.decorators</code>","text":""},{"location":"api/cellects/utils/decorators/#cellects.utils.decorators.njit","title":"<code>njit(*args, **kwargs)</code>","text":"<p>numba.njit decorator that can be disabled. Useful for testing.</p> Source code in <code>src/cellects/utils/decorators.py</code> <pre><code>def njit(*args, **kwargs):\n    \"\"\" numba.njit decorator that can be disabled. Useful for testing.\n    \"\"\"\n    if USE_NUMBA:\n        return _real_njit(*args, **kwargs)\n    # test mode: return identity decorator\n    def deco(func):\n        return func\n    return deco\n</code></pre>"},{"location":"api/cellects/utils/formulas/","title":"<code>cellects.utils.formulas</code>","text":""},{"location":"api/cellects/utils/formulas/#cellects.utils.formulas","title":"<code>cellects.utils.formulas</code>","text":"<p>Statistical and geometric analysis tools for numerical arrays.</p> <p>This module provides a collection of functions and unit tests for calculating distances, statistical properties (skewness, kurtosis), array transformations, and image moment-based analysis. The tools are optimized for applications involving binary images, coordinate data, and mathematical modeling operations where performance-critical calculations benefit from vectorized or JIT-compiled implementations.</p> <p>Functions: eudist : Calculate Euclidean distance between two vectors to_uint8 : Convert array to 8-bit unsigned integers using NumBA acceleration translate_dict : Transform dictionary structures into alternative formats linear_model : Compute y = a*x + b regression model values (JIT-compiled) moving_average : Calculate sliding window averages with specified step size get_var : Derive variance from image moments and spatial coordinates find_common_coord : Identify shared coordinate pairs between two arrays get_skewness/get_kurtosis : Calculate third/fourth standardized moment statistics sum_of_abs_differences : Compute total absolute differences between arrays (JIT) bracket_to_uint8_image_contrast : Convert images to 8-bit with contrast normalization find_duplicates_coord : Locate rows with duplicate coordinate values get_power_dists : Generate radial distance measures from image centers get_inertia_axes : Calculate principal axes of inertia for binary shapes</p> <p>Notes: - All Numba-accelerated functions require congruent NumPy arrays as inputs - Image processing functions expect binary (boolean/int8) input matrices</p>"},{"location":"api/cellects/utils/formulas/#cellects.utils.formulas.bracket_to_uint8_image_contrast","title":"<code>bracket_to_uint8_image_contrast(image)</code>","text":"<p>Convert an image with bracket contrast values to uint8 type.</p> <p>This function normalizes an input image by scaling the minimum and maximum values of the image to the range [0, 255] and then converts it to uint8 data type.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>Input image as a numpy array with floating-point values.</p> required <p>Returns:</p> Type Description <code>ndarray of uint8</code> <p>Output image converted to uint8 type after normalization.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; image = np.random.randint(0, 255, (10, 10), dtype=np.uint8)\n&gt;&gt;&gt; res = bracket_to_uint8_image_contrast(image)\n&gt;&gt;&gt; print(res)\n</code></pre> <pre><code>&gt;&gt;&gt; image = np.zeros((10, 10), dtype=np.uint8)\n&gt;&gt;&gt; res = bracket_to_uint8_image_contrast(image)\n&gt;&gt;&gt; print(res)\n</code></pre> Source code in <code>src/cellects/utils/formulas.py</code> <pre><code>@njit()\ndef bracket_to_uint8_image_contrast(image: NDArray):\n    \"\"\"\n    Convert an image with bracket contrast values to uint8 type.\n\n    This function normalizes an input image by scaling the minimum and maximum\n    values of the image to the range [0, 255] and then converts it to uint8\n    data type.\n\n    Parameters\n    ----------\n    image : ndarray\n        Input image as a numpy array with floating-point values.\n\n    Returns\n    -------\n    ndarray of uint8\n        Output image converted to uint8 type after normalization.\n\n    Examples\n    --------\n    &gt;&gt;&gt; image = np.random.randint(0, 255, (10, 10), dtype=np.uint8)\n    &gt;&gt;&gt; res = bracket_to_uint8_image_contrast(image)\n    &gt;&gt;&gt; print(res)\n\n    &gt;&gt;&gt; image = np.zeros((10, 10), dtype=np.uint8)\n    &gt;&gt;&gt; res = bracket_to_uint8_image_contrast(image)\n    &gt;&gt;&gt; print(res)\n    \"\"\"\n    image -= image.min()\n    if image.max() == 0:\n        return np.zeros_like(image, dtype=np.uint8)\n    else:\n        return to_uint8(255 * (image / np.max(image)))\n</code></pre>"},{"location":"api/cellects/utils/formulas/#cellects.utils.formulas.detect_first_move","title":"<code>detect_first_move(size_dynamics, growth_threshold)</code>","text":"<p>Detects the first move in a time series where the value exceeds the initial value by a given threshold.</p> <p>Parameters:</p> Name Type Description Default <code>size_dynamics</code> <code>ndarray</code> <p>The time series data of dynamics.</p> required <code>growth_threshold</code> <p>The threshold value for detecting the first move.</p> required <p>Returns:</p> Type Description <code>int or NA</code> <p>The index of the first move where the condition is met. Returns <code>pandas.NA</code> if no such index exists.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; size_dynamics = np.array([10, 12, 15, 18])\n&gt;&gt;&gt; growth_threshold = 5\n&gt;&gt;&gt; detect_first_move(size_dynamics, growth_threshold)\n2\n</code></pre> Source code in <code>src/cellects/utils/formulas.py</code> <pre><code>def detect_first_move(size_dynamics: NDArray, growth_threshold)-&gt; int:\n    \"\"\"\n    Detects the first move in a time series where the value exceeds the initial value by a given threshold.\n\n    Parameters\n    ----------\n    size_dynamics : numpy.ndarray\n        The time series data of dynamics.\n    growth_threshold: int or float\n        The threshold value for detecting the first move.\n\n    Returns\n    -------\n    int or pandas.NA\n        The index of the first move where the condition is met.\n        Returns `pandas.NA` if no such index exists.\n\n    Examples\n    --------\n    &gt;&gt;&gt; size_dynamics = np.array([10, 12, 15, 18])\n    &gt;&gt;&gt; growth_threshold = 5\n    &gt;&gt;&gt; detect_first_move(size_dynamics, growth_threshold)\n    2\n    \"\"\"\n    first_move = pd.NA\n    thresh_reached = np.nonzero(size_dynamics &gt;= (size_dynamics[0] + growth_threshold))[0]\n    if len(thresh_reached) &gt; 0:\n        first_move = thresh_reached[0]\n    return first_move\n</code></pre>"},{"location":"api/cellects/utils/formulas/#cellects.utils.formulas.eudist","title":"<code>eudist(v1, v2)</code>","text":"<p>Calculate the Euclidean distance between two points in n-dimensional space.</p> <p>Parameters:</p> Name Type Description Default <code>v1</code> <code>iterable of float</code> <p>The coordinates of the first point.</p> required <code>v2</code> <code>iterable of float</code> <p>The coordinates of the second point.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The Euclidean distance between <code>v1</code> and <code>v2</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>v1</code> and <code>v2</code> do not have the same length.</p> Notes <p>The Euclidean distance is calculated using the standard formula: \u221a((x2 \u2212 x1)^2 + (y2 \u2212 y1)^2 + ...).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; v1 = [1.0, 2.0]\n&gt;&gt;&gt; v2 = [4.0, 6.0]\n&gt;&gt;&gt; eudist(v1, v2)\n5.0\n</code></pre> <pre><code>&gt;&gt;&gt; v1 = [1.0, 2.0, 3.0]\n&gt;&gt;&gt; v2 = [4.0, 6.0, 8.0]\n&gt;&gt;&gt; eudist(v1, v2)\n7.0710678118654755\n</code></pre> Source code in <code>src/cellects/utils/formulas.py</code> <pre><code>def eudist(v1, v2) -&gt; float:\n    \"\"\"\n    Calculate the Euclidean distance between two points in n-dimensional space.\n\n    Parameters\n    ----------\n    v1 : iterable of float\n        The coordinates of the first point.\n    v2 : iterable of float\n        The coordinates of the second point.\n\n    Returns\n    -------\n    float\n        The Euclidean distance between `v1` and `v2`.\n\n    Raises\n    ------\n    ValueError\n        If `v1` and `v2` do not have the same length.\n\n    Notes\n    -----\n    The Euclidean distance is calculated using the standard formula:\n    \u221a((x2 \u2212 x1)^2 + (y2 \u2212 y1)^2 + ...).\n\n    Examples\n    --------\n    &gt;&gt;&gt; v1 = [1.0, 2.0]\n    &gt;&gt;&gt; v2 = [4.0, 6.0]\n    &gt;&gt;&gt; eudist(v1, v2)\n    5.0\n\n    &gt;&gt;&gt; v1 = [1.0, 2.0, 3.0]\n    &gt;&gt;&gt; v2 = [4.0, 6.0, 8.0]\n    &gt;&gt;&gt; eudist(v1, v2)\n    7.0710678118654755\n    \"\"\"\n    dist = [(a - b)**2 for a, b in zip(v1, v2)]\n    dist = np.sqrt(np.sum(dist))\n    return dist\n</code></pre>"},{"location":"api/cellects/utils/formulas/#cellects.utils.formulas.find_common_coord","title":"<code>find_common_coord(array1, array2)</code>","text":"<p>Find common coordinates between two arrays.</p> <p>This function compares the given 2D <code>array1</code> and <code>array2</code> to determine if there are any common coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>array1</code> <code>ndarray of int</code> <p>A 2D numpy ndarray.</p> required <code>array2</code> <code>ndarray of int</code> <p>Another 2D numpy ndarray.</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>ndarray of bool</code> <p>A boolean numpy ndarray where True indicates common coordinates.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; array1 = np.array([[1, 2], [3, 4]])\n&gt;&gt;&gt; array2 = np.array([[5, 6], [1, 2]])\n&gt;&gt;&gt; result = find_common_coord(array1, array2)\n&gt;&gt;&gt; print(result)\narray([ True, False])\n</code></pre> Source code in <code>src/cellects/utils/formulas.py</code> <pre><code>def find_common_coord(array1: NDArray[int], array2: NDArray[int]) -&gt; NDArray[bool]:\n    \"\"\"Find common coordinates between two arrays.\n\n    This function compares the given 2D `array1` and `array2`\n    to determine if there are any common coordinates.\n\n    Parameters\n    ----------\n    array1 : ndarray of int\n        A 2D numpy ndarray.\n    array2 : ndarray of int\n        Another 2D numpy ndarray.\n\n    Returns\n    -------\n    out : ndarray of bool\n        A boolean numpy ndarray where True indicates common\n        coordinates.\n\n    Examples\n    --------\n    &gt;&gt;&gt; array1 = np.array([[1, 2], [3, 4]])\n    &gt;&gt;&gt; array2 = np.array([[5, 6], [1, 2]])\n    &gt;&gt;&gt; result = find_common_coord(array1, array2)\n    &gt;&gt;&gt; print(result)\n    array([ True, False])\"\"\"\n    return (array1[:, None, :] == array2[None, :, :]).all(-1).any(-1)\n</code></pre>"},{"location":"api/cellects/utils/formulas/#cellects.utils.formulas.find_duplicates_coord","title":"<code>find_duplicates_coord(array1)</code>","text":"<p>Find duplicate rows in a 2D array and return their coordinate indices.</p> <p>Given a 2D NumPy array, this function identifies rows that are duplicated (i.e., appear more than once) and returns a boolean array indicating their positions.</p> <p>Parameters:</p> Name Type Description Default <code>array1</code> <code>ndarray of int</code> <p>Input 2D array of shape (n_rows, n_columns) from which to find duplicate rows.</p> required <p>Returns:</p> Name Type Description <code>duplicates</code> <code>ndarray of bool</code> <p>Boolean array of shape (n_rows,), where <code>True</code> indicates that the corresponding row in <code>array1</code> is a duplicate.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; array1 = np.array([[1, 2], [3, 4], [1, 2], [5, 6]])\n&gt;&gt;&gt; find_duplicates_coord(array1)\narray([ True, False,  True, False])\n</code></pre> Source code in <code>src/cellects/utils/formulas.py</code> <pre><code>def find_duplicates_coord(array1: NDArray[int]) -&gt; NDArray[bool]:\n    \"\"\"\n    Find duplicate rows in a 2D array and return their coordinate indices.\n\n    Given a 2D NumPy array, this function identifies rows that are duplicated (i.e., appear more than once) and returns a boolean array indicating their positions.\n\n    Parameters\n    ----------\n    array1 : ndarray of int\n        Input 2D array of shape (n_rows, n_columns) from which to find duplicate rows.\n\n    Returns\n    -------\n    duplicates : ndarray of bool\n        Boolean array of shape (n_rows,), where `True` indicates that the corresponding row in `array1` is a duplicate.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; array1 = np.array([[1, 2], [3, 4], [1, 2], [5, 6]])\n    &gt;&gt;&gt; find_duplicates_coord(array1)\n    array([ True, False,  True, False])\"\"\"\n    unique_rows, inverse_indices = np.unique(array1, axis=0, return_inverse=True)\n    counts = np.bincount(inverse_indices)\n    # A row is duplicate if its count &gt; 1\n    return counts[inverse_indices] &gt; 1\n</code></pre>"},{"location":"api/cellects/utils/formulas/#cellects.utils.formulas.get_contour_width_from_im_shape","title":"<code>get_contour_width_from_im_shape(im_shape)</code>","text":"<p>Calculate the contour width based on image shape.</p> <p>Parameters:</p> Name Type Description Default <code>im_shape</code> <code>tuple of int, two items</code> <p>The dimensions of the image.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The calculated contour width.</p> Source code in <code>src/cellects/utils/formulas.py</code> <pre><code>def get_contour_width_from_im_shape(im_shape: Tuple) -&gt; int:\n    \"\"\"\n    Calculate the contour width based on image shape.\n\n    Parameters\n    ----------\n    im_shape : tuple of int, two items\n        The dimensions of the image.\n\n    Returns\n    -------\n    int\n        The calculated contour width.\n    \"\"\"\n    return np.max((np.round(np.log10(np.max(im_shape)) - 2).astype(int), 2))\n</code></pre>"},{"location":"api/cellects/utils/formulas/#cellects.utils.formulas.get_inertia_axes","title":"<code>get_inertia_axes(mo)</code>","text":"<p>Calculate the inertia axes of a moment object.</p> <p>This function computes the barycenters, central moments, and the lengths of the major and minor axes, as well as their orientation.</p> <p>Parameters:</p> Name Type Description Default <code>mo</code> <code>dict</code> <p>Dictionary containing moments, which should include keys: 'm00', 'm10', 'm01', 'm20', and 'm11'.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing:     - cx : float         The x-coordinate of the barycenter.     - cy : float         The y-coordinate of the barycenter.     - major_axis_len : float         The length of the major axis.     - minor_axis_len : float         The length of the minor axis.     - axes_orientation : float         The orientation of the axes in radians.</p> Notes <p>This function uses Numba's @njit decorator for performance. The moments in the input dictionary should be computed from the same image region.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; mo = {'m00': 1.0, 'm10': 2.0, 'm01': 3.0, 'm20': 4.0, 'm11': 5.0}\n&gt;&gt;&gt; get_inertia_axes(mo)\n(2.0, 3.0, 9.165151389911677, 0.8421875803239, 0.7853981633974483)\n</code></pre> Source code in <code>src/cellects/utils/formulas.py</code> <pre><code>@njit()\ndef get_inertia_axes(mo: dict) -&gt; Tuple[float, float, float, float, float]:\n    \"\"\"\n    Calculate the inertia axes of a moment object.\n\n    This function computes the barycenters, central moments,\n    and the lengths of the major and minor axes, as well as\n    their orientation.\n\n    Parameters\n    ----------\n    mo : dict\n        Dictionary containing moments, which should include keys:\n        'm00', 'm10', 'm01', 'm20', and 'm11'.\n\n    Returns\n    -------\n    tuple\n        A tuple containing:\n            - cx : float\n                The x-coordinate of the barycenter.\n            - cy : float\n                The y-coordinate of the barycenter.\n            - major_axis_len : float\n                The length of the major axis.\n            - minor_axis_len : float\n                The length of the minor axis.\n            - axes_orientation : float\n                The orientation of the axes in radians.\n\n    Notes\n    -----\n    This function uses Numba's @njit decorator for performance.\n    The moments in the input dictionary should be computed from\n    the same image region.\n\n    Examples\n    --------\n    &gt;&gt;&gt; mo = {'m00': 1.0, 'm10': 2.0, 'm01': 3.0, 'm20': 4.0, 'm11': 5.0}\n    &gt;&gt;&gt; get_inertia_axes(mo)\n    (2.0, 3.0, 9.165151389911677, 0.8421875803239, 0.7853981633974483)\n\n    \"\"\"\n    #L. Rocha, L. Velho and P.C.P. Calvalho (2002)\n    #http://sibgrapi.sid.inpe.br/col/sid.inpe.br/banon/2002/10.23.11.34/doc/35.pdf\n    # http://raphael.candelier.fr/?blog=Image%20Moments\n\n    # Calculate barycenters\n    cx = mo[\"m10\"] / mo[\"m00\"]\n    cy = mo[\"m01\"] / mo[\"m00\"]\n    # Calculate central moments\n    c20 = (mo[\"m20\"] / mo[\"m00\"]) - np.square(cx)\n    c02 = (mo[\"m02\"] / mo[\"m00\"]) - np.square(cy)\n    c11 = (mo[\"m11\"] / mo[\"m00\"]) - (cx * cy)\n    # Calculate major and minor axi lengths OK\n    major_axis_len = np.sqrt(6 * (c20 + c02 + np.sqrt(np.square(2 * c11) + np.square(c20 - c02))))\n    minor_axis_len = np.sqrt(6 * (c20 + c02 - np.sqrt(np.square(2 * c11) + np.square(c20 - c02))))\n    if (c20 - c02) != 0:\n        axes_orientation = (0.5 * np.arctan((2 * c11) / (c20 - c02))) + ((c20 &lt; c02) * (np.pi /2))\n    else:\n        axes_orientation = 0.\n    return cx, cy, major_axis_len, minor_axis_len, axes_orientation\n</code></pre>"},{"location":"api/cellects/utils/formulas/#cellects.utils.formulas.get_kurtosis","title":"<code>get_kurtosis(mo, binary_image, cx, cy, sx, sy)</code>","text":"<p>Calculate the kurtosis of a binary image.</p> <p>The function calculates the fourth moment (kurtosis) of the given binary image around the specified center coordinates with an option to specify the size of the square window.</p> <p>Parameters:</p> Name Type Description Default <code>mo</code> <code>dict</code> <p>Dictionary containing moments of binary image.</p> required <code>binary_image</code> <code>ndarray</code> <p>A 2D numpy ndarray representing a binary image.</p> required <code>cx</code> <code>int or float</code> <p>The x-coordinate of the center point of the square window.</p> required <code>cy</code> <code>int or float</code> <p>The y-coordinate of the center point of the square window.</p> required <code>sx</code> <code>int or float</code> <p>The x-length of the square window (width).</p> required <code>sy</code> <code>int or float</code> <p>The y-length of the square window (height).</p> required <p>Returns:</p> Type Description <code>float</code> <p>The kurtosis value calculated from the moments.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; mo = np.array([[0, 1], [2, 3]])\n&gt;&gt;&gt; binary_image = np.array([[1, 0], [0, 1]])\n&gt;&gt;&gt; cx = 2\n&gt;&gt;&gt; cy = 3\n&gt;&gt;&gt; sx = 5\n&gt;&gt;&gt; sy = 6\n&gt;&gt;&gt; result = get_kurtosis(mo, binary_image, cx, cy, sx, sy)\n&gt;&gt;&gt; print(result)\nexpected output\n</code></pre> Source code in <code>src/cellects/utils/formulas.py</code> <pre><code>def get_kurtosis(mo: dict, binary_image: NDArray, cx: float, cy: float, sx: float, sy: float) -&gt; Tuple[float, float]:\n    \"\"\"\n    Calculate the kurtosis of a binary image.\n\n    The function calculates the fourth moment (kurtosis) of the given\n    binary image around the specified center coordinates with an option\n    to specify the size of the square window.\n\n    Parameters\n    ----------\n    mo : dict\n        Dictionary containing moments of binary image.\n    binary_image : np.ndarray\n        A 2D numpy ndarray representing a binary image.\n    cx : int or float\n        The x-coordinate of the center point of the square window.\n    cy : int or float\n        The y-coordinate of the center point of the square window.\n    sx : int or float\n        The x-length of the square window (width).\n    sy : int or float\n        The y-length of the square window (height).\n\n    Returns\n    -------\n    float\n        The kurtosis value calculated from the moments.\n\n    Examples\n    --------\n    &gt;&gt;&gt; mo = np.array([[0, 1], [2, 3]])\n    &gt;&gt;&gt; binary_image = np.array([[1, 0], [0, 1]])\n    &gt;&gt;&gt; cx = 2\n    &gt;&gt;&gt; cy = 3\n    &gt;&gt;&gt; sx = 5\n    &gt;&gt;&gt; sy = 6\n    &gt;&gt;&gt; result = get_kurtosis(mo, binary_image, cx, cy, sx, sy)\n    &gt;&gt;&gt; print(result)\n    expected output\n    \"\"\"\n    x4, y4 = get_power_dists(binary_image, cx, cy, 4)\n    X4, Y4 = np.meshgrid(x4, y4)\n    m4x, m4y = get_var(mo, binary_image, X4, Y4)\n    return get_skewness_kurtosis(m4x, m4y, sx, sy, 4)\n</code></pre>"},{"location":"api/cellects/utils/formulas/#cellects.utils.formulas.get_newly_explored_area","title":"<code>get_newly_explored_area(binary_vid)</code>","text":"<p>Get newly explored area in a binary video.</p> <p>Calculate the number of new pixels that have become active (==1) from the previous frame in a binary video representation.</p> <p>Parameters:</p> Name Type Description Default <code>binary_vid</code> <code>ndarray</code> <p>The current frame of the binary video.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>An array containing the number of new active pixels for each row.</p> Notes <p>This function uses Numba's @njit decorator for performance.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; binary_vid=np.zeros((4, 5, 5), dtype=np.uint8)\n&gt;&gt;&gt; binary_vid[:2, 3, 3] = 1\n&gt;&gt;&gt; binary_vid[1, 4, 3] = 1\n&gt;&gt;&gt; binary_vid[2, 3, 4] = 1\n&gt;&gt;&gt; binary_vid[3, 2, 3] = 1\n&gt;&gt;&gt; get_newly_explored_area(binary_vid)\narray([0, 1, 1, 1])\n</code></pre> <pre><code>&gt;&gt;&gt; binary_vid=np.zeros((5, 5), dtype=np.uint8)[None, :, :]\n&gt;&gt;&gt; get_newly_explored_area(binary_vid)\narray([0])\n</code></pre> Source code in <code>src/cellects/utils/formulas.py</code> <pre><code>@njit()\ndef get_newly_explored_area(binary_vid: NDArray[np.uint8]) -&gt; NDArray:\n    \"\"\"\n    Get newly explored area in a binary video.\n\n    Calculate the number of new pixels that have become active (==1) from\n    the previous frame in a binary video representation.\n\n    Parameters\n    ----------\n    binary_vid : np.ndarray\n        The current frame of the binary video.\n\n    Returns\n    -------\n    np.ndarray\n        An array containing the number of new active pixels for each row.\n\n    Notes\n    -----\n    This function uses Numba's @njit decorator for performance.\n\n    Examples\n    --------\n    &gt;&gt;&gt; binary_vid=np.zeros((4, 5, 5), dtype=np.uint8)\n    &gt;&gt;&gt; binary_vid[:2, 3, 3] = 1\n    &gt;&gt;&gt; binary_vid[1, 4, 3] = 1\n    &gt;&gt;&gt; binary_vid[2, 3, 4] = 1\n    &gt;&gt;&gt; binary_vid[3, 2, 3] = 1\n    &gt;&gt;&gt; get_newly_explored_area(binary_vid)\n    array([0, 1, 1, 1])\n\n    &gt;&gt;&gt; binary_vid=np.zeros((5, 5), dtype=np.uint8)[None, :, :]\n    &gt;&gt;&gt; get_newly_explored_area(binary_vid)\n    array([0])\n    \"\"\"\n    return ((binary_vid - binary_vid[0, ...]) == 1).reshape(binary_vid.shape[0], - 1).sum(1)\n</code></pre>"},{"location":"api/cellects/utils/formulas/#cellects.utils.formulas.get_power_dists","title":"<code>get_power_dists(binary_image, cx, cy, n)</code>","text":"<p>Calculate the power distributions based on the given center coordinates and exponent.</p> <p>This function computes the <code>n</code>th powers of x and y distances from a given center point <code>(cx, cy)</code> for each pixel in the binary image.</p> <p>Parameters:</p> Name Type Description Default <code>binary_image</code> <code>ndarray</code> <p>A 2D array (binary image) where the power distributions are calculated.</p> required <code>cx</code> <code>float</code> <p>The x-coordinate of the center point.</p> required <code>cy</code> <code>float</code> <p>The y-coordinate of the center point.</p> required <code>n</code> <code>int</code> <p>The exponent for power distribution calculation.</p> required <p>Returns:</p> Type Description <code>tuple[ndarray, ndarray]</code> <p>A tuple containing two arrays: - The first array contains the <code>n</code>th power of x distances from the center. - The second array contains the <code>n</code>th power of y distances from the center.</p> Notes <p>This function uses Numba's <code>@njit</code> decorator for performance optimization. Ensure that <code>binary_image</code> is a NumPy ndarray to avoid type issues.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; binary_image = np.zeros((10, 10))\n&gt;&gt;&gt; xn, yn = get_power_dists(binary_image, 5.0, 5.0, 2)\n&gt;&gt;&gt; print(xn.shape), print(yn.shape)\n(10,) (10,)\n</code></pre> Source code in <code>src/cellects/utils/formulas.py</code> <pre><code>@njit()\ndef get_power_dists(binary_image: np.ndarray, cx: float, cy: float, n: int):\n    \"\"\"\n    Calculate the power distributions based on the given center coordinates and exponent.\n\n    This function computes the `n`th powers of x and y distances from\n    a given center point `(cx, cy)` for each pixel in the binary image.\n\n    Parameters\n    ----------\n    binary_image : np.ndarray\n        A 2D array (binary image) where the power distributions are calculated.\n    cx : float\n        The x-coordinate of the center point.\n    cy : float\n        The y-coordinate of the center point.\n    n : int\n        The exponent for power distribution calculation.\n\n    Returns\n    -------\n    tuple[np.ndarray, np.ndarray]\n        A tuple containing two arrays:\n        - The first array contains the `n`th power of x distances from the center.\n        - The second array contains the `n`th power of y distances from the center.\n\n    Notes\n    -----\n    This function uses Numba's `@njit` decorator for performance optimization.\n    Ensure that `binary_image` is a NumPy ndarray to avoid type issues.\n\n    Examples\n    --------\n    &gt;&gt;&gt; binary_image = np.zeros((10, 10))\n    &gt;&gt;&gt; xn, yn = get_power_dists(binary_image, 5.0, 5.0, 2)\n    &gt;&gt;&gt; print(xn.shape), print(yn.shape)\n    (10,) (10,)\n    \"\"\"\n    xn = (np.arange(binary_image.shape[1]) - cx) ** n\n    yn = (np.arange(binary_image.shape[0]) - cy) ** n\n    return xn, yn\n</code></pre>"},{"location":"api/cellects/utils/formulas/#cellects.utils.formulas.get_skewness","title":"<code>get_skewness(mo, binary_image, cx, cy, sx, sy)</code>","text":"<p>Calculate skewness of the given moment.</p> <p>This function computes the skewness based on the third moments and the central moments of a binary image.</p> <p>Parameters:</p> Name Type Description Default <code>mo</code> <code>dict</code> <p>Dictionary containing moments of binary image.</p> required <code>binary_image</code> <code>ndarray</code> <p>Binary image as a 2D numpy array.</p> required <code>cx</code> <code>float</code> <p>Description of parameter <code>cx</code>.</p> required <code>cy</code> <code>float</code> <p>Description of parameter <code>cy</code>.</p> required <code>sx</code> <code>float</code> <p>Description of parameter <code>sx</code>.</p> required <code>sy</code> <code>float</code> <p>Description of parameter <code>sy</code>.</p> required <p>Returns:</p> Type Description <code>Tuple[float, float]</code> <p>Tuple containing skewness values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; result = get_skewness(mo=example_mo, binary_image=binary_img,\n... cx=0.5, cy=0.5, sx=1.0, sy=1.0)\n&gt;&gt;&gt; print(result)\n(skewness_x, skewness_y)  # Example output\n</code></pre> Source code in <code>src/cellects/utils/formulas.py</code> <pre><code>def get_skewness(mo: dict, binary_image: NDArray, cx: float, cy: float, sx: float, sy: float) -&gt; Tuple[float, float]:\n    \"\"\"Calculate skewness of the given moment.\n\n    This function computes the skewness based on the third moments\n    and the central moments of a binary image.\n\n    Parameters\n    ----------\n    mo : dict\n        Dictionary containing moments of binary image.\n    binary_image : ndarray\n        Binary image as a 2D numpy array.\n    cx : float\n        Description of parameter `cx`.\n    cy : float\n        Description of parameter `cy`.\n    sx : float\n        Description of parameter `sx`.\n    sy : float\n        Description of parameter `sy`.\n\n    Returns\n    -------\n    Tuple[float, float]\n        Tuple containing skewness values.\n\n    Examples\n    --------\n    &gt;&gt;&gt; result = get_skewness(mo=example_mo, binary_image=binary_img,\n    ... cx=0.5, cy=0.5, sx=1.0, sy=1.0)\n    &gt;&gt;&gt; print(result)\n    (skewness_x, skewness_y)  # Example output\n    \"\"\"\n    x3, y3 = get_power_dists(binary_image, cx, cy, 3)\n    X3, Y3 = np.meshgrid(x3, y3)\n    m3x, m3y = get_var(mo, binary_image, X3, Y3)\n    return get_skewness_kurtosis(m3x, m3y, sx, sy, 3)\n</code></pre>"},{"location":"api/cellects/utils/formulas/#cellects.utils.formulas.get_skewness_kurtosis","title":"<code>get_skewness_kurtosis(mnx, mny, sx, sy, n)</code>","text":"<p>Calculates skewness and kurtosis of a distribution.</p> <p>This function computes the skewness and kurtosis from given statistical moments, standard deviations, and order of moments.</p> <p>Parameters:</p> Name Type Description Default <code>mnx</code> <code>float</code> <p>The third moment about the mean for x.</p> required <code>mny</code> <code>float</code> <p>The fourth moment about the mean for y.</p> required <code>sx</code> <code>float</code> <p>The standard deviation of x.</p> required <code>sy</code> <code>float</code> <p>The standard deviation of y.</p> required <code>n</code> <code>int</code> <p>Order of the moment (3 for skewness, 4 for kurtosis).</p> required <p>Returns:</p> Name Type Description <code>skewness</code> <code>float</code> <p>The computed skewness.</p> <code>kurtosis</code> <code>float</code> <p>The computed kurtosis.</p> Notes <p>This function uses Numba's <code>@njit</code> decorator for performance. Ensure that the values of <code>mnx</code>, <code>mny</code>, <code>sx</code>, and <code>sy</code> are non-zero to avoid division by zero. If <code>n = 3</code>, the function calculates skewness. If <code>n = 4</code>, it calculates kurtosis.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; skewness, kurtosis = get_skewness_kurtosis(1.5, 2.0, 0.5, 0.75, 3)\n&gt;&gt;&gt; print(\"Skewness:\", skewness)\nSkewness: 8.0\n&gt;&gt;&gt; print(\"Kurtosis:\", kurtosis)\nKurtosis: nan\n</code></pre> Source code in <code>src/cellects/utils/formulas.py</code> <pre><code>@njit()\ndef get_skewness_kurtosis(mnx: float, mny: float, sx: float, sy: float, n: int) -&gt; Tuple[float, float]:\n    \"\"\"\n    Calculates skewness and kurtosis of a distribution.\n\n    This function computes the skewness and kurtosis from given statistical\n    moments, standard deviations, and order of moments.\n\n    Parameters\n    ----------\n    mnx : float\n        The third moment about the mean for x.\n    mny : float\n        The fourth moment about the mean for y.\n    sx : float\n        The standard deviation of x.\n    sy : float\n        The standard deviation of y.\n    n : int\n        Order of the moment (3 for skewness, 4 for kurtosis).\n\n    Returns\n    -------\n    skewness : float\n        The computed skewness.\n    kurtosis : float\n        The computed kurtosis.\n\n    Notes\n    -----\n    This function uses Numba's `@njit` decorator for performance.\n    Ensure that the values of `mnx`, `mny`, `sx`, and `sy` are non-zero to avoid division by zero.\n    If `n = 3`, the function calculates skewness. If `n = 4`, it calculates kurtosis.\n\n    Examples\n    --------\n    &gt;&gt;&gt; skewness, kurtosis = get_skewness_kurtosis(1.5, 2.0, 0.5, 0.75, 3)\n    &gt;&gt;&gt; print(\"Skewness:\", skewness)\n    Skewness: 8.0\n    &gt;&gt;&gt; print(\"Kurtosis:\", kurtosis)\n    Kurtosis: nan\n\n    \"\"\"\n    if sx == 0:\n        fx = 0\n    else:\n        fx = mnx / sx ** n\n\n    if sy == 0:\n        fy = 0\n    else:\n        fy = mny / sy ** n\n\n    return fx, fy\n</code></pre>"},{"location":"api/cellects/utils/formulas/#cellects.utils.formulas.get_standard_deviations","title":"<code>get_standard_deviations(mo, binary_image, cx, cy)</code>","text":"<p>Return spatial standard deviations for a given moment and binary image.</p> <p>This function computes the square root of variances along <code>x</code> (horizontal) and <code>y</code> (vertical) axes for the given binary image and moment.</p> <p>Parameters:</p> Name Type Description Default <code>mo</code> <code>dict</code> <p>Dictionary containing moments of binary image.</p> required <code>binary_image</code> <code>ndarray of bool or int8</code> <p>The binary input image where the moments are computed.</p> required <code>cx</code> <code>float64</code> <p>X-coordinate of center of mass (horizontal position).</p> required <code>cy</code> <code>float64</code> <p>Y-coordinate of center of mass (vertical position).</p> required <p>Returns:</p> Type Description <code>tuple[ndarray of float64, ndarray of float64]</code> <p>Tuple containing the standard deviations along the x and y axes.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>binary_image</code> is not a binary image or has an invalid datatype.</p> Notes <p>This function uses the <code>get_power_dists</code> and <code>get_var</code> functions to compute the distributed variances, which are then transformed into standard deviations.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; binary_image = np.array([[0, 1], [1, 0]], dtype=np.int8)\n&gt;&gt;&gt; mo = np.array([[2.0], [3.0]])\n&gt;&gt;&gt; cx, cy = 1.5, 1.5\n&gt;&gt;&gt; stdx, stdy = get_standard_deviations(mo, binary_image, cx, cy)\n&gt;&gt;&gt; print(stdx)\n[1.1]\n&gt;&gt;&gt; print(stdy)\n[0.8366600265...]\n</code></pre> Source code in <code>src/cellects/utils/formulas.py</code> <pre><code>def get_standard_deviations(mo: dict, binary_image: NDArray, cx: float, cy: float) -&gt; Tuple[float, float]:\n    \"\"\"\n    Return spatial standard deviations for a given moment and binary image.\n\n    This function computes the square root of variances along `x` (horizontal)\n    and `y` (vertical) axes for the given binary image and moment.\n\n    Parameters\n    ----------\n    mo : dict\n        Dictionary containing moments of binary image.\n    binary_image : ndarray of bool or int8\n        The binary input image where the moments are computed.\n    cx : float64\n        X-coordinate of center of mass (horizontal position).\n    cy : float64\n        Y-coordinate of center of mass (vertical position).\n\n    Returns\n    -------\n    tuple[ndarray of float64, ndarray of float64]\n        Tuple containing the standard deviations along the x and y axes.\n\n    Raises\n    ------\n    ValueError\n        If `binary_image` is not a binary image or has an invalid datatype.\n\n    Notes\n    -----\n    This function uses the `get_power_dists` and `get_var` functions to compute\n    the distributed variances, which are then transformed into standard deviations.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; binary_image = np.array([[0, 1], [1, 0]], dtype=np.int8)\n    &gt;&gt;&gt; mo = np.array([[2.0], [3.0]])\n    &gt;&gt;&gt; cx, cy = 1.5, 1.5\n    &gt;&gt;&gt; stdx, stdy = get_standard_deviations(mo, binary_image, cx, cy)\n    &gt;&gt;&gt; print(stdx)\n    [1.1]\n    &gt;&gt;&gt; print(stdy)\n    [0.8366600265...]\n    \"\"\"\n    x2, y2 = get_power_dists(binary_image, cx, cy, 2)\n    X2, Y2 = np.meshgrid(x2, y2)\n    vx, vy = get_var(mo, binary_image, X2, Y2)\n    return np.sqrt(vx), np.sqrt(vy)\n</code></pre>"},{"location":"api/cellects/utils/formulas/#cellects.utils.formulas.get_var","title":"<code>get_var(mo, binary_image, Xn, Yn)</code>","text":"<p>Compute the center of mass in 2D space.</p> <p>This function calculates the weighted average position (centroid) of a binary image using given pixel coordinates and moments.</p> <p>Parameters:</p> Name Type Description Default <code>mo</code> <code>dict</code> <p>Dictionary containing moments of binary image.</p> required <code>binary_image</code> <code>ndarray</code> <p>2D binary image where non-zero pixels are considered.</p> required <code>Xn</code> <code>ndarray</code> <p>Array of x-coordinates for each pixel in <code>binary_image</code>.</p> required <code>Yn</code> <code>ndarray</code> <p>Array of y-coordinates for each pixel in <code>binary_image</code>.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple of two floats <code>(vx, vy)</code> representing the centroid coordinates.</p> <p>Raises:</p> Type Description <code>ZeroDivisionError</code> <p>If <code>mo['m00']</code> is zero, indicating no valid pixels in the image. The function raises a <code>ZeroDivisionError</code>.</p> Notes <p>Performance considerations: This function uses Numba's <code>@njit</code> decorator for performance.</p> Source code in <code>src/cellects/utils/formulas.py</code> <pre><code>@njit()\ndef get_var(mo: dict, binary_image: NDArray, Xn: NDArray, Yn: NDArray) -&gt; Tuple[float, float]:\n    \"\"\"\n    Compute the center of mass in 2D space.\n\n    This function calculates the weighted average position (centroid) of\n    a binary image using given pixel coordinates and moments.\n\n    Parameters\n    ----------\n    mo : dict\n        Dictionary containing moments of binary image.\n    binary_image : ndarray\n        2D binary image where non-zero pixels are considered.\n    Xn : ndarray\n        Array of x-coordinates for each pixel in `binary_image`.\n    Yn : ndarray\n        Array of y-coordinates for each pixel in `binary_image`.\n\n    Returns\n    -------\n    tuple\n        A tuple of two floats `(vx, vy)` representing the centroid coordinates.\n\n    Raises\n    ------\n    ZeroDivisionError\n        If `mo['m00']` is zero, indicating no valid pixels in the image.\n        The function raises a `ZeroDivisionError`.\n\n    Notes\n    -----\n    Performance considerations: This function uses Numba's `@njit` decorator for performance.\n    \"\"\"\n    if mo['m00'] == 0:\n        vx, vy = 0., 0.\n    else:\n        vx = np.sum(binary_image * Xn) / mo[\"m00\"]\n        vy = np.sum(binary_image * Yn) / mo[\"m00\"]\n    return vx, vy\n</code></pre>"},{"location":"api/cellects/utils/formulas/#cellects.utils.formulas.linear_model","title":"<code>linear_model(x, a, b)</code>","text":"<p>Perform a linear transformation on input data using slope and intercept.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array_like</code> <p>Input data.</p> required <code>a</code> <code>float</code> <p>Slope coefficient.</p> required <code>b</code> <code>float</code> <p>Intercept.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Resulting value from linear transformation: <code>a</code> * <code>x</code> + <code>b</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; result = linear_model(5, 2.0, 1.5)\n&gt;&gt;&gt; print(result)\n11.5\n</code></pre> Notes <p>This function uses Numba's @njit decorator for performance.</p> Source code in <code>src/cellects/utils/formulas.py</code> <pre><code>@njit()\ndef linear_model(x: NDArray, a: float, b: float) -&gt; float:\n    \"\"\"\n    Perform a linear transformation on input data using slope and intercept.\n\n    Parameters\n    ----------\n    x : array_like\n        Input data.\n    a : float\n        Slope coefficient.\n    b : float\n        Intercept.\n\n    Returns\n    -------\n    float\n        Resulting value from linear transformation: `a` * `x` + `b`.\n\n    Examples\n    --------\n    &gt;&gt;&gt; result = linear_model(5, 2.0, 1.5)\n    &gt;&gt;&gt; print(result)  # doctest: +SKIP\n    11.5\n\n    Notes\n    -----\n    This function uses Numba's @njit decorator for performance.\n    \"\"\"\n    return a * x + b\n</code></pre>"},{"location":"api/cellects/utils/formulas/#cellects.utils.formulas.moving_average","title":"<code>moving_average(vector, step)</code>","text":"<p>Calculate the moving average of a given vector with specified step size.</p> <p>Computes the moving average of input <code>vector</code> using specified <code>step</code> size. NaN values are treated as zeros in the calculation to allow for continuous averaging.</p> <p>Parameters:</p> Name Type Description Default <code>vector</code> <code>ndarray</code> <p>Input vector for which to calculate the moving average.</p> required <code>step</code> <code>int</code> <p>Size of the window for computing the moving average.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Vector containing the moving averages of the input vector.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>step</code> is less than 1.</p> <code>ValueError</code> <p>If the input vector has no valid (non-NaN) elements.</p> Notes <ul> <li>The function considers NaN values as zeros during the averaging process.</li> <li>If <code>step</code> is greater than or equal to the length of the vector, a warning will be raised.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; vector = np.array([1.0, 2.0, np.nan, 4.0, 5.0])\n&gt;&gt;&gt; step = 3\n&gt;&gt;&gt; result = moving_average(vector, step)\n&gt;&gt;&gt; print(result)\n[1.5 2.33333333 3.66666667 4.         nan]\n</code></pre> Source code in <code>src/cellects/utils/formulas.py</code> <pre><code>def moving_average(vector: NDArray, step: int) -&gt; NDArray[float]:\n    \"\"\"\n    Calculate the moving average of a given vector with specified step size.\n\n    Computes the moving average of input `vector` using specified `step`\n    size. NaN values are treated as zeros in the calculation to allow\n    for continuous averaging.\n\n    Parameters\n    ----------\n    vector : ndarray\n        Input vector for which to calculate the moving average.\n    step : int\n        Size of the window for computing the moving average.\n\n    Returns\n    -------\n    numpy.ndarray\n        Vector containing the moving averages of the input vector.\n\n    Raises\n    ------\n    ValueError\n        If `step` is less than 1.\n    ValueError\n        If the input vector has no valid (non-NaN) elements.\n\n    Notes\n    -----\n    - The function considers NaN values as zeros during the averaging process.\n    - If `step` is greater than or equal to the length of the vector, a warning will be raised.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; vector = np.array([1.0, 2.0, np.nan, 4.0, 5.0])\n    &gt;&gt;&gt; step = 3\n    &gt;&gt;&gt; result = moving_average(vector, step)\n    &gt;&gt;&gt; print(result)\n    [1.5 2.33333333 3.66666667 4.         nan]\n    \"\"\"\n    substep = np.array((- int(np.floor((step - 1) / 2)), int(np.ceil((step - 1) / 2))))\n    sums = np.zeros(vector.shape)\n    n_okays = deepcopy(sums)\n    true_numbers = np.logical_not(np.isnan(vector))\n    vector[np.logical_not(true_numbers)] = 0\n    for step_i in np.arange(substep[1] + 1):\n        sums[step_i: (sums.size - step_i)] = sums[step_i: (sums.size - step_i)] + vector[(2 * step_i):]\n        n_okays[step_i: (sums.size - step_i)] = n_okays[step_i: (sums.size - step_i)] + true_numbers[(2 * step_i):]\n        if np.logical_and(step_i &gt; 0, step_i &lt; np.absolute(substep[0])):\n            sums[step_i: (sums.size - step_i)] = sums[step_i: (sums.size - step_i)] + vector[:(sums.size - (2 * step_i)):]\n            n_okays[step_i: (sums.size - step_i)] = n_okays[step_i: (sums.size - step_i)] + true_numbers[:(\n                        true_numbers.size - (2 * step_i))]\n    vector = sums / n_okays\n    return vector\n</code></pre>"},{"location":"api/cellects/utils/formulas/#cellects.utils.formulas.scale_coordinates","title":"<code>scale_coordinates(coord, scale, dims)</code>","text":"<p>Scale coordinates based on given scale factors and dimensions.</p> <p>Parameters:</p> Name Type Description Default <code>coord</code> <code>ndarray</code> <p>A 2x2 array of coordinates to be scaled.</p> required <code>scale</code> <code>tuple of float</code> <p>Scaling factors for the x and y coordinates, respectively.</p> required <code>dims</code> <code>tuple of int</code> <p>Maximum dimensions (height, width) for the scaled coordinates.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Scaled and rounded coordinates.</p> <code>int</code> <p>Minimum y-coordinate.</p> <code>int</code> <p>Maximum y-coordinate.</p> <code>int</code> <p>Minimum x-coordinate.</p> <code>int</code> <p>Maximum x-coordinate.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; coord = np.array(((47, 38), (59, 37)))\n&gt;&gt;&gt; scale = (0.92, 0.87)\n&gt;&gt;&gt; dims = (245, 300, 3)\n&gt;&gt;&gt; scaled_coord, min_y, max_y, min_x, max_x = scale_coordinates(coord, scale, dims)\n&gt;&gt;&gt; scaled_coord\narray([[43, 33],\n       [54, 32]])\n&gt;&gt;&gt; min_y, max_y\n(np.int64(43), np.int64(54))\n&gt;&gt;&gt; min_x, max_x\n(np.int64(32), np.int64(33))\n</code></pre> Notes <p>This function assumes that the input coordinates are in a specific format and will fail if not. The scaling factors should be positive.</p> Source code in <code>src/cellects/utils/formulas.py</code> <pre><code>def scale_coordinates(coord: NDArray, scale: Tuple, dims: Tuple) -&gt; Tuple[NDArray[np.int64], np.int64, np.int64, np.int64, np.int64]:\n    \"\"\"\n    Scale coordinates based on given scale factors and dimensions.\n\n    Parameters\n    ----------\n    coord : numpy.ndarray\n        A 2x2 array of coordinates to be scaled.\n    scale : tuple of float\n        Scaling factors for the x and y coordinates, respectively.\n    dims : tuple of int\n        Maximum dimensions (height, width) for the scaled coordinates.\n\n    Returns\n    -------\n    numpy.ndarray\n        Scaled and rounded coordinates.\n    int\n        Minimum y-coordinate.\n    int\n        Maximum y-coordinate.\n    int\n        Minimum x-coordinate.\n    int\n        Maximum x-coordinate.\n\n    Examples\n    --------\n    &gt;&gt;&gt; coord = np.array(((47, 38), (59, 37)))\n    &gt;&gt;&gt; scale = (0.92, 0.87)\n    &gt;&gt;&gt; dims = (245, 300, 3)\n    &gt;&gt;&gt; scaled_coord, min_y, max_y, min_x, max_x = scale_coordinates(coord, scale, dims)\n    &gt;&gt;&gt; scaled_coord\n    array([[43, 33],\n           [54, 32]])\n    &gt;&gt;&gt; min_y, max_y\n    (np.int64(43), np.int64(54))\n    &gt;&gt;&gt; min_x, max_x\n    (np.int64(32), np.int64(33))\n\n    Notes\n    -----\n    This function assumes that the input coordinates are in a specific format\n    and will fail if not. The scaling factors should be positive.\n    \"\"\"\n    coord = np.array(((np.round(coord[0][0] * scale[0]), np.round(coord[0][1] * scale[1])),\n                    (np.round(coord[1][0] * scale[0]), np.round(coord[1][1] * scale[1]))), dtype=np.int64)\n    min_y = np.max((0, np.min(coord[:, 0])))\n    max_y = np.min((dims[0], np.max(coord[:, 0])))\n    min_x = np.max((0, np.min(coord[:, 1])))\n    max_x = np.min((dims[1], np.max(coord[:, 1])))\n    return coord, min_y, max_y, min_x, max_x\n</code></pre>"},{"location":"api/cellects/utils/formulas/#cellects.utils.formulas.sum_of_abs_differences","title":"<code>sum_of_abs_differences(array1, array2)</code>","text":"<p>Compute the sum of absolute differences between two arrays.</p> <p>Parameters:</p> Name Type Description Default <code>array1</code> <code>NDArray</code> <p>The first input array.</p> required <code>array2</code> <code>NDArray</code> <p>The second input array.</p> required <p>Returns:</p> Type Description <code>int</code> <p>Sum of absolute differences between elements of <code>array1</code> and <code>array2</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; arr1 = np.array([1.2, 2.5, -3.7])\n&gt;&gt;&gt; arr2 = np.array([12, 25, -37])\n&gt;&gt;&gt; result = sum_of_abs_differences(arr1, arr2)\n&gt;&gt;&gt; print(result)\n66.6\n</code></pre> Source code in <code>src/cellects/utils/formulas.py</code> <pre><code>@njit()\ndef sum_of_abs_differences(array1: NDArray, array2: NDArray):\n    \"\"\"\n    Compute the sum of absolute differences between two arrays.\n\n    Parameters\n    ----------\n    array1 : NDArray\n        The first input array.\n    array2 : NDArray\n        The second input array.\n\n    Returns\n    -------\n    int\n        Sum of absolute differences between elements of `array1` and `array2`.\n\n    Examples\n    --------\n    &gt;&gt;&gt; arr1 = np.array([1.2, 2.5, -3.7])\n    &gt;&gt;&gt; arr2 = np.array([12, 25, -37])\n    &gt;&gt;&gt; result = sum_of_abs_differences(arr1, arr2)\n    &gt;&gt;&gt; print(result)\n    66.6\n    \"\"\"\n    return np.sum(np.absolute(array1 - array2))\n</code></pre>"},{"location":"api/cellects/utils/formulas/#cellects.utils.formulas.to_uint8","title":"<code>to_uint8(an_array)</code>","text":"<p>Convert an array to unsigned 8-bit integers.</p> <p>Parameters:</p> Name Type Description Default <code>an_array</code> <code>ndarray</code> <p>Input array to be converted. It can be of any numeric dtype.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>The input array rounded to the nearest integer and then cast to unsigned 8-bit integers.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>an_array</code> is not a ndarray.</p> Notes <p>This function uses Numba's <code>@njit</code> decorator for performance optimization.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; result = to_uint8(np.array([1.2, 2.5, -3.7]))\n&gt;&gt;&gt; print(result)\n[1 3 0]\n</code></pre> Source code in <code>src/cellects/utils/formulas.py</code> <pre><code>@njit()\ndef to_uint8(an_array: NDArray):\n    \"\"\"\n    Convert an array to unsigned 8-bit integers.\n\n    Parameters\n    ----------\n    an_array : ndarray\n        Input array to be converted. It can be of any numeric dtype.\n\n    Returns\n    -------\n    ndarray\n        The input array rounded to the nearest integer and then cast to\n        unsigned 8-bit integers.\n\n    Raises\n    ------\n    TypeError\n        If `an_array` is not a ndarray.\n\n    Notes\n    -----\n    This function uses Numba's `@njit` decorator for performance optimization.\n\n    Examples\n    --------\n    &gt;&gt;&gt; result = to_uint8(np.array([1.2, 2.5, -3.7]))\n    &gt;&gt;&gt; print(result)\n    [1 3 0]\n    \"\"\"\n    out = np.empty_like(an_array)\n    return np.round(an_array, 0, out).astype(np.uint8)\n</code></pre>"},{"location":"api/cellects/utils/load_display_save/","title":"<code>cellects.utils.load_display_save</code>","text":""},{"location":"api/cellects/utils/load_display_save/#cellects.utils.load_display_save","title":"<code>cellects.utils.load_display_save</code>","text":"<p>This script contains functions and classes to load, display and save various files For example:     - PickleRick: to write and read files without conflicts     - See: Display an image using opencv     - write_video: Write a video on hard drive</p>"},{"location":"api/cellects/utils/load_display_save/#cellects.utils.load_display_save.PickleRick","title":"<code>PickleRick</code>","text":"<p>A class to handle safe file reading and writing operations using pickle.</p> <p>This class ensures that files are not being accessed concurrently by creating a lock file (PickleRickX.pkl) to signal that the file is open. It includes methods to check for the lock file, write data safely, and read data safely.</p> Source code in <code>src/cellects/utils/load_display_save.py</code> <pre><code>class PickleRick:\n    \"\"\"\n    A class to handle safe file reading and writing operations using pickle.\n\n    This class ensures that files are not being accessed concurrently by\n    creating a lock file (PickleRickX.pkl) to signal that the file is open.\n    It includes methods to check for the lock file, write data safely,\n    and read data safely.\n    \"\"\"\n    def __init__(self, pickle_rick_number=\"\"):\n        \"\"\"\n        Initialize a new instance of the class.\n\n        This constructor sets up initial attributes for tracking Rick's state, including\n        a boolean flag for waiting for Pickle Rick, a counter, the provided pickle Rick number,\n        and the time when the first check was performed.\n\n        Parameters\n        ----------\n        pickle_rick_number : str, optional\n            The number associated with Pickle Rick. Defaults to an empty string.\n        \"\"\"\n        self.wait_for_pickle_rick: bool = False\n        self.counter = 0\n        self.pickle_rick_number = pickle_rick_number\n        self.first_check_time = default_timer()\n\n    def _check_that_file_is_not_open(self):\n        \"\"\"\n        Check if a specific pickle file exists and handle it accordingly.\n\n        This function checks whether a file named `PickleRick{self.pickle_rick_number}.pkl`\n        exists. If the file has not been modified for more than 2 seconds, it is removed.\n        The function then updates an attribute to indicate whether the file exists.\n\n        Parameters\n        ----------\n        self : PickleRickObject\n            The instance of the class containing this method.\n\n        Returns\n        -------\n        None\n            This function does not return any value.\n            It updates the `self.wait_for_pickle_rick` attribute.\n\n        Notes\n        -----\n        This function removes the pickle file if it has not been modified for more than 2 seconds.\n        The `self.wait_for_pickle_rick` attribute is updated based on the existence of the file.\n        \"\"\"\n        if os.path.isfile(f\"PickleRick{self.pickle_rick_number}.pkl\"):\n            if default_timer() - self.first_check_time &gt; 2:\n                os.remove(f\"PickleRick{self.pickle_rick_number}.pkl\")\n            # logging.error((f\"Cannot read/write, Trying again... tip: unlock by deleting the file named PickleRick{self.pickle_rick_number}.pkl\"))\n        self.wait_for_pickle_rick = os.path.isfile(f\"PickleRick{self.pickle_rick_number}.pkl\")\n\n    def _write_pickle_rick(self):\n        \"\"\"\n        Write pickle data to a file for Pickle Rick.\n\n        Parameters\n        ----------\n        self : object\n            The instance of the class that this method belongs to.\n            This typically contains attributes and methods relevant to managing\n            pickle operations for Pickle Rick.\n\n        Raises\n        ------\n        Exception\n            General exception raised if there is any issue with writing the file.\n            The error details are logged.\n\n        Notes\n        -----\n        This function creates a file named `PickleRick{self.pickle_rick_number}.pkl`\n        with a dictionary indicating readiness for Pickle Rick.\n\n        Examples\n        --------\n        &gt;&gt;&gt; obj = PickleRick()  # Assuming `YourClassInstance` is the class containing this method\n        &gt;&gt;&gt; obj.pickle_rick_number = 1  # Set an example value for the attribute\n        &gt;&gt;&gt; obj._write_pickle_rick()     # Call the method to create and write to file\n        \"\"\"\n        try:\n            with open(f\"PickleRick{self.pickle_rick_number}.pkl\", 'wb') as file_to_write:\n                pickle.dump({'wait_for_pickle_rick': True}, file_to_write)\n        except Exception as exc:\n            logging.error(f\"Don't know how but Pickle Rick failed... Error is: {exc}\")\n\n    def _delete_pickle_rick(self):\n        \"\"\"\n\n        Delete a specific Pickle Rick file.\n\n        Deletes the pickle file associated with the current instance's\n        `pickle_rick_number`.\n\n        Raises\n        ------\n        FileNotFoundError\n            If the file with name `PickleRick{self.pickle_rick_number}.pkl` does not exist.\n        \"\"\"\n        if os.path.isfile(f\"PickleRick{self.pickle_rick_number}.pkl\"):\n            os.remove(f\"PickleRick{self.pickle_rick_number}.pkl\")\n\n    def write_file(self, file_content, file_name):\n        \"\"\"\n        Write content to a file with error handling and retry logic.\n\n        This function attempts to write the provided content into a file.\n        If it fails, it retries up to 100 times with some additional checks\n        and delays. Note that the content is serialized using pickle.\n\n        Parameters\n        ----------\n        file_content : Any\n            The data to be written into the file. This will be pickled.\n        file_name : str\n            The name of the file where data should be written.\n\n        Returns\n        -------\n        None\n\n        Raises\n        ------\n        Exception\n            If the file cannot be written after 100 attempts, an error is logged.\n\n        Notes\n        -----\n        This function uses pickle to serialize the data, which can introduce security risks\n        if untrusted content is being written. It performs some internal state checks,\n        such as verifying that the target file isn't open and whether it should delete\n        some internal state, represented by `_delete_pickle_rick`.\n\n        The function implements a retry mechanism with a backoff strategy that can include\n        random delays, though the example code does not specify these details explicitly.\n\n        Examples\n        --------\n        &gt;&gt;&gt; result = PickleRick().write_file({'key': 'value'}, 'test.pkl')\n        Success to write file\n        \"\"\"\n        self.counter += 1\n        if self.counter &lt; 100:\n            if self.counter &gt; 95:\n                self._delete_pickle_rick()\n            # time.sleep(np.random.choice(np.arange(1, os.cpu_count(), 0.5)))\n            self._check_that_file_is_not_open()\n            if self.wait_for_pickle_rick:\n                time.sleep(2)\n                self.write_file(file_content, file_name)\n            else:\n                self._write_pickle_rick()\n                try:\n                    with open(file_name, 'wb') as file_to_write:\n                        pickle.dump(file_content, file_to_write, protocol=0)\n                    self._delete_pickle_rick()\n                    logging.info(f\"Success to write file\")\n                except Exception as exc:\n                    logging.error(f\"The Pickle error on the file {file_name} is: {exc}\")\n                    self._delete_pickle_rick()\n                    self.write_file(file_content, file_name)\n        else:\n            logging.error(f\"Failed to write {file_name}\")\n\n    def read_file(self, file_name):\n        \"\"\"\n        Reads the contents of a file using pickle and returns it.\n\n        Parameters\n        ----------\n        file_name : str\n            The name of the file to be read.\n\n        Returns\n        -------\n        Union[Any, None]\n            The content of the file if successfully read; otherwise, `None`.\n\n        Raises\n        ------\n        Exception\n            If there is an error reading the file.\n\n        Notes\n        -----\n        This function attempts to read a file multiple times if it fails.\n        If the number of attempts exceeds 1000, it logs an error and returns `None`.\n\n        Examples\n        --------\n        &gt;&gt;&gt; PickleRick().read_file(\"example.pkl\")\n        \"\"\"\n        self.counter += 1\n        if self.counter &lt; 1000:\n            if self.counter &gt; 950:\n                self._delete_pickle_rick()\n            self._check_that_file_is_not_open()\n            if self.wait_for_pickle_rick:\n                time.sleep(2)\n                self.read_file(file_name)\n            else:\n                self._write_pickle_rick()\n                try:\n                    with open(file_name, 'rb') as fileopen:\n                        file_content = pickle.load(fileopen)\n                except Exception as exc:\n                    logging.error(f\"The Pickle error on the file {file_name} is: {exc}\")\n                    file_content = None\n                self._delete_pickle_rick()\n                if file_content is None:\n                    self.read_file(file_name)\n                else:\n                    logging.info(f\"Success to read file\")\n                return file_content\n        else:\n            logging.error(f\"Failed to read {file_name}\")\n</code></pre>"},{"location":"api/cellects/utils/load_display_save/#cellects.utils.load_display_save.PickleRick.__init__","title":"<code>__init__(pickle_rick_number='')</code>","text":"<p>Initialize a new instance of the class.</p> <p>This constructor sets up initial attributes for tracking Rick's state, including a boolean flag for waiting for Pickle Rick, a counter, the provided pickle Rick number, and the time when the first check was performed.</p> <p>Parameters:</p> Name Type Description Default <code>pickle_rick_number</code> <code>str</code> <p>The number associated with Pickle Rick. Defaults to an empty string.</p> <code>''</code> Source code in <code>src/cellects/utils/load_display_save.py</code> <pre><code>def __init__(self, pickle_rick_number=\"\"):\n    \"\"\"\n    Initialize a new instance of the class.\n\n    This constructor sets up initial attributes for tracking Rick's state, including\n    a boolean flag for waiting for Pickle Rick, a counter, the provided pickle Rick number,\n    and the time when the first check was performed.\n\n    Parameters\n    ----------\n    pickle_rick_number : str, optional\n        The number associated with Pickle Rick. Defaults to an empty string.\n    \"\"\"\n    self.wait_for_pickle_rick: bool = False\n    self.counter = 0\n    self.pickle_rick_number = pickle_rick_number\n    self.first_check_time = default_timer()\n</code></pre>"},{"location":"api/cellects/utils/load_display_save/#cellects.utils.load_display_save.PickleRick.read_file","title":"<code>read_file(file_name)</code>","text":"<p>Reads the contents of a file using pickle and returns it.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>The name of the file to be read.</p> required <p>Returns:</p> Type Description <code>Union[Any, None]</code> <p>The content of the file if successfully read; otherwise, <code>None</code>.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If there is an error reading the file.</p> Notes <p>This function attempts to read a file multiple times if it fails. If the number of attempts exceeds 1000, it logs an error and returns <code>None</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; PickleRick().read_file(\"example.pkl\")\n</code></pre> Source code in <code>src/cellects/utils/load_display_save.py</code> <pre><code>def read_file(self, file_name):\n    \"\"\"\n    Reads the contents of a file using pickle and returns it.\n\n    Parameters\n    ----------\n    file_name : str\n        The name of the file to be read.\n\n    Returns\n    -------\n    Union[Any, None]\n        The content of the file if successfully read; otherwise, `None`.\n\n    Raises\n    ------\n    Exception\n        If there is an error reading the file.\n\n    Notes\n    -----\n    This function attempts to read a file multiple times if it fails.\n    If the number of attempts exceeds 1000, it logs an error and returns `None`.\n\n    Examples\n    --------\n    &gt;&gt;&gt; PickleRick().read_file(\"example.pkl\")\n    \"\"\"\n    self.counter += 1\n    if self.counter &lt; 1000:\n        if self.counter &gt; 950:\n            self._delete_pickle_rick()\n        self._check_that_file_is_not_open()\n        if self.wait_for_pickle_rick:\n            time.sleep(2)\n            self.read_file(file_name)\n        else:\n            self._write_pickle_rick()\n            try:\n                with open(file_name, 'rb') as fileopen:\n                    file_content = pickle.load(fileopen)\n            except Exception as exc:\n                logging.error(f\"The Pickle error on the file {file_name} is: {exc}\")\n                file_content = None\n            self._delete_pickle_rick()\n            if file_content is None:\n                self.read_file(file_name)\n            else:\n                logging.info(f\"Success to read file\")\n            return file_content\n    else:\n        logging.error(f\"Failed to read {file_name}\")\n</code></pre>"},{"location":"api/cellects/utils/load_display_save/#cellects.utils.load_display_save.PickleRick.write_file","title":"<code>write_file(file_content, file_name)</code>","text":"<p>Write content to a file with error handling and retry logic.</p> <p>This function attempts to write the provided content into a file. If it fails, it retries up to 100 times with some additional checks and delays. Note that the content is serialized using pickle.</p> <p>Parameters:</p> Name Type Description Default <code>file_content</code> <code>Any</code> <p>The data to be written into the file. This will be pickled.</p> required <code>file_name</code> <code>str</code> <p>The name of the file where data should be written.</p> required <p>Returns:</p> Type Description <code>None</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If the file cannot be written after 100 attempts, an error is logged.</p> Notes <p>This function uses pickle to serialize the data, which can introduce security risks if untrusted content is being written. It performs some internal state checks, such as verifying that the target file isn't open and whether it should delete some internal state, represented by <code>_delete_pickle_rick</code>.</p> <p>The function implements a retry mechanism with a backoff strategy that can include random delays, though the example code does not specify these details explicitly.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; result = PickleRick().write_file({'key': 'value'}, 'test.pkl')\nSuccess to write file\n</code></pre> Source code in <code>src/cellects/utils/load_display_save.py</code> <pre><code>def write_file(self, file_content, file_name):\n    \"\"\"\n    Write content to a file with error handling and retry logic.\n\n    This function attempts to write the provided content into a file.\n    If it fails, it retries up to 100 times with some additional checks\n    and delays. Note that the content is serialized using pickle.\n\n    Parameters\n    ----------\n    file_content : Any\n        The data to be written into the file. This will be pickled.\n    file_name : str\n        The name of the file where data should be written.\n\n    Returns\n    -------\n    None\n\n    Raises\n    ------\n    Exception\n        If the file cannot be written after 100 attempts, an error is logged.\n\n    Notes\n    -----\n    This function uses pickle to serialize the data, which can introduce security risks\n    if untrusted content is being written. It performs some internal state checks,\n    such as verifying that the target file isn't open and whether it should delete\n    some internal state, represented by `_delete_pickle_rick`.\n\n    The function implements a retry mechanism with a backoff strategy that can include\n    random delays, though the example code does not specify these details explicitly.\n\n    Examples\n    --------\n    &gt;&gt;&gt; result = PickleRick().write_file({'key': 'value'}, 'test.pkl')\n    Success to write file\n    \"\"\"\n    self.counter += 1\n    if self.counter &lt; 100:\n        if self.counter &gt; 95:\n            self._delete_pickle_rick()\n        # time.sleep(np.random.choice(np.arange(1, os.cpu_count(), 0.5)))\n        self._check_that_file_is_not_open()\n        if self.wait_for_pickle_rick:\n            time.sleep(2)\n            self.write_file(file_content, file_name)\n        else:\n            self._write_pickle_rick()\n            try:\n                with open(file_name, 'wb') as file_to_write:\n                    pickle.dump(file_content, file_to_write, protocol=0)\n                self._delete_pickle_rick()\n                logging.info(f\"Success to write file\")\n            except Exception as exc:\n                logging.error(f\"The Pickle error on the file {file_name} is: {exc}\")\n                self._delete_pickle_rick()\n                self.write_file(file_content, file_name)\n    else:\n        logging.error(f\"Failed to write {file_name}\")\n</code></pre>"},{"location":"api/cellects/utils/load_display_save/#cellects.utils.load_display_save.create_empty_videos","title":"<code>create_empty_videos(image_list, cr, lose_accuracy_to_save_memory, already_greyscale, csc_dict)</code>","text":"<p>Create empty video arrays based on input parameters.</p> <p>Parameters:</p> Name Type Description Default <code>image_list</code> <code>list</code> <p>List of images.</p> required <code>cr</code> <code>list</code> <p>Crop region defined by [x_start, y_start, x_end, y_end].</p> required <code>lose_accuracy_to_save_memory</code> <code>bool</code> <p>Boolean flag to determine if memory should be saved by using uint8 data type.</p> required <code>already_greyscale</code> <code>bool</code> <p>Boolean flag indicating if the images are already in greyscale format.</p> required <code>csc_dict</code> <code>dict</code> <p>Dictionary containing color space conversion settings, including 'logical' key.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing three elements:     - <code>visu</code>: NumPy array with shape (len(image_list), cr[1] - cr[0] + 1, cr[3] - cr[2] + 1, 3) and dtype uint8 for RGB images.     - <code>converted_video</code>: NumPy array with shape (len(image_list), cr[1] - cr[0] + 1, cr[3] - cr[2] + 1) and dtype uint8 or float according to <code>lose_accuracy_to_save_memory</code>.     - <code>converted_video2</code>: NumPy array with shape same as <code>converted_video</code> and dtype uint8 or float according to <code>lose_accuracy_to_save_memory</code>.</p> Notes <p>Performance considerations:     - If <code>lose_accuracy_to_save_memory</code> is True, the function uses np.uint8 for memory efficiency.     - If <code>already_greyscale</code> is False, additional arrays are created to store RGB data.</p> Source code in <code>src/cellects/utils/load_display_save.py</code> <pre><code>def create_empty_videos(image_list: list, cr: list, lose_accuracy_to_save_memory: bool,\n                        already_greyscale: bool, csc_dict: dict):\n    \"\"\"\n\n    Create empty video arrays based on input parameters.\n\n    Parameters\n    ----------\n    image_list : list\n        List of images.\n    cr : list\n        Crop region defined by [x_start, y_start, x_end, y_end].\n    lose_accuracy_to_save_memory : bool\n        Boolean flag to determine if memory should be saved by using uint8 data type.\n    already_greyscale : bool\n        Boolean flag indicating if the images are already in greyscale format.\n    csc_dict : dict\n        Dictionary containing color space conversion settings, including 'logical' key.\n\n    Returns\n    -------\n    tuple\n        A tuple containing three elements:\n            - `visu`: NumPy array with shape (len(image_list), cr[1] - cr[0] + 1, cr[3] - cr[2] + 1, 3) and dtype uint8 for RGB images.\n            - `converted_video`: NumPy array with shape (len(image_list), cr[1] - cr[0] + 1, cr[3] - cr[2] + 1) and dtype uint8 or float according to `lose_accuracy_to_save_memory`.\n            - `converted_video2`: NumPy array with shape same as `converted_video` and dtype uint8 or float according to `lose_accuracy_to_save_memory`.\n\n    Notes\n    -----\n    Performance considerations:\n        - If `lose_accuracy_to_save_memory` is True, the function uses np.uint8 for memory efficiency.\n        - If `already_greyscale` is False, additional arrays are created to store RGB data.\n    \"\"\"\n    visu, converted_video, converted_video2 = None, None, None\n    dims = len(image_list), cr[1] - cr[0], cr[3] - cr[2]\n    if lose_accuracy_to_save_memory:\n        converted_video = np.zeros(dims, dtype=np.uint8)\n    else:\n        converted_video = np.zeros(dims, dtype=float)\n    if not already_greyscale:\n        visu = np.zeros((dims[0], dims[1], dims[2], 3), dtype=np.uint8)\n        if csc_dict['logical'] != 'None':\n            if lose_accuracy_to_save_memory:\n                converted_video2 = np.zeros(dims, dtype=np.uint8)\n            else:\n                converted_video2 = np.zeros(dims, dtype=float)\n    return visu, converted_video, converted_video2\n</code></pre>"},{"location":"api/cellects/utils/load_display_save/#cellects.utils.load_display_save.display_boxes","title":"<code>display_boxes(binary_image, box_diameter, show=True)</code>","text":"<p>Display grid lines on a binary image at specified box diameter intervals.</p> <p>This function displays the given binary image with vertical and horizontal grid lines drawn at regular intervals defined by <code>box_diameter</code>. The function returns the total number of grid lines drawn.</p> <p>Parameters:</p> Name Type Description Default <code>binary_image</code> <code>ndarray</code> <p>Binary image on which to draw the grid lines.</p> required <code>box_diameter</code> <code>int</code> <p>Diameter of each box in pixels.</p> required <p>Returns:</p> Name Type Description <code>line_nb</code> <code>int</code> <p>Number of grid lines drawn, both vertical and horizontal.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; binary_image = np.random.randint(0, 2, (100, 100), dtype=np.uint8)\n&gt;&gt;&gt; display_boxes(binary_image, box_diameter=25)\n</code></pre> Source code in <code>src/cellects/utils/load_display_save.py</code> <pre><code>def display_boxes(binary_image: NDArray, box_diameter: int, show: bool = True):\n    \"\"\"\n    Display grid lines on a binary image at specified box diameter intervals.\n\n    This function displays the given binary image with vertical and horizontal\n    grid lines drawn at regular intervals defined by `box_diameter`. The function\n    returns the total number of grid lines drawn.\n\n    Parameters\n    ----------\n    binary_image : ndarray\n        Binary image on which to draw the grid lines.\n    box_diameter : int\n        Diameter of each box in pixels.\n\n    Returns\n    -------\n    line_nb : int\n        Number of grid lines drawn, both vertical and horizontal.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; binary_image = np.random.randint(0, 2, (100, 100), dtype=np.uint8)\n    &gt;&gt;&gt; display_boxes(binary_image, box_diameter=25)\n    \"\"\"\n    plt.imshow(binary_image, cmap='gray', extent=(0, binary_image.shape[1], 0, binary_image.shape[0]))\n    height, width = binary_image.shape\n    line_nb = 0\n    for x in range(0, width + 1, box_diameter):\n        line_nb += 1\n        plt.axvline(x=x, color='white', linewidth=1)\n    for y in range(0, height + 1, box_diameter):\n        line_nb += 1\n        plt.axhline(y=y, color='white', linewidth=1)\n\n    if show:\n        plt.show()\n\n    return line_nb\n</code></pre>"},{"location":"api/cellects/utils/load_display_save/#cellects.utils.load_display_save.display_network_methods","title":"<code>display_network_methods(network_detection, save_path=None)</code>","text":"<p>Display segmentation results from a network detection object.</p> Extended Description <p>Plots the binary segmentation results for various methods stored in <code>network_detection.all_results</code>. Highlights the best result based on quality metrics and allows for saving the figure to a file.</p> <p>Parameters:</p> Name Type Description Default <code>network_detection</code> <code>object</code> <p>An object containing segmentation results and quality metrics.</p> required <code>save_path</code> <code>str</code> <p>Path to save the figure. If <code>None</code>, the plot is displayed.</p> <code>None</code> Source code in <code>src/cellects/utils/load_display_save.py</code> <pre><code>def display_network_methods(network_detection: object, save_path: str=None):\n    \"\"\"\n\n    Display segmentation results from a network detection object.\n\n    Extended Description\n    --------------------\n\n    Plots the binary segmentation results for various methods stored in ``network_detection.all_results``.\n    Highlights the best result based on quality metrics and allows for saving the figure to a file.\n\n    Parameters\n    ----------\n    network_detection : object\n        An object containing segmentation results and quality metrics.\n    save_path : str, optional\n        Path to save the figure. If ``None``, the plot is displayed.\n\n    \"\"\"\n    row_nb = 6\n    fig, axes = plt.subplots(int(np.ceil(len(network_detection.all_results) / row_nb)), row_nb, figsize=(100, 100))\n    fig.suptitle(f'Segmentation Comparison: Frangi + Sato Variations', fontsize=16)\n\n    # Plot all results\n    for idx, result in enumerate(network_detection.all_results):\n        row = idx // row_nb\n        col = idx % row_nb\n\n        ax = axes[row, col]\n\n        # Display binary segmentation result\n        ax.imshow(result['binary'], cmap='gray')\n\n        # Create title with filter info and quality score\n        title = f\"{result['method']}: {str(np.round(network_detection.quality_metrics[idx], 0))}\"\n\n        # Highlight the best result\n        if idx == network_detection.best_idx:\n            ax.set_title(title, fontsize=8, color='red', fontweight='bold')\n            ax.add_patch(plt.Rectangle((0, 0), result['binary'].shape[1] - 1,\n                                       result['binary'].shape[0] - 1,\n                                       fill=False, edgecolor='red', linewidth=3))\n        else:\n            ax.set_title(title, fontsize=8)\n\n        ax.axis('off')\n    plt.tight_layout()\n\n    if save_path is not None:\n        plt.savefig(save_path, bbox_inches='tight', pad_inches=0., transparent=True, dpi=500)\n        plt.close()\n    else:\n        plt.show()\n</code></pre>"},{"location":"api/cellects/utils/load_display_save/#cellects.utils.load_display_save.extract_time","title":"<code>extract_time(image_list, pathway='', raw_images=False)</code>","text":"<p>Extract timestamps from a list of images.</p> <p>This function extracts the DateTimeOriginal or datetime values from the EXIF data of a list of image files, and computes the total time in seconds.</p> <p>Parameters:</p> Name Type Description Default <code>image_list</code> <code>list of str</code> <p>List of image file names.</p> required <code>pathway</code> <code>str</code> <p>Path to the directory containing the images. Default is an empty string.</p> <code>''</code> <code>raw_images</code> <code>bool</code> <p>If True, use the exifread library. Otherwise, use the exif library. Default is False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>time</code> <code>ndarray of int64</code> <p>Array containing the total time in seconds for each image.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pathway = Path(__name__).resolve().parents[0] / \"data\" / \"single_experiment\"\n&gt;&gt;&gt; image_list = ['image1.tif', 'image2.tif']\n&gt;&gt;&gt; time = extract_time(image_list, pathway)\n&gt;&gt;&gt; print(time)\narray([0, 0])\n</code></pre> Source code in <code>src/cellects/utils/load_display_save.py</code> <pre><code>def extract_time(image_list: list, pathway=\"\", raw_images:bool=False):\n    \"\"\"\n    Extract timestamps from a list of images.\n\n    This function extracts the DateTimeOriginal or datetime values from\n    the EXIF data of a list of image files, and computes the total time in seconds.\n\n    Parameters\n    ----------\n    image_list : list of str\n        List of image file names.\n    pathway : str, optional\n        Path to the directory containing the images. Default is an empty string.\n    raw_images : bool, optional\n        If True, use the exifread library. Otherwise, use the exif library.\n        Default is False.\n\n    Returns\n    -------\n    time : ndarray of int64\n        Array containing the total time in seconds for each image.\n\n    Examples\n    --------\n    &gt;&gt;&gt; pathway = Path(__name__).resolve().parents[0] / \"data\" / \"single_experiment\"\n    &gt;&gt;&gt; image_list = ['image1.tif', 'image2.tif']\n    &gt;&gt;&gt; time = extract_time(image_list, pathway)\n    &gt;&gt;&gt; print(time)\n    array([0, 0])\n\n    \"\"\"\n    if isinstance(pathway, str):\n        pathway = Path(pathway)\n    nb = len(image_list)\n    timings = np.zeros((nb, 6), dtype=np.int64)\n    if raw_images:\n        for i in np.arange(nb):\n            with open(pathway / image_list, 'rb') as image_file:\n                my_image = exifread.process_file(image_file, details=False, stop_tag='DateTimeOriginal')\n                datetime = my_image[\"EXIF DateTimeOriginal\"]\n            datetime = datetime.values[:10] + ':' + datetime.values[11:]\n            timings[i, :] = datetime.split(':')\n    else:\n        for i in np.arange(nb):\n            with open(pathway / image_list[i], 'rb') as image_file:\n                my_image = Image(image_file)\n                if my_image.has_exif:\n                    datetime = my_image.datetime\n                    datetime = datetime[:10] + ':' + datetime[11:]\n                    timings[i, :] = datetime.split(':')\n\n    if np.all(timings[:, 0] == timings[0, 0]):\n        if np.all(timings[:, 1] == timings[0, 1]):\n            if np.all(timings[:, 2] == timings[0, 2]):\n                time = timings[:, 3] * 3600 + timings[:, 4] * 60 + timings[:, 5]\n            else:\n                time = timings[:, 2] * 86400 + timings[:, 3] * 3600 + timings[:, 4] * 60 + timings[:, 5]\n        else:\n            days_per_month = (31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31)\n            for j in np.arange(nb):\n                month_number = timings[j, 1]#int(timings[j, 1])\n                timings[j, 1] = days_per_month[month_number] * month_number\n            time = (timings[:, 1] + timings[:, 2]) * 86400 + timings[:, 3] * 3600 + timings[:, 4] * 60 + timings[:, 5]\n        #time = int(time)\n    else:\n        time = np.repeat(0, nb)#arange(1, nb * 60, 60)#\"Do not experiment the 31th of december!!!\"\n    if time.sum() == 0:\n        time = np.repeat(0, nb)#arange(1, nb * 60, 60)\n    return time\n</code></pre>"},{"location":"api/cellects/utils/load_display_save/#cellects.utils.load_display_save.get_h5_keys","title":"<code>get_h5_keys(file_name)</code>","text":"<p>Retrieve all keys from a given HDF5 file.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>The path to the HDF5 file from which keys are to be retrieved.</p> required <p>Returns:</p> Type Description <code>list of str</code> <p>A list containing all the keys present in the specified HDF5 file.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the specified HDF5 file does not exist.</p> Source code in <code>src/cellects/utils/load_display_save.py</code> <pre><code>def get_h5_keys(file_name):\n    \"\"\"\n    Retrieve all keys from a given HDF5 file.\n\n    Parameters\n    ----------\n    file_name : str\n        The path to the HDF5 file from which keys are to be retrieved.\n\n    Returns\n    -------\n    list of str\n        A list containing all the keys present in the specified HDF5 file.\n\n    Raises\n    ------\n    FileNotFoundError\n        If the specified HDF5 file does not exist.\n    \"\"\"\n    try:\n        with h5py.File(file_name, 'r') as h5f:\n            all_keys = list(h5f.keys())\n            return all_keys\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"The file '{file_name}' does not exist.\")\n</code></pre>"},{"location":"api/cellects/utils/load_display_save/#cellects.utils.load_display_save.get_mpl_colormap","title":"<code>get_mpl_colormap(cmap_name)</code>","text":"<p>Returns a linear color range array for the given matplotlib colormap.</p> <p>Parameters:</p> Name Type Description Default <code>cmap_name</code> <code>str</code> <p>The name of the colormap to get.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>A 256x1x3 array of bytes representing the linear color range.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; result = get_mpl_colormap('viridis')\n&gt;&gt;&gt; print(result.shape)\n(256, 1, 3)\n</code></pre> Source code in <code>src/cellects/utils/load_display_save.py</code> <pre><code>def get_mpl_colormap(cmap_name: str):\n    \"\"\"\n    Returns a linear color range array for the given matplotlib colormap.\n\n    Parameters\n    ----------\n    cmap_name : str\n        The name of the colormap to get.\n\n    Returns\n    -------\n    numpy.ndarray\n        A 256x1x3 array of bytes representing the linear color range.\n\n    Examples\n    --------\n    &gt;&gt;&gt; result = get_mpl_colormap('viridis')\n    &gt;&gt;&gt; print(result.shape)\n    (256, 1, 3)\n\n    \"\"\"\n    cmap = plt.get_cmap(cmap_name)\n\n    # Initialize the matplotlib color map\n    sm = plt.cm.ScalarMappable(cmap=cmap)\n\n    # Obtain linear color range\n    color_range = sm.to_rgba(np.linspace(0, 1, 256), bytes=True)[:, 2::-1]\n\n    return color_range.reshape(256, 1, 3)\n</code></pre>"},{"location":"api/cellects/utils/load_display_save/#cellects.utils.load_display_save.is_raw_image","title":"<code>is_raw_image(image_path)</code>","text":"<p>Determine if the image path corresponds to a raw image.</p> <p>Parameters:</p> Name Type Description Default <code>image_path</code> <code>str</code> <p>The file path of the image.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the image is considered raw, False otherwise.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; result = is_raw_image(\"image.jpg\")\n&gt;&gt;&gt; print(result)\nFalse\n</code></pre> Source code in <code>src/cellects/utils/load_display_save.py</code> <pre><code>def is_raw_image(image_path) -&gt; bool:\n    \"\"\"\n    Determine if the image path corresponds to a raw image.\n\n    Parameters\n    ----------\n    image_path : str\n        The file path of the image.\n\n    Returns\n    -------\n    bool\n        True if the image is considered raw, False otherwise.\n\n    Examples\n    --------\n    &gt;&gt;&gt; result = is_raw_image(\"image.jpg\")\n    &gt;&gt;&gt; print(result)\n    False\n    \"\"\"\n    ext = image_path.split(\".\")[-1]\n    if np.isin(ext, opencv_accepted_formats):\n        raw_image = False\n    else:\n        raw_image = True\n    return raw_image\n</code></pre>"},{"location":"api/cellects/utils/load_display_save/#cellects.utils.load_display_save.movie","title":"<code>movie(video, increase_contrast=True)</code>","text":"Summary <p>Processes a video to display each frame with optional contrast increase and resizing.</p> <p>Parameters:</p> Name Type Description Default <code>video</code> <code>ndarray</code> <p>The input video represented as a 3D NumPy array.</p> required <code>increase_contrast</code> <code>bool</code> <p>Flag to increase the contrast of each frame (default is True).</p> <code>True</code> <p>Other Parameters:</p> Name Type Description <code>keyboard</code> <code>int</code> <p>Key to wait for during the display of each frame.</p> <code>increase_contrast</code> <code>bool</code> <p>Whether to increase contrast for the displayed frames.</p> <p>Returns:</p> Type Description <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>video</code> is not a 3D NumPy array.</p> Notes <p>This function uses OpenCV's <code>imshow</code> to display each frame. Ensure that the required OpenCV dependencies are met.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; movie(video)\nProcesses and displays a video with default settings.\n&gt;&gt;&gt; movie(video, keyboard=0)\nProcesses and displays a video waiting for the SPACE key between frames.\n&gt;&gt;&gt; movie(video, increase_contrast=False)\nProcesses and displays a video without increasing contrast.\n</code></pre> Source code in <code>src/cellects/utils/load_display_save.py</code> <pre><code>def movie(video, increase_contrast: bool=True):\n    \"\"\"\n    Summary\n    -------\n    Processes a video to display each frame with optional contrast increase and resizing.\n\n    Parameters\n    ----------\n    video : numpy.ndarray\n        The input video represented as a 3D NumPy array.\n    increase_contrast : bool, optional\n        Flag to increase the contrast of each frame (default is True).\n\n    Other Parameters\n    ----------------\n    keyboard : int, optional\n        Key to wait for during the display of each frame.\n    increase_contrast : bool, optional\n        Whether to increase contrast for the displayed frames.\n\n    Returns\n    -------\n    None\n\n    Raises\n    ------\n    ValueError\n        If `video` is not a 3D NumPy array.\n\n    Notes\n    -----\n    This function uses OpenCV's `imshow` to display each frame. Ensure that the required\n    OpenCV dependencies are met.\n\n    Examples\n    --------\n    &gt;&gt;&gt; movie(video)\n    Processes and displays a video with default settings.\n    &gt;&gt;&gt; movie(video, keyboard=0)\n    Processes and displays a video waiting for the SPACE key between frames.\n    &gt;&gt;&gt; movie(video, increase_contrast=False)\n    Processes and displays a video without increasing contrast.\n\n    \"\"\"\n    for i in np.arange(video.shape[0]):\n        image = video[i, :, :]\n        if np.any(image):\n            if increase_contrast:\n                image = bracket_to_uint8_image_contrast(image)\n            final_img = cv2.resize(image, (500, 500))\n            cv2.imshow('Motion analysis', final_img)\n            if cv2.waitKey(25) &amp; 0xFF == ord('q'):\n                break\n    cv2.destroyAllWindows()\n</code></pre>"},{"location":"api/cellects/utils/load_display_save/#cellects.utils.load_display_save.read_and_rotate","title":"<code>read_and_rotate(image_name, prev_img=None, raw_images=False, is_landscape=True, crop_coord=None)</code>","text":"<p>Read and rotate an image based on specified parameters.</p> <p>This function reads an image from the given file name, optionally rotates it by 90 degrees clockwise or counterclockwise based on its dimensions and the <code>is_landscape</code> flag, and applies cropping if specified. It also compares rotated images against a previous image to choose the best rotation.</p> <p>Parameters:</p> Name Type Description Default <code>image_name</code> <code>str</code> <p>Name of the image file to read.</p> required <code>prev_img</code> <code>ndarray</code> <p>Previous image for comparison. Default is <code>None</code>.</p> <code>None</code> <code>raw_images</code> <code>bool</code> <p>Flag to read raw images. Default is <code>False</code>.</p> <code>False</code> <code>is_landscape</code> <code>bool</code> <p>Flag to determine if the image should be considered in landscape mode. Default is <code>True</code>.</p> <code>True</code> <code>crop_coord</code> <code>ndarray</code> <p>Coordinates for cropping the image. Default is <code>None</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Rotated and optionally cropped image.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the specified image file does not exist.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pathway = Path(__name__).resolve().parents[0] / \"data\" / \"single_experiment\"\n&gt;&gt;&gt; image_name = 'image1.tif'\n&gt;&gt;&gt; image = read_and_rotate(pathway /image_name)\n&gt;&gt;&gt; print(image.shape)\n(245, 300, 3)\n</code></pre> Source code in <code>src/cellects/utils/load_display_save.py</code> <pre><code>def read_and_rotate(image_name, prev_img: NDArray=None, raw_images: bool=False, is_landscape: bool=True, crop_coord: NDArray=None) -&gt; NDArray:\n    \"\"\"\n    Read and rotate an image based on specified parameters.\n\n    This function reads an image from the given file name, optionally rotates\n    it by 90 degrees clockwise or counterclockwise based on its dimensions and\n    the `is_landscape` flag, and applies cropping if specified. It also compares\n    rotated images against a previous image to choose the best rotation.\n\n    Parameters\n    ----------\n    image_name : str\n        Name of the image file to read.\n    prev_img : ndarray, optional\n        Previous image for comparison. Default is `None`.\n    raw_images : bool, optional\n        Flag to read raw images. Default is `False`.\n    is_landscape : bool, optional\n        Flag to determine if the image should be considered in landscape mode.\n        Default is `True`.\n    crop_coord : ndarray, optional\n        Coordinates for cropping the image. Default is `None`.\n\n    Returns\n    -------\n    ndarray\n        Rotated and optionally cropped image.\n\n    Raises\n    ------\n    FileNotFoundError\n        If the specified image file does not exist.\n\n    Examples\n    ------\n    &gt;&gt;&gt; pathway = Path(__name__).resolve().parents[0] / \"data\" / \"single_experiment\"\n    &gt;&gt;&gt; image_name = 'image1.tif'\n    &gt;&gt;&gt; image = read_and_rotate(pathway /image_name)\n    &gt;&gt;&gt; print(image.shape)\n    (245, 300, 3)\n    \"\"\"\n    if not os.path.exists(image_name):\n        raise FileNotFoundError(image_name)\n    img = readim(image_name, raw_images)\n    if (img.shape[0] &gt; img.shape[1] and is_landscape) or (img.shape[0] &lt; img.shape[1] and not is_landscape):\n        clockwise = cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)\n        if crop_coord is not None:\n            clockwise = clockwise[crop_coord[0]:crop_coord[1], crop_coord[2]:crop_coord[3], ...]\n        if prev_img is not None:\n            prev_img = np.int16(prev_img)\n            clock_diff = sum_of_abs_differences(prev_img, np.int16(clockwise))\n            counter_clockwise = cv2.rotate(img, cv2.ROTATE_90_COUNTERCLOCKWISE)\n            if crop_coord is not None:\n                counter_clockwise = counter_clockwise[crop_coord[0]:crop_coord[1], crop_coord[2]:crop_coord[3], ...]\n            counter_clock_diff = sum_of_abs_differences(prev_img, np.int16(counter_clockwise))\n            if clock_diff &gt; counter_clock_diff:\n                img = counter_clockwise\n            else:\n                img = clockwise\n        else:\n            img = clockwise\n    else:\n        if crop_coord is not None:\n            img = img[crop_coord[0]:crop_coord[1], crop_coord[2]:crop_coord[3], ...]\n    return img\n</code></pre>"},{"location":"api/cellects/utils/load_display_save/#cellects.utils.load_display_save.read_h5_array","title":"<code>read_h5_array(file_name, key='data')</code>","text":"<p>Read data array from an HDF5 file.</p> <p>This function reads a specific dataset from an HDF5 file using the provided key.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>The path to the HDF5 file.</p> required <code>key</code> <code>str</code> <p>The dataset name within the HDF5 file.</p> <code>'data'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The data array from the specified dataset in the HDF5 file.</p> Source code in <code>src/cellects/utils/load_display_save.py</code> <pre><code>def read_h5_array(file_name, key: str=\"data\"):\n    \"\"\"\n    Read data array from an HDF5 file.\n\n    This function reads a specific dataset from an HDF5 file using the provided key.\n\n    Parameters\n    ----------\n    file_name : str\n        The path to the HDF5 file.\n    key : str, optional, default: 'data'\n        The dataset name within the HDF5 file.\n\n    Returns\n    -------\n    ndarray\n        The data array from the specified dataset in the HDF5 file.\n    \"\"\"\n    try:\n        with h5py.File(file_name, 'r') as h5f:\n            if key in h5f:\n                data = h5f[key][:]\n                return data\n            else:\n                raise KeyError(f\"Dataset '{key}' not found in file '{file_name}'.\")\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"The file '{file_name}' does not exist.\")\n</code></pre>"},{"location":"api/cellects/utils/load_display_save/#cellects.utils.load_display_save.read_one_arena","title":"<code>read_one_arena(arena_label, already_greyscale, csc_dict, videos_already_in_ram=None, true_frame_width=None, vid_name=None, background=None, background2=None)</code>","text":"<p>Read a single arena's video data, potentially converting it from color to greyscale.</p> <p>Parameters:</p> Name Type Description Default <code>arena_label</code> <code>int</code> <p>The label of the arena.</p> required <code>already_greyscale</code> <code>bool</code> <p>Whether the video is already in greyscale format.</p> required <code>csc_dict</code> <code>dict</code> <p>Dictionary containing color space conversion settings.</p> required <code>videos_already_in_ram</code> <code>ndarray</code> <p>Pre-loaded video frames in memory. Default is None.</p> <code>None</code> <code>true_frame_width</code> <code>int</code> <p>The true width of the video frames. Default is None.</p> <code>None</code> <code>vid_name</code> <code>str</code> <p>Name of the video file. Default is None.</p> <code>None</code> <code>background</code> <code>ndarray</code> <p>Background image for subtractions. Default is None.</p> <code>None</code> <code>background2</code> <code>ndarray</code> <p>Second background image for subtractions. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing:     - visu: np.ndarray or None, the visual frame.     - converted_video: np.ndarray or None, the video data converted as needed.     - converted_video2: np.ndarray or None, additional video data if necessary.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the specified video file does not exist.</p> <code>ValueError</code> <p>If the video data shape is invalid.</p> Notes <p>This function assumes that <code>video2numpy</code> is a helper function available in the scope. For optimal performance, ensure all video data fits in RAM.</p> Source code in <code>src/cellects/utils/load_display_save.py</code> <pre><code>def read_one_arena(arena_label, already_greyscale:bool, csc_dict: dict, videos_already_in_ram=None,\n                   true_frame_width=None, vid_name: str=None, background: NDArray=None, background2: NDArray=None):\n    \"\"\"\n    Read a single arena's video data, potentially converting it from color to greyscale.\n\n    Parameters\n    ----------\n    arena_label : int\n        The label of the arena.\n    already_greyscale : bool\n        Whether the video is already in greyscale format.\n    csc_dict : dict\n        Dictionary containing color space conversion settings.\n    videos_already_in_ram : np.ndarray, optional\n        Pre-loaded video frames in memory. Default is None.\n    true_frame_width : int, optional\n        The true width of the video frames. Default is None.\n    vid_name : str, optional\n        Name of the video file. Default is None.\n    background : np.ndarray, optional\n        Background image for subtractions. Default is None.\n    background2 : np.ndarray, optional\n        Second background image for subtractions. Default is None.\n\n    Returns\n    -------\n    tuple\n        A tuple containing:\n            - visu: np.ndarray or None, the visual frame.\n            - converted_video: np.ndarray or None, the video data converted as needed.\n            - converted_video2: np.ndarray or None, additional video data if necessary.\n\n    Raises\n    ------\n    FileNotFoundError\n        If the specified video file does not exist.\n    ValueError\n        If the video data shape is invalid.\n\n    Notes\n    -----\n    This function assumes that `video2numpy` is a helper function available in the scope.\n    For optimal performance, ensure all video data fits in RAM.\n    \"\"\"\n    visu, converted_video, converted_video2 = None, None, None\n    logging.info(f\"Arena n\u00b0{arena_label}. Load images and videos\")\n    if videos_already_in_ram is not None:\n        if already_greyscale:\n            converted_video = videos_already_in_ram\n        else:\n            if csc_dict['logical'] == 'None':\n                visu, converted_video = videos_already_in_ram\n            else:\n                visu, converted_video, converted_video2 = videos_already_in_ram\n    else:\n        if vid_name is not None:\n            if already_greyscale:\n                converted_video = video2numpy(vid_name, None, background, background2, true_frame_width)\n                if len(converted_video.shape) == 4:\n                    converted_video = converted_video[:, :, :, 0]\n            else:\n                visu = video2numpy(vid_name, None, background, background2, true_frame_width)\n        else:\n            vid_name = f\"ind_{arena_label}.npy\"\n            if os.path.isfile(vid_name):\n                if already_greyscale:\n                    converted_video = video2numpy(vid_name, None, background, background2, true_frame_width)\n                    if len(converted_video.shape) == 4:\n                        converted_video = converted_video[:, :, :, 0]\n                else:\n                    visu = video2numpy(vid_name, None, background, background2, true_frame_width)\n    return visu, converted_video, converted_video2\n</code></pre>"},{"location":"api/cellects/utils/load_display_save/#cellects.utils.load_display_save.readim","title":"<code>readim(image_path, raw_image=False)</code>","text":"<p>Read an image from a file and optionally process it.</p> <p>Parameters:</p> Name Type Description Default <code>image_path</code> <code>str</code> <p>Path to the image file.</p> required <code>raw_image</code> <code>bool</code> <p>If True, logs an error message indicating that the raw image format cannot be processed. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The decoded image represented as a NumPy array of shape (height, width, channels).</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If <code>raw_image</code> is set to True, logs an error indicating that the raw image format cannot be processed.</p> Notes <p>Although <code>raw_image</code> is set to False by default, currently it does not perform any raw image processing.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; cv2.imread(\"example.jpg\")\narray([[[255, 0, 0],\n        [255, 0, 0]],\n</code></pre> <pre><code>   [[  0, 255, 0],\n    [  0, 255, 0]],\n\n   [[  0,   0, 255],\n    [  0,   0, 255]]], dtype=np.uint8)\n</code></pre> Source code in <code>src/cellects/utils/load_display_save.py</code> <pre><code>def readim(image_path, raw_image: bool=False):\n    \"\"\"\n    Read an image from a file and optionally process it.\n\n    Parameters\n    ----------\n    image_path : str\n        Path to the image file.\n    raw_image : bool, optional\n        If True, logs an error message indicating that the raw image format cannot be processed. Default is False.\n\n    Returns\n    -------\n    ndarray\n        The decoded image represented as a NumPy array of shape (height, width, channels).\n\n    Raises\n    ------\n    RuntimeError\n        If `raw_image` is set to True, logs an error indicating that the raw image format cannot be processed.\n\n    Notes\n    -----\n    Although `raw_image` is set to False by default, currently it does not perform any raw image processing.\n\n    Examples\n    --------\n    &gt;&gt;&gt; cv2.imread(\"example.jpg\")\n    array([[[255, 0, 0],\n            [255, 0, 0]],\n\n           [[  0, 255, 0],\n            [  0, 255, 0]],\n\n           [[  0,   0, 255],\n            [  0,   0, 255]]], dtype=np.uint8)\n    \"\"\"\n    if raw_image:\n        logging.error(\"Cannot read this image format. If the rawpy package can, ask for a version of Cellects using it.\")\n        # import rawpy\n        # raw = rawpy.imread(image_path)\n        # raw = raw.postprocess()\n        # return cv2.cvtColor(raw, COLOR_RGB2BGR)\n        return cv2.imread(image_path)\n    else:\n        return cv2.imread(image_path)\n</code></pre>"},{"location":"api/cellects/utils/load_display_save/#cellects.utils.load_display_save.remove_h5_key","title":"<code>remove_h5_key(file_name, key='data')</code>","text":"<p>Remove a specified key from an HDF5 file.</p> <p>This function opens an HDF5 file in append mode and deletes the specified key if it exists. It handles exceptions related to file not found and other runtime errors.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>The path to the HDF5 file from which the key should be removed.</p> required <code>key</code> <code>str</code> <p>The name of the dataset or group to delete from the HDF5 file. Default is \"data\".</p> <code>'data'</code> <p>Returns:</p> Type Description <code>None</code> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the specified file does not exist.</p> <code>RuntimeError</code> <p>If any other error occurs during file operations.</p> Notes <p>This function modifies the HDF5 file in place. Ensure you have a backup if necessary.</p> Source code in <code>src/cellects/utils/load_display_save.py</code> <pre><code>def remove_h5_key(file_name, key: str=\"data\"):\n    \"\"\"\n    Remove a specified key from an HDF5 file.\n\n    This function opens an HDF5 file in append mode and deletes the specified\n    key if it exists. It handles exceptions related to file not found\n    and other runtime errors.\n\n    Parameters\n    ----------\n    file_name : str\n        The path to the HDF5 file from which the key should be removed.\n    key : str, optional\n        The name of the dataset or group to delete from the HDF5 file.\n        Default is \"data\".\n\n    Returns\n    -------\n    None\n\n    Raises\n    ------\n    FileNotFoundError\n        If the specified file does not exist.\n    RuntimeError\n        If any other error occurs during file operations.\n\n    Notes\n    -----\n    This function modifies the HDF5 file in place. Ensure you have a backup if necessary.\n    \"\"\"\n    try:\n        with h5py.File(file_name, 'a') as h5f:  # Open in append mode to modify the file\n            if key in h5f:\n                del h5f[key]\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"The file '{file_name}' does not exist.\")\n    except Exception as e:\n        raise RuntimeError(f\"An error occurred: {e}\")\n</code></pre>"},{"location":"api/cellects/utils/load_display_save/#cellects.utils.load_display_save.save_fig","title":"<code>save_fig(img, full_path, cmap=None)</code>","text":"<p>Save an image figure to a file with specified options.</p> <p>This function creates a matplotlib figure from the given image, optionally applies a colormap, displays it briefly, saves the figure to disk at high resolution, and closes the figure.</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>array_like(M, N, 3)</code> <p>Input image to be saved as a figure. Expected to be in RGB format.</p> required <code>full_path</code> <code>str</code> <p>The complete file path where the figure will be saved. Must include extension (e.g., '.png', '.jpg').</p> required <code>cmap</code> <code>str or None</code> <p>Colormap to be applied if the image should be displayed with a specific color map. If <code>None</code>, no colormap is applied.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>This function does not return any value. It saves the figure to disk at the specified location.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the directory in <code>full_path</code> does not exist.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; img = np.random.rand(100, 100, 3) * 255\n&gt;&gt;&gt; save_fig(img, 'test.png')\nCreates and saves a figure from the random image to 'test.png'.\n</code></pre> <pre><code>&gt;&gt;&gt; save_fig(img, 'colored_test.png', cmap='viridis')\nCreates and saves a figure from the random image with 'viridis' colormap\nto 'colored_test.png'.\n</code></pre> Source code in <code>src/cellects/utils/load_display_save.py</code> <pre><code>def save_fig(img: NDArray, full_path, cmap=None):\n    \"\"\"\n    Save an image figure to a file with specified options.\n\n    This function creates a matplotlib figure from the given image,\n    optionally applies a colormap, displays it briefly, saves the\n    figure to disk at high resolution, and closes the figure.\n\n    Parameters\n    ----------\n    img : array_like (M, N, 3)\n        Input image to be saved as a figure. Expected to be in RGB format.\n    full_path : str\n        The complete file path where the figure will be saved. Must include\n        extension (e.g., '.png', '.jpg').\n    cmap : str or None, optional\n        Colormap to be applied if the image should be displayed with a specific\n        color map. If `None`, no colormap is applied.\n\n    Returns\n    -------\n    None\n\n        This function does not return any value. It saves the figure to disk\n        at the specified location.\n\n    Raises\n    ------\n    FileNotFoundError\n        If the directory in `full_path` does not exist.\n\n    Examples\n    --------\n    &gt;&gt;&gt; img = np.random.rand(100, 100, 3) * 255\n    &gt;&gt;&gt; save_fig(img, 'test.png')\n    Creates and saves a figure from the random image to 'test.png'.\n\n    &gt;&gt;&gt; save_fig(img, 'colored_test.png', cmap='viridis')\n    Creates and saves a figure from the random image with 'viridis' colormap\n    to 'colored_test.png'.\n    \"\"\"\n    sizes = img.shape[0] / 100,  img.shape[1] / 100\n    fig = plt.figure(figsize=(sizes[0], sizes[1]))\n    ax = fig.gca()\n    if cmap is None:\n        ax.imshow(img, interpolation=\"none\")\n    else:\n        ax.imshow(img, cmap=cmap, interpolation=\"none\")\n    plt.axis('off')\n    if np.min(img.shape) &gt; 50:\n        fig.tight_layout()\n\n    fig.savefig(full_path, bbox_inches='tight', pad_inches=0., transparent=True, dpi=500)\n    plt.close(fig)\n</code></pre>"},{"location":"api/cellects/utils/load_display_save/#cellects.utils.load_display_save.show","title":"<code>show(img, interactive=True, cmap=None, show=True)</code>","text":"<p>Display an image using Matplotlib with optional interactivity and colormap.</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>ndarray</code> <p>The image data to be displayed.</p> required <code>interactive</code> <code>bool</code> <p>If <code>True</code>, turn on interactive mode. Default is <code>True</code>.</p> <code>True</code> <code>cmap</code> <code>str or Colormap</code> <p>The colormap to be used. If <code>None</code>, the default colormap will be used.</p> <code>None</code> <p>Other Parameters:</p> Name Type Description <code>interactive</code> <code>bool</code> <p>If <code>True</code>, turn on interactive mode. Default is <code>True</code>.</p> <code>cmap</code> <code>str or Colormap</code> <p>The colormap to be used. If <code>None</code>, the default colormap will be used.</p> <p>Returns:</p> Name Type Description <code>fig</code> <code>Figure</code> <p>The Matplotlib figure object containing the displayed image.</p> <code>ax</code> <code>AxesSubplot</code> <p>The axes on which the image is plotted.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>cmap</code> is not a recognized colormap name or object.</p> Notes <p>If interactive mode is enabled, the user can manipulate the figure window interactively.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; img = np.random.rand(100, 50)\n&gt;&gt;&gt; fig, ax = show(img)\n&gt;&gt;&gt; print(fig)\n&lt;Figure size ... with ... Axes&gt;\n</code></pre> <pre><code>&gt;&gt;&gt; fig, ax = show(img, interactive=False)\n&gt;&gt;&gt; print(fig)\n&lt;Figure size ... with ... Axes&gt;\n</code></pre> <pre><code>&gt;&gt;&gt; fig, ax = show(img, cmap='gray')\n&gt;&gt;&gt; print(fig)\n&lt;Figure size ... with .... Axes&gt;\n</code></pre> Source code in <code>src/cellects/utils/load_display_save.py</code> <pre><code>def show(img, interactive: bool=True, cmap=None, show: bool=True):\n    \"\"\"\n    Display an image using Matplotlib with optional interactivity and colormap.\n\n    Parameters\n    ----------\n    img : ndarray\n        The image data to be displayed.\n    interactive : bool, optional\n        If ``True``, turn on interactive mode. Default is ``True``.\n    cmap : str or Colormap, optional\n        The colormap to be used. If ``None``, the default colormap will\n        be used.\n\n    Other Parameters\n    ----------------\n    interactive : bool, optional\n        If ``True``, turn on interactive mode. Default is ``True``.\n    cmap : str or Colormap, optional\n        The colormap to be used. If ``None``, the default colormap will\n        be used.\n\n    Returns\n    -------\n    fig : Figure\n        The Matplotlib figure object containing the displayed image.\n    ax : AxesSubplot\n        The axes on which the image is plotted.\n\n    Raises\n    ------\n    ValueError\n        If `cmap` is not a recognized colormap name or object.\n\n    Notes\n    -----\n    If interactive mode is enabled, the user can manipulate the figure\n    window interactively.\n\n    Examples\n    --------\n    &gt;&gt;&gt; img = np.random.rand(100, 50)\n    &gt;&gt;&gt; fig, ax = show(img)\n    &gt;&gt;&gt; print(fig) # doctest: +SKIP\n    &lt;Figure size ... with ... Axes&gt;\n\n    &gt;&gt;&gt; fig, ax = show(img, interactive=False)\n    &gt;&gt;&gt; print(fig) # doctest: +SKIP\n    &lt;Figure size ... with ... Axes&gt;\n\n    &gt;&gt;&gt; fig, ax = show(img, cmap='gray')\n    &gt;&gt;&gt; print(fig) # doctest: +SKIP\n    &lt;Figure size ... with .... Axes&gt;\n    \"\"\"\n    if interactive:\n        plt.ion()\n    else:\n        plt.ioff()\n    sizes = img.shape[0] / 100,  img.shape[1] / 100\n    fig = plt.figure(figsize=(sizes[1], sizes[0]))\n    ax = fig.gca()\n    if cmap is None:\n        ax.imshow(img, interpolation=\"none\", extent=(0, sizes[1], 0, sizes[0]))\n    else:\n        ax.imshow(img, cmap=cmap, interpolation=\"none\", extent=(0, sizes[1], 0, sizes[0]))\n\n    if show:\n        fig.tight_layout()\n        fig.show()\n\n    return fig, ax\n</code></pre>"},{"location":"api/cellects/utils/load_display_save/#cellects.utils.load_display_save.video2numpy","title":"<code>video2numpy(vid_name, conversion_dict=None, background=None, background2=None, true_frame_width=None)</code>","text":"<p>Convert a video file to a NumPy array.</p> <p>Parameters:</p> Name Type Description Default <code>vid_name</code> <code>str</code> <p>The path to the video file. Can be a <code>.mp4</code> or <code>.npy</code>.</p> required <code>conversion_dict</code> <code>dict</code> <p>Dictionary containing color space conversion parameters.</p> <code>None</code> <code>background</code> <code>NDArray</code> <p>Background image for processing.</p> <code>None</code> <code>background2</code> <code>NDArray</code> <p>Second background image for processing.</p> <code>None</code> <code>true_frame_width</code> <code>int</code> <p>True width of the frame. If specified and the current width is double this value, adjusts to true_frame_width.</p> <code>None</code> <p>Returns:</p> Type Description <code>NDArray or tuple of NDArrays</code> <p>If conversion_dict is None, returns the video as a NumPy array. Otherwise, returns a tuple containing the original video and converted video.</p> Notes <p>This function uses OpenCV to read the contents of a <code>.mp4</code> video file.</p> Source code in <code>src/cellects/utils/load_display_save.py</code> <pre><code>def video2numpy(vid_name: str, conversion_dict=None, background: NDArray=None, background2: NDArray=None,\n                true_frame_width: int=None):\n    \"\"\"\n    Convert a video file to a NumPy array.\n\n    Parameters\n    ----------\n    vid_name : str\n        The path to the video file. Can be a `.mp4` or `.npy`.\n    conversion_dict : dict, optional\n        Dictionary containing color space conversion parameters.\n    background : NDArray, optional\n        Background image for processing.\n    background2 : NDArray, optional\n        Second background image for processing.\n    true_frame_width : int, optional\n        True width of the frame. If specified and the current width is double this value,\n        adjusts to true_frame_width.\n\n    Returns\n    -------\n    NDArray or tuple of NDArrays\n        If conversion_dict is None, returns the video as a NumPy array.\n        Otherwise, returns a tuple containing the original video and converted video.\n\n    Notes\n    -----\n    This function uses OpenCV to read the contents of a `.mp4` video file.\n    \"\"\"\n    np_loading = vid_name[-4:] == \".npy\"\n    if np_loading:\n        video = np.load(vid_name)\n        dims = list(video.shape)\n    else:\n        cap = cv2.VideoCapture(vid_name)\n        dims = [int(cap.get(cv2.CAP_PROP_FRAME_COUNT)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)), int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))]\n\n    if true_frame_width is not None:\n        if dims[2] == 2 * true_frame_width:\n            dims[2] = true_frame_width\n\n    if conversion_dict is not None:\n        first_dict, second_dict, c_spaces = split_dict(conversion_dict)\n        converted_video = np.empty(dims[:3], dtype=np.uint8)\n        if conversion_dict['logical'] == 'None':\n            converted_video2 = np.empty(dims[:3], dtype=np.uint8)\n        if np_loading:\n            for counter in np.arange(video.shape[0]):\n                img = video[counter, :, :dims[2], :]\n                greyscale_image, greyscale_image2, all_c_spaces, first_pc_vector = generate_color_space_combination(img, c_spaces,\n                                                                                     first_dict, second_dict, background=background,background2=background2,\n                                                                                     convert_to_uint8=True)\n                converted_video[counter, ...] = greyscale_image\n                if conversion_dict['logical'] == 'None':\n                    converted_video2[counter, ...] = greyscale_image2\n            video = video[:, :, :dims[2], ...]\n\n    if not np_loading:\n        # 2) Create empty arrays to store video analysis data\n        video = np.empty((dims[0], dims[1], dims[2], 3), dtype=np.uint8)\n        # 3) Read and convert the video frame by frame\n        counter = 0\n        while cap.isOpened() and counter &lt; dims[0]:\n            ret, frame = cap.read()\n            frame = frame[:, :dims[2], ...]\n            video[counter, ...] = frame\n            if conversion_dict is not None:\n                greyscale_image, greyscale_image2, all_c_spaces, first_pc_vector = generate_color_space_combination(frame, c_spaces,\n                                                                                     first_dict, second_dict, background=background,background2=background2,\n                                                                                     convert_to_uint8=True)\n                converted_video[counter, ...] = greyscale_image\n                if conversion_dict['logical'] == 'None':\n                    converted_video2[counter, ...] = greyscale_image2\n            counter += 1\n        cap.release()\n\n    if conversion_dict is None:\n        return video\n    else:\n        return video, converted_video\n</code></pre>"},{"location":"api/cellects/utils/load_display_save/#cellects.utils.load_display_save.vstack_h5_array","title":"<code>vstack_h5_array(file_name, table, key='data')</code>","text":"<p>Stack tables vertically in an HDF5 file.</p> <p>This function either appends the input table to an existing dataset in the specified HDF5 file or creates a new dataset if the key doesn't exist.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>Path to the HDF5 file.</p> required <code>table</code> <code>NDArray[uint8]</code> <p>The table to be stacked vertically with the existing data.</p> required <code>key</code> <code>str</code> <p>Key under which the dataset will be stored. Defaults to 'data'.</p> <code>'data'</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; table = np.array([[1, 2], [3, 4]], dtype=np.uint8)\n&gt;&gt;&gt; vstack_h5_array('example.h5', table)\n</code></pre> Source code in <code>src/cellects/utils/load_display_save.py</code> <pre><code>def vstack_h5_array(file_name, table: NDArray, key: str=\"data\"):\n    \"\"\"\n    Stack tables vertically in an HDF5 file.\n\n    This function either appends the input table to an existing dataset\n    in the specified HDF5 file or creates a new dataset if the key doesn't exist.\n\n    Parameters\n    ----------\n    file_name : str\n        Path to the HDF5 file.\n    table : NDArray[np.uint8]\n        The table to be stacked vertically with the existing data.\n    key : str, optional\n        Key under which the dataset will be stored. Defaults to 'data'.\n\n    Examples\n    --------\n    &gt;&gt;&gt; table = np.array([[1, 2], [3, 4]], dtype=np.uint8)\n    &gt;&gt;&gt; vstack_h5_array('example.h5', table)\n    \"\"\"\n    if os.path.exists(file_name):\n        # Open the file in append mode\n        with h5py.File(file_name, 'a') as h5f:\n            if key in h5f:\n                # Append to the existing dataset\n                existing_data = h5f[key][:]\n                new_data = np.vstack((existing_data, table))\n                del h5f[key]\n                h5f.create_dataset(key, data=new_data)\n            else:\n                # Create a new dataset if the key doesn't exist\n                h5f.create_dataset(key, data=table)\n    else:\n        with h5py.File(file_name, 'w') as h5f:\n            h5f.create_dataset(key, data=table)\n</code></pre>"},{"location":"api/cellects/utils/load_display_save/#cellects.utils.load_display_save.write_video","title":"<code>write_video(np_array, vid_name, is_color=True, fps=40)</code>","text":"<p>Write video from numpy array.</p> <p>Save a numpy array as a video file. Supports .npy format for saving raw numpy arrays and various video formats (mp4, avi, mkv) using OpenCV. For video formats, automatically selects a suitable codec and handles file extensions.</p> <p>Parameters:</p> Name Type Description Default <code>np_array</code> <code>ndarray of uint8</code> <p>Input array containing video frames.</p> required <code>vid_name</code> <code>str</code> <p>Filename for the output video. Can include extension or not (defaults to .mp4).</p> required <code>is_color</code> <code>bool</code> <p>Whether the video should be written in color. Defaults to True.</p> <code>True</code> <code>fps</code> <code>int</code> <p>Frame rate for the video in frames per second. Defaults to 40.</p> <code>40</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; video_array = np.random.randint(0, 255, size=(10, 100, 100, 3), dtype=np.uint8)\n&gt;&gt;&gt; write_video(video_array, 'output.mp4', True, 30)\nSaves `video_array` as a color video 'output.mp4' with FPS 30.\n&gt;&gt;&gt; video_array = np.random.randint(0, 255, size=(10, 100, 100), dtype=np.uint8)\n&gt;&gt;&gt; write_video(video_array, 'raw_data.npy')\nSaves `video_array` as a raw numpy array file without frame rate.\n</code></pre> Source code in <code>src/cellects/utils/load_display_save.py</code> <pre><code>def write_video(np_array: NDArray[np.uint8], vid_name: str, is_color: bool=True, fps: int=40):\n    \"\"\"\n    Write video from numpy array.\n\n    Save a numpy array as a video file. Supports .npy format for saving raw\n    numpy arrays and various video formats (mp4, avi, mkv) using OpenCV.\n    For video formats, automatically selects a suitable codec and handles\n    file extensions.\n\n    Parameters\n    ----------\n    np_array : ndarray of uint8\n        Input array containing video frames.\n    vid_name : str\n        Filename for the output video. Can include extension or not (defaults to .mp4).\n    is_color : bool, optional\n        Whether the video should be written in color. Defaults to True.\n    fps : int, optional\n        Frame rate for the video in frames per second. Defaults to 40.\n\n    Examples\n    --------\n    &gt;&gt;&gt; video_array = np.random.randint(0, 255, size=(10, 100, 100, 3), dtype=np.uint8)\n    &gt;&gt;&gt; write_video(video_array, 'output.mp4', True, 30)\n    Saves `video_array` as a color video 'output.mp4' with FPS 30.\n    &gt;&gt;&gt; video_array = np.random.randint(0, 255, size=(10, 100, 100), dtype=np.uint8)\n    &gt;&gt;&gt; write_video(video_array, 'raw_data.npy')\n    Saves `video_array` as a raw numpy array file without frame rate.\n    \"\"\"\n    #h265 ou h265 (mp4)\n    # linux: fourcc = 0x00000021 -&gt; don't forget to change it bellow as well\n    if vid_name[-4:] == '.npy':\n        with open(vid_name, 'wb') as file:\n             np.save(file, np_array)\n    else:\n        valid_extensions = ['.mp4', '.avi', '.mkv']\n        vid_ext = vid_name[-4:]\n        if vid_ext not in valid_extensions:\n            vid_name = vid_name[:-4]\n            vid_name += '.mp4'\n            vid_ext = '.mp4'\n        if vid_ext =='.mp4':\n            fourcc = 0x7634706d# VideoWriter_fourcc(*'FMP4') #(*'MP4V') (*'h265') (*'x264') (*'DIVX')\n        else:\n            fourcc = cv2.VideoWriter_fourcc('F', 'F', 'V', '1')  # lossless\n        size = np_array.shape[2], np_array.shape[1]\n        vid = cv2.VideoWriter(vid_name, fourcc, float(fps), tuple(size), is_color)\n        for image_i in np.arange(np_array.shape[0]):\n            image = np_array[image_i, ...]\n            vid.write(image)\n        vid.release()\n</code></pre>"},{"location":"api/cellects/utils/load_display_save/#cellects.utils.load_display_save.write_video_sets","title":"<code>write_video_sets(img_list, sizes, vid_names, crop_coord, bounding_boxes, bunch_nb, video_nb_per_bunch, remaining, raw_images, is_landscape, use_list_of_vid, in_colors=False, reduce_image_dim=False, pathway='')</code>","text":"<p>Write video sets from a list of images, applying cropping and optional rotation.</p> <p>Parameters:</p> Name Type Description Default <code>img_list</code> <code>list</code> <p>List of image file names.</p> required <code>sizes</code> <code>NDArray</code> <p>Array containing the dimensions of each video frame.</p> required <code>vid_names</code> <code>list</code> <p>List of video file names to be saved.</p> required <code>crop_coord</code> <code>dict or tuple</code> <p>Coordinates for cropping regions of interest in images/videos.</p> required <code>bounding_boxes</code> <code>tuple</code> <p>Bounding box coordinates to extract sub-images from the original images.</p> required <code>bunch_nb</code> <code>int</code> <p>Number of bunches to divide the videos into.</p> required <code>video_nb_per_bunch</code> <code>int</code> <p>Number of videos per bunch.</p> required <code>remaining</code> <code>int</code> <p>Number of videos remaining after the last full bunch.</p> required <code>raw_images</code> <code>bool</code> <p>Whether the images are in raw format.</p> required <code>is_landscape</code> <code>bool</code> <p>If true, rotate the images to landscape orientation before processing.</p> required <code>use_list_of_vid</code> <code>bool</code> <p>Flag indicating if the output should be a list of videos.</p> required <code>in_colors</code> <code>bool</code> <p>If true, process images with color information. Default is False.</p> <code>False</code> <code>reduce_image_dim</code> <code>bool</code> <p>If true, reduce image dimensions. Default is False.</p> <code>False</code> <code>pathway</code> <code>str</code> <p>Path where the videos should be saved. Default is an empty string.</p> <code>''</code> Source code in <code>src/cellects/utils/load_display_save.py</code> <pre><code>def write_video_sets(img_list: list, sizes: NDArray, vid_names: list, crop_coord, bounding_boxes,\n                      bunch_nb: int, video_nb_per_bunch: int, remaining: int,\n                      raw_images: bool, is_landscape: bool, use_list_of_vid: bool,\n                      in_colors: bool=False, reduce_image_dim: bool=False, pathway: str=\"\"):\n    \"\"\"\n    Write video sets from a list of images, applying cropping and optional rotation.\n\n    Parameters\n    ----------\n    img_list : list\n        List of image file names.\n    sizes : NDArray\n        Array containing the dimensions of each video frame.\n    vid_names : list\n        List of video file names to be saved.\n    crop_coord : dict or tuple\n        Coordinates for cropping regions of interest in images/videos.\n    bounding_boxes : tuple\n        Bounding box coordinates to extract sub-images from the original images.\n    bunch_nb : int\n        Number of bunches to divide the videos into.\n    video_nb_per_bunch : int\n        Number of videos per bunch.\n    remaining : int\n        Number of videos remaining after the last full bunch.\n    raw_images : bool\n        Whether the images are in raw format.\n    is_landscape : bool\n        If true, rotate the images to landscape orientation before processing.\n    use_list_of_vid : bool\n        Flag indicating if the output should be a list of videos.\n    in_colors : bool, optional\n        If true, process images with color information. Default is False.\n    reduce_image_dim : bool, optional\n        If true, reduce image dimensions. Default is False.\n    pathway : str, optional\n        Path where the videos should be saved. Default is an empty string.\n    \"\"\"\n    top, bot, left, right = bounding_boxes\n    for bunch in np.arange(bunch_nb):\n        print(f'\\nSaving the bunch n: {bunch + 1} / {bunch_nb} of videos:', end=' ')\n        if bunch == (bunch_nb - 1) and remaining &gt; 0:\n            arenas = np.arange(bunch * video_nb_per_bunch, bunch * video_nb_per_bunch + remaining, dtype=np.uint32)\n        else:\n            arenas = np.arange(bunch * video_nb_per_bunch, (bunch + 1) * video_nb_per_bunch, dtype=np.uint32)\n        if use_list_of_vid:\n            video_bunch = [np.zeros(sizes[i, :], dtype=np.uint8) for i in arenas]\n        else:\n            video_bunch = np.zeros(np.append(sizes[0, :], len(arenas)), dtype=np.uint8)\n        prev_img = None\n        images_done = bunch * len(img_list)\n        for image_i, image_name in enumerate(img_list):\n            img = read_and_rotate(image_name, prev_img, raw_images, is_landscape, crop_coord)\n            prev_img = img.copy()\n            if not in_colors and reduce_image_dim:\n                img = img[:, :, 0]\n\n            for arena_i, arena_name in enumerate(arenas):\n                # arena_i = 0; arena_name = arena[arena_i]\n                sub_img = img[top[arena_name]: (bot[arena_name] + 1), left[arena_name]: (right[arena_name] + 1), ...]\n                if use_list_of_vid:\n                    video_bunch[arena_i][image_i, ...] = sub_img\n                else:\n                    if len(video_bunch.shape) == 5:\n                        video_bunch[image_i, :, :, :, arena_i] = sub_img\n                    else:\n                        video_bunch[image_i, :, :, arena_i] = sub_img\n        for arena_i, arena_name in enumerate(arenas):\n            if use_list_of_vid:\n                 np.save(pathway + vid_names[arena_name], video_bunch[arena_i])\n            else:\n                if len(video_bunch.shape) == 5:\n                     np.save(pathway + vid_names[arena_name], video_bunch[:, :, :, :, arena_i])\n                else:\n                     np.save(pathway + vid_names[arena_name], video_bunch[:, :, :, arena_i])\n</code></pre>"},{"location":"api/cellects/utils/load_display_save/#cellects.utils.load_display_save.zoom_on_nonzero","title":"<code>zoom_on_nonzero(binary_image, padding=2, return_coord=True)</code>","text":"<p>Crops a binary image around non-zero elements with optional padding and returns either coordinates or cropped region.</p> <p>Parameters:</p> Name Type Description Default <code>binary_image</code> <code>NDArray</code> <p>2D NumPy array containing binary values (0/1)</p> required <code>padding</code> <code>int</code> <p>Amount of zero-padding to add around the minimum bounding box</p> <code>2</code> <code>return_coord</code> <code>bool</code> <p>If True, return slice coordinates instead of cropped image</p> <code>True</code> <p>Returns:</p> Type Description <code>    If `return_coord` is True: [y_min, y_max, x_min, x_max] as 4-element Tuple.</code> <p>If False: 2D binary array representing the cropped region defined by non-zero elements plus padding.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; img = np.zeros((10,10))\n&gt;&gt;&gt; img[3:7,4:6] = 1\n&gt;&gt;&gt; result = zoom_on_nonzero(img)\n&gt;&gt;&gt; print(result)\n[1 8 2 7]\n&gt;&gt;&gt; cropped = zoom_on_nonzero(img, return_coord=False)\n&gt;&gt;&gt; print(cropped.shape)\n(6, 5)\n</code></pre> Notes <ul> <li>Returns empty slice coordinates if input contains no non-zero elements.</li> <li>Coordinate indices are 0-based and compatible with NumPy array slicing syntax.</li> </ul> Source code in <code>src/cellects/utils/load_display_save.py</code> <pre><code>def zoom_on_nonzero(binary_image:NDArray, padding: int = 2, return_coord: bool=True):\n    \"\"\"\n    Crops a binary image around non-zero elements with optional padding and returns either coordinates or cropped region.\n\n    Parameters\n    ----------\n    binary_image : NDArray\n        2D NumPy array containing binary values (0/1)\n    padding : int, default=2\n        Amount of zero-padding to add around the minimum bounding box\n    return_coord : bool, default=True\n        If True, return slice coordinates instead of cropped image\n\n    Returns\n    -------\n        If `return_coord` is True: [y_min, y_max, x_min, x_max] as 4-element Tuple.\n        If False: 2D binary array representing the cropped region defined by non-zero elements plus padding.\n\n    Examples\n    --------\n    &gt;&gt;&gt; img = np.zeros((10,10))\n    &gt;&gt;&gt; img[3:7,4:6] = 1\n    &gt;&gt;&gt; result = zoom_on_nonzero(img)\n    &gt;&gt;&gt; print(result)\n    [1 8 2 7]\n    &gt;&gt;&gt; cropped = zoom_on_nonzero(img, return_coord=False)\n    &gt;&gt;&gt; print(cropped.shape)\n    (6, 5)\n\n    Notes\n    -----\n    - Returns empty slice coordinates if input contains no non-zero elements.\n    - Coordinate indices are 0-based and compatible with NumPy array slicing syntax.\n    \"\"\"\n    y, x = np.nonzero(binary_image)\n    cy_min = np.max((0, y.min() - padding))\n    cy_max = np.min((binary_image.shape[0], y.max() + padding + 1))\n    cx_min = np.max((0, x.min() - padding))\n    cx_max = np.min((binary_image.shape[1], x.max() + padding + 1))\n    if return_coord:\n        return cy_min, cy_max, cx_min, cx_max\n    else:\n        return binary_image[cy_min:cy_max, cx_min:cx_max]\n</code></pre>"},{"location":"api/cellects/utils/utilitarian/","title":"<code>cellects.utils.utilitarian</code>","text":""},{"location":"api/cellects/utils/utilitarian/#cellects.utils.utilitarian","title":"<code>cellects.utils.utilitarian</code>","text":"<p>Utility module with array operations, path manipulation, and progress tracking.</p> <p>This module provides performance-optimized utilities for numerical comparisons using Numba, path string truncation, dictionary filtering, and iteration progress monitoring. It is designed for applications requiring efficient data processing pipelines with both low-level optimization and human-readable output formatting.</p> <p>Classes:</p> Name Description <code>PercentAndTimeTracker : Track iteration progress with time estimates and percentage completion</code> <p>Functions:</p> Name Description <code>greater_along_first_axis : Compare arrays element-wise along first axis</code> <code>less_along_first_axis    : Compare arrays element-wise along first axis</code> <code>translate_dict           : Convert standard dict to typed dict, filtering non-string values</code> <code>reduce_path_len          : Truncate long path strings with ellipsis insertion</code> <code>find_nearest             : Find array element closest to target value</code> Notes <p>Numba-optimized functions (greater_along_first_axis and less_along_first_axis) require input arrays of identical shape. String manipulation utilities include automatic type conversion. The progress tracker records initialization time for potential performance analysis.</p>"},{"location":"api/cellects/utils/utilitarian/#cellects.utils.utilitarian.PercentAndTimeTracker","title":"<code>PercentAndTimeTracker</code>","text":"<p>Initialize a progress bar object to track and display the progress of an iteration.</p> <p>Parameters:</p> Name Type Description Default <code>total</code> <code>int</code> <p>The total number of iterations.</p> required <code>compute_with_elements_number</code> <code>bool</code> <p>If True, create an element vector. Default is False.</p> <code>False</code> <code>core_number</code> <code>int</code> <p>The number of cores to use. Default is 1.</p> <code>1</code> <p>Attributes:</p> Name Type Description <code>starting_time</code> <code>float</code> <p>The time when the ProgressBar object is initialized.</p> <code>total</code> <code>int</code> <p>The total number of iterations.</p> <code>current_step</code> <code>int</code> <p>The current iteration step (initialized to 0).</p> <code>element_vector</code> <code>(ndarray, optional)</code> <p>A vector of zeros with the same length as <code>total</code>, created if <code>compute_with_elements_number</code> is True.</p> <code>core_number</code> <code>int</code> <p>The number of cores.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; p = PercentAndTimeTracker(10)\n&gt;&gt;&gt; print(p.total)  # prints: 10\n&gt;&gt;&gt; p = PercentAndTimeTracker(10, compute_with_elements_number=True)\n&gt;&gt;&gt; print(p.element_vector)  # prints: [0 0 0 0 0 0 0 0 0 0]\n</code></pre> Notes <p>Starting time is recorded for potential performance tracking.</p> Source code in <code>src/cellects/utils/utilitarian.py</code> <pre><code>class PercentAndTimeTracker:\n    \"\"\"\n    Initialize a progress bar object to track and display the progress of an iteration.\n\n    Parameters\n    ----------\n    total : int\n        The total number of iterations.\n    compute_with_elements_number : bool, optional\n        If True, create an element vector. Default is False.\n    core_number : int, optional\n        The number of cores to use. Default is 1.\n\n    Attributes\n    ----------\n    starting_time : float\n        The time when the ProgressBar object is initialized.\n    total : int\n        The total number of iterations.\n    current_step : int\n        The current iteration step (initialized to 0).\n    element_vector : numpy.ndarray, optional\n        A vector of zeros with the same length as `total`, created if\n        `compute_with_elements_number` is True.\n    core_number : int\n        The number of cores.\n\n    Examples\n    --------\n    &gt;&gt;&gt; p = PercentAndTimeTracker(10)\n    &gt;&gt;&gt; print(p.total)  # prints: 10\n    &gt;&gt;&gt; p = PercentAndTimeTracker(10, compute_with_elements_number=True)\n    &gt;&gt;&gt; print(p.element_vector)  # prints: [0 0 0 0 0 0 0 0 0 0]\n\n    Notes\n    -----\n    Starting time is recorded for potential performance tracking.\n\n    \"\"\"\n    def __init__(self, total: int, compute_with_elements_number: bool=False, core_number:int =1):\n        \"\"\"Initialize an instance of the class.\n\n        This constructor sets up the initial attributes including\n        a starting time, total value, current step, and an optional\n        element vector if ``compute_with_elements_number`` is set to True.\n        The core number can be specified, defaulting to 1.\n\n        Parameters\n        ----------\n        total : int\n            The total number of elements or steps.\n        compute_with_elements_number : bool, optional\n            If True, initialize an element vector of zeros. Defaults to False.\n        core_number : int, optional\n            The number of cores to use. Defaults to 1.\n\n        Attributes\n        ----------\n        starting_time : float\n            The time of instantiation.\n        total : int\n            The total number of elements or steps.\n        current_step : int\n            The current step in the process.\n        element_vector : ndarray of int64, optional\n            A vector initialized with zeros. Exists if ``compute_with_elements_number`` is True.\n        core_number : int\n            The number of cores to use.\n        \"\"\"\n        self.starting_time = default_timer()\n        self.total = total\n        self.current_step = 0\n        if compute_with_elements_number:\n            self.element_vector = np.zeros(total, dtype=np.int64)\n        self.core_number = core_number\n\n    def get_progress(self, step=None, element_number=None):\n        \"\"\"\n        Calculate and update the current progress, including elapsed time and estimated remaining time.\n\n        This function updates the internal state of the object to reflect progress\n        based on the current step and element number. It calculates elapsed time,\n        estimates total time, and computes the estimated time of arrival (ETA).\n\n        Parameters\n        ----------\n        step : int or None, optional\n            The current step of the process. If ``None``, the internal counter is incremented.\n        element_number : int or None, optional\n            The current element number. If ``None``, no update is made to the element vector.\n\n        Returns\n        -------\n        tuple\n            A tuple containing:\n            - `int`: The current progress percentage.\n            - `str`: A string with the ETA and remaining time.\n\n        Raises\n        ------\n        ValueError\n            If ``step`` or ``element_number`` are invalid.\n\n        Notes\n        -----\n        The function uses linear regression to estimate future progress values when the current step is sufficiently large.\n\n        Examples\n        --------\n        &gt;&gt;&gt; PercentAndTimeTracker(10, compute_with_elements_number=True).get_progress(9, 5)\n        (0, ', wait to get a more accurate ETA...')\n        \"\"\"\n        if step is not None:\n            self.current_step = step\n        if element_number is not None:\n            self.element_vector[self.current_step] = element_number\n\n        if self.current_step &gt; 0:\n            elapsed_time = default_timer() - self.starting_time\n            if element_number is None or element_number == 0 or self.current_step &lt; 15:\n                if self.current_step &lt; self.core_number:\n                    current_prop = self.core_number / self.total\n                else:\n                    current_prop = (self.current_step + 1) / self.total\n            else:\n                x_mat = np.array([np.ones(self.current_step - 4), np.arange(5, self.current_step + 1)]).T\n                coefs = np.linalg.lstsq(x_mat, self.element_vector[5:self.current_step + 1], rcond=-1)[0]\n                self.element_vector = coefs[0] + (np.arange(self.total) * coefs[1])\n                self.element_vector[self.element_vector &lt; 0] = 0\n                current_prop = self.element_vector[:self.current_step + 1].sum() / self.element_vector.sum()\n\n            total_time = elapsed_time / current_prop\n            current_prop = int(np.round(current_prop * 100))\n            remaining_time_s = total_time - elapsed_time\n\n            local_time = time.localtime()\n            local_m = int(time.strftime(\"%M\", local_time))\n            local_h = int(time.strftime(\"%H\", local_time))\n            remaining_time_h = remaining_time_s // 3600\n            reste_s = remaining_time_s % 3600\n            reste_m = reste_s // 60\n            # + str(int(np.floor(reste_s % 60))) + \"S\"\n            hours = int(np.floor(remaining_time_h))\n            minutes = int(np.floor(reste_m))\n\n            if (local_m + minutes) &lt; 60:\n                eta_m = local_m + minutes\n            else:\n                eta_m = (local_m + minutes) % 60\n                local_h += 1\n\n            if (local_h + hours) &lt; 24:\n                output = current_prop, f\", ETA {local_h + hours}:{eta_m} ({hours}:{minutes} left)\"\n            else:\n                days = (local_h + hours) // 24\n                eta_h = (local_h + hours) % 24\n                eta_d = time.strftime(\"%m\", local_time) + \"/\" + str(int(time.strftime(\"%d\", local_time)) + days)\n                output = current_prop, f\", ETA {eta_d} {eta_h}:{eta_m} ({hours}:{minutes} left)\"\n            # return current_prop, str(local_h + hours) + \":\" + str(local_m + minutes) + \"(\" + str()\n        else:\n            output = int(np.round(100 / self.total)), \", wait...\"\n        if step is None:\n            self.current_step += 1\n        if element_number is not None:\n            if self.current_step &lt; 50:\n                output = int(0), \", wait to get a more accurate ETA...\"\n        return output\n</code></pre>"},{"location":"api/cellects/utils/utilitarian/#cellects.utils.utilitarian.PercentAndTimeTracker.__init__","title":"<code>__init__(total, compute_with_elements_number=False, core_number=1)</code>","text":"<p>Initialize an instance of the class.</p> <p>This constructor sets up the initial attributes including a starting time, total value, current step, and an optional element vector if <code>compute_with_elements_number</code> is set to True. The core number can be specified, defaulting to 1.</p> <p>Parameters:</p> Name Type Description Default <code>total</code> <code>int</code> <p>The total number of elements or steps.</p> required <code>compute_with_elements_number</code> <code>bool</code> <p>If True, initialize an element vector of zeros. Defaults to False.</p> <code>False</code> <code>core_number</code> <code>int</code> <p>The number of cores to use. Defaults to 1.</p> <code>1</code> <p>Attributes:</p> Name Type Description <code>starting_time</code> <code>float</code> <p>The time of instantiation.</p> <code>total</code> <code>int</code> <p>The total number of elements or steps.</p> <code>current_step</code> <code>int</code> <p>The current step in the process.</p> <code>element_vector</code> <code>ndarray of int64, optional</code> <p>A vector initialized with zeros. Exists if <code>compute_with_elements_number</code> is True.</p> <code>core_number</code> <code>int</code> <p>The number of cores to use.</p> Source code in <code>src/cellects/utils/utilitarian.py</code> <pre><code>def __init__(self, total: int, compute_with_elements_number: bool=False, core_number:int =1):\n    \"\"\"Initialize an instance of the class.\n\n    This constructor sets up the initial attributes including\n    a starting time, total value, current step, and an optional\n    element vector if ``compute_with_elements_number`` is set to True.\n    The core number can be specified, defaulting to 1.\n\n    Parameters\n    ----------\n    total : int\n        The total number of elements or steps.\n    compute_with_elements_number : bool, optional\n        If True, initialize an element vector of zeros. Defaults to False.\n    core_number : int, optional\n        The number of cores to use. Defaults to 1.\n\n    Attributes\n    ----------\n    starting_time : float\n        The time of instantiation.\n    total : int\n        The total number of elements or steps.\n    current_step : int\n        The current step in the process.\n    element_vector : ndarray of int64, optional\n        A vector initialized with zeros. Exists if ``compute_with_elements_number`` is True.\n    core_number : int\n        The number of cores to use.\n    \"\"\"\n    self.starting_time = default_timer()\n    self.total = total\n    self.current_step = 0\n    if compute_with_elements_number:\n        self.element_vector = np.zeros(total, dtype=np.int64)\n    self.core_number = core_number\n</code></pre>"},{"location":"api/cellects/utils/utilitarian/#cellects.utils.utilitarian.PercentAndTimeTracker.get_progress","title":"<code>get_progress(step=None, element_number=None)</code>","text":"<p>Calculate and update the current progress, including elapsed time and estimated remaining time.</p> <p>This function updates the internal state of the object to reflect progress based on the current step and element number. It calculates elapsed time, estimates total time, and computes the estimated time of arrival (ETA).</p> <p>Parameters:</p> Name Type Description Default <code>step</code> <code>int or None</code> <p>The current step of the process. If <code>None</code>, the internal counter is incremented.</p> <code>None</code> <code>element_number</code> <code>int or None</code> <p>The current element number. If <code>None</code>, no update is made to the element vector.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing: - <code>int</code>: The current progress percentage. - <code>str</code>: A string with the ETA and remaining time.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>step</code> or <code>element_number</code> are invalid.</p> Notes <p>The function uses linear regression to estimate future progress values when the current step is sufficiently large.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; PercentAndTimeTracker(10, compute_with_elements_number=True).get_progress(9, 5)\n(0, ', wait to get a more accurate ETA...')\n</code></pre> Source code in <code>src/cellects/utils/utilitarian.py</code> <pre><code>def get_progress(self, step=None, element_number=None):\n    \"\"\"\n    Calculate and update the current progress, including elapsed time and estimated remaining time.\n\n    This function updates the internal state of the object to reflect progress\n    based on the current step and element number. It calculates elapsed time,\n    estimates total time, and computes the estimated time of arrival (ETA).\n\n    Parameters\n    ----------\n    step : int or None, optional\n        The current step of the process. If ``None``, the internal counter is incremented.\n    element_number : int or None, optional\n        The current element number. If ``None``, no update is made to the element vector.\n\n    Returns\n    -------\n    tuple\n        A tuple containing:\n        - `int`: The current progress percentage.\n        - `str`: A string with the ETA and remaining time.\n\n    Raises\n    ------\n    ValueError\n        If ``step`` or ``element_number`` are invalid.\n\n    Notes\n    -----\n    The function uses linear regression to estimate future progress values when the current step is sufficiently large.\n\n    Examples\n    --------\n    &gt;&gt;&gt; PercentAndTimeTracker(10, compute_with_elements_number=True).get_progress(9, 5)\n    (0, ', wait to get a more accurate ETA...')\n    \"\"\"\n    if step is not None:\n        self.current_step = step\n    if element_number is not None:\n        self.element_vector[self.current_step] = element_number\n\n    if self.current_step &gt; 0:\n        elapsed_time = default_timer() - self.starting_time\n        if element_number is None or element_number == 0 or self.current_step &lt; 15:\n            if self.current_step &lt; self.core_number:\n                current_prop = self.core_number / self.total\n            else:\n                current_prop = (self.current_step + 1) / self.total\n        else:\n            x_mat = np.array([np.ones(self.current_step - 4), np.arange(5, self.current_step + 1)]).T\n            coefs = np.linalg.lstsq(x_mat, self.element_vector[5:self.current_step + 1], rcond=-1)[0]\n            self.element_vector = coefs[0] + (np.arange(self.total) * coefs[1])\n            self.element_vector[self.element_vector &lt; 0] = 0\n            current_prop = self.element_vector[:self.current_step + 1].sum() / self.element_vector.sum()\n\n        total_time = elapsed_time / current_prop\n        current_prop = int(np.round(current_prop * 100))\n        remaining_time_s = total_time - elapsed_time\n\n        local_time = time.localtime()\n        local_m = int(time.strftime(\"%M\", local_time))\n        local_h = int(time.strftime(\"%H\", local_time))\n        remaining_time_h = remaining_time_s // 3600\n        reste_s = remaining_time_s % 3600\n        reste_m = reste_s // 60\n        # + str(int(np.floor(reste_s % 60))) + \"S\"\n        hours = int(np.floor(remaining_time_h))\n        minutes = int(np.floor(reste_m))\n\n        if (local_m + minutes) &lt; 60:\n            eta_m = local_m + minutes\n        else:\n            eta_m = (local_m + minutes) % 60\n            local_h += 1\n\n        if (local_h + hours) &lt; 24:\n            output = current_prop, f\", ETA {local_h + hours}:{eta_m} ({hours}:{minutes} left)\"\n        else:\n            days = (local_h + hours) // 24\n            eta_h = (local_h + hours) % 24\n            eta_d = time.strftime(\"%m\", local_time) + \"/\" + str(int(time.strftime(\"%d\", local_time)) + days)\n            output = current_prop, f\", ETA {eta_d} {eta_h}:{eta_m} ({hours}:{minutes} left)\"\n        # return current_prop, str(local_h + hours) + \":\" + str(local_m + minutes) + \"(\" + str()\n    else:\n        output = int(np.round(100 / self.total)), \", wait...\"\n    if step is None:\n        self.current_step += 1\n    if element_number is not None:\n        if self.current_step &lt; 50:\n            output = int(0), \", wait to get a more accurate ETA...\"\n    return output\n</code></pre>"},{"location":"api/cellects/utils/utilitarian/#cellects.utils.utilitarian.find_nearest","title":"<code>find_nearest(array, value)</code>","text":"<p>Find the element in an array that is closest to a given value.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>array_like</code> <p>Input array. Can be any array-like data structure.</p> required <code>value</code> <code>int or float</code> <p>The value to find the closest element to.</p> required <p>Returns:</p> Type Description <code>obj:`array` type</code> <p>The element in <code>array</code> that is closest to <code>value</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; find_nearest([1, 2, 3, 4], 2.5)\n2\n</code></pre> Source code in <code>src/cellects/utils/utilitarian.py</code> <pre><code>def find_nearest(array: NDArray, value):\n    \"\"\"\n    Find the element in an array that is closest to a given value.\n\n    Parameters\n    ----------\n    array : array_like\n        Input array. Can be any array-like data structure.\n    value : int or float\n        The value to find the closest element to.\n\n    Returns\n    -------\n    :obj:`array` type\n        The element in `array` that is closest to `value`.\n\n    Examples\n    --------\n    &gt;&gt;&gt; find_nearest([1, 2, 3, 4], 2.5)\n    2\n    \"\"\"\n    array = np.asarray(array)\n    idx = (np.abs(array - value)).argmin()\n    return array[idx]\n</code></pre>"},{"location":"api/cellects/utils/utilitarian/#cellects.utils.utilitarian.greater_along_first_axis","title":"<code>greater_along_first_axis(array_in_1, array_in_2)</code>","text":"<p>Compare two arrays along the first axis and store the result in a third array.</p> <p>This function performs a comparison between two input arrays along their first axis and stores the result in a third array. The comparison is made to determine which elements of each row of the first array are greater than the elements(s) corresponding to that row in the second array.</p> <p>Parameters:</p> Name Type Description Default <code>array_in_1</code> <code>ndarray</code> <p>First input array.</p> required <code>array_in_2</code> <code>ndarray</code> <p>Second input array.</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>ndarray of uint8</code> <p>Boolean ndarray with same shape as input arrays, containing the result of element-wise comparison.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; array_in_1 = np.array([[2, 4], [5, 8]])\n&gt;&gt;&gt; array_in_2 = np.array([3, 6])\n&gt;&gt;&gt; array_out = greater_along_first_axis(array_in_1, array_in_2)\n&gt;&gt;&gt; print(array_out)\n[[0 1]\n [0 1]]\n</code></pre> Source code in <code>src/cellects/utils/utilitarian.py</code> <pre><code>@njit()\ndef greater_along_first_axis(array_in_1: NDArray, array_in_2: NDArray) -&gt; NDArray[np.uint8]:\n    \"\"\"\n    Compare two arrays along the first axis and store the result in a third array.\n\n    This function performs a comparison between two input arrays\n    along their first axis and stores the result in a third array. The comparison is\n    made to determine which elements of each row of the first array are greater than\n    the elements(s) corresponding to that row in the second array.\n\n    Parameters\n    ----------\n    array_in_1 : ndarray\n        First input array.\n    array_in_2 : ndarray\n        Second input array.\n\n    Returns\n    -------\n    out : ndarray of uint8\n        Boolean ndarray with same shape as input arrays,\n        containing the result of element-wise comparison.\n\n    Examples\n    --------\n    &gt;&gt;&gt; array_in_1 = np.array([[2, 4], [5, 8]])\n    &gt;&gt;&gt; array_in_2 = np.array([3, 6])\n    &gt;&gt;&gt; array_out = greater_along_first_axis(array_in_1, array_in_2)\n    &gt;&gt;&gt; print(array_out)\n    [[0 1]\n     [0 1]]\n    \"\"\"\n    array_out = np.zeros(array_in_1.shape, dtype=np.uint8)\n    for i, value in enumerate(array_in_2):\n        array_out[i, ...] = array_in_1[i, ...] &gt; value\n    return array_out\n</code></pre>"},{"location":"api/cellects/utils/utilitarian/#cellects.utils.utilitarian.insensitive_glob","title":"<code>insensitive_glob(pattern)</code>","text":"<p>Generates a glob pattern that matches both lowercase and uppercase letters.</p> <p>Parameters:</p> Name Type Description Default <code>pattern</code> <code>str</code> <p>The glob pattern to be made case-insensitive.</p> required <p>Returns:</p> Type Description <code>str</code> <p>A new glob pattern that will match both lowercase and uppercase letters.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; insensitive_glob('*.TXT')\n</code></pre> Source code in <code>src/cellects/utils/utilitarian.py</code> <pre><code>def insensitive_glob(pattern: str):\n    \"\"\"\n    Generates a glob pattern that matches both lowercase and uppercase letters.\n\n    Parameters\n    ----------\n    pattern : str\n        The glob pattern to be made case-insensitive.\n\n    Returns\n    -------\n    str\n        A new glob pattern that will match both lowercase and uppercase letters.\n\n    Examples\n    --------\n    &gt;&gt;&gt; insensitive_glob('*.TXT')\n    \"\"\"\n    def either(c):\n        return '[%s%s]' % (c.lower(), c.upper()) if c.isalpha() else c\n    return glob(''.join(map(either, pattern)))\n</code></pre>"},{"location":"api/cellects/utils/utilitarian/#cellects.utils.utilitarian.less_along_first_axis","title":"<code>less_along_first_axis(array_in_1, array_in_2)</code>","text":"<p>Compare two arrays along the first axis and store the result in a third array.</p> <p>This function performs a comparison between two input arrays along their first axis and stores the result in a third array. The comparison is made to determine which elements of each row of the first array are lesser than the elements(s) corresponding to that row in the second array.</p> <p>Parameters:</p> Name Type Description Default <code>array_in_1</code> <code>ndarray</code> <p>The first input array.</p> required <code>array_in_2</code> <code>ndarray</code> <p>The second input array.</p> required <p>Returns:</p> Type Description <code>ndarray of uint8</code> <p>A boolean array where each element is <code>True</code> if the corresponding element in <code>array_in_1</code> is lesser than the corresponding element in <code>array_in_2</code>, and <code>False</code> otherwise.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; array_in_1 = np.array([[2, 4], [5, 8]])\n&gt;&gt;&gt; array_in_2 = np.array([3, 6])\n&gt;&gt;&gt; array_out = less_along_first_axis(array_in_1, array_in_2)\n&gt;&gt;&gt; print(array_out)\n[[1 0]\n [1 0]]\n</code></pre> Source code in <code>src/cellects/utils/utilitarian.py</code> <pre><code>@njit()\ndef less_along_first_axis(array_in_1: NDArray, array_in_2: NDArray) -&gt; NDArray[np.uint8]:\n    \"\"\"\n    Compare two arrays along the first axis and store the result in a third array.\n\n    This function performs a comparison between two input arrays\n    along their first axis and stores the result in a third array. The comparison is\n    made to determine which elements of each row of the first array are lesser than\n    the elements(s) corresponding to that row in the second array.\n\n    Parameters\n    ----------\n    array_in_1 : ndarray\n        The first input array.\n    array_in_2 : ndarray\n        The second input array.\n\n    Returns\n    -------\n    ndarray of uint8\n        A boolean array where each element is `True` if the corresponding\n        element in `array_in_1` is lesser than the corresponding element\n        in `array_in_2`, and `False` otherwise.\n\n    Examples\n    --------\n    &gt;&gt;&gt; array_in_1 = np.array([[2, 4], [5, 8]])\n    &gt;&gt;&gt; array_in_2 = np.array([3, 6])\n    &gt;&gt;&gt; array_out = less_along_first_axis(array_in_1, array_in_2)\n    &gt;&gt;&gt; print(array_out)\n    [[1 0]\n     [1 0]]\n    \"\"\"\n    array_out = np.zeros(array_in_1.shape, dtype=np.uint8)\n    for i, value in enumerate(array_in_2):\n        array_out[i, ...] = array_in_1[i, ...] &lt; value\n    return array_out\n</code></pre>"},{"location":"api/cellects/utils/utilitarian/#cellects.utils.utilitarian.reduce_path_len","title":"<code>reduce_path_len(pathway, to_start, from_end)</code>","text":"<p>Reduce the length of a given pathway string by truncating it from both ends.</p> <p>The function is used to shorten the <code>pathway</code> string if its length exceeds a calculated maximum size. If it does, the function truncates it from both ends, inserting an ellipsis (\"...\") in between.</p> <p>Parameters:</p> Name Type Description Default <code>pathway</code> <code>str</code> <p>The pathway string to be reduced. If an integer is provided, it will be converted into a string.</p> required <code>to_start</code> <code>int</code> <p>Number of characters from the start to keep in the pathway string.</p> required <code>from_end</code> <code>int</code> <p>Number of characters from the end to keep in the pathway string.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The reduced version of the <code>pathway</code> string. If truncation is not necessary, returns the original pathway string.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; reduce_path_len(\"example/complicated/path/to/resource\", 8, 12)\n'example/.../to/resource'\n</code></pre> Source code in <code>src/cellects/utils/utilitarian.py</code> <pre><code>def reduce_path_len(pathway: str, to_start: int, from_end: int) -&gt; str:\n    \"\"\"\n    Reduce the length of a given pathway string by truncating it from both ends.\n\n    The function is used to shorten the `pathway` string if its length exceeds\n    a calculated maximum size. If it does, the function truncates it from both ends,\n    inserting an ellipsis (\"...\") in between.\n\n    Parameters\n    ----------\n    pathway : str\n        The pathway string to be reduced. If an integer is provided,\n        it will be converted into a string.\n    to_start : int\n        Number of characters from the start to keep in the pathway string.\n    from_end : int\n        Number of characters from the end to keep in the pathway string.\n\n    Returns\n    -------\n    str\n        The reduced version of the `pathway` string. If truncation is not necessary,\n        returns the original pathway string.\n\n    Examples\n    --------\n    &gt;&gt;&gt; reduce_path_len(\"example/complicated/path/to/resource\", 8, 12)\n    'example/.../to/resource'\n    \"\"\"\n    if not isinstance(pathway, str):\n        pathway = str(pathway)\n    max_size = to_start + from_end + 3\n    if len(pathway) &gt; max_size:\n        pathway = pathway[:to_start] + \"...\" + pathway[-from_end:]\n    return pathway\n</code></pre>"},{"location":"api/cellects/utils/utilitarian/#cellects.utils.utilitarian.remove_coordinates","title":"<code>remove_coordinates(arr1, arr2)</code>","text":"<p>Remove coordinates from <code>arr1</code> that are present in <code>arr2</code>.</p> <p>Given two arrays of coordinates, remove rows from the first array that match any row in the second array.</p> <p>Parameters:</p> Name Type Description Default <code>arr1</code> <code>ndarray of shape (n, 2)</code> <p>Array containing coordinates to filter.</p> required <code>arr2</code> <code>ndarray of shape (m, 2)</code> <p>Array containing coordinates to match for removal.</p> required <p>Returns:</p> Type Description <code>ndarray of shape (k, 2)</code> <p>Array with coordinates from <code>arr1</code> that are not in <code>arr2</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; arr1 = np.array([[1, 2], [3, 4]])\n&gt;&gt;&gt; arr2 = np.array([[3, 4]])\n&gt;&gt;&gt; remove_coordinates(arr1, arr2)\narray([[1, 2],\n       [3, 4]])\n</code></pre> <pre><code>&gt;&gt;&gt; arr1 = np.array([[1, 2], [3, 4]])\n&gt;&gt;&gt; arr2 = np.array([[3, 2], [1, 4]])\n&gt;&gt;&gt; remove_coordinates(arr1, arr2)\narray([[1, 2],\n       [3, 4]])\n</code></pre> <pre><code>&gt;&gt;&gt; arr1 = np.array([[1, 2], [3, 4]])\n&gt;&gt;&gt; arr2 = np.array([[3, 2], [1, 2]])\n&gt;&gt;&gt; remove_coordinates(arr1, arr2)\narray([[3, 4]])\n</code></pre> <pre><code>&gt;&gt;&gt; arr1 = np.arange(200).reshape(100, 2)\n&gt;&gt;&gt; arr2 = np.array([[196, 197], [198, 199]])\n&gt;&gt;&gt; new_arr1 = remove_coordinates(arr1, arr2)\n&gt;&gt;&gt; new_arr1.shape\n(98, 2)\n</code></pre> Source code in <code>src/cellects/utils/utilitarian.py</code> <pre><code>def remove_coordinates(arr1: NDArray, arr2: NDArray) -&gt; NDArray:\n    \"\"\"\n    Remove coordinates from `arr1` that are present in `arr2`.\n\n    Given two arrays of coordinates, remove rows from the first array\n    that match any row in the second array.\n\n    Parameters\n    ----------\n    arr1 : ndarray of shape (n, 2)\n        Array containing coordinates to filter.\n    arr2 : ndarray of shape (m, 2)\n        Array containing coordinates to match for removal.\n\n    Returns\n    -------\n    ndarray of shape (k, 2)\n        Array with coordinates from `arr1` that are not in `arr2`.\n\n    Examples\n    --------\n    &gt;&gt;&gt; arr1 = np.array([[1, 2], [3, 4]])\n    &gt;&gt;&gt; arr2 = np.array([[3, 4]])\n    &gt;&gt;&gt; remove_coordinates(arr1, arr2)\n    array([[1, 2],\n           [3, 4]])\n\n    &gt;&gt;&gt; arr1 = np.array([[1, 2], [3, 4]])\n    &gt;&gt;&gt; arr2 = np.array([[3, 2], [1, 4]])\n    &gt;&gt;&gt; remove_coordinates(arr1, arr2)\n    array([[1, 2],\n           [3, 4]])\n\n    &gt;&gt;&gt; arr1 = np.array([[1, 2], [3, 4]])\n    &gt;&gt;&gt; arr2 = np.array([[3, 2], [1, 2]])\n    &gt;&gt;&gt; remove_coordinates(arr1, arr2)\n    array([[3, 4]])\n\n    &gt;&gt;&gt; arr1 = np.arange(200).reshape(100, 2)\n    &gt;&gt;&gt; arr2 = np.array([[196, 197], [198, 199]])\n    &gt;&gt;&gt; new_arr1 = remove_coordinates(arr1, arr2)\n    &gt;&gt;&gt; new_arr1.shape\n    (98, 2)\n    \"\"\"\n    if arr2.shape[0] == 0:\n        return arr1\n    else:\n        if arr1.shape[1] != 2 or arr2.shape[1] != 2:\n            raise ValueError(\"Both arrays must have shape (n, 2)\")\n        c_to_keep = ~np.all(arr1 == arr2[0], axis=1)\n        for row in arr2[1:]:\n            c_to_keep *= ~np.all(arr1 == row, axis=1)\n        return arr1[c_to_keep]\n</code></pre>"},{"location":"api/cellects/utils/utilitarian/#cellects.utils.utilitarian.smallest_memory_array","title":"<code>smallest_memory_array(array_object, array_type='uint')</code>","text":"<p>Convert input data to the smallest possible NumPy array type that can hold it.</p> <p>Parameters:</p> Name Type Description Default <code>array_object</code> <code>ndarray or list of lists</code> <p>The input data to be converted.</p> required <code>array_type</code> <code>str</code> <p>The type of NumPy data type to use ('uint').</p> <code>is 'uint'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>A NumPy array of the smallest data type that can hold all values in <code>array_object</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; array = [[1, 2], [3, 4]]\n&gt;&gt;&gt; smallest_memory_array(array)\narray([[1, 2],\n       [3, 4]], dtype=np.uint8)\n</code></pre> <pre><code>&gt;&gt;&gt; array = [[1000, 2000], [3000, 4000]]\n&gt;&gt;&gt; smallest_memory_array(array)\narray([[1000, 2000],\n       [3000, 4000]], dtype=uint16)\n</code></pre> <pre><code>&gt;&gt;&gt; array = [[2**31, 2**32], [2**33, 2**34]]\n&gt;&gt;&gt; smallest_memory_array(array)\narray([[         2147483648,          4294967296],\n       [         8589934592,        17179869184]], dtype=uint64)\n</code></pre> Source code in <code>src/cellects/utils/utilitarian.py</code> <pre><code>def smallest_memory_array(array_object, array_type='uint') -&gt; NDArray:\n    \"\"\"\n    Convert input data to the smallest possible NumPy array type that can hold it.\n\n    Parameters\n    ----------\n    array_object : ndarray or list of lists\n        The input data to be converted.\n    array_type : str, optional, default is 'uint'\n        The type of NumPy data type to use ('uint').\n\n    Returns\n    -------\n    ndarray\n        A NumPy array of the smallest data type that can hold all values in `array_object`.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; array = [[1, 2], [3, 4]]\n    &gt;&gt;&gt; smallest_memory_array(array)\n    array([[1, 2],\n           [3, 4]], dtype=np.uint8)\n\n    &gt;&gt;&gt; array = [[1000, 2000], [3000, 4000]]\n    &gt;&gt;&gt; smallest_memory_array(array)\n    array([[1000, 2000],\n           [3000, 4000]], dtype=uint16)\n\n    &gt;&gt;&gt; array = [[2**31, 2**32], [2**33, 2**34]]\n    &gt;&gt;&gt; smallest_memory_array(array)\n    array([[         2147483648,          4294967296],\n           [         8589934592,        17179869184]], dtype=uint64)\n    \"\"\"\n    if isinstance(array_object, list):\n        array_object = np.array(array_object)\n    if isinstance(array_object, np.ndarray):\n        value_max = array_object.max()\n    else:\n        if len(array_object[0]) &gt; 0:\n            value_max = np.max((array_object[0].max(), array_object[1].max()))\n        else:\n            value_max = 0\n\n    if array_type == 'uint':\n        if value_max &lt;= np.iinfo(np.uint8).max:\n            array_object = np.array(array_object, dtype=np.uint8)\n        elif value_max &lt;= np.iinfo(np.uint16).max:\n            array_object = np.array(array_object, dtype=np.uint16)\n        elif value_max &lt;= np.iinfo(np.uint32).max:\n            array_object = np.array(array_object, dtype=np.uint32)\n        else:\n            array_object = np.array(array_object, dtype=np.uint64)\n    return array_object\n</code></pre>"},{"location":"api/cellects/utils/utilitarian/#cellects.utils.utilitarian.split_dict","title":"<code>split_dict(c_space_dict)</code>","text":"<p>Split a dictionary into two dictionaries based on specific criteria and return their keys.</p> <p>Split the input dictionary <code>c_space_dict</code> into two dictionaries: one for items not ending with '2' and another where the key is truncated by removing its last character if it does end with '2'. Additionally, return the keys that have been processed.</p> <p>Parameters:</p> Name Type Description Default <code>c_space_dict</code> <code>dict</code> <p>The dictionary to be split. Expected keys are strings and values can be any type.</p> required <p>Returns:</p> Name Type Description <code>first_dict</code> <code>dict</code> <p>Dictionary containing items from <code>c_space_dict</code> whose keys do not end with '2'.</p> <code>second_dict</code> <code>dict</code> <p>Dictionary containing items from <code>c_space_dict</code> whose keys end with '2', where the key is truncated by removing its last character.</p> <code>c_spaces</code> <code>list</code> <p>List of keys from <code>c_space_dict</code> that have been processed.</p> <p>Raises:</p> Type Description <code>None</code> Notes <p>No critical information to share.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; c_space_dict = {'key1': 10, 'key2': 20, 'logical': 30}\n&gt;&gt;&gt; first_dict, second_dict, c_spaces = split_dict(c_space_dict)\n&gt;&gt;&gt; print(first_dict)\n{'key1': 10}\n&gt;&gt;&gt; print(second_dict)\n{'key': 20}\n&gt;&gt;&gt; print(c_spaces)\n['key1', 'key']\n</code></pre> Source code in <code>src/cellects/utils/utilitarian.py</code> <pre><code>def split_dict(c_space_dict: dict) -&gt; Tuple[Dict, Dict, list]:\n    \"\"\"\n\n    Split a dictionary into two dictionaries based on specific criteria and return their keys.\n\n    Split the input dictionary `c_space_dict` into two dictionaries: one for items not\n    ending with '2' and another where the key is truncated by removing its last\n    character if it does end with '2'. Additionally, return the keys that have been\n    processed.\n\n    Parameters\n    ----------\n    c_space_dict : dict\n        The dictionary to be split. Expected keys are strings and values can be any type.\n\n    Returns\n    -------\n    first_dict : dict\n        Dictionary containing items from `c_space_dict` whose keys do not end with '2'.\n    second_dict : dict\n        Dictionary containing items from `c_space_dict` whose keys end with '2',\n        where the key is truncated by removing its last character.\n    c_spaces : list\n        List of keys from `c_space_dict` that have been processed.\n\n    Raises\n    ------\n    None\n\n    Notes\n    -----\n    No critical information to share.\n\n    Examples\n    --------\n    &gt;&gt;&gt; c_space_dict = {'key1': 10, 'key2': 20, 'logical': 30}\n    &gt;&gt;&gt; first_dict, second_dict, c_spaces = split_dict(c_space_dict)\n    &gt;&gt;&gt; print(first_dict)\n    {'key1': 10}\n    &gt;&gt;&gt; print(second_dict)\n    {'key': 20}\n    &gt;&gt;&gt; print(c_spaces)\n    ['key1', 'key']\n\n    \"\"\"\n    first_dict = Dict()\n    second_dict = Dict()\n    c_spaces = []\n    for k, v in c_space_dict.items():\n        if k == 'PCA' or k != 'logical' and np.absolute(v).sum() &gt; 0:\n            if k[-1] != '2':\n                first_dict[k] = v\n                c_spaces.append(k)\n            else:\n                second_dict[k[:-1]] = v\n                c_spaces.append(k[:-1])\n    return first_dict, second_dict, c_spaces\n</code></pre>"},{"location":"api/cellects/utils/utilitarian/#cellects.utils.utilitarian.translate_dict","title":"<code>translate_dict(old_dict)</code>","text":"<p>Translate a dictionary to a typed dictionary and filter out non-string values.</p> <p>Parameters:</p> Name Type Description Default <code>old_dict</code> <code>dict</code> <p>The input dictionary that may contain non-string values</p> required <p>Returns:</p> Name Type Description <code>numba_dict</code> <code>Dict</code> <p>A typed dictionary containing only the items from <code>old_dict</code> where the value is not a string</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; result = translate_dict({'a': 1., 'b': 'string', 'c': 2.0})\n&gt;&gt;&gt; print(result)\n{a: 1.0, c: 2.0}\n</code></pre> Source code in <code>src/cellects/utils/utilitarian.py</code> <pre><code>def translate_dict(old_dict: dict) -&gt; Dict:\n    \"\"\"\n    Translate a dictionary to a typed dictionary and filter out non-string values.\n\n    Parameters\n    ----------\n    old_dict : dict\n        The input dictionary that may contain non-string values\n\n    Returns\n    -------\n    numba_dict : Dict\n        A typed dictionary containing only the items from `old_dict` where the value is not a string\n\n    Examples\n    --------\n    &gt;&gt;&gt; result = translate_dict({'a': 1., 'b': 'string', 'c': 2.0})\n    &gt;&gt;&gt; print(result)\n    {a: 1.0, c: 2.0}\n    \"\"\"\n    numba_dict = Dict()\n    for k, v in old_dict.items():\n        if not isinstance(v, str):\n            numba_dict[k] = v\n    return numba_dict\n</code></pre>"},{"location":"first-analysis/","title":"Setting up a first analysis","text":"<p>This section describes the three main steps of a first analysis:</p> <ul> <li>Data localisation in the first window </li> <li>Finding where the specimens are in the image analysis window </li> <li>Tuning parameters of the video tracking window</li> </ul>"},{"location":"first-analysis/data-localisation/","title":"Data localisation in the first window","text":"<p>Before diving into detailed analysis workflows, Cellects requires initial setup (Fig. 1) to define the scope and format of the data being processed. This first window serves as the gateway to configuring foundational parameters that underpin all subsequent steps: from basic image/video tracking to advanced video analytics.  Users will specify whether their input consists of an image stack or a single video file, establish naming conventions for targeted files (e.g., prefixes like \"exp_\" and extensions like \".jpg\"), and define the root folder(s) containing experimental data. Additionally, this interface allows users to declare how many independent arenas are present in each dataset\u2014a critical step for ensuring accurate analysis.  These configurations directly inform later stages (e.g., image analysis, video tracking) and enable automation across multiple folders if required (See Fig. 11).</p>"},{"location":"first-analysis/data-localisation/#detailed-description","title":"Detailed description","text":"Figure 1: Cellects first window"},{"location":"first-analysis/data-localisation/#image-list-or-videos","title":"Image list or videos:","text":"<p>The Image list or video option indicates whether the data have been stored as an image stack (i.e. a set of files where each file contains a single image) or as a video. Images must be named alphanumerically so the program can read them in the right order.</p>"},{"location":"first-analysis/data-localisation/#image-prefix-and-extension","title":"Image prefix and extension:","text":"<p>The Images prefix and Images extension fields allow Cellects to consider relevant data. For example, setting 'exp_' as image prefix and '.jpg' as image extension will cause Cellects to only consider JPG files whose name starts with 'exp_'. Remaining labels should indicate the order in which images were taken.</p> <p>Note</p> <ul> <li>Image prefix is optional</li> <li>If every .jpg files start with IMG_ but other .jpg files exist, use the prefix to excludeirrelevant files.</li> <li>Supported formats: bmp, dib, exr, exr, hdr, jp2, jpe, jpeg, jpg, pbm, pfm, pgm, pic, png,  pnm,ppm, ras, sr, tif, tiff, webp, cr2, cr3, nef, arw, sr2, raf, prf, rw2, pef, dng, 3fr, iiq.</li> </ul>"},{"location":"first-analysis/data-localisation/#folder","title":"Folder:","text":"<p>The Folder field must specify the directory path to the folder(s) for Cellects to be able to run the analysis. The user can copy/paste this path into the field or navigate to the folder using the Browse push button. For batch analysis, provide a path leading directly to the parent folder containing all subfolders.</p>"},{"location":"first-analysis/data-localisation/#arena-number-per-folder","title":"Arena number per folder:","text":"<p>The Arena number per folder specifies how many arenas are present in the images. Cellects will process and analyze each arena separately.</p> <p>Note</p> <ul> <li>For batch processing, assign different arena counts for each subfolder (see Fig. 11: the severalfolder window).</li> </ul>"},{"location":"first-analysis/data-localisation/#browse","title":"Browse:","text":"<p>Clicking the Browse button opens a dialog to select a folder for analysis.</p>"},{"location":"first-analysis/data-localisation/#advanced-parameters","title":"Advanced parameters:","text":"<p>Clicking the Advanced parameters button opens the window containing all secondary parameters of the software.  Find details about this window in the advanced documentation.</p>"},{"location":"first-analysis/data-localisation/#required-outputs","title":"Required outputs:","text":"<p>Clicking the Required outputs button opens the window allowing to choose what descriptors Cellects will compute on the selected data. Find details about this window in the advanced documentation.</p>"},{"location":"first-analysis/data-localisation/#run-all-directly","title":"Run all directly:","text":"<p>This option appears when image analysis has already been performed for the current folder. It is a shortcut to bypass the image analysis step and proceed directly to video tracking refinement.</p>"},{"location":"first-analysis/data-localisation/#next","title":"Next:","text":"<p>Click the Next button to go to the image analysis window (Fig. 2), or  to the window showing the list of folders (Fig. 11) if applicable.</p>"},{"location":"first-analysis/image-analysis/","title":"Finding where the specimens are in the image analysis window","text":"<p>After defining the data source (see Data localisation), the next step consists in setting up Cellects to detect the specimen on single images. This interface allows users to guide automated segmentation by specifying whether specimens are isolated per arena, selecting reference images for analysis, and defining spatial scaling parameters (e.g., converting pixel measurements to real-world units).  Interactive tools like Select and draw enable manual correction of segmentation errors, while the Advanced mode caters to power users seeking fine-grained control over color spaces, filters, and algorithms.  By iteratively testing segmentation methods and validating results through visual feedback (Figures 2\u20134), researchers calibrate Cellects to reliably distinguish specimens from background noise across heterogeneous datasets.  Proper configuration here directly informs downstream video tracking workflows (see Video tracking), ensuring reproducible, high-fidelity analysis of dynamic processes such as cell migration or colony growth.</p>"},{"location":"first-analysis/image-analysis/#detailed-description","title":"Detailed description","text":"Figure 2: Cellects image analysis window"},{"location":"first-analysis/image-analysis/#image-number","title":"Image number:","text":"<p>Selects the image number to analyze. This number should only be changed when specimen(s) are invisible on the first image (e.g., in the case of appearing colonies of bacteria), never otherwise. When the specimen(s) are invisible, read more advanced images until some material can be detected.</p> <p>Note</p> <ul> <li>When the data is stored as images, this image number comes from alphanumerical sorting of originalimage labels.</li> </ul>"},{"location":"first-analysis/image-analysis/#one-specimen-per-arena","title":"One specimen per arena:","text":"<p>Select this option if there is only one specimen (e.g., a cell or connected colony) per arena. If multiple specimens exist (or will be present) in an arena, unselect this option.</p> <p>Note</p> <ul> <li>This option is selected by default.</li> </ul>"},{"location":"first-analysis/image-analysis/#scale-with","title":"Scale with:","text":"<p>Specify how to compute true pixel size (in mm). Cellects can determine this scale using:</p> <ul> <li>Image width (horizontal dimension)</li> <li>Specimen width on first image (usable when specimens share consistent width)</li> </ul> <p>Note</p> <ul> <li>Advanced parameters allow disabling scaling and outputting in pixels.</li> <li>Using specimen width reduces initial detection efficiency. We recommend using image width unlessspecimen dimensions are known with higher accuracy than imaging equipment.</li> <li>Pixel size is stored in a file named <code>software_settings.csv</code>.</li> </ul>"},{"location":"first-analysis/image-analysis/#scale-size","title":"Scale size:","text":"<p>The Scale size is the actual length (in mm) corresponding to scaling reference.</p> <p>Note</p> <ul> <li>This value enables conversion from pixel coordinates to physical dimensions.</li> </ul>"},{"location":"first-analysis/image-analysis/#select-and-draw","title":"Select and draw:","text":"<p>Select and draw allows the user to inform Cellects about specimen (Cell) and background (Back) areas in images. To use, click Cell or Back button (button color changes), then:</p> <ul> <li>Click and drag mouse on image to mark corresponding area</li> <li>Numbered drawings appear below buttons for reference</li> <li>(if needed) Click numbered drawing to remove selection.</li> </ul> <p>Note</p> <ul> <li>By default, this tool only works for the first folder when analyzing multiple folders. Advancedparameters include an option to use these same masks in multiple folders.</li> <li>To apply saved masks (e.g., background or specimen initiation regions) across selected folders,enable Keep Cell and Back drawing for all folders in Advanced parameters.</li> </ul>"},{"location":"first-analysis/image-analysis/#draw-buttons","title":"Draw buttons:","text":"<p>Click the Cell or Back button and draw a corresponding area on the image by clicking and holding mouse on the image.</p>"},{"location":"first-analysis/image-analysis/#generate-analysis-options","title":"Generate analysis options:","text":"<p>Cellects proposes algorithms to automatically determine optimal specimen detection parameters on the first or last image:</p> <ul> <li>Basic \u2192 provides suggestions in minutes. Alternatively, the user can switch to Advanced mode to review or modify more specific settings.</li> </ul> <p>Note</p> <ul> <li>Selecting Basic (or Apply current config) will trigger an orange working message duringprocessing.</li> </ul> Figure 3: Cellects image analysis window after analysis option generation"},{"location":"first-analysis/image-analysis/#select-option-to-read","title":"Select option to read:","text":"<p>Choose the option producing optimal segmentation results. This menu appears after generating analysis options, allowing direct quality assessment. For example, if Option 1 shows correct detection (e.g., 6 spots in 6 arenas), click Yes. Otherwise, improve analysis via:</p> <ul> <li>Adjusting arena/spot shapes or sizes</li> <li>Using Select and draw to annotate specimens/background</li> <li>Manual configuration in advanced mode \u2192 Test changes with Apply current config</li> </ul> <p>Note</p> <ul> <li>Confirm when magenta/pink contours match expected positions and counts.</li> </ul>"},{"location":"first-analysis/image-analysis/#arena-shape","title":"Arena shape:","text":"<p>Specifies whether the specimen(s) can move in a circular or rectangular arena.</p>"},{"location":"first-analysis/image-analysis/#set-spot-shape","title":"Set spot shape:","text":"<p>Defines the expected shape of specimens within arenas.</p>"},{"location":"first-analysis/image-analysis/#set-spot-size","title":"Set spot size:","text":"<p>Initial horizontal size of the specimen(s) (in mm). If similar across all specimens, this can also be used as a scale.</p>"},{"location":"first-analysis/image-analysis/#advanced-mode","title":"Advanced mode:","text":"<p>The Advanced mode enables fine tuning of image analysis parameters for:</p> <ul> <li>Custom color space combinations (e.g., HSV, HLS)</li> <li>Applying filters before segmentation</li> <li>Combining segmentations using logical operators</li> <li>Accessing rolling window and Kmeans algorithms</li> </ul> <p>Note</p> <ul> <li>Useful for reusing validated parameter sets or testing alternative methods.</li> </ul> Figure 4: Image analysis advanced mode"},{"location":"first-analysis/image-analysis/#color-combination","title":"Color combination:","text":"<p>Color spaces are transformations of the original BGR (Blue Green Red) image Instead of defining an image by 3 colors,  they transform it into 3 different visual properties</p> <ul> <li>hsv: hue (color), saturation, value (lightness)</li> <li>hls: hue (color), lightness, saturation</li> <li>lab: Lightness, Red/Green, Blue/Yellow</li> <li>luv and yuv: l and y are Lightness, u and v are related to colors</li> </ul>"},{"location":"first-analysis/image-analysis/#filter","title":"Filter:","text":"<p>Apply a filter to preprocess images before segmentation.</p> <p>Note</p> <ul> <li>Filtering can improve segmentation accuracy by emphasizing relevant image features.</li> </ul>"},{"location":"first-analysis/image-analysis/#logical-operator","title":"Logical operator:","text":"<p>The Logical operator defines how to combine results from distinct segmentations (e.g., merging the segmentation resulting from a specific color space transformation and filtering with a different one). Supported operators: AND, OR, XOR.</p>"},{"location":"first-analysis/image-analysis/#rolling-window-segmentation","title":"Rolling window segmentation:","text":"<p>Segments image regions using a rolling window approach to detect local intensity valleys. The method applies Otsu's thresholding locally on each window.</p>"},{"location":"first-analysis/image-analysis/#kmeans","title":"Kmeans:","text":"<p>The Kmeans algorithm clusters pixels into a specified number of categories (2</p> <p>-5) to identify specimen regions within the image.</p>"},{"location":"first-analysis/image-analysis/#video-delimitation","title":"Video delimitation:","text":"<p>After confirming initial detection, automatic video delimitation results appear in blue.  Click Yes if accurate or No for:</p> <ul> <li>A slower, higher precision algorithm.</li> <li>Manual delineation option.</li> </ul> Figure 5: Cellects image analysis window, after arena delineation"},{"location":"first-analysis/image-analysis/#last-image-question","title":"Last image question:","text":"<p>If parameters might fail on later images, test them first on the final frame:</p> <ul> <li>Yes \u2192 validates with last image before tracking.</li> <li>No \u2192 proceeds directly to video analysis.</li> </ul>"},{"location":"first-analysis/image-analysis/#check-if-the-medium-at-starting-position-differs-from-the-rest-of-the-arena","title":"Check if the medium at starting position differs from the rest of the arena:","text":"<p>Enable if the substrate differs between initial position and arena growth area (e.g., nutritive gel vs. agar). Disable for homogeneous substrates.</p>"},{"location":"first-analysis/image-analysis/#save-image-analysis","title":"Save image analysis:","text":"<p>Complete the analysis of the current image. Clicking this button analyzes only one image. To analyze video(s), click Next.</p> <p>Note</p> <ul> <li>When analyzing a single specimen per arena, keeps the largest connected component.</li> <li>Saves all selected descriptors in .csv format for the current image and generates a validationimage to assess segmentation accuracy.</li> </ul>"},{"location":"first-analysis/video-tracking/","title":"Tuning parameters of the video tracking window","text":"<p>Following successful specimen detection (Image analysis), fine-tune the video tracking algorithms in this window.  Here, users adjust segmentation methods (e.g., Frame, Threshold, and Slope), define spatial constraints like the maximal growth factor, and apply post-processing filters to eliminate noise and refine detection accuracy (Figures 5\u20137).  By iteratively testing tracking parameters and validating results through visual feedback, researchers ensure reproducible quantification of temporal changes such as cell migration, colony expansion, or morphological shifts.</p>"},{"location":"first-analysis/video-tracking/#detailed-description","title":"Detailed description","text":"Figure 6: Cellects video tracking window"},{"location":"first-analysis/video-tracking/#arena-to-analyze","title":"Arena to analyze:","text":"<p>This arena number selects a specific arena in the current folder. The user can choose an arena, click Detection to load and analyze it, then Read results.</p> <p>Note</p> <ul> <li>Cellects automatically names the arena by their position (left to right, top to bottom).</li> <li>For single arena setups, use 1.</li> <li>Post processing triggers Detection, which in turn triggers Load One arena.</li> <li>Videos can be saved (as .npy files) for later analysis using the Advanced parameter Keepunaltered videos.</li> </ul>"},{"location":"first-analysis/video-tracking/#maximal-growth-factor","title":"Maximal growth factor:","text":"<p>This is the maximum allowable proportion of image area that may be covered by specimen movement between frames. Adjust accordingly:</p> <ul> <li>Increase if specimen size is underestimated.</li> <li>Decrease if specimen size is overestimated.</li> </ul> <p>Note</p> <ul> <li>Precisely, this defines an upper bound on relative coverage changes between sequential images.</li> </ul>"},{"location":"first-analysis/video-tracking/#temporal-smoothing","title":"Temporal smoothing:","text":"<p>Applies temporal smoothing to reduce noise and highlight long</p> <p>-term trends by averaging pixel intensity changes. Use when analyzing slope -based segmentation results.</p> <p>Note</p> <ul> <li>This uses a moving window algorithm on pixel intensity curves over time.</li> <li>Excessive iterations produce constant values, preventing accurate detection.</li> </ul>"},{"location":"first-analysis/video-tracking/#segmentation-method","title":"Segmentation method:","text":"<p>Cellects includes five video tracking options:</p> <ul> <li>Frame option: Applies the image analysis algorithm frame by frame, without temporal dynamics.</li> <li>Threshold option: Compares pixel intensity with the average intensity of the whole image at each time step.</li> <li>Slope option: Compares pixel intensity slopes with an automatically defined threshold.</li> <li>T and S option: logical AND of threshold and slope options.</li> <li>T or S option: logical OR of threshold and slope options.</li> </ul> <p>Note</p> <ul> <li>Selecting the Compute all options before dunning Detection allows method comparison.  Onceanalysis completes. Once the analysis completed, select one option and click Read.</li> <li>Computing only one option is faster and requires less memory.</li> <li>When Heterogeneous background or Grid segmentation has been selected in the image analysiswindow, only the Frame option remains available.</li> </ul>"},{"location":"first-analysis/video-tracking/#load-one-arena","title":"Load one arena:","text":"<p>Clicking this button loads the arena associated with Arena to analyze. The center of the window displays the first frame of that arena's video. Click Read to review the full video.</p>"},{"location":"first-analysis/video-tracking/#detection","title":"Detection:","text":"<p>Detection applies a (or all) segmentation methods to one arena. Once finished, click Read  to view the detection result. If correct, answer Done to proceed with tuning parameters for post processing.</p>"},{"location":"first-analysis/video-tracking/#read","title":"Read:","text":"<p>Clicking Read starts the video display corresponding to the current state of the analysis.</p> Figure 7: Cellects video tracking window during detection visualization"},{"location":"first-analysis/video-tracking/#fading-detection","title":"Fading detection:","text":"<p>Fading detection monitors when specimens leave previously occupied areas, useful for  moving organisms rather than static growth. Uncheck this option if not needed. Set a value  between minus one and one to control sensitivity:</p> <ul> <li>Near minus one: Minimal false removal of specimen traces.</li> <li>Near one: High risk of over -removal from all areas.</li> </ul>"},{"location":"first-analysis/video-tracking/#post-processing","title":"Post processing:","text":"<p>Post processing applies detection algorithms with additional enhancements:</p> <ul> <li>Binary operations: opening, closing, logical ops.</li> <li>Fading detection* tracking: when specimen(s) may leave areas (optional).</li> <li>Correct errors around initial shape: when the contour of the initial position of the specimen is hard to detect (optional).</li> <li>Connect distant shapes: when the specimen's heterogeneity create wrong disconnections in the video detection (optional).</li> <li>Prevent fast growth near periphery: when arena's border (typically petri dishes) may be wrongly detected as specimen (optional).</li> </ul> <p>Note</p> <ul> <li>Once Post processing works, the user can click \u201cDone\u201d to Step 2: Tune fading and advancedparameters to improve Post processing, and then Run All arenas.</li> </ul>"},{"location":"first-analysis/video-tracking/#save-one-result","title":"Save one result:","text":"<p>Complete the current video analysis by clicking this button for single arena processing. Saving includes:</p> <ul> <li>Calculating all selected descriptors (.csv) per frame.</li> <li>Generating validation videos for detection verification.</li> <li>Storing configuration parameters for reproducibility.</li> </ul> <p>Note</p> <ul> <li>This action will overwrite results and validation data for the current arena.</li> </ul> Figure 8: Cellects video tracking window, running all arenas"},{"location":"first-analysis/video-tracking/#run-all","title":"Run All:","text":"<p>Apply validated parameters to all arenas by clicking Run All. This action:</p> <ul> <li>Generates full -resolution video outputs (storage -intensive)</li> <li>Processes videos sequentially with real time visualization</li> <li>Calculates selected descriptors for each frame</li> <li>Produces validation content at multiple intervals</li> <li>Preserves current configuration settings</li> </ul>"},{"location":"first-analysis/video-tracking/#save-all-choices","title":"Save all choices:","text":"<p>Clicking Save all choices writes/updates configuration files to preserve analysis parameters for future replication.</p>"}]}